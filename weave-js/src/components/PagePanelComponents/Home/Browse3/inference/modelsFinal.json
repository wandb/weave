{
  "models": [
    {
      "provider": "coreweave",
      "id": "cw_meta-llama_Llama-3.1-8B-Instruct",
      "idPlayground": "meta-llama/Llama-3.1-8B-Instruct",
      "idHuggingFace": "meta-llama/Llama-3.1-8B-Instruct",
      "label": "Llama 3.1 8B",
      "status": "hosted",
      "descriptionShort": "Efficient conversational model optimized for responsive multilingual chatbot interactions.",
      "descriptionMedium": "Llama 3.1 8B provides efficient multilingual conversational support ideal for applications where responsiveness and computational efficiency are critical. Effective for building chatbots, automated customer interactions, and applications needing fast yet reliable language understanding.",
      "launchDate": "2024-07-01T09:00:00Z",
      "parameterCountTotal": 8000000000,
      "contextWindow": 128000,
      "priceCentsPerBillionTokensInput": 22000,
      "priceCentsPerBillionTokensOutput": 22000,
      "apiStyle": "chat",
      "modalities": ["Text"],
      "likesHuggingFace": 4299,
      "downloadsHuggingFace": 5763207,
      "license": "llama3.1"
    },
    {
      "provider": "coreweave",
      "id": "cw_moonshotai_Kimi-K2-Instruct",
      "isNew": true,
      "idPlayground": "moonshotai/Kimi-K2-Instruct",
      "idHuggingFace": "moonshotai/Kimi-K2-Instruct",
      "label": "MoonshotAI Kimi K2",
      "status": "hosted",
      "descriptionShort": "Mixture-of-Experts model optimized for complex tool use, reasoning, and code synthesis.",
      "descriptionMedium": "Kimi K2 is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It is optimized for agentic capabilities, including advanced tool use, reasoning, and code synthesis. Kimi K2 excels across a broad range of benchmarks, particularly in coding (LiveCodeBench, SWE-bench), reasoning (ZebraLogic, GPQA), and tool-use (Tau2, AceBench) tasks. It supports long-context inference up to 128K tokens and is designed with a novel training stack that includes the MuonClip optimizer for stable large-scale MoE training.",
      "launchDate": "2025-07-14T09:00:00Z",
      "parameterCountTotal": 1000000000000,
      "parameterCountActive": 32000000000,
      "contextWindow": 128000,
      "priceCentsPerBillionTokensInput": 135000,
      "priceCentsPerBillionTokensOutput": 400000,
      "apiStyle": "chat",
      "modalities": ["Text"],
      "likesHuggingFace": 1140,
      "downloadsHuggingFace": 25070,
      "license": "other"
    },
    {
      "provider": "coreweave",
      "id": "cw_deepseek-ai_DeepSeek-R1-0528",
      "idPlayground": "deepseek-ai/DeepSeek-R1-0528",
      "idHuggingFace": "deepseek-ai/DeepSeek-R1-0528",
      "label": "DeepSeek R1-0528",
      "status": "hosted",
      "descriptionShort": "Optimized for precise reasoning tasks including complex coding, math, and structured document analysis.",
      "descriptionMedium": "DeepSeek R1-0528 specializes in tasks requiring detailed reasoning, including mathematics, programming, and logical problem-solving. It's particularly effective in scenarios like complex coding tasks, planning workflows, and analyzing structured documents with enhanced accuracy and reduced hallucinations.",
      "launchDate": "2025-05-28T09:00:00Z",
      "parameterCountTotal": 680000000000,
      "parameterCountActive": 37000000000,
      "contextWindow": 161000,
      "priceCentsPerBillionTokensInput": 135000,
      "priceCentsPerBillionTokensOutput": 540000,
      "apiStyle": "chat",
      "modalities": ["Text"],
      "likesHuggingFace": 2265,
      "downloadsHuggingFace": 312351,
      "license": "mit"
    },
    {
      "provider": "coreweave",
      "id": "cw_deepseek-ai_DeepSeek-V3-0324",
      "idPlayground": "deepseek-ai/DeepSeek-V3-0324",
      "idHuggingFace": "deepseek-ai/DeepSeek-V3-0324",
      "label": "DeepSeek V3-0324",
      "status": "hosted",
      "descriptionShort": "Robust Mixture-of-Experts model tailored for high-complexity language processing and comprehensive document analysis.",
      "descriptionMedium": "DeepSeek V3-0324 is a powerful Mixture-of-Experts model designed for demanding language tasks, including comprehensive information extraction, content summarization, document analysis, and handling complex, structured textual data. It excels in balancing depth and accuracy in high-complexity scenarios.",
      "launchDate": "2025-03-24T09:00:00Z",
      "parameterCountTotal": 680000000000,
      "parameterCountActive": 37000000000,
      "contextWindow": 161000,
      "priceCentsPerBillionTokensInput": 114000,
      "priceCentsPerBillionTokensOutput": 275000,
      "apiStyle": "chat",
      "modalities": ["Text"],
      "likesHuggingFace": 3000,
      "downloadsHuggingFace": 530927,
      "license": "mit"
    },
    {
      "provider": "coreweave",
      "id": "cw_meta-llama_Llama-3.3-70B-Instruct",
      "idPlayground": "meta-llama/Llama-3.3-70B-Instruct",
      "idHuggingFace": "meta-llama/Llama-3.3-70B-Instruct",
      "label": "Llama 3.3 70B",
      "status": "hosted",
      "descriptionShort": "Multilingual model excelling in conversational tasks, detailed instruction-following, and coding.",
      "descriptionMedium": "Llama 3.3 70B excels in multilingual conversational interactions, providing strong capabilities in detailed instruction-following, coding tasks, and mathematical reasoning. Ideal for general-purpose chatbots, complex text-based queries, and multilingual user interactions.",
      "launchDate": "2024-12-01T09:00:00Z",
      "parameterCountTotal": 70000000000,
      "contextWindow": 128000,
      "priceCentsPerBillionTokensInput": 71000,
      "priceCentsPerBillionTokensOutput": 71000,
      "apiStyle": "chat",
      "modalities": ["Text"],
      "likesHuggingFace": 2433,
      "downloadsHuggingFace": 473530,
      "license": "llama3.3"
    },
    {
      "provider": "coreweave",
      "id": "cw_meta-llama_Llama-4-Scout-17B-16E-Instruct",
      "idPlayground": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
      "idHuggingFace": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
      "label": "Llama 4 Scout",
      "status": "hosted",
      "descriptionShort": "Multimodal model integrating text and image understanding, ideal for visual tasks and combined analysis.",
      "descriptionMedium": "Llama 4 Scout integrates text and image understanding, making it suitable for multimodal applications such as visual Q&A, content moderation, captioning, and analysis tasks involving images combined with textual data. It efficiently balances computational load via a mixture-of-experts architecture.",
      "launchDate": "2025-04-01T09:00:00Z",
      "parameterCountTotal": 109000000000,
      "parameterCountActive": 17000000000,
      "contextWindow": 64000,
      "priceCentsPerBillionTokensInput": 17000,
      "priceCentsPerBillionTokensOutput": 66000,
      "apiStyle": "chat",
      "modalities": ["Text", "Vision"],
      "likesHuggingFace": 996,
      "downloadsHuggingFace": 540504,
      "license": "other"
    },
    {
      "provider": "coreweave",
      "id": "cw_microsoft_Phi-4-mini-instruct",
      "idPlayground": "microsoft/Phi-4-mini-instruct",
      "idHuggingFace": "microsoft/Phi-4-mini-instruct",
      "label": "Phi 4 Mini",
      "status": "hosted",
      "descriptionShort": "Compact, efficient model ideal for fast responses in resource-constrained environments.",
      "descriptionMedium": "Phi 4 Mini is optimized for use in lightweight environments, excelling at quick interactions, and constrained-resource deployments. It supports strong mathematical and logical inference, suitable for embedded applications, mobile environments, or real-time AI assistants.",
      "launchDate": "2025-02-01T09:00:00Z",
      "parameterCountTotal": 3800000000,
      "contextWindow": 128000,
      "priceCentsPerBillionTokensInput": 8000,
      "priceCentsPerBillionTokensOutput": 35000,
      "apiStyle": "chat",
      "modalities": ["Text"],
      "likesHuggingFace": 547,
      "downloadsHuggingFace": 250397,
      "license": "mit"
    }
  ]
}
