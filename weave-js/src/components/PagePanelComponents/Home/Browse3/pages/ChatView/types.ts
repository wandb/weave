/**
 * Our Chat View is based on OpenAI's Chat API, with an extension to support
 * Placeholders in Prompts.
 */

export type Placeholder = {
  name: string;
  type: string;
  default?: string;
};

export type ImageUrl = {
  url: string;
};

export type InternalMessage = {
  type: 'text' | 'image_url';
  text?: string;
  image_url?: ImageUrl;
};

export type MessagePart = string | Placeholder | InternalMessage;

export type ToolCall = {
  id: string;
  type: string;
  function: {
    name: string;

    // The arguments to call the function with, as generated by the model in JSON format.
    // Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema.
    // Validate the arguments in your code before calling your function.
    arguments: string;
  };
};

export type Message = {
  role: string;
  content?: string | MessagePart[];
  tool_calls?: ToolCall[];
};

export type Messages = Message[];

// See https://platform.openai.com/docs/api-reference/chat/create
export type ChatRequest = {
  model: string;
  messages: Messages;
  seed?: number | null;

  // Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message.
  logprobs?: boolean | null;

  // An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used.
  top_logprobs?: number | null;

  //   Defaults to 1
  // What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.

  // We generally recommend altering this or top_p but not both.

  temperature?: number | null;

  response_format?: Record<string, any>;
};

export type Choice = {
  index: number;
  message: Message;
  finish_reason: string;
  // logprobs
};

export type Usage = {
  // Number of tokens in the generated completion.
  completion_tokens: number;

  // Number of tokens in the prompt.
  prompt_tokens: number;

  // Total number of tokens used in the request (prompt + completion).
  total_tokens: number;

  // completion_tokens_details
};

// See https://platform.openai.com/docs/api-reference/chat/object
export type ChatCompletion = {
  id: string;
  choices: Choice[];

  // The Unix timestamp (in seconds) of when the chat completion was created.
  created: number;

  // The model used for the chat completion.
  model: string;

  // This fingerprint represents the backend configuration that the model runs with.
  // Can be used in conjunction with the seed request parameter to understand when backend changes have been made that might impact determinism.
  system_fingerprint: string;

  usage: Usage;
};

export type Chat = {
  // TODO: Maybe optional information linking back to Call?
  isStructuredOutput: boolean;
  request: ChatRequest;
  result: ChatCompletion | null;
};

export type ChoicesMode = 'linear' | 'carousel';
