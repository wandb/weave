{".class": "MypyFile", "_fullname": "transformers.models.speech_to_text.tokenization_speech_to_text", "future_import_flags": [], "is_partial_stub_package": false, "is_stub": false, "names": {".class": "SymbolTable", "Any": {".class": "SymbolTableNode", "cross_ref": "typing.Any", "kind": "Gdef"}, "Dict": {".class": "SymbolTableNode", "cross_ref": "typing.Dict", "kind": "Gdef"}, "LANGUAGES": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_inferred", "has_explicit_value"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.LANGUAGES", "name": "LANGUAGES", "type": {".class": "Instance", "args": ["builtins.str", {".class": "Instance", "args": ["builtins.str"], "type_ref": "builtins.list"}], "type_ref": "builtins.dict"}}}, "List": {".class": "SymbolTableNode", "cross_ref": "typing.List", "kind": "Gdef"}, "MAX_MODEL_INPUT_SIZES": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_inferred", "has_explicit_value"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.MAX_MODEL_INPUT_SIZES", "name": "MAX_MODEL_INPUT_SIZES", "type": {".class": "Instance", "args": ["builtins.str", "builtins.int"], "type_ref": "builtins.dict"}}}, "MUSTC_LANGS": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_inferred", "has_explicit_value"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.MUSTC_LANGS", "name": "MUSTC_LANGS", "type": {".class": "Instance", "args": ["builtins.str"], "type_ref": "builtins.list"}}}, "Optional": {".class": "SymbolTableNode", "cross_ref": "typing.Optional", "kind": "Gdef"}, "PRETRAINED_VOCAB_FILES_MAP": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_inferred", "has_explicit_value"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.PRETRAINED_VOCAB_FILES_MAP", "name": "PRETRAINED_VOCAB_FILES_MAP", "type": {".class": "Instance", "args": ["builtins.str", {".class": "Instance", "args": ["builtins.str", "builtins.str"], "type_ref": "builtins.dict"}], "type_ref": "builtins.dict"}}}, "Path": {".class": "SymbolTableNode", "cross_ref": "pathlib.Path", "kind": "Gdef"}, "PreTrainedTokenizer": {".class": "SymbolTableNode", "cross_ref": "transformers.tokenization_utils.PreTrainedTokenizer", "kind": "Gdef"}, "SPIECE_UNDERLINE": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_ready", "is_inferred", "has_explicit_value"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.SPIECE_UNDERLINE", "name": "SPIECE_UNDERLINE", "type": "builtins.str"}}, "Speech2TextTokenizer": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "TypeInfo", "_promote": [], "abstract_attributes": [], "bases": ["transformers.tokenization_utils.PreTrainedTokenizer"], "declared_metaclass": null, "defn": {".class": "ClassDef", "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer", "name": "Speech2TextTokenizer", "type_vars": []}, "deletable_attributes": [], "flags": [], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer", "has_param_spec_type": false, "metaclass_type": null, "metadata": {}, "module_name": "transformers.models.speech_to_text.tokenization_speech_to_text", "mro": ["transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer", "transformers.tokenization_utils.PreTrainedTokenizer", "transformers.tokenization_utils_base.PreTrainedTokenizerBase", "transformers.tokenization_utils_base.SpecialTokensMixin", "transformers.utils.hub.PushToHubMixin", "builtins.object"], "names": {".class": "SymbolTable", "__getstate__": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0], "arg_names": ["self"], "flags": [], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.__getstate__", "name": "__getstate__", "type": {".class": "CallableType", "arg_kinds": [0], "arg_names": ["self"], "arg_types": ["transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer"], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "__getstate__ of Speech2TextTokenizer", "ret_type": {".class": "Instance", "args": [{".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 4}, {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 4}], "type_ref": "builtins.dict"}, "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "__init__": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4], "arg_names": ["self", "vocab_file", "spm_file", "bos_token", "eos_token", "pad_token", "unk_token", "do_upper_case", "do_lower_case", "tgt_lang", "lang_codes", "sp_model_kwargs", "kwargs"], "flags": [], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.__init__", "name": "__init__", "type": {".class": "CallableType", "arg_kinds": [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4], "arg_names": ["self", "vocab_file", "spm_file", "bos_token", "eos_token", "pad_token", "unk_token", "do_upper_case", "do_lower_case", "tgt_lang", "lang_codes", "sp_model_kwargs", "kwargs"], "arg_types": ["transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer", {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}, {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}, {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}, {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}, {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}, {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}, {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}, {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}, {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}, {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}, {".class": "UnionType", "items": [{".class": "Instance", "args": ["builtins.str", {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 2}], "type_ref": "builtins.dict"}, {".class": "NoneType"}]}, {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "__init__ of Speech2TextTokenizer", "ret_type": {".class": "NoneType"}, "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "__setstate__": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["self", "d"], "flags": [], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.__setstate__", "name": "__setstate__", "type": {".class": "CallableType", "arg_kinds": [0, 0], "arg_names": ["self", "d"], "arg_types": ["transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer", {".class": "Instance", "args": [{".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 4}, {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 4}], "type_ref": "builtins.dict"}], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "__setstate__ of Speech2TextTokenizer", "ret_type": {".class": "NoneType"}, "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "_convert_id_to_token": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["self", "index"], "flags": [], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer._convert_id_to_token", "name": "_convert_id_to_token", "type": {".class": "CallableType", "arg_kinds": [0, 0], "arg_names": ["self", "index"], "arg_types": ["transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer", "builtins.int"], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "_convert_id_to_token of Speech2TextTokenizer", "ret_type": "builtins.str", "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "_convert_token_to_id": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["self", "token"], "flags": [], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer._convert_token_to_id", "name": "_convert_token_to_id", "type": null}}, "_tgt_lang": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer._tgt_lang", "name": "_tgt_lang", "type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}}}, "_tokenize": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["self", "text"], "flags": [], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer._tokenize", "name": "_tokenize", "type": {".class": "CallableType", "arg_kinds": [0, 0], "arg_names": ["self", "text"], "arg_types": ["transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer", "builtins.str"], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "_tokenize of Speech2TextTokenizer", "ret_type": {".class": "Instance", "args": ["builtins.str"], "type_ref": "builtins.list"}, "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "build_inputs_with_special_tokens": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0, 1], "arg_names": ["self", "token_ids_0", "token_ids_1"], "flags": [], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.build_inputs_with_special_tokens", "name": "build_inputs_with_special_tokens", "type": {".class": "CallableType", "arg_kinds": [0, 0, 1], "arg_names": ["self", "token_ids_0", "token_ids_1"], "arg_types": ["transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer", {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}, {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "build_inputs_with_special_tokens of Speech2TextTokenizer", "ret_type": {".class": "Instance", "args": ["builtins.int"], "type_ref": "builtins.list"}, "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "convert_tokens_to_string": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["self", "tokens"], "flags": [], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.convert_tokens_to_string", "name": "convert_tokens_to_string", "type": {".class": "CallableType", "arg_kinds": [0, 0], "arg_names": ["self", "tokens"], "arg_types": ["transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer", {".class": "Instance", "args": ["builtins.str"], "type_ref": "builtins.list"}], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "convert_tokens_to_string of Speech2TextTokenizer", "ret_type": "builtins.str", "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "decoder": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.decoder", "name": "decoder", "type": {".class": "Instance", "args": [{".class": "AnyType", "missing_import_name": null, "source_any": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 4}, "type_of_any": 7}, {".class": "AnyType", "missing_import_name": null, "source_any": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 4}, "type_of_any": 7}], "type_ref": "builtins.dict"}}}, "do_lower_case": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.do_lower_case", "name": "do_lower_case", "type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}}}, "do_upper_case": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.do_upper_case", "name": "do_upper_case", "type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}}}, "encoder": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.encoder", "name": "encoder", "type": {".class": "UnionType", "items": [{".class": "Instance", "args": [{".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 4}, {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 4}], "type_ref": "builtins.dict"}, {".class": "Instance", "args": [{".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 4}], "type_ref": "builtins.list"}]}}}, "get_special_tokens_mask": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0, 1, 1], "arg_names": ["self", "token_ids_0", "token_ids_1", "already_has_special_tokens"], "flags": [], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.get_special_tokens_mask", "name": "get_special_tokens_mask", "type": {".class": "CallableType", "arg_kinds": [0, 0, 1, 1], "arg_names": ["self", "token_ids_0", "token_ids_1", "already_has_special_tokens"], "arg_types": ["transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer", {".class": "Instance", "args": ["builtins.int"], "type_ref": "builtins.list"}, {".class": "UnionType", "items": [{".class": "Instance", "args": ["builtins.int"], "type_ref": "builtins.list"}, {".class": "NoneType"}]}, "builtins.bool"], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "get_special_tokens_mask of Speech2TextTokenizer", "ret_type": {".class": "Instance", "args": ["builtins.int"], "type_ref": "builtins.list"}, "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "get_vocab": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0], "arg_names": ["self"], "flags": [], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.get_vocab", "name": "get_vocab", "type": {".class": "CallableType", "arg_kinds": [0], "arg_names": ["self"], "arg_types": ["transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer"], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "get_vocab of Speech2TextTokenizer", "ret_type": {".class": "Instance", "args": [{".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 4}, {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 4}], "type_ref": "builtins.dict"}, "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "lang_code_to_id": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.lang_code_to_id", "name": "lang_code_to_id", "type": {".class": "Instance", "args": ["builtins.str", {".class": "AnyType", "missing_import_name": "transformers.models.speech_to_text.tokenization_speech_to_text.sentencepiece", "source_any": {".class": "AnyType", "missing_import_name": "transformers.models.speech_to_text.tokenization_speech_to_text.sentencepiece", "source_any": null, "type_of_any": 3}, "type_of_any": 7}], "type_ref": "builtins.dict"}}}, "lang_codes": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.lang_codes", "name": "lang_codes", "type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}}}, "lang_tokens": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.lang_tokens", "name": "lang_tokens", "type": {".class": "Instance", "args": ["builtins.str"], "type_ref": "builtins.list"}}}, "langs": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.langs", "name": "langs", "type": {".class": "Instance", "args": ["builtins.str"], "type_ref": "builtins.list"}}}, "max_model_input_sizes": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "Var", "flags": ["is_initialized_in_class", "is_inferred", "has_explicit_value"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.max_model_input_sizes", "name": "max_model_input_sizes", "type": null}}, "model_input_names": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "Var", "flags": ["is_initialized_in_class", "is_inferred", "has_explicit_value"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.model_input_names", "name": "model_input_names", "type": {".class": "Instance", "args": ["builtins.str"], "type_ref": "builtins.list"}}}, "prefix_tokens": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "Var", "flags": ["is_initialized_in_class", "is_ready", "has_explicit_value"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.prefix_tokens", "name": "prefix_tokens", "type": {".class": "Instance", "args": ["builtins.int"], "type_ref": "builtins.list"}}}, "pretrained_vocab_files_map": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "Var", "flags": ["is_initialized_in_class", "is_inferred", "has_explicit_value"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.pretrained_vocab_files_map", "name": "pretrained_vocab_files_map", "type": {".class": "Instance", "args": ["builtins.str", {".class": "Instance", "args": ["builtins.str", "builtins.str"], "type_ref": "builtins.dict"}], "type_ref": "builtins.dict"}}}, "save_vocabulary": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0, 1], "arg_names": ["self", "save_directory", "filename_prefix"], "flags": [], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.save_vocabulary", "name": "save_vocabulary", "type": {".class": "CallableType", "arg_kinds": [0, 0, 1], "arg_names": ["self", "save_directory", "filename_prefix"], "arg_types": ["transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer", "builtins.str", {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "save_vocabulary of Speech2TextTokenizer", "ret_type": {".class": "TupleType", "implicit": false, "items": ["builtins.str"], "partial_fallback": {".class": "Instance", "args": [{".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 6}], "type_ref": "builtins.tuple"}}, "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "set_tgt_lang_special_tokens": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["self", "tgt_lang"], "flags": [], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.set_tgt_lang_special_tokens", "name": "set_tgt_lang_special_tokens", "type": {".class": "CallableType", "arg_kinds": [0, 0], "arg_names": ["self", "tgt_lang"], "arg_types": ["transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer", "builtins.str"], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "set_tgt_lang_special_tokens of Speech2TextTokenizer", "ret_type": {".class": "NoneType"}, "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "sp_model": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.sp_model", "name": "sp_model", "type": {".class": "AnyType", "missing_import_name": "transformers.models.speech_to_text.tokenization_speech_to_text.sentencepiece", "source_any": null, "type_of_any": 3}}}, "sp_model_kwargs": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.sp_model_kwargs", "name": "sp_model_kwargs", "type": {".class": "Instance", "args": ["builtins.str", {".class": "AnyType", "missing_import_name": null, "source_any": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 2}, "type_of_any": 7}], "type_ref": "builtins.dict"}}}, "spm_file": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.spm_file", "name": "spm_file", "type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}}}, "tgt_lang": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "OverloadedFuncDef", "flags": ["is_property"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.tgt_lang", "impl": null, "items": [{".class": "Decorator", "func": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0], "arg_names": ["self"], "flags": ["is_property", "is_overload", "is_decorated"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.tgt_lang", "name": "tgt_lang", "type": {".class": "CallableType", "arg_kinds": [0], "arg_names": ["self"], "arg_types": ["transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer"], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "tgt_lang of Speech2TextTokenizer", "ret_type": "builtins.str", "type_guard": null, "unpack_kwargs": false, "variables": []}}, "is_overload": true, "var": {".class": "Var", "flags": ["is_initialized_in_class", "is_property", "is_settable_property", "is_ready", "is_inferred"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.tgt_lang", "name": "tgt_lang", "type": {".class": "CallableType", "arg_kinds": [0], "arg_names": ["self"], "arg_types": ["transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer"], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "tgt_lang of Speech2TextTokenizer", "ret_type": "builtins.str", "type_guard": null, "unpack_kwargs": false, "variables": []}}}, {".class": "Decorator", "func": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["self", "new_tgt_lang"], "flags": ["is_decorated"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.tgt_lang", "name": "tgt_lang", "type": {".class": "CallableType", "arg_kinds": [0, 0], "arg_names": ["self", "new_tgt_lang"], "arg_types": ["transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer", {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "tgt_lang of Speech2TextTokenizer", "ret_type": {".class": "NoneType"}, "type_guard": null, "unpack_kwargs": false, "variables": []}}, "is_overload": false, "var": {".class": "Var", "flags": ["is_inferred"], "fullname": null, "name": "tgt_lang", "type": null}}], "type": {".class": "Overloaded", "items": [{".class": "CallableType", "arg_kinds": [0], "arg_names": ["self"], "arg_types": ["transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer"], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "tgt_lang of Speech2TextTokenizer", "ret_type": "builtins.str", "type_guard": null, "unpack_kwargs": false, "variables": []}]}}}, "vocab_files_names": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "Var", "flags": ["is_initialized_in_class", "is_inferred", "has_explicit_value"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.vocab_files_names", "name": "vocab_files_names", "type": {".class": "Instance", "args": ["builtins.str", "builtins.str"], "type_ref": "builtins.dict"}}}, "vocab_size": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "Decorator", "func": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0], "arg_names": ["self"], "flags": ["is_property", "is_decorated"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.vocab_size", "name": "vocab_size", "type": {".class": "CallableType", "arg_kinds": [0], "arg_names": ["self"], "arg_types": ["transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer"], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "vocab_size of Speech2TextTokenizer", "ret_type": "builtins.int", "type_guard": null, "unpack_kwargs": false, "variables": []}}, "is_overload": false, "var": {".class": "Var", "flags": ["is_initialized_in_class", "is_property", "is_ready", "is_inferred"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer.vocab_size", "name": "vocab_size", "type": {".class": "CallableType", "arg_kinds": [0], "arg_names": ["self"], "arg_types": ["transformers.models.speech_to_text.tokenization_speech_to_text.Speech2TextTokenizer"], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "vocab_size of Speech2TextTokenizer", "ret_type": "builtins.int", "type_guard": null, "unpack_kwargs": false, "variables": []}}}}}, "slots": null, "tuple_type": null, "type_vars": [], "typeddict_type": null}}, "Tuple": {".class": "SymbolTableNode", "cross_ref": "typing.Tuple", "kind": "Gdef"}, "Union": {".class": "SymbolTableNode", "cross_ref": "typing.Union", "kind": "Gdef"}, "VOCAB_FILES_NAMES": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_inferred", "has_explicit_value"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.VOCAB_FILES_NAMES", "name": "VOCAB_FILES_NAMES", "type": {".class": "Instance", "args": ["builtins.str", "builtins.str"], "type_ref": "builtins.dict"}}}, "__annotations__": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_ready"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.__annotations__", "name": "__annotations__", "type": {".class": "Instance", "args": ["builtins.str", {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 6}], "type_ref": "builtins.dict"}}}, "__doc__": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_ready"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.__doc__", "name": "__doc__", "type": "builtins.str"}}, "__file__": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_ready"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.__file__", "name": "__file__", "type": "builtins.str"}}, "__name__": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_ready"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.__name__", "name": "__name__", "type": "builtins.str"}}, "__package__": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_ready"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.__package__", "name": "__package__", "type": "builtins.str"}}, "copyfile": {".class": "SymbolTableNode", "cross_ref": "shutil.copyfile", "kind": "Gdef"}, "json": {".class": "SymbolTableNode", "cross_ref": "json", "kind": "Gdef"}, "load_json": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0], "arg_names": ["path"], "flags": [], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.load_json", "name": "load_json", "type": {".class": "CallableType", "arg_kinds": [0], "arg_names": ["path"], "arg_types": ["builtins.str"], "bound_args": [], "def_extras": {"first_arg": null}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "load_json", "ret_type": {".class": "UnionType", "items": [{".class": "Instance", "args": [{".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 4}, {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 4}], "type_ref": "builtins.dict"}, {".class": "Instance", "args": [{".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 4}], "type_ref": "builtins.list"}]}, "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "load_spm": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["path", "sp_model_kwargs"], "flags": [], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.load_spm", "name": "load_spm", "type": {".class": "CallableType", "arg_kinds": [0, 0], "arg_names": ["path", "sp_model_kwargs"], "arg_types": ["builtins.str", {".class": "Instance", "args": ["builtins.str", {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 2}], "type_ref": "builtins.dict"}], "bound_args": [], "def_extras": {"first_arg": null}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "load_spm", "ret_type": {".class": "AnyType", "missing_import_name": "transformers.models.speech_to_text.tokenization_speech_to_text.sentencepiece", "source_any": null, "type_of_any": 3}, "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "logger": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_inferred", "has_explicit_value"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.logger", "name": "logger", "type": "logging.Logger"}}, "logging": {".class": "SymbolTableNode", "cross_ref": "transformers.utils.logging", "kind": "Gdef"}, "os": {".class": "SymbolTableNode", "cross_ref": "os", "kind": "Gdef"}, "save_json": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["data", "path"], "flags": [], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.save_json", "name": "save_json", "type": {".class": "CallableType", "arg_kinds": [0, 0], "arg_names": ["data", "path"], "arg_types": [{".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}, "builtins.str"], "bound_args": [], "def_extras": {"first_arg": null}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "save_json", "ret_type": {".class": "NoneType"}, "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "sentencepiece": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_suppressed_import", "is_ready", "is_inferred"], "fullname": "transformers.models.speech_to_text.tokenization_speech_to_text.sentencepiece", "name": "sentencepiece", "type": {".class": "AnyType", "missing_import_name": "transformers.models.speech_to_text.tokenization_speech_to_text.sentencepiece", "source_any": null, "type_of_any": 3}}}}, "path": "/home/paperspace/.pyenv/versions/3.10.4/lib/python3.10/site-packages/transformers/models/speech_to_text/tokenization_speech_to_text.py"}