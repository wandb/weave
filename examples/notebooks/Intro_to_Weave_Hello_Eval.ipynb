{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- docusaurus_head_meta::start\n",
    "---\n",
    "title: Introduction to Evaluations\n",
    "---\n",
    "docusaurus_head_meta::end -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tjh24iCFw8TH"
   },
   "source": [
    "# Introduction to Evaluations\n",
    "\n",
    "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "\n",
    "Weave is a toolkit for developing AI-powered applications.\n",
    "\n",
    "This notebook demonstrates how to evaluate a model or function using Weave‚Äôs Evaluation API. Evaluation is a core concept in Weave that helps you measure and iterate on your application by running it against a dataset of examples and scoring the outputs using custom-defined functions. You'll define a simple model, create a labeled dataset, track scoring functions with `@weave.op`, and run an evaluation that automatically tracks results in the Weave UI. This forms the foundation for more advanced workflows like LLM fine-tuning, regression testing, or model comparison.\n",
    "\n",
    "To get started, complete the prerequisites. Then, define a Weave `Model` with a `predict` method, create a labeled dataset and scoring function, and run an evaluation using `weave.Evaluation.evaluate()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McE7cuqSxMiP"
   },
   "source": [
    "## üêù Running your first evaluation\n",
    "\n",
    "In this example, we're using W&B Inference. [Learn more](https://docs.wandb.ai/inference) about our inference API.\n",
    "Using another provider? [We support all major clients and frameworks](https://docs.wandb.ai/weave/guides/integrations).\n",
    "\n",
    "We also have an OpenAI version of this example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56XteuP7s7sm"
   },
   "outputs": [],
   "source": [
    "# Ensure your dependencies are installed with:\n",
    "!pip install openai pandas weave\n",
    "\n",
    "# Find your wandb API key at: https://wandb.ai/authorize\n",
    "# Ensure that your wandb API key is available at:\n",
    "# os.environ['WANDB_API_KEY'] = \"<your_wandb_api_key>\"\n",
    "\n",
    "import asyncio\n",
    "import os\n",
    "import re\n",
    "from textwrap import dedent\n",
    "\n",
    "import weave\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "class JsonModel(weave.Model):\n",
    "    prompt: weave.Prompt = weave.StringPrompt(\n",
    "        dedent(\"\"\"\n",
    "You are an assistant that answers questions about JSON data provided by the user. The JSON data represents structured information of various kinds, and may be deeply nested. In the first user message, you will receive the JSON data under a label called 'context', and a question under a label called 'question'. Your job is to answer the question with as much accuracy and brevity as possible. Give only the answer with no preamble. You must output the answer in XML format, between <answer> and </answer> tags.\n",
    "\"\"\")\n",
    "    )\n",
    "    model: str = \"OpenPipe/Qwen3-14B-Instruct\"\n",
    "    _client: OpenAI\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._client = OpenAI(\n",
    "            base_url='https://api.inference.wandb.ai/v1',\n",
    "            api_key=os.environ['WANDB_API_KEY'],\n",
    "            project='martin-test/another-new-test',\n",
    "        )\n",
    "\n",
    "    @weave.op\n",
    "    def predict(self, context: str, question: str) -> str:\n",
    "        response = self._client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.prompt.format()},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Context: {context}\\nQuestion: {question}\",\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "\n",
    "@weave.op\n",
    "def correct_answer_format(answer: str, output: str) -> dict[str, bool]:\n",
    "    parsed_output = re.search(r\"<answer>(.*?)</answer>\", output, re.DOTALL)\n",
    "    if parsed_output is None:\n",
    "        return {\"correct_answer\": False, \"correct_format\": False}\n",
    "    return {\"correct_answer\": parsed_output.group(1) == answer, \"correct_format\": True}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.environ.get('WANDB_API_KEY'):\n",
    "        print(\"WANDB_API_KEY is not set - make sure to export it in your environment or assign it in this script\")\n",
    "        exit(1)\n",
    "\n",
    "    # Find your wandb API key at: https://wandb.ai/authorize\n",
    "    weave.init(\"martin-test/another-new-test\")\n",
    "\n",
    "    jsonqa = weave.Dataset.from_uri(\n",
    "        \"weave:///wandb/json-qa/object/json-qa:v3\"\n",
    "    ).to_pandas()\n",
    "\n",
    "    model = JsonModel()\n",
    "\n",
    "    eval = weave.Evaluation(\n",
    "        name=\"json-qa-eval\",\n",
    "        dataset=weave.Dataset.from_pandas(jsonqa),\n",
    "        scorers=[correct_answer_format],\n",
    "    )\n",
    "\n",
    "    asyncio.run(eval.evaluate(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üêù Running your first evaluation (using OpenAI)\n",
    "\n",
    "Your can find your OpenAI API keys here: https://platform.openai.com/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure your dependencies are installed with:\n",
    "!pip install openai pandas weave\n",
    "\n",
    "# Find your OpenAI API key at: https://platform.openai.com/api-keys\n",
    "# Ensure that your OpenAI API key is available at:\n",
    "# os.environ['OPENAI_API_KEY'] = \"<your_openai_api_key>\"\n",
    "\n",
    "import asyncio\n",
    "import os\n",
    "import re\n",
    "from textwrap import dedent\n",
    "\n",
    "import openai\n",
    "import weave\n",
    "\n",
    "\n",
    "class JsonModel(weave.Model):\n",
    "    prompt: weave.Prompt = weave.StringPrompt(\n",
    "        dedent(\"\"\"\n",
    "You are an assistant that answers questions about JSON data provided by the user. The JSON data represents structured information of various kinds, and may be deeply nested. In the first user message, you will receive the JSON data under a label called 'context', and a question under a label called 'question'. Your job is to answer the question with as much accuracy and brevity as possible. Give only the answer with no preamble. You must output the answer in XML format, between <answer> and </answer> tags.\n",
    "\"\"\")\n",
    "    )\n",
    "    model: str = \"gpt-4.1-nano\"\n",
    "    _client: openai.OpenAI\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._client = openai.OpenAI()\n",
    "\n",
    "    @weave.op\n",
    "    def predict(self, context: str, question: str) -> str:\n",
    "        response = self._client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.prompt.format()},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Context: {context}\\nQuestion: {question}\",\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "        assert response.choices[0].message.content is not None\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "\n",
    "@weave.op\n",
    "def correct_answer_format(answer: str, output: str) -> dict[str, bool]:\n",
    "    parsed_output = re.search(r\"<answer>(.*?)</answer>\", output, re.DOTALL)\n",
    "    if parsed_output is None:\n",
    "        return {\"correct_answer\": False, \"correct_format\": False}\n",
    "    return {\"correct_answer\": parsed_output.group(1) == answer, \"correct_format\": True}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.environ.get('OPENAI_API_KEY'):\n",
    "        print(\"OPENAI_API_KEY is not set - make sure to export it in your environment or assign it in this script\")\n",
    "        exit(1)\n",
    "\n",
    "    # Find your wandb API key at: https://wandb.ai/authorize\n",
    "    weave.init(\"martin-test/another-new-test\")\n",
    "\n",
    "    jsonqa = weave.Dataset.from_uri(\n",
    "        \"weave:///wandb/json-qa/object/json-qa:v3\"\n",
    "    ).to_pandas()\n",
    "\n",
    "    model = JsonModel()\n",
    "\n",
    "    eval = weave.Evaluation(\n",
    "        name=\"json-qa-eval\",\n",
    "        dataset=weave.Dataset.from_pandas(jsonqa),\n",
    "        scorers=[correct_answer_format],\n",
    "    )\n",
    "\n",
    "    asyncio.run(eval.evaluate(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGqeyYMmw7Hl"
   },
   "source": [
    "## üöÄ Looking for more examples?\n",
    "\n",
    "- Learn how to build an [evlauation pipeline end-to-end](https://weave-docs.wandb.ai/tutorial-eval). \n",
    "- Learn how to evaluate a [RAG application by building](https://weave-docs.wandb.ai/tutorial-rag)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
