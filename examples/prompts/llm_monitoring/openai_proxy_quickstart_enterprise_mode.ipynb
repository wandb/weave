{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Openai Proxy Quickstart - Enterprise Mode\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/wandb/weave/blob/master/examples/prompts/llm_monitoring/openai_proxy_quickstart_enterprise_mode.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "A simple tool to capture metadata about your interactions with the OpenAI API.  Currently supports ChatCompletion endpoint, including streaming.\n",
    "\n",
    "Fundamentally, you can directly request data from the proxy using any technique you prefer (eg. `curl`), or use the openai client library. This notebook will highlight both apporaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not already installed\n",
    "!pip install -qqq openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings & Setup\n",
    "\n",
    "The following setup the openai client to log correctly. Concretely, we need to:\n",
    "1. Change your openai api base url.\n",
    "2. Change your openai api key wandb api key.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Configure target URL\n",
    "WANDB_BASE_URL = # SET ME! - unlikely to be \"https://api.wandb.ai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Ensure WANDB_API_KEY is set in env\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "WANDB_APP_URL = WANDB_BASE_URL if WANDB_BASE_URL != \"https://api.wandb.ai\" else \"https://app.wandb.ai\"\n",
    "WEAVE_ROOT_URL = WANDB_BASE_URL + \"/weave\" if WANDB_BASE_URL != \"https://api.wandb.ai\" else \"https://weave.wandb.ai\"\n",
    "if os.getenv(\"WANDB_API_KEY\") is None:\n",
    "    os.environ[\"WANDB_API_KEY\"] = getpass(f\"Paste your W&B key from: {WANDB_APP_URL}/authorize \\n\")\n",
    "print(\"W&B API key configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Perform setup\n",
    "import openai\n",
    "openai.api_base = f\"{WANDB_BASE_URL}/proxy/openai/v1\"\n",
    "openai.api_key = os.getenv(\"WANDB_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start logging\n",
    "You can use your existing API calls as-is, now with extra logging!\n",
    "\n",
    "By default, this will log to `{enterprise_admin_entity}/monitoring/openai`, where the project is `monitoring` and the table name is `openai`.\n",
    "\n",
    "### Advanced configuration\n",
    "1. Optionally add custom attributes to your table with the `X-Wandb-Attribute-` prefix.  For example, adding the header `X-Wandb-Attribute-Foo: bar` will add a column `Foo` with the value `bar` to your table.\n",
    "2. Optionally add the `X-Wandb-Client-Id` header if you want to further group related requests together.  By default, each request will have its own unique Client ID.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style 1: Using OpenAI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke about gradients as if you were Shakespeare\"}],\n",
    "    headers={\n",
    "        # Optional Client ID\n",
    "        \"X-Wandb-Client-Id\": \"Shakespearean-Gradients-001\",\n",
    "        # Optional Key-Value pairs:\n",
    "        \"X-Wandb-Attribute-Persona\": \"Shakespeare\",\n",
    "    }\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Admins can view logs at {WEAVE_ROOT_URL}/browse/wandb/[ADMIN_ENTITY]/monitoring/openai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style 2: Using CURL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell simply exposes the correct strings as env variables\n",
    "# so that they can be used in the next cell to demonstrate curling\n",
    "# the endpoint. These env vars are not required - it is just an easy\n",
    "# way to pass data from the notebook to the bash command\n",
    "os.environ[\"OPENAI_PROXY_API_BASE\"] = openai.api_base\n",
    "os.environ[\"OPENAI_PROXY_API_KEY\"] = openai.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to the openai sdk example, the X-Wandb-Client-Id and X-Wandb-Attribute-[KEY] headers are optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl \"$OPENAI_PROXY_API_BASE/chat/completions\" \\\n",
    "-H \"Authorization: Bearer $OPENAI_PROXY_API_KEY\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-H \"X-Wandb-Client-Id: Shakespearean-Gradients-001\" \\\n",
    "-H \"X-Wandb-Attribute-Persona: Shakespeare\" \\\n",
    "-d '{\n",
    "     \"model\": \"gpt-3.5-turbo\",\n",
    "     \"messages\": [{\"role\": \"user\", \"content\": \"Tell me a joke about loss functions if you were Shakespeare!\"}]\n",
    "   }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Admins can view logs at {WEAVE_ROOT_URL}/browse/wandb/[ADMIN_ENTITY]/monitoring/openai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## Use Proxy with Weave SDK\n",
    "\n",
    "Now, users will typically want to track their own logs so they can analyze them independently. In this case we combine the monitoring techiques discussed in [OpenAI Monitoring Demo](./openai_client_quickstart.ipynb) with the proxy concepts here.\n",
    "\n",
    "You can use this proxy with the Weave SDK to log out to a personal streamtable and a proxy-configured streamtable simultaneously.  This can be useful for admins who want to monitor API calls in their organization.\n",
    "- Users specify a personal table path with `init_monitor(entity/project/table)`.  Any `ChatCompletion.create` calls with automatically log to this table.\n",
    "- Then, if the `openai.api_base` and `openai.api_key` are set to proxy format, the above calls will also log the proxy-configured table.\n",
    "\n",
    "NOTE: This currently does not work with streaming `ChatCompletion`, but we hope to support it soon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WB_PROJECT = \"llmon\"\n",
    "STREAM_NAME = \"openai_logs\"\n",
    "WB_ENTITY = \"\"  # optional: wandb username or team name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The WB_ENTITY will match the default entity associated with your [W&B API key](wandb.ai/authorize). You can optionally set WB_ENTITY to a different wandb username or team name. Log in to W&B and navigate to [the Home Page](https://wandb.ai/home) to see any other valid options for your WB_ENTITY under your \"Profile\" and \"Teams\" in the left sidebar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install weave tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_BASE_URL\"] = WANDB_BASE_URL\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from weave.legacy.monitoring import openai as openaimon\n",
    "from weave.legacy.monitoring import init_monitor\n",
    "\n",
    "m = init_monitor(f\"{WB_ENTITY}/{WB_PROJECT}/{STREAM_NAME}\")\n",
    "\n",
    "r = openaimon.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": f\"hello world!\"}])\n",
    "print(r)\n",
    "r = openaimon.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": f\"what is 2+2?\"}])\n",
    "print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
