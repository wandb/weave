{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "<!--- @wandbcode{weave_openai_client_qs} -->\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/wandb/weave/blob/master/examples/prompts/llm_monitoring/openai_proxy_quickstart.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "**Note:** you will need an [OpenAI API key](https://platform.openai.com/account/api-keys) to run this colab.\n",
    "\n",
    "# Openai Proxy Quickstart\n",
    "> A simple tool to capture metadata about your interactions with the OpenAI API.  Currently supports ChatCompletion endpoint, including streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not already installed\n",
    "!pip install -qqq openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authenticate with OpenAI\n",
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI key from: https://platform.openai.com/account/api-keys\\n\")\n",
    "assert os.getenv(\"OPENAI_API_KEY\", \"\").startswith(\"sk-\"), \"This doesn't look like a valid OpenAI API key\"\n",
    "print(\"OpenAI API key configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authenticate with W&B\n",
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "if os.getenv(\"WANDB_API_KEY\") is None:\n",
    "  os.environ[\"WANDB_API_KEY\"] = getpass(\"Paste your W&B API key from: https://wandb.ai/authorize\\n\")\n",
    "print(\"W&B API key configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "All you need to do is:\n",
    "1. Change your openai api key to the format `{wandb_api_key}:{openai_api_key}`; and\n",
    "2. Change your openai api base url to `https://wandb.ai/proxy/openai/v1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "wandb_api_key = os.getenv(\"WANDB_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_base = \"https://api.wandb.ai/proxy/openai/v1\"\n",
    "openai.api_key = f\"{wandb_api_key}:{openai_api_key}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start logging\n",
    "You can use your existing API calls as-is, now with extra logging!\n",
    "\n",
    "By default, this will log to `{your_entity}/monitoring/openai`, where the project is `monitoring` and the table name is `openai`.\n",
    "\n",
    "**View your logs at [https://weave.wandb.ai/browse/wandb](https://weave.wandb.ai/browse/wandb)/{your_entity}/monitoring/openai** - click `Monitor OpenAI API usage` to launch into an interactive dashboard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using OpenAI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke about gradients!\"}],\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CURL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl \"https://api.wandb.ai/proxy/openai/v1/chat/completions\" \\\n",
    "-H \"Authorization: Bearer $WANDB_API_KEY:$OPENAI_API_KEY\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-d '{\n",
    "     \"model\": \"gpt-3.5-turbo\",\n",
    "     \"messages\": [{\"role\": \"user\", \"content\": \"Tell me a joke about loss functions!\"}]\n",
    "   }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Optionally specify headers to determine where you want your data to be created.\n",
    "\n",
    "| Header | Description | Default |\n",
    "| --- | --- | --- |\n",
    "| `X-Wandb-Entity` | W&B entity name | Your default entity |\n",
    "| `X-Wandb-Project` | W&B project name | `monitoring` |\n",
    "| `X-Wandb-Stream` | StreamTable name | `openai` |\n",
    "\n",
    "By default, proxy data will show up in `{your_entity}/monitoring/openai`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced configuration\n",
    "1. Optionally add custom attributes to your table with the `X-Wandb-Attribute-` prefix.  For example, adding the header `X-Wandb-Attribute-Foo: bar` will add a column `Foo` with the value `bar` to your table.\n",
    "2. Optionally add the `X-Wandb-Client-Id` header if you want to further group related requests together.  By default, each request will have its own unique Client ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "WB_ENTITY = # set to your wandb username or team name\n",
    "WB_PROJECT = \"weave\" # top-level directory for this work\n",
    "STREAM_NAME = \"openai_logs\" # record table which stores the logs of OpenAI API calls as they stream in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using OpenAI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke about gradients as if you were Shakespeare\"}],\n",
    "    headers={\n",
    "        \"X-Wandb-Entity\": WB_ENTITY,\n",
    "        \"X-Wandb-Project\": WB_PROJECT,\n",
    "        \"X-Wandb-Stream\": STREAM_NAME,\n",
    "        \"X-Wandb-Client-Id\": \"Shakespearean-Gradients-001\",\n",
    "        \"X-Wandb-Attribute-Persona\": \"Shakespeare\",\n",
    "    }\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use with CURL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Replace WB_ENTITY, WB_PROJECT, and STREAM_NAME to specify where to store the logs.\n",
    "# You will likely get an authentication error if you don't do this.\n",
    "\n",
    "%%bash\n",
    "curl \"https://api.wandb.ai/proxy/openai/v1/chat/completions\" \\\n",
    "-H \"Authorization: Bearer $WANDB_API_KEY:$OPENAI_API_KEY\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-H \"X-Wandb-Entity: WB_ENTITY\" \\ \n",
    "-H \"X-Wandb-Project: WB_PROJECT\" \\\n",
    "-H \"X-Wandb-Stream: STREAM_NAME\" \\\n",
    "-H \"X-Wandb-Client-Id: Shakespearean-Gradients-001\" \\\n",
    "-H \"X-Wandb-Attribute-Persona: Shakespeare\" \\\n",
    "-d '{\n",
    "     \"model\": \"gpt-3.5-turbo\",\n",
    "     \"messages\": [{\"role\": \"user\", \"content\": \"Tell me a joke about loss functions!\"}]\n",
    "   }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## Use Proxy with Weave SDK\n",
    "You can use this proxy with the Weave SDK to log out to a personal streamtable and a proxy-configured streamtable simultaneously.  This can be useful for admins who want to monitor API calls in their organization.\n",
    "- Users specify a personal table path with `init_monitor(entity/project/table)`.  Any `ChatCompletion.create` calls with automatically log to this table.\n",
    "- Then, if the `openai.api_base` and `openai.api_key` are set to proxy format, the above calls will also log the proxy-configured table.\n",
    "\n",
    "NOTE: This currently does not work with streaming `ChatCompletion`, but we hope to support it soon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -qqq weave tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weave.monitoring import openai as openaimon\n",
    "from weave.monitoring import init_monitor\n",
    "\n",
    "m = init_monitor(f\"{WB_ENTITY}/{WB_PROJECT}/{STREAM_NAME}\")\n",
    "\n",
    "openai.api_base = \"https://api.wandb.ai/proxy/openai/v1\"\n",
    "openai.api_key = f\"{wandb_api_key}:{openai_api_key}\"\n",
    "\n",
    "OPENAI_MODEL = 'gpt-3.5-turbo'\n",
    "\n",
    "r = openaimon.ChatCompletion.create(model=OPENAI_MODEL, messages=[{\"role\": \"user\", \"content\": f\"hello world!\"}])\n",
    "r = openaimon.ChatCompletion.create(model=OPENAI_MODEL, messages=[{\"role\": \"user\", \"content\": f\"what is 2+2?\"}])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
