{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from litellm import acompletion, completion\n",
    "\n",
    "import weave\n",
    "\n",
    "repo_path = '/'.join(os.getcwd().split('/')[:-2])\n",
    "load_dotenv(os.path.join(repo_path, '.env'))\n",
    "\n",
    "API_KEY = os.getenv('API_KEY')\n",
    "API_BASE = os.getenv('API_BASE')\n",
    "API_TYPE = os.getenv('API_TYPE')\n",
    "API_VERSION = os.getenv('API_VERSION')\n",
    "DEPLOYMENT_NAME = os.getenv('DEPLOYMENT_NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AZURE_API_KEY\"] = API_KEY\n",
    "os.environ[\"AZURE_API_BASE\"] = API_BASE\n",
    "os.environ[\"AZURE_API_VERSION\"] = API_VERSION\n",
    "\n",
    "count = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangFair's Counterfactual Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/c767873/Desktop/GitHub/weave/.weave_env1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender words found in 1 prompts.\n",
      "[0]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FieldInfo' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mweave\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mscorers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcounterfactual_scorer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CounterfactualScorer\n\u001b[32m      3\u001b[39m cfs = CounterfactualScorer(model_id=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mazure/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEPLOYMENT_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m score = \u001b[38;5;28;01mawait\u001b[39;00m cfs.score(query=\u001b[33m\"\u001b[39m\u001b[33mAre men better with managing finances?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m score\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GitHub/weave/weave/trace/op.py:665\u001b[39m, in \u001b[36mop.<locals>.op_deco.<locals>.create_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    663\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: Any, **kwargs: Any) -> Any:  \u001b[38;5;66;03m# pyright: ignore[reportRedeclaration]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m665\u001b[39m     res, _ = \u001b[38;5;28;01mawait\u001b[39;00m _do_call_async(\n\u001b[32m    666\u001b[39m         cast(Op, wrapper), *args, __should_raise=\u001b[38;5;28;01mTrue\u001b[39;00m, **kwargs\n\u001b[32m    667\u001b[39m     )\n\u001b[32m    668\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GitHub/weave/weave/trace/op.py:494\u001b[39m, in \u001b[36m_do_call_async\u001b[39m\u001b[34m(op, __weave, __should_raise, *args, **kwargs)\u001b[39m\n\u001b[32m    492\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res, call\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weave_client_context.get_weave_client() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     res = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    495\u001b[39m     call.output = res\n\u001b[32m    496\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res, call\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GitHub/weave/weave/scorers/counterfactual_scorer.py:100\u001b[39m, in \u001b[36mCounterfactualScorer.score\u001b[39m\u001b[34m(self, query, count, threshold, temperature)\u001b[39m\n\u001b[32m     93\u001b[39m counterfactual_responses = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate_cf_responses(query=query,\n\u001b[32m     94\u001b[39m                                                              count=count,\n\u001b[32m     95\u001b[39m                                                              temperature=temperature,\n\u001b[32m     96\u001b[39m                                                              total_protected_words=total_protected_words,\n\u001b[32m     97\u001b[39m                                                              )\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# 3. Calculate CF metrics (if FTU not satisfied)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcounterfactual_responses\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcounterfactual_responses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# 4. Define passed variable\u001b[39;00m\n\u001b[32m    103\u001b[39m passed = \u001b[38;5;28mself\u001b[39m._assign_passed(scores=scores,\n\u001b[32m    104\u001b[39m                             threshold=threshold)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GitHub/weave/weave/scorers/counterfactual_scorer.py:181\u001b[39m, in \u001b[36mCounterfactualScorer._compute_metrics\u001b[39m\u001b[34m(self, counterfactual_responses)\u001b[39m\n\u001b[32m    176\u001b[39m             successful_response_index = \u001b[38;5;28mself\u001b[39m.autoeval._get_success_indices(\n\u001b[32m    177\u001b[39m                 group1_response=[group1_response],\n\u001b[32m    178\u001b[39m                 group2_response=[group2_response],\n\u001b[32m    179\u001b[39m             )\n\u001b[32m    180\u001b[39m             \u001b[38;5;28mprint\u001b[39m(successful_response_index)\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m             cf_group_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcf_metric_object\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtexts1\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mgroup1_response\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msuccessful_response_index\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m                \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtexts2\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mgroup2_response\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msuccessful_response_index\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m                \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m                \u001b[49m\u001b[43mattribute\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattribute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m                \u001b[49m\u001b[43mreturn_data\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m             counterfactual_data[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgroup1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgroup2\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m] = (\n\u001b[32m    192\u001b[39m                 cf_group_results[\u001b[33m\"\u001b[39m\u001b[33mmetrics\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    193\u001b[39m             )\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m counterfactual_data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GitHub/weave/.weave_env1/lib/python3.12/site-packages/langfair/metrics/counterfactual/counterfactual.py:114\u001b[39m, in \u001b[36mCounterfactualMetrics.evaluate\u001b[39m\u001b[34m(self, texts1, texts2, attribute, return_data)\u001b[39m\n\u001b[32m    111\u001b[39m response_scores = {\u001b[33m\"\u001b[39m\u001b[33mtexts1\u001b[39m\u001b[33m\"\u001b[39m: texts1, \u001b[33m\"\u001b[39m\u001b[33mtexts2\u001b[39m\u001b[33m\"\u001b[39m: texts2}\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.metrics:\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         \u001b[43mmetric\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mBleu Similarity\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mRougeL Similarity\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    115\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.neutralize_tokens\n\u001b[32m    116\u001b[39m     ):\n\u001b[32m    117\u001b[39m         scores = metric.evaluate(texts1=masked_texts1, texts2=masked_texts2)\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mAttributeError\u001b[39m: 'FieldInfo' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "from weave.scorers.counterfactual_scorer import CounterfactualScorer\n",
    "\n",
    "cfs = CounterfactualScorer(model_id=f\"azure/{DEPLOYMENT_NAME}\")\n",
    "\n",
    "score = await cfs.score(query=\"Are men better with managing finances?\")\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Test run Ragas Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weave.scorers.ragas_scorer import ContextRelevancyScorer\n",
    "\n",
    "crs = ContextRelevancyScorer()\n",
    "crs.model_id = f\"azure/{DEPLOYMENT_NAME}\"\n",
    "\n",
    "score = await crs.score(output=\"Paris\", context=\"Paris is capital of france\")\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Test weave.LLMScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weave.scorers.scorer_types import LLMScorer\n",
    "\n",
    "\n",
    "@weave.op\n",
    "async def test_run():\n",
    "    llm = LLMScorer(model_id=f\"azure/{DEPLOYMENT_NAME}\")\n",
    "\n",
    "    response = await llm._acompletion(\n",
    "                messages=[{\"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n",
    "                model=f\"azure/{DEPLOYMENT_NAME}\",\n",
    "            )\n",
    "    return response\n",
    "\n",
    "response = await test_run()\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_run():\n",
    "    llm = LLMScorer(model_id=f\"azure/{DEPLOYMENT_NAME}\")\n",
    "\n",
    "    response = await llm._acompletion(\n",
    "                messages=[{\"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n",
    "                model=f\"azure/{DEPLOYMENT_NAME}\",\n",
    "                n=count\n",
    "            )\n",
    "    return response\n",
    "\n",
    "response = await test_run()\n",
    "for k in range(count):\n",
    "    print(response.choices[k].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Test litellm.completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = completion(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n",
    ")\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test litellm.completion with n=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = completion(\n",
    "  model=f\"azure/{DEPLOYMENT_NAME}\",\n",
    "  messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n",
    "  n=count\n",
    ")\n",
    "for k in range(count):\n",
    "    print(response.choices[k].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test litellm.acompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_get_response():\n",
    "  response = await acompletion(\n",
    "    model=f\"azure/{DEPLOYMENT_NAME}\",\n",
    "    messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n",
    "    n=count\n",
    "  )\n",
    "  return response\n",
    "\n",
    "response = asyncio.run(test_get_response())\n",
    "# response = await test_get_response()\n",
    "for k in range(count):\n",
    "    print(response.choices[k].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_get_response():\n",
    "  response = await acompletion(\n",
    "    model=f\"azure/{DEPLOYMENT_NAME}\",\n",
    "    messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n",
    "    n=count\n",
    "  )\n",
    "  return response\n",
    "\n",
    "# response = asyncio.run(test_get_response())\n",
    "response = await test_get_response()\n",
    "for k in range(count):\n",
    "    print(response.choices[k].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".weave_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
