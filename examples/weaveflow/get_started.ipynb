{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Weave Library\n",
    "# (note: this will become `pip install weave` soon - this is a temp dev state)\n",
    "# !pip uninstall weave -y >/dev/null 2>&1\n",
    "# !pip install git+https://github.com/wandb/weave@master -U -qqq >/dev/null 2>&1\n",
    "# !echo \"Installed Weave!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with W&B\n",
    "# (note: we can eliminate this step import in the future)\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weave\n",
    "# (note: we can eliminate the `weaveflow` import in future)\n",
    "\n",
    "import weave\n",
    "\n",
    "client = weave.init(\"wf_eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with OpenAI\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\n",
    "        \"Paste your OpenAI key from: https://platform.openai.com/account/api-keys\\n\"\n",
    "    )\n",
    "assert os.getenv(\"OPENAI_API_KEY\", \"\").startswith(\n",
    "    \"sk-\"\n",
    "), \"This doesn't look like a valid OpenAI API key\"\n",
    "print(\"OpenAI API key configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def simple_openai_complete(message: str, model: str, system_prompt: str) -> str:\n",
    "    completion = OpenAI().chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": message},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def lookup_docs(user_message: str) -> list[str]:\n",
    "    docs = simple_openai_complete(\n",
    "        f\"Query: {user_message}\",\n",
    "        \"gpt-3.5-turbo\",\n",
    "        \"Please act like a vector database, returning up to 3 documents that relate to a query. Feel free to be creative, make up names & details, etc... - as if you have access to relevant documents containing critical information. Do not include docuemnt titles, just content. You must format your response as a JSON.load-able array of strings.\",\n",
    "    )\n",
    "    try:\n",
    "        json_res = json.loads(docs)\n",
    "        res = []\n",
    "        for item in json_res:\n",
    "            res.append(\"\" + item)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "@weave.type()\n",
    "class GptRagModel:\n",
    "    base_model: str\n",
    "    system_prompt: str\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, input: str) -> str:\n",
    "        docs = lookup_docs(input)\n",
    "        prompt = f\"Given the following documents, please formulate a short, consise answer.\\n Documents: {','.join(docs)}.\\n\\n Query: {input}.\"\n",
    "        res = simple_openai_complete(prompt, self.base_model, self.system_prompt)\n",
    "        return res\n",
    "\n",
    "\n",
    "@weave.type()\n",
    "class SimpleDataset:\n",
    "    examples: list[str]\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def brevity_score(example: str, prediction: str) -> float:\n",
    "    # returns a dict of scores\n",
    "    return 1 / (1 + len(prediction))\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def relevance_score(example: str, prediction: str) -> int:\n",
    "    # returns a dict of scores\n",
    "    return int(\n",
    "        simple_openai_complete(\n",
    "            f\"Prompt:{example}.\\nAnswer:{prediction}.\\nScore:\",\n",
    "            \"gpt-3.5-turbo\",\n",
    "            \"Score relevance of the output to the input. Emit ONLY a number between 0 and 9 inclusive. Nothing else\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def score(example: str, prediction: str) -> dict:\n",
    "    # returns a dict of scores\n",
    "    return {\n",
    "        \"brevity\": brevity_score(example, prediction),\n",
    "        \"relevance\": relevance_score(example, prediction),\n",
    "    }\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def eval_iter(model: GptRagModel, example: str) -> dict:\n",
    "    return score(example, model.predict(example))\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def aggregate_scores(score_dicts) -> float:\n",
    "    return sum(\n",
    "        [score_dict[\"brevity\"] * score_dict[\"relevance\"] for score_dict in score_dicts]\n",
    "    ) / len(score_dicts)\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def evaluate(model: GptRagModel, dataset: SimpleDataset):\n",
    "    scores = []\n",
    "    for example in dataset.examples:\n",
    "        score_dict = eval_iter(model, example)\n",
    "        scores.append(score_dict)\n",
    "    score_agg = aggregate_scores(scores)\n",
    "    return score_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    GptRagModel(\"gpt-3.5-turbo\", \"You are a helpful assistant.\"),\n",
    "    GptRagModel(\"gpt-3.5-turbo\", \"You are a very bored, sarcastic assistant.\"),\n",
    "    GptRagModel(\"gpt-3.5-turbo\", \"You are a evil, unhelpful assistant.\"),\n",
    "]\n",
    "\n",
    "dataset = SimpleDataset(\n",
    "    [\n",
    "        \"What is the square root of pi?\",\n",
    "        \"Describe inception, not the movie, the concept.\",\n",
    "        \"How much is Apple worth today?\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "for model in models:\n",
    "    evaluate(model, dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wandb-weave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
