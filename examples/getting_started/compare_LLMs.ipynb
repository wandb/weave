{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f47d996",
   "metadata": {},
   "source": [
    "# Compare LLMs\n",
    "\n",
    "Creating programs that rely on LLMs is an iterative process so we need a workflow to compare each new pipeline/prompt/technique systematically.\n",
    "\n",
    "In this tutorial, we'll experiment with models and create a workflow using Weave to:\n",
    "- Run the same evaluation set for every new program and store the responses, token counts, etc.\n",
    "- Display a table with any two pipelines' responses side-by-side, with the ability to page through examples and group/sort/filter from the UI\n",
    "- Display a bar chart to compare each metric, like token count sum\n",
    "\n",
    "# Evaluate different pipelines\n",
    "\n",
    "So that we're comparing apples with apples, we'll create a single evaluation dataset for our pipelines. We'll save it to Weave so that any time we update it, it will be versioned and we can easily get the latest version by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f00181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "import random\n",
    "\n",
    "classes = ['positive', 'negative', 'neutral']\n",
    "prompts = [\"I absolutely love this product!\", \n",
    "           \"I'm really disappointed with this service.\", \n",
    "           \"The movie was just average.\"]\n",
    "labels = ['positive', 'negative', 'neutral']\n",
    "dataset = [{'prompt': prompt, 'label': label} for prompt, label in zip(prompts, labels)]\n",
    "dataset_name = 'classification_test_set'\n",
    "test_set = weave.save(dataset, name=dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974c84f5",
   "metadata": {},
   "source": [
    "For this example, we'll just mock a few pipelines (this will also save us from using up our LLM budget while we build out this example :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85837eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(prompt: str) -> str:\n",
    "    latency = random.uniform(0, 10)\n",
    "    tokens = random.choice(range(0, 10))\n",
    "    response = random.choice(classes)\n",
    "    return response, latency, tokens\n",
    "\n",
    "pipeline_1 = pipeline_2 = pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8c11f0",
   "metadata": {},
   "source": [
    "Now that we have our pipelines defined, we can run them through our data and capture the responses. Here is where we'll capture other metrics we care about like latency and token count so we can use them for comparison later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d6fc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = weave.get(f\"local-artifact:///{dataset_name}:latest/obj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f9b6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(pipeline, pipeline_name):\n",
    "    outs = []\n",
    "    dataset_name = 'classification_test_set'\n",
    "    test_set = weave.get(f\"local-artifact:///{dataset_name}:latest/obj\")\n",
    "    for examples in test_set.val:\n",
    "        prompt, label = examples['prompt'], examples['label']\n",
    "        response, latency, tokens = pipeline(prompt)\n",
    "        outs.append({'label': label,\n",
    "                     'prompt': prompt,\n",
    "                     'response': response,\n",
    "                     'latency': latency,\n",
    "                     'tokens': tokens})\n",
    "    return weave.save(outs, name=pipeline_name)\n",
    "\n",
    "pipeline_1_w = evaluate(pipeline_1, 'pipeline_1')\n",
    "pipeline_2_w = evaluate(pipeline_2, 'pipeline_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1075426e",
   "metadata": {},
   "source": [
    "We use `weave.save(<responses and metrics>, name=<name>)` to save metrics and responses to weave. We can choose any name for the models, and we collect responses and metrics in a list of Python dictionaries. \n",
    "\n",
    "Weave will intelligently display our data in a clear and useful format, so we can view `pipeline_1_w` in a **Weave Table** panel.\n",
    "To view a panel in a notebook, enter just the variable name on a line by itself and run the cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e7eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_1_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5274b9a5",
   "metadata": {},
   "source": [
    "In this table, you can page through examples, filter results, and even create new columns by using data in other columns.\n",
    "\n",
    "# Join prediction tables\n",
    "\n",
    "Because we want to compare our pipelines for each prompt, we'll use a **Weave Op** (operation) to join the two tables on the `prompt`. We'll give the pipelines aliases `1` & `2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1077c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = weave.ops.join_2(\n",
    "    pipeline_1_w,\n",
    "    pipeline_2_w,\n",
    "    lambda row: row['prompt'], \n",
    "    lambda row: row['prompt'],\n",
    "    '1',\n",
    "    '2',\n",
    "    False,\n",
    "    False)\n",
    "\n",
    "joined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009a8cfa",
   "metadata": {},
   "source": [
    "We only want to display our prompts, labels, and each model's response, so we'll need to use `weave.panels.Table` to do so. The `columns` argument may look a bit complex, but it's a very flexible abstraction for us to define which columns are used in the table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8b4e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = weave.panels.Table(\n",
    "    joined,\n",
    "    columns=[\n",
    "        lambda row: row[\"1.label\"],\n",
    "        lambda row: row[\"1.prompt\"],\n",
    "        lambda row: row[\"1.response\"],\n",
    "        lambda row: row[\"1.response\"] == row[\"1.label\"],\n",
    "        lambda row: row[\"2.response\"],\n",
    "        lambda row: row[\"2.response\"] == row[\"2.label\"],\n",
    "    ],\n",
    ")\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be4a2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "latency_bar = weave.ops.dict_(\n",
    "    pipeline_1=joined[\"1.latency\"].avg(),\n",
    "    pipeline_2=joined[\"2.latency\"].avg(),\n",
    ")\n",
    "latency_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098cd450",
   "metadata": {},
   "source": [
    "This might seem a bit magic, but it's the same as we saw above—Weave makes a best guess how to display your data. Here, we created a dict like `{'pipeline_1': metric_a, 'pipeline_2': metric_b}`, and it chooses to display this dictionary as a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a442d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count_bar = weave.ops.dict_(\n",
    "    pipeline_1=joined[\"1.tokens\"].sum(),\n",
    "    pipeline_2=joined[\"2.tokens\"].sum(),\n",
    ")\n",
    "token_count_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3f9e84",
   "metadata": {},
   "source": [
    "Again, we created a dictionary with weave, and it knew to display it as a bar chart.\n",
    "\n",
    "# Putting it all together in a Board\n",
    "\n",
    "We want to easily jump between comparing different models, so we'll need some way to change which models we're comparing. To do this, we'll use a **Weave Board**.\n",
    "\n",
    "Weave Boards can have variables, or **vars** which you can change dynamically to make your panels and plots update. \n",
    "Here, we'll define which models we're comparing, and we can change the local artifact path to get new models. We'll put each of our panels in a **BoardPanel** and we'll define the layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1abb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weave.panels.Board(\n",
    "    vars={\n",
    "        \"pipeline_1\": pipeline_1_w,\n",
    "        \"pipeline_2\": pipeline_2_w,\n",
    "        \"joined\": lambda pipeline_1, pipeline_2: \n",
    "        weave.ops.join_2(\n",
    "            pipeline_1, \n",
    "            pipeline_2,\n",
    "            lambda row: row['prompt'], \n",
    "            lambda row: row['prompt'],\n",
    "            '1',\n",
    "            '2',\n",
    "            False,\n",
    "            False)                                                 \n",
    "            },\n",
    "    panels=[\n",
    "        weave.panels.BoardPanel(\n",
    "            lambda joined: weave.panels.Table(\n",
    "                                joined,\n",
    "                                columns=[\n",
    "                                    lambda row: row[\"1.label\"],\n",
    "                                    lambda row: row[\"1.prompt\"],\n",
    "                                    lambda row: row[\"1.response\"],\n",
    "                                    lambda row: row[\"1.response\"] == row[\"1.label\"],\n",
    "                                    lambda row: row[\"2.response\"],\n",
    "                                    lambda row: row[\"2.response\"] == row[\"2.label\"],\n",
    "                                ],\n",
    "                            ),\n",
    "            layout=weave.panels.BoardPanelLayout(x=0, y=0, w=24, h=9)\n",
    "        ),\n",
    "         weave.panels.BoardPanel(\n",
    "            lambda joined: weave.ops.dict_(\n",
    "                                pipeline_1=joined[\"1.tokens\"].sum(),\n",
    "                                pipeline_2=joined[\"2.tokens\"].sum(),\n",
    "                            ),\n",
    "             layout=weave.panels.BoardPanelLayout(x=0, y=9, w=12, h=8)\n",
    "        ),\n",
    "        weave.panels.BoardPanel(\n",
    "            lambda joined: weave.ops.dict_(\n",
    "                                pipeline_1=joined[\"1.latency\"].avg(),\n",
    "                                pipeline_2=joined[\"2.latency\"].avg(),\n",
    "                            ),\n",
    "             layout=weave.panels.BoardPanelLayout(x=12, y=9, w=12, h=8)\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88665131",
   "metadata": {},
   "source": [
    "You can now open the Board in a new tab to make it full screen. Hover over the right side of the panel to expand a drawer menu and click the arrow to \"Open in a new tab\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e40dcc",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "And that's it. We've created a **Weave Board** to compare our pipeline responses, token counts, and latency. We've seen how Weave intelligently decides how to display data, whether that's in a table, bar chart or some other type of panel. We've learned how to define **Weave Panels** ourselves and how to define computations using **Weave Ops**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
