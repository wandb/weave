{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/wandb/weave/blob/master/examples/cookbooks/summarization/chain-of-density-arxiv.ipynb)\n",
    "<!--- @wandbcode{weave-cod-summarization-cookbook} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arxiv PDF Summarization Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "from itertools import product\n",
    "\n",
    "import anthropic\n",
    "import filetype\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import requests\n",
    "import arxiv\n",
    "from arxiv_models import ArxivPaper, Author, Link, convert_raw_arxiv_to_pydantic\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pdf2image import convert_from_bytes\n",
    "from PIL import Image\n",
    "\n",
    "import weave\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weave version 0.50.11 is available!  To upgrade, please run:\n",
      " $ pip install weave --upgrade\n",
      "Logged in as Weights & Biases user: a-sh0ts.\n",
      "View Weave data at https://wandb.ai/a-sh0ts/arxiv-papers-anthropic-testv7-4/weave\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<weave.weave_client.WeaveClient at 0x16afd4ce0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weave.init(\"arxiv-chain-of-density-summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "anthropic_client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(d, parent_key='', sep='_'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Fetch Arxiv Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section demonstrates how to fetch relevant papers from the ArXiv database based on a given research instruction. This step is optional but can be useful if you want to dynamically retrieve papers for summarization instead of using predefined examples.\n",
    "\n",
    "### Generate ArXiv Query Arguments\n",
    "\n",
    "We use the `generate_arxiv_query_args` function to create an optimal ArXiv search query and determine the appropriate number of results to fetch. This function leverages Claude to generate a well-crafted query string and suggest a suitable `max_results` value.\n",
    "\n",
    "```python\n",
    "instruction = \"Answer the following question: What are the latest advancements in audio music information retrieval?\"\n",
    "arxiv_query, max_results = generate_arxiv_query_args(instruction)\n",
    "print(f\"ArXiv query: {arxiv_query}\")\n",
    "print(f\"Max results: {max_results}\")\n",
    "```\n",
    "\n",
    "### Fetch ArXiv Papers\n",
    "\n",
    "Once we have the query and max_results, we can use the `fetch_arxiv_papers` function to retrieve the relevant papers from ArXiv. This function returns a list of `ArxivPaper` objects, which contain metadata about each paper, including its title, authors, abstract, and PDF URL.\n",
    "\n",
    "```python\n",
    "arxiv_papers = fetch_arxiv_papers(arxiv_query, max_results)\n",
    "```\n",
    "\n",
    "By uncommenting and running these code snippets, you can dynamically fetch ArXiv papers based on your research interests. This allows for a more flexible and customizable summarization pipeline, enabling you to process and summarize the most recent and relevant research in your field of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def generate_arxiv_query_args(instruction, model=\"claude-3-sonnet-20240229\"):\n",
    "    tools = [{\n",
    "        \"name\": \"prepare_arxiv_search\",\n",
    "        \"description\": \"Prepare arguments for ArXiv paper search. This tool generates an optimal query string utilizing Boolean operators, field-specific syntax, and precise search terms. It also determines an efficient maximum number of results to fetch, balancing comprehensive coverage with processing efficiency. The output is tailored to the given research instruction, aiming to provide relevant and focused search results.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The ArXiv search query string. Supports Boolean operators (AND, OR, NOT), field-specific syntax (e.g., 'ti:' for title, 'au:' for author), quotation marks for exact phrases, and wildcards. Can include multiple search terms to refine results based on title, abstract, authors, comments, journal reference, subject category, or report number.\"\n",
    "                },\n",
    "                \"max_results\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"The maximum number of paper results to return from the ArXiv search. Aims to minimize the number of results while ensuring sufficient coverage of the topic. Defaults to 5 if not specified. Increasing this value broadens the search but may increase processing time and resource usage. Aim to be below 10 articles.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\", \"max_results\"]\n",
    "        }\n",
    "    }]\n",
    "\n",
    "    system_prompt = \"\"\"You are an expert at generating ArXiv queries. Use the prepare_arxiv_search tool to create an optimal query and determine the appropriate maximum number of results for the given research question. The query should utilize advanced search techniques including Boolean operators, field-specific syntax, and precise terms to ensure comprehensive yet focused results.\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Use the prepare_arxiv_search tool to generate an optimal ArXiv query and determine the maximum number of results for the following research instruction: {instruction}\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response = anthropic_client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=4096,\n",
    "        messages=messages,\n",
    "        system=system_prompt,\n",
    "        tools=tools\n",
    "    )\n",
    "\n",
    "    # Extract the query and max_results from the response\n",
    "    for content in response.content:\n",
    "        if content.type == 'tool_use' and content.name == 'prepare_arxiv_search':\n",
    "            args = content.input\n",
    "            return args.get('query'), args.get('max_results')\n",
    "\n",
    "    # If no tool use was found, return a default query and the provided max_results\n",
    "    return f\"{instruction}\", 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instruction = \"Answer the following question: What are the latest advancements in audio music information retrieval?\"\n",
    "# arxiv_query, max_results = generate_arxiv_query_args(instruction)\n",
    "# print(f\"ArXiv query: {arxiv_query}\")\n",
    "# print(f\"Max results: {max_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def fetch_arxiv_papers(query, max_results=5):\n",
    "    # Initialize the arxiv Client\n",
    "    arxiv_client = arxiv.Client()\n",
    "    \n",
    "    # Create the search object\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.Relevance,\n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "    \n",
    "    # Fetch the results using client.results() and convert them to ArxivPaper objects\n",
    "    papers = []\n",
    "    for result in arxiv_client.results(search):\n",
    "        paper = convert_raw_arxiv_to_pydantic(result)\n",
    "        papers.append(paper)\n",
    "    \n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arxiv_papers = fetch_arxiv_papers(arxiv_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a sample Arxiv paper object and load its PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we demonstrate how to create a sample `ArxivPaper` object and load its corresponding PDF. This process is crucial for our summarization pipeline, as it provides both the metadata and the actual content of the paper.\n",
    "\n",
    "### Creating the ArxivPaper object\n",
    "\n",
    "The `ArxivPaper` class is a custom data structure that encapsulates various attributes of an arXiv paper, including:\n",
    "\n",
    "- `entry_id`: A unique identifier for the paper\n",
    "- `updated` and `published`: Timestamps for when the paper was last updated and initially published\n",
    "- `title`: The title of the paper\n",
    "- `authors`: A list of `Author` objects representing the paper's authors\n",
    "- `summary`: An abstract or brief description of the paper's content\n",
    "- `doi`: The Digital Object Identifier for the paper\n",
    "- `categories`: The arXiv categories the paper belongs to\n",
    "- `links`: Various URLs associated with the paper, including its abstract and PDF\n",
    "- `pdf_url`: A direct link to the paper's PDF\n",
    "\n",
    "In the code snippet below, we create an `ArxivPaper` object for a paper titled \"CRAG -- Comprehensive RAG Benchmark\". This paper discusses a new benchmark for Retrieval-Augmented Generation (RAG) systems, which is highly relevant to our summarization task.\n",
    "\n",
    "### Loading the PDF\n",
    "\n",
    "After creating the `ArxivPaper` object, we use the `load_pdf` function to fetch and load the actual PDF content. This function:\n",
    "\n",
    "1. Retrieves the PDF URL from the `ArxivPaper` object\n",
    "2. Downloads the PDF content using the `requests` library\n",
    "3. Creates a `BytesIO` object from the downloaded content\n",
    "4. Uses `PyPDF2.PdfReader` to create a PDF reader object\n",
    "\n",
    "The `load_pdf` function allows us to work with the actual content of the paper, which is essential for our summarization task.\n",
    "\n",
    "By using this sample object and loading its PDF, we can proceed with our chain of density summarization process and evaluate its performance on a known, controlled input. This approach helps in debugging, fine-tuning, and showcasing the capabilities of our summarization pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper = ArxivPaper(\n",
    "    entry_id=\"http://arxiv.org/abs/2406.04744v1\",\n",
    "    updated=datetime(2024, 6, 7, 8, 43, 7, tzinfo=timezone.utc),\n",
    "    published=datetime(2024, 6, 7, 8, 43, 7, tzinfo=timezone.utc),\n",
    "    title=\"CRAG -- Comprehensive RAG Benchmark\",\n",
    "    authors=[\n",
    "        Author(full_name=\"Xiao Yang\"),\n",
    "        Author(full_name=\"Kai Sun\"),\n",
    "        Author(full_name=\"Hao Xin\"),\n",
    "        Author(full_name=\"Yushi Sun\"),\n",
    "        Author(full_name=\"Nikita Bhalla\"),\n",
    "        Author(full_name=\"Xiangsen Chen\"),\n",
    "        Author(full_name=\"Sajal Choudhary\"),\n",
    "        Author(full_name=\"Rongze Daniel Gui\"),\n",
    "        Author(full_name=\"Ziran Will Jiang\"),\n",
    "        Author(full_name=\"Ziyu Jiang\"),\n",
    "        Author(full_name=\"Lingkun Kong\"),\n",
    "        Author(full_name=\"Brian Moran\"),\n",
    "        Author(full_name=\"Jiaqi Wang\"),\n",
    "        Author(full_name=\"Yifan Ethan Xu\"),\n",
    "        Author(full_name=\"An Yan\"),\n",
    "        Author(full_name=\"Chenyu Yang\"),\n",
    "        Author(full_name=\"Eting Yuan\"),\n",
    "        Author(full_name=\"Hanwen Zha\"),\n",
    "        Author(full_name=\"Nan Tang\"),\n",
    "        Author(full_name=\"Lei Chen\"),\n",
    "        Author(full_name=\"Nicolas Scheffer\"),\n",
    "        Author(full_name=\"Yue Liu\"),\n",
    "        Author(full_name=\"Nirav Shah\"),\n",
    "        Author(full_name=\"Rakesh Wanga\"),\n",
    "        Author(full_name=\"Anuj Kumar\"),\n",
    "        Author(full_name=\"Wen-tau Yih\"),\n",
    "        Author(full_name=\"Xin Luna Dong\")\n",
    "    ],\n",
    "    summary=\"Retrieval-Augmented Generation (RAG) has recently emerged as a promising solution to alleviate Large Language Model (LLM)'s deficiency in lack of knowledge. Existing RAG datasets, however, do not adequately represent the diverse and dynamic nature of real-world Question Answering (QA) tasks. To bridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual question answering benchmark of 4,409 question-answer pairs and mock APIs to simulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a diverse array of questions across five domains and eight question categories, reflecting varied entity popularity from popular to long-tail, and temporal dynamisms ranging from years to seconds. Our evaluation on this benchmark highlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve <=34% accuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. State-of-the-art industry RAG solutions only answer 63% questions without any hallucination. CRAG also reveals much lower accuracy in answering questions regarding facts with higher dynamism, lower popularity, or higher complexity, suggesting future research directions. The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge, attracting thousands of participants and submissions within the first 50 days of the competition. We commit to maintaining CRAG to serve research communities in advancing RAG solutions and general QA solutions.\",\n",
    "    comment=\"\",\n",
    "    journal_ref=None,\n",
    "    doi=\"10.48550/arXiv.2406.04744\",\n",
    "    primary_category=\"cs.CL\",\n",
    "    categories=[\"cs.CL\"],\n",
    "    links=[\n",
    "        Link(href=\"https://arxiv.org/abs/2406.04744\", title=\"Abstract\", rel=\"alternate\", content_type=None),\n",
    "        Link(href=\"https://arxiv.org/pdf/2406.04744\", title=\"pdf\", rel=\"related\", content_type=None)\n",
    "    ],\n",
    "    pdf_url=\"https://arxiv.org/pdf/2406.04744\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://arxiv.org/pdf/2406.04744'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_paper.pdf_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(arxiv_result):\n",
    "    pdf_url = arxiv_result[\"pdf_url\"]\n",
    "    response = requests.get(pdf_url)\n",
    "    pdf_file = io.BytesIO(response.content)\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "    return pdf_reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Images to Text using Sonnet's vision capabilities\n",
    "\n",
    "In this section, we leverage Claude 3 Sonnet's advanced vision capabilities to convert images from ArXiv PDFs into detailed textual descriptions. This process is crucial for creating a comprehensive text-based representation of the entire paper, including figures and diagrams.\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **Vector Graphic Conversion**: \n",
    "   - The `convert_vector_graphic_page_to_image` function handles vector graphics in PDFs, converting them to PNG images for further processing.\n",
    "   - This step is essential for capturing complex diagrams and charts that are often present in scientific papers.\n",
    "   - If direct image extraction is not possible (e.g., for SVGs or other vector graphics), the function converts the entire page to an image.\n",
    "   - In such cases, the LLM is instructed to focus solely on describing the images on the page, ignoring any text content.\n",
    "\n",
    "2. **Image Processing**:\n",
    "   - Two main functions, `process_figure_image` and `process_vector_image_pdf`, utilize Claude 3 Sonnet to analyze and describe images.\n",
    "   - `process_figure_image` focuses on individual figures, providing detailed technical descriptions.\n",
    "   - `process_vector_image_pdf` handles full PDF pages that may contain multiple vector graphics.\n",
    "\n",
    "3. **Image Extraction and Description**:\n",
    "   - The `extract_images` function iterates through PDF pages, extracting both raster images and vector graphics.\n",
    "   - It calls the appropriate processing function for each image type, generating textual descriptions.\n",
    "\n",
    "4. **Text Integration**:\n",
    "   - `replace_images_with_descriptions` combines the extracted text from the PDF with the generated image descriptions.\n",
    "   - This creates a unified text document that includes both the original text and detailed descriptions of all visual elements.\n",
    "\n",
    "By converting images to text, we ensure that the chain of density summarization process can incorporate information from all aspects of the paper, including visual data. This comprehensive approach allows for more accurate and informative summaries, especially for papers with significant visual content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_vector_graphic_page_to_image(pdf_page, scale_factor=0.5):\n",
    "    def get_object(obj):\n",
    "        if isinstance(obj, PyPDF2.generic.IndirectObject):\n",
    "            return obj.get_object()\n",
    "        return obj\n",
    "\n",
    "    resources = get_object(pdf_page.get('/Resources', {}))\n",
    "    xobject = get_object(resources.get('/XObject', {}))\n",
    "    \n",
    "    # Check if there's a figure that's not an image\n",
    "    if xobject:\n",
    "        for obj in xobject.values():\n",
    "            obj = get_object(obj)\n",
    "            if isinstance(obj, dict) and obj.get('/Subtype') == '/Form':  # This indicates a vector graphic\n",
    "                # Convert the page to a PIL Image\n",
    "                pdf_bytes = io.BytesIO()\n",
    "                pdf_writer = PyPDF2.PdfWriter()\n",
    "                pdf_writer.add_page(pdf_page)\n",
    "                pdf_writer.write(pdf_bytes)\n",
    "                pdf_bytes.seek(0)\n",
    "                \n",
    "                # Convert PDF to image\n",
    "                images = convert_from_bytes(pdf_bytes.getvalue(), fmt='png')\n",
    "                \n",
    "                if images:\n",
    "                    image = images[0]\n",
    "                    # Resize the image\n",
    "                    new_size = (int(image.width * scale_factor), int(image.height * scale_factor))\n",
    "                    image = image.resize(new_size, Image.LANCZOS)\n",
    "                    img_byte_arr = io.BytesIO()\n",
    "                    image.save(img_byte_arr, format='PNG')\n",
    "                    img_byte_arr = img_byte_arr.getvalue()\n",
    "                    img_str = base64.b64encode(img_byte_arr).decode(\"utf-8\")\n",
    "                    data_url = f\"data:image/png;base64,{img_str}\"\n",
    "                    return data_url\n",
    "    \n",
    "    return None  # Return None if no conversion was needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def process_figure_image(data_url, model=\"claude-3-5-sonnet-20240620\"):\n",
    "    \"\"\"Process image data and return a detailed technical description.\"\"\"\n",
    "    img_str = data_url.split(\",\")[1]\n",
    "\n",
    "    response = anthropic_client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=4096,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"image/png\",\n",
    "                            \"data\": img_str,\n",
    "                        },\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"\"\"Analyze this image as if it's a figure from a scientific research paper. Provide a detailed technical description addressing the following:\n",
    "\n",
    "1. Type of figure (e.g., graph, diagram, flowchart, experimental setup)\n",
    "2. Key components or variables represented\n",
    "3. Relationships or trends depicted\n",
    "4. Quantitative information (if present)\n",
    "5. Methodology or process illustrated (if applicable)\n",
    "6. Potential implications or conclusions that can be drawn\n",
    "7. Any limitations or assumptions evident in the figure\n",
    "\n",
    "Focus on technical accuracy and relevance to scientific research. Avoid general descriptions and concentrate on the specific scientific content presented.\"\"\",\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def process_vector_image_pdf(data_url, model=\"claude-3-5-sonnet-20240620\"):\n",
    "    img_str = data_url.split(\",\")[1]\n",
    "\n",
    "    response = anthropic_client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=4096,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"image/png\",\n",
    "                            \"data\": img_str,\n",
    "                        },\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"\"\"This image is a full page from a scientific paper PDF, converted to PNG format. It may contain one or more vector graphic figures or charts. Your task is to:\n",
    "\n",
    "1. Identify and focus solely on the vector graphic figures or charts within the page.\n",
    "2. For each identified figure or chart, provide a detailed technical analysis addressing:\n",
    "\n",
    "   a. Type of figure (e.g., graph, diagram, flowchart)\n",
    "   b. Key components or variables represented\n",
    "   c. Relationships or trends depicted\n",
    "   d. Quantitative information (if present)\n",
    "   e. Methodology or process illustrated (if applicable)\n",
    "   f. Potential implications or conclusions that can be drawn\n",
    "\n",
    "3. Ignore any text or other elements on the page that are not part of the vector graphic figures.\n",
    "4. If multiple figures are present, analyze each separately and clearly indicate which figure you are describing.\n",
    "\n",
    "Focus on providing accurate, technical descriptions of the vector graphic content only.\"\"\",\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def extract_images(paper, model=\"claude-3-5-sonnet-20240620\"):\n",
    "    \"\"\"Extract text and images from PDF content.\"\"\"\n",
    "    pdf_reader = load_pdf(paper)\n",
    "    all_images = []\n",
    "\n",
    "    for page in pdf_reader.pages:\n",
    "        images = []\n",
    "\n",
    "        for image in page.images:\n",
    "            img_data = image.data\n",
    "            kind = filetype.guess(img_data)\n",
    "            if kind is None:\n",
    "                print(\"Cannot guess file type!\")\n",
    "                continue\n",
    "            \n",
    "            img_str = base64.b64encode(img_data).decode(\"utf-8\")\n",
    "            data_url = f\"data:{kind.mime};base64,{img_str}\"\n",
    "            try:\n",
    "                images.append(\n",
    "                    {\"image\": data_url, \"description\": process_figure_image(data_url, model=model)}\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image: {e}\")\n",
    "                images.append({\"image\": data_url, \"description\": \"\"})\n",
    "        \n",
    "        vector_graphics_image_data_url = convert_vector_graphic_page_to_image(page)\n",
    "        if vector_graphics_image_data_url:\n",
    "            images.append({\"image\": vector_graphics_image_data_url, \"description\": process_vector_image_pdf(vector_graphics_image_data_url, model=model)})\n",
    "        all_images.append(images)\n",
    "\n",
    "    return all_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def replace_images_with_descriptions(paper, images):\n",
    "    pdf_reader = load_pdf(paper)\n",
    "    text = \"\"\n",
    "    for page_num, page in enumerate(pdf_reader.pages):\n",
    "        text += page.extract_text() + \"\\n\\n\"\n",
    "        if images[page_num] and len(images[page_num]) > 0:\n",
    "            text += f\"\\n\\n[Image Descriptions for page {page_num+1}]\\n\"\n",
    "            for image_num, image in enumerate(images[page_num]):\n",
    "                text += f\"\\n[Image {image_num+1}]: {image['description']}\\n\"\n",
    "            text += \"[END OF IMAGE DESCRIPTIONS]\\n\"\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain of Density Summarization\n",
    "\n",
    "The Chain of Density (CoD) summarization technique is a powerful method for creating increasingly dense and informative summaries. In this section, we'll explore how to implement CoD for ArXiv PDF summarization, including specific preprocessing and postprocessing steps to evaluate the model's performance.\n",
    "\n",
    "Chain of Density is an iterative approach to summarization that progressively refines and condenses information. The process involves several key steps:\n",
    "\n",
    "1. **Initial Summarization**: Starting with the full document, the `summarize_current_summary` function creates an initial summary focused on a specific instruction.\n",
    "\n",
    "2. **Iterative Refinement**: The `iterative_density_summarization` function repeatedly calls `summarize_current_summary`, each time taking the previous summary as input. This process:\n",
    "   - Identifies new, important technical entities or ideas from the original text\n",
    "   - Incorporates these new elements into the summary\n",
    "   - Increases overall information density while maintaining focus on the instruction\n",
    "\n",
    "3. **Final Condensation**: After multiple iterations, the `final_summary` function creates an extremely dense summary, aiming to reduce length by 30-40% while retaining all critical technical content.\n",
    "\n",
    "The `chain_of_density_summarization` function orchestrates this entire process:\n",
    "\n",
    "```python\n",
    "@weave.op()\n",
    "def chain_of_density_summarization(document, instruction, current_summary=\"\", model=\"claude-3-5-sonnet-20240620\", density_iterations=2):\n",
    "    current_summary, iteration_summaries = iterative_density_summarization(document, instruction, current_summary, density_iterations, model)\n",
    "    final_summary_text = final_summary(instruction, current_summary, model)\n",
    "    print(f\"Final Summary:\\n{final_summary_text}\\n\")\n",
    "\n",
    "    return {\n",
    "        \"final_summary\": final_summary_text,\n",
    "        \"accumulated_summary\": current_summary,\n",
    "        \"iteration_summaries\": iteration_summaries,\n",
    "    }\n",
    "```\n",
    "\n",
    "This function takes the preprocessed document, a specific instruction to focus on, the model to use, and the number of density iterations. It returns a dictionary containing:\n",
    "\n",
    "- The final, highly condensed summary\n",
    "- The accumulated summary from all iterations\n",
    "- Individual summaries from each iteration\n",
    "\n",
    "By using this approach, Chain of Density creates summaries that are progressively more concise, technically precise, and information-dense, while remaining focused on the specific instruction provided. This makes it particularly well-suited for summarizing complex technical documents like ArXiv papers, where maintaining accuracy and depth of information is crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def summarize_current_summary(document, instruction, current_summary=\"\", iteration=1, model=\"claude-3-5-sonnet-20240620\"):\n",
    "    max_tokens = 4096  # Adjust this value based on the model's context window\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Document:\n",
    "    {document}\n",
    "    \n",
    "    Current summary:\n",
    "    {current_summary}\n",
    "\n",
    "    Instruction to focus on: {instruction}\n",
    "\n",
    "    Iteration: {iteration}\n",
    "\n",
    "    Generate an increasingly concise, entity-dense, and highly technical summary from the provided document that specifically addresses the given instruction using the below approach:\n",
    "\n",
    "    1. Carefully read the current summary and the instruction.\n",
    "\n",
    "    2. Identify 1-3 new, important technical entities or ideas from the original text that:\n",
    "       - Are directly relevant to the instruction\n",
    "       - Are not yet present in the current summary\n",
    "       - Add significant, specific information to the summary\n",
    "       - Are preferably 5 words or fewer\n",
    "       - May include methodologies, algorithms, metrics, or key findings\n",
    "       - Ensure to include this in the output before the summary\n",
    "\n",
    "    3. Write a new summary that:\n",
    "       - Incorporates the newly identified entities/ideas\n",
    "       - Retains all crucial information from the current summary\n",
    "       - Increases overall information density\n",
    "       - Remains focused on addressing the instruction\n",
    "       - Utilizes the response window of {max_tokens} tokens\n",
    "\n",
    "    Guidelines:\n",
    "    - Prioritize technical accuracy and specificity over general readability\n",
    "    - Use precise terminology, domain-specific jargon, and include quantitative details where relevant\n",
    "    - Ensure all information is directly related to the instruction\n",
    "    - Make every word count: rewrite to improve density and make space for new technical entities\n",
    "    - Employ fusion, compression, and removal of less informative phrases to increase density\n",
    "    - Never drop entities or technical details from the current summary that are relevant to the instruction\n",
    "    - Maintain coherence while maximizing information density\n",
    "\n",
    "    Your goal is to create a summary that is noticeably denser, more technical, and more informative than the previous one, utilizing the response window of {max_tokens} tokens while staying laser-focused on the instruction. The summary should be suitable for an expert audience in the field.\"\"\"\n",
    "\n",
    "    response = anthropic_client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=max_tokens,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def iterative_density_summarization(document, instruction, current_summary, density_iterations, model):\n",
    "    iteration_summaries = []\n",
    "    for iteration in range(1, density_iterations + 1):\n",
    "        current_summary = summarize_current_summary(document, instruction, current_summary, iteration, model)\n",
    "        iteration_summaries.append(current_summary)\n",
    "        print(f\"Iteration {iteration}:\\n{current_summary}\\n\")\n",
    "    return current_summary, iteration_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def final_summary(instruction, current_summary, model):\n",
    "    return anthropic_client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=4096,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"Given this summary:\n",
    "\n",
    "{current_summary}\n",
    "\n",
    "And this instruction to focus on:\n",
    "\n",
    "{instruction}\n",
    "\n",
    "Create an extremely dense, final summary that captures all key technical information in the most concise form possible, while specifically addressing the given instruction. Follow these guidelines:\n",
    "\n",
    "1. Aim to reduce length by 30-40% while retaining all critical technical content relevant to the instruction.\n",
    "2. Prioritize highly specific methodologies, algorithms, metrics, and findings that directly address the instruction.\n",
    "3. Preserve precise quantitative data, including statistical significance and error margins where applicable and relevant to the instruction.\n",
    "4. Maintain the use of domain-specific terminology and technical jargon pertinent to the instruction.\n",
    "5. Ensure that all key entities and concepts from the original summary that relate to the instruction are represented.\n",
    "6. Use compact phrasing and remove any remaining non-essential information that doesn't directly contribute to addressing the instruction.\n",
    "7. If relevant to the instruction, include brief mentions of limitations, assumptions, or conflicting viewpoints.\n",
    "8. Optimize for information density while maintaining coherence for an expert audience, always keeping the focus on the given instruction.\n",
    "\n",
    "The final summary should be a highly concentrated, technical distillation of the research that specifically addresses the given instruction, suitable for specialists in the field.\"\"\",\n",
    "            }\n",
    "        ],\n",
    "    ).content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def chain_of_density_summarization(document, instruction, current_summary=\"\", model=\"claude-3-5-sonnet-20240620\", density_iterations=2):\n",
    "    current_summary, iteration_summaries = iterative_density_summarization(document, instruction, current_summary, density_iterations, model)\n",
    "    final_summary_text = final_summary(instruction, current_summary, model)\n",
    "    print(f\"Final Summary:\\n{final_summary_text}\\n\")\n",
    "\n",
    "    return {\n",
    "        \"final_summary\": final_summary_text,\n",
    "        \"accumulated_summary\": current_summary,\n",
    "        \"iteration_summaries\": iteration_summaries,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Weave Model Object to better serialize the model for experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Weave Model Object to better serialize the model for experimentation\n",
    "\n",
    "This section defines an `ArxivChainOfDensityPipeline` class that encapsulates our summarization pipeline as a `weave.Model`. Key features:\n",
    "\n",
    "- Configurable parameters: `model` and `density_iterations`\n",
    "- `predict` method: Processes an `ArxivPaper` object and instruction through the entire pipeline\n",
    "\n",
    "The class structure enables easy serialization, parameter adjustment, and reproducibility of experiments. Usage example is provided for instantiation and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArxivChainOfDensityPipeline(weave.Model):\n",
    "\n",
    "    model: str = \"claude-3-5-sonnet-20240620\"\n",
    "    density_iterations: int = 3\n",
    "\n",
    "    def __init__(self, model: str = \"claude-3-5-sonnet-20240620\", density_iterations: int = 3):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.density_iterations = density_iterations\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, paper: ArxivPaper, instruction: str) -> dict:\n",
    "        extracted_images = extract_images(paper)\n",
    "        cleaned_text = replace_images_with_descriptions(paper, extracted_images)\n",
    "        result = chain_of_density_summarization(cleaned_text, instruction, model=self.model, density_iterations=self.density_iterations)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_chain_of_density_pipeline = ArxivChainOfDensityPipeline()\n",
    "# arxiv_chain_of_density_pipeline.predict(arxiv_paper, \"Determine how I would best incorporate these benchmarks for my customer support RAG system. What evaluations would work best specifically for me?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our Evaluation Dataset\n",
    "\n",
    "In this section, we prepare a dataset for evaluating our Chain of Density (CoD) summarization pipeline on ArXiv papers. This dataset will allow us to assess the performance of our model across different papers and instructions.\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **Sample ArXiv Papers**: We create `ArxivPaper` objects for three different papers:\n",
    "   - `arxiv_paper1`: \"Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?\"\n",
    "   - `arxiv_paper2`: \"Many-Shot In-Context Learning\"\n",
    "   - `arxiv_paper3`: \"LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks\"\n",
    "\n",
    "   Each `ArxivPaper` object contains metadata such as title, authors, summary, and PDF URL.\n",
    "\n",
    "2. **Evaluation Instructions**: We define a list of instructions that will guide the summarization process:\n",
    "   ```python\n",
    "   eval_instructions = [\n",
    "       \"Summarize the key methodologies and novel contributions of this research, focusing on their potential impact in the field.\",\n",
    "       \"Analyze the experimental setup, results, and limitations of this study, highlighting any statistical significance and error margins.\",\n",
    "       \"Compare this paper's approach to existing methods in the field, explaining how it addresses current challenges or limitations.\"\n",
    "   ]\n",
    "   ```\n",
    "\n",
    "3. **Creating Evaluation Data**: We use `itertools.product()` to create combinations of papers and instructions:\n",
    "   ```python\n",
    "   eval_data = list(product(eval_papers, eval_instructions))\n",
    "   ```\n",
    "\n",
    "4. **Weave Dataset**: Finally, we create a Weave Dataset object that combines the paper, instruction, and original summary for each evaluation item:\n",
    "   ```python\n",
    "   dataset = weave.Dataset(name=\"we-paper-reading-eval-data\", \n",
    "                           rows=[{\"paper\": arxiv_paper, \n",
    "                                  \"instruction\": instruction, \n",
    "                                  \"summary\": arxiv_paper.summary} \n",
    "                                 for arxiv_paper, instruction in eval_data])\n",
    "   ```\n",
    "\n",
    "5. **Publishing the Dataset**: We publish the dataset to make it available for evaluation:\n",
    "   ```python\n",
    "   weave.publish(dataset)\n",
    "   ```\n",
    "\n",
    "This evaluation dataset provides a structured way to assess our CoD summarization pipeline across different papers and instructions, allowing for comprehensive testing of the model's performance and adaptability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper1 = ArxivPaper(\n",
    "    entry_id=\"http://arxiv.org/abs/2405.05904\",\n",
    "    updated=datetime(2024, 5, 13, 7, 29, 58, tzinfo=timezone.utc),\n",
    "    published=datetime(2024, 5, 9, 17, 0, 22, tzinfo=timezone.utc),\n",
    "    title=\"Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?\",\n",
    "    authors=[\n",
    "        Author(full_name=\"Zorik Gekhman\"),\n",
    "        Author(full_name=\"Gal Yona\"),\n",
    "        Author(full_name=\"Roee Aharoni\"),\n",
    "        Author(full_name=\"Matan Eyal\"),\n",
    "        Author(full_name=\"Amir Feder\"),\n",
    "        Author(full_name=\"Roi Reichart\"),\n",
    "        Author(full_name=\"Jonathan Herzig\")\n",
    "    ],\n",
    "    summary=(\"When large language models are aligned via supervised fine-tuning, they may encounter new factual information \"\n",
    "             \"that was not acquired through pre-training. It is often conjectured that this can teach the model the behavior \"\n",
    "             \"of hallucinating factually incorrect responses, as the model is trained to generate facts that are not grounded \"\n",
    "             \"in its pre-existing knowledge. In this work, we study the impact of such exposure to new knowledge on the capability \"\n",
    "             \"of the fine-tuned model to utilize its pre-existing knowledge. To this end, we design a controlled setup, focused on \"\n",
    "             \"closed-book QA, where we vary the proportion of the fine-tuning examples that introduce new knowledge. We demonstrate \"\n",
    "             \"that large language models struggle to acquire new factual knowledge through fine-tuning, as fine-tuning examples that \"\n",
    "             \"introduce new knowledge are learned significantly slower than those consistent with the model's knowledge. However, we \"\n",
    "             \"also find that as the examples with new knowledge are eventually learned, they linearly increase the model's tendency \"\n",
    "             \"to hallucinate. Taken together, our results highlight the risk in introducing new factual knowledge through fine-tuning, \"\n",
    "             \"and support the view that large language models mostly acquire factual knowledge through pre-training, whereas fine-tuning \"\n",
    "             \"teaches them to use it more efficiently.\"),\n",
    "    comment=None,\n",
    "    journal_ref=None,\n",
    "    doi=\"10.48550/arXiv.2405.05904\",\n",
    "    primary_category=\"cs.CL\",\n",
    "    categories=[\"cs.CL\"],\n",
    "    links=[\n",
    "        Link(href=\"https://arxiv.org/abs/2405.05904\", title=\"Abstract\", rel=\"alternate\"),\n",
    "        Link(href=\"https://arxiv.org/pdf/2405.05904\", title=\"pdf\", rel=\"related\")\n",
    "    ],\n",
    "    pdf_url=\"https://arxiv.org/pdf/2405.05904\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper2 = ArxivPaper(\n",
    "    entry_id=\"http://arxiv.org/abs/2404.11018\",\n",
    "    updated=datetime(2024, 5, 22, 17, 6, 10, tzinfo=timezone.utc),\n",
    "    published=datetime(2024, 4, 17, 2, 49, 26, tzinfo=timezone.utc),\n",
    "    title=\"Many-Shot In-Context Learning\",\n",
    "    authors=[\n",
    "        Author(full_name=\"Rishabh Agarwal\"),\n",
    "        Author(full_name=\"Avi Singh\"),\n",
    "        Author(full_name=\"Lei M. Zhang\"),\n",
    "        Author(full_name=\"Bernd Bohnet\"),\n",
    "        Author(full_name=\"Luis Rosias\"),\n",
    "        Author(full_name=\"Stephanie Chan\"),\n",
    "        Author(full_name=\"Biao Zhang\"),\n",
    "        Author(full_name=\"Ankesh Anand\"),\n",
    "        Author(full_name=\"Zaheer Abbas\"),\n",
    "        Author(full_name=\"Azade Nova\"),\n",
    "        Author(full_name=\"John D. Co-Reyes\"),\n",
    "        Author(full_name=\"Eric Chu\"),\n",
    "        Author(full_name=\"Feryal Behbahani\"),\n",
    "        Author(full_name=\"Aleksandra Faust\"),\n",
    "        Author(full_name=\"Hugo Larochelle\")\n",
    "    ],\n",
    "    summary=(\"Large language models (LLMs) excel at few-shot in-context learning (ICL) -- learning from a few examples provided in context at inference, \"\n",
    "             \"without any weight updates. Newly expanded context windows allow us to investigate ICL with hundreds or thousands of examples -- the many-shot regime. \"\n",
    "             \"Going from few-shot to many-shot, we observe significant performance gains across a wide variety of generative and discriminative tasks. While promising, \"\n",
    "             \"many-shot ICL can be bottlenecked by the available amount of human-generated examples. To mitigate this limitation, we explore two new settings: Reinforced \"\n",
    "             \"and Unsupervised ICL. Reinforced ICL uses model-generated chain-of-thought rationales in place of human examples. Unsupervised ICL removes rationales from the \"\n",
    "             \"prompt altogether, and prompts the model only with domain-specific questions. We find that both Reinforced and Unsupervised ICL can be quite effective in the \"\n",
    "             \"many-shot regime, particularly on complex reasoning tasks. Finally, we demonstrate that, unlike few-shot learning, many-shot learning is effective at overriding \"\n",
    "             \"pretraining biases, can learn high-dimensional functions with numerical inputs, and performs comparably to fine-tuning. Our analysis also reveals the limitations \"\n",
    "             \"of next-token prediction loss as an indicator of downstream ICL performance.\"),\n",
    "    comment=None,\n",
    "    journal_ref=None,\n",
    "    doi=\"10.48550/arXiv.2404.11018\",\n",
    "    primary_category=\"cs.LG\",\n",
    "    categories=[\"cs.LG\", \"cs.AI\", \"cs.CL\"],\n",
    "    links=[\n",
    "        Link(href=\"https://arxiv.org/abs/2404.11018\", title=\"Abstract\", rel=\"alternate\"),\n",
    "        Link(href=\"https://arxiv.org/pdf/2404.11018\", title=\"pdf\", rel=\"related\")\n",
    "    ],\n",
    "    pdf_url=\"https://arxiv.org/pdf/2404.11018\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper3 = ArxivPaper(\n",
    "    entry_id=\"http://arxiv.org/abs/2406.18403\",\n",
    "    updated=datetime(2024, 6, 26, 14, 56, 13, tzinfo=timezone.utc),\n",
    "    published=datetime(2024, 6, 26, 14, 56, 13, tzinfo=timezone.utc),\n",
    "    title=\"LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks\",\n",
    "    authors=[\n",
    "        Author(full_name=\"Anna Bavaresco\"),\n",
    "        Author(full_name=\"Raffaella Bernardi\"),\n",
    "        Author(full_name=\"Leonardo Bertolazzi\"),\n",
    "        Author(full_name=\"Desmond Elliott\"),\n",
    "        Author(full_name=\"Raquel Fernndez\"),\n",
    "        Author(full_name=\"Albert Gatt\"),\n",
    "        Author(full_name=\"Esam Ghaleb\"),\n",
    "        Author(full_name=\"Mario Giulianelli\"),\n",
    "        Author(full_name=\"Michael Hanna\"),\n",
    "        Author(full_name=\"Alexander Koller\"),\n",
    "        Author(full_name=\"Andr F. T. Martins\"),\n",
    "        Author(full_name=\"Philipp Mondorf\"),\n",
    "        Author(full_name=\"Vera Neplenbroek\"),\n",
    "        Author(full_name=\"Sandro Pezzelle\"),\n",
    "        Author(full_name=\"Barbara Plank\"),\n",
    "        Author(full_name=\"David Schlangen\"),\n",
    "        Author(full_name=\"Alessandro Suglia\"),\n",
    "        Author(full_name=\"Aditya K Surikuchi\"),\n",
    "        Author(full_name=\"Ece Takmaz\"),\n",
    "        Author(full_name=\"Alberto Testoni\")\n",
    "    ],\n",
    "    summary=(\"There is an increasing trend towards evaluating NLP models with LLM-generated judgments instead of human judgments. \"\n",
    "             \"In the absence of a comparison against human data, this raises concerns about the validity of these evaluations; in case they are conducted with proprietary models, \"\n",
    "             \"this also raises concerns over reproducibility. We provide JUDGE-BENCH, a collection of 20 NLP datasets with human annotations, and comprehensively evaluate 11 current LLMs, \"\n",
    "             \"covering both open-weight and proprietary models, for their ability to replicate the annotations. Our evaluations show that each LLM exhibits a large variance across datasets in its correlation to human judgments. \"\n",
    "             \"We conclude that LLMs are not yet ready to systematically replace human judges in NLP.\"),\n",
    "    comment=None,\n",
    "    journal_ref=None,\n",
    "    doi=\"10.48550/arXiv.2406.18403\",\n",
    "    primary_category=\"cs.CL\",\n",
    "    categories=[\"cs.CL\"],\n",
    "    links=[\n",
    "        Link(href=\"https://arxiv.org/abs/2406.18403\", title=\"Abstract\", rel=\"alternate\"),\n",
    "        Link(href=\"https://arxiv.org/pdf/2406.18403\", title=\"pdf\", rel=\"related\")\n",
    "    ],\n",
    "    pdf_url=\"https://arxiv.org/pdf/2406.18403\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://arxiv.org/pdf/2406.18403'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_paper3.pdf_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_papers = [\n",
    "    # arxiv_paper1,\n",
    "    # arxiv_paper2,\n",
    "    arxiv_paper3\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_instructions = [\n",
    "    \"Summarize the key methodologies and novel contributions of this research, focusing on their potential impact in the field.\",\n",
    "    # \"Analyze the experimental setup, results, and limitations of this study, highlighting any statistical significance and error margins.\",\n",
    "    # \"Compare this paper's approach to existing methods in the field, explaining how it addresses current challenges or limitations.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "eval_data = list(product(eval_papers, eval_instructions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = weave.Dataset(name=\"we-paper-reading-eval-data\", rows=[{\"paper\": arxiv_paper, \"instruction\": instruction, \"summary\": arxiv_paper.summary} for arxiv_paper, instruction in eval_data])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Published to https://wandb.ai/a-sh0ts/arxiv-papers-anthropic-testv7-4/weave/objects/we-paper-reading-eval-data/versions/0FtjvygYZHJQ38c5NLYP1fmFKA2u7s8YOGXcmewR1hE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectRef(entity='a-sh0ts', project='arxiv-papers-anthropic-testv7-4', name='we-paper-reading-eval-data', digest='0FtjvygYZHJQ38c5NLYP1fmFKA2u7s8YOGXcmewR1hE', extra=())"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weave.publish(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our metrics\n",
    "\n",
    "In this section, we establish a set of metrics to evaluate the quality and effectiveness of our Chain of Density (CoD) summarization pipeline for ArXiv PDFs. These metrics are designed to provide a comprehensive assessment of the summarization process, focusing on relevance, technical quality, and conciseness.\n",
    "\n",
    "### Key Metrics:\n",
    "\n",
    "1. **Summary Scoring (`score_summary`)**: \n",
    "   - Evaluates individual summaries based on three criteria:\n",
    "     - Relevance (0-5): How well the summary addresses the given instruction\n",
    "     - Technical Quality (0-5): Accuracy and depth of technical content\n",
    "     - Conciseness (0-5): Information density and brevity\n",
    "   - Uses GPT-4 to perform the evaluation, ensuring a nuanced assessment\n",
    "\n",
    "2. **Long-tail Statistics (`calculate_long_tail_stats`)**:\n",
    "   - Analyzes the distribution of scores across multiple summaries\n",
    "   - Calculates mean scores and tail ratios for each aspect (relevance, technical quality, conciseness)\n",
    "   - Helps identify overall performance and potential outliers\n",
    "\n",
    "3. **Iteration Impact Analysis (`analyze_iteration_impact`)**:\n",
    "   - Assesses the improvement of summaries across iterations\n",
    "   - Identifies the point of diminishing returns and cumulative improvement\n",
    "   - Useful for optimizing the number of iterations in the CoD process\n",
    "\n",
    "4. **Optimal Improvement Range (`find_optimal_improvement_range`)**:\n",
    "   - Determines the most effective range of iterations for improvement\n",
    "   - Considers moving averages of improvements to find sustained progress\n",
    "\n",
    "5. **Optimal Score Range (`find_optimal_score_range`)**:\n",
    "   - Identifies the iteration range that produces the highest quality summaries\n",
    "   - Helps in fine-tuning the CoD process for maximum effectiveness\n",
    "\n",
    "6. **Iteration Summary Processing (`process_iteration_summaries`)**:\n",
    "   - Aggregates and analyzes scores across all iterations\n",
    "   - Provides a holistic view of the summarization process's progression\n",
    "\n",
    "7. **Quality Scorer (`quality_scorer`)**:\n",
    "   - Combines all the above metrics into a comprehensive evaluation\n",
    "   - Analyzes iteration summaries, accumulated summary, and final summary\n",
    "   - Produces a flattened, easy-to-analyze score dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def score_summary(summary, summary_type, instruction, model):\n",
    "    openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    prompt = f\"\"\"Evaluate the quality of the following {summary_type} based on how well it addresses the given instruction. Use the scoring rules below to calculate three numerical scores between 0 and 10.\n",
    "\n",
    "Instruction: {instruction}\n",
    "\n",
    "{summary_type}:\n",
    "{summary}\n",
    "Scoring Rules:\n",
    "1. Relevance (0-5):\n",
    "   - 5: Perfectly addresses all aspects of the instruction, focusing on key methodologies and novel contributions\n",
    "     Example: \"The paper introduces JUDGE-BENCH, a comprehensive evaluation framework comprising 20 NLP datasets with human annotations, designed to assess LLMs' capacity to replicate human judgments across diverse NLP tasks. The study employs a rigorous comparative analysis of 11 state-of-the-art LLMs, including both open-weight and proprietary models, utilizing correlation metrics to quantify alignment with human annotations.\"\n",
    "   - 4: Addresses most aspects of the instruction with minor omissions\n",
    "     Example: \"The research presents JUDGE-BENCH, a novel evaluation framework for LLMs consisting of 20 NLP datasets. It conducts a thorough assessment of 11 LLMs, analyzing their ability to replicate human judgments. The methodology involves correlation analysis between LLM outputs and human annotations.\"\n",
    "   - 3: Addresses the main points of the instruction but misses some details about methodologies or contributions\n",
    "     Example: \"The study proposes JUDGE-BENCH, a new benchmark for evaluating LLMs against human judgments in NLP tasks. It assesses multiple LLMs and finds significant variability in their performance across different datasets.\"\n",
    "   - 2: Partially addresses the instruction, missing significant aspects of methodologies or contributions\n",
    "     Example: \"The paper discusses a new method for evaluating language models using human-annotated datasets. It compares several LLMs and concludes that they are not yet ready to replace human judges in NLP tasks.\"\n",
    "   - 1: Barely addresses the instruction, focusing on tangential information\n",
    "     Example: \"The research explores various natural language processing tasks and the performance of language models. It suggests that human evaluation is still important in NLP.\"\n",
    "   - 0: Completely irrelevant to the instruction\n",
    "     Example: \"The paper discusses advancements in computer vision algorithms for image recognition using convolutional neural networks.\"\n",
    "\n",
    "2. Technical Quality (0-5):\n",
    "   - 5: Exceptionally accurate, detailed, and technically sound, with precise descriptions of methodologies and contributions\n",
    "     Example: \"JUDGE-BENCH employs a multi-faceted evaluation protocol, utilizing Pearson correlation coefficients (r) to quantify LLM-human judgment alignment across 20 diverse NLP tasks. The framework incorporates both discriminative and generative tasks, with a particular focus on nuanced linguistic phenomena such as pragmatic inference and discourse coherence. The study reports a mean correlation of r = 0.47 ( = 0.18) across all models and tasks, with significant inter-task variability (range: 0.12  r  0.83). Notably, the best-performing LLM (GPT-4) achieved a maximum mean correlation of r = 0.62, still substantially below perfect alignment (r = 1.0), underscoring the persistent gap between LLM and human judgment capabilities.\"\n",
    "   - 4: Highly accurate with comprehensive technical details about research methods and findings\n",
    "     Example: \"The JUDGE-BENCH framework evaluates 11 LLMs across 20 NLP datasets using Pearson correlation to measure alignment with human judgments. The study reports a mean correlation of 0.47 across all models and tasks, with significant variability ( = 0.18). The best-performing model (GPT-4) achieved a maximum mean correlation of 0.62, indicating a substantial gap between LLM and human judgment capabilities.\"\n",
    "   - 3: Generally accurate with good technical depth, but may lack some specifics\n",
    "     Example: \"JUDGE-BENCH evaluates LLMs using correlation analysis with human judgments across multiple NLP tasks. The study finds variable performance across models and tasks, with the best model achieving a mean correlation of 0.62. This suggests LLMs are not yet capable of consistently replicating human judgments in NLP tasks.\"\n",
    "   - 2: Mostly accurate but lacks important technical details about methodologies or contributions\n",
    "     Example: \"The study uses a new benchmark called JUDGE-BENCH to evaluate language models. It compares LLM outputs to human judgments using correlation analysis and finds that even the best models don't consistently match human performance across different NLP tasks.\"\n",
    "   - 1: Contains technical inaccuracies or lacks significant depth in describing research approaches\n",
    "     Example: \"The paper discusses a method for evaluating AI language models using human-annotated datasets. It shows that AI models don't always agree with human judgments, suggesting they need improvement.\"\n",
    "   - 0: Technically unsound or extremely superficial in describing methodologies and contributions\n",
    "     Example: \"The research uses AI to compare computer-generated text to human writing. It finds that AI is not as good as humans at understanding language.\"\n",
    "\n",
    "3. Conciseness (0-5):\n",
    "   - 5: Maximally information-dense without any unnecessary content, perfectly balancing detail and brevity\n",
    "     Example: \"JUDGE-BENCH: 20-dataset NLP evaluation framework. 11 LLMs assessed. Mean correlation with human judgments: r = 0.47 ( = 0.18). Best model (GPT-4): r = 0.62. Significant inter-task variability: 0.12  r  0.83. Conclusion: LLMs not ready to replace human judges in NLP.\"\n",
    "   - 4: Highly concise with minimal extraneous information, efficiently describing methodologies and contributions\n",
    "     Example: \"JUDGE-BENCH: 20 NLP datasets for LLM evaluation. 11 models tested. Mean human-LLM judgment correlation: 0.47. Best model: 0.62. High variability across tasks. LLMs currently inadequate for replacing human NLP judges.\"\n",
    "   - 3: Generally concise but could be slightly more compact in describing research approaches\n",
    "     Example: \"JUDGE-BENCH evaluates 11 LLMs on 20 NLP datasets. Uses correlation with human judgments. Finds variable performance across tasks. Best model achieves 0.62 correlation. Concludes LLMs can't reliably replace human judges in NLP yet.\"\n",
    "   - 2: Contains some unnecessary information or repetition, diluting the focus on key methodologies and contributions\n",
    "     Example: \"The paper introduces JUDGE-BENCH, a new way to evaluate language models. It looks at how well 11 different AI models can match human judgments on 20 NLP tasks. The researchers found that even the best AI model wasn't consistently as good as humans at judging language tasks. They conclude that AI models aren't ready to replace human judges in NLP research yet.\"\n",
    "   - 1: Verbose with significant redundancy, obscuring the main research points\n",
    "     Example: \"In this study, the researchers created something called JUDGE-BENCH. It's a way to test how good AI language models are at understanding and judging language like humans do. They tested 11 different AI models on 20 different types of language tasks. They found out that the AI models weren't as consistent as humans in judging these tasks. Even the best AI model wasn't always as good as humans. So, they say that right now, we can't use AI to replace humans when we need to judge language in research.\"\n",
    "   - 0: Extremely verbose or filled with irrelevant information unrelated to methodologies and contributions\n",
    "     Example: \"The researchers in this study were interested in natural language processing, which is a field of artificial intelligence that deals with how computers understand and generate human language. They created a new tool called JUDGE-BENCH to test AI models. They used many different language tasks and compared how the AI did compared to humans. It's important to test AI models because we want to know if they can understand language as well as humans can. This kind of research helps us improve AI technology.\"\n",
    "\n",
    "     Examples:\n",
    "\n",
    "1. High-quality summary (Instruction: \"Summarize the key methodologies and novel contributions of this research, focusing on their potential impact in the field.\"):\n",
    "{{\n",
    "    \"relevance\": {{\n",
    "        \"score\": 4.75\n",
    "    }},\n",
    "    \"technical_quality\": {{\n",
    "        \"score\": 4.5\n",
    "    }},\n",
    "    \"conciseness\": {{\n",
    "        \"score\": 4.25\n",
    "    }}\n",
    "}}\n",
    "\n",
    "2. Average-quality summary (Instruction: \"Analyze the experimental setup, results, and limitations of this study.\"):\n",
    "{{\n",
    "    \"relevance\": {{\n",
    "        \"score\": 3.0\n",
    "    }},\n",
    "    \"technical_quality\": {{\n",
    "        \"score\": 2.75\n",
    "    }},\n",
    "    \"conciseness\": {{\n",
    "        \"score\": 3.5\n",
    "    }}\n",
    "}}\n",
    "\n",
    "3. Low-quality summary (Instruction: \"Explain how this paper's approach compares to existing methods in the field.\"):\n",
    "{{\n",
    "    \"relevance\": {{\n",
    "        \"score\": 1.5\n",
    "    }},\n",
    "    \"technical_quality\": {{\n",
    "        \"score\": 1.25\n",
    "    }},\n",
    "    \"conciseness\": {{\n",
    "        \"score\": 2.0\n",
    "    }}\n",
    "}}\n",
    "\n",
    "Provide your evaluation in the following JSON format:\n",
    "{{\n",
    "    \"relevance\": {{\n",
    "        \"score\": <float>\n",
    "    }},\n",
    "    \"technical_quality\": {{\n",
    "        \"score\": <float>\n",
    "    }},\n",
    "    \"conciseness\": {{\n",
    "        \"score\": <float>\n",
    "    }}\n",
    "}}\n",
    "\n",
    "Ensure your response is ONLY valid JSON. Do not include any other text outside the JSON object.\n",
    "Ensure you have the keys: relevance, technical_quality, conciseness, each containing only a score.\n",
    "Ensure each score is a float between 0 and 10, using the scoring rules provided above.\n",
    "\"\"\"\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    return json.loads(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def calculate_long_tail_stats(scores):\n",
    "    if not scores:\n",
    "        return None\n",
    "    aspects = ['relevance', 'technical_quality', 'conciseness']\n",
    "    stats = {}\n",
    "    for aspect in aspects:\n",
    "        try:\n",
    "            if isinstance(scores[0], list):\n",
    "                flattened_scores = [score[aspect]['score'] for sublist in scores for score in sublist]\n",
    "            elif isinstance(scores[0], dict):\n",
    "                flattened_scores = [score[aspect]['score'] for score in scores]\n",
    "            else:\n",
    "                print(f\"Unexpected format for scores: {scores}\")\n",
    "                return None\n",
    "            \n",
    "            stats[aspect] = {\n",
    "                \"mean\": np.mean(flattened_scores),\n",
    "                # \"median\": np.median(flattened_scores),\n",
    "                # \"top_5_percent\": np.mean(sorted(flattened_scores)[-max(1, int(len(flattened_scores)*0.05)):]),\n",
    "                # \"bottom_5_percent\": np.mean(sorted(flattened_scores)[:max(1, int(len(flattened_scores)*0.05))]),\n",
    "                # \"top_1_percent\": np.mean(sorted(flattened_scores)[-max(1, int(len(flattened_scores)*0.01)):]),\n",
    "                # \"interquartile_range\": np.percentile(flattened_scores, 75) - np.percentile(flattened_scores, 25),\n",
    "                \"tail_ratio\": np.mean(sorted(flattened_scores)[-max(1, int(len(flattened_scores)*0.05)):]) / np.mean(flattened_scores),\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating stats for {aspect}: {str(e)}\")\n",
    "            stats[aspect] = None\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def analyze_iteration_impact(scores):\n",
    "    if len(scores) < 2:\n",
    "        return {aspect: {\"mean_improvement\": 0, \"diminishing_returns_point\": 0, \"cumulative_improvement\": 0, \"improvement_variability\": 0} for aspect in ['relevance', 'technical_quality', 'conciseness']}\n",
    "    \n",
    "    aspects = ['relevance', 'technical_quality', 'conciseness']\n",
    "    results = {}\n",
    "    \n",
    "    for aspect in aspects:\n",
    "        aspect_scores = [s[aspect]['score'] for s in scores]\n",
    "        improvements = [aspect_scores[i+1] - aspect_scores[i] for i in range(len(aspect_scores)-1)]\n",
    "        \n",
    "        results[aspect] = {\n",
    "            # \"mean_improvement\": np.mean(improvements),\n",
    "            \"diminishing_returns_point\": next((i for i, imp in enumerate(improvements) if imp <= 0), len(improvements)),\n",
    "            \"cumulative_improvement\": sum(improvements),\n",
    "            # \"improvement_variability\": np.std(improvements) / np.mean(improvements) if np.mean(improvements) != 0 else 0\n",
    "        }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def find_optimal_improvement_range(scores):\n",
    "    if len(scores) < 3:\n",
    "        return {aspect: {\"optimal_range_start\": 0, \"optimal_range_end\": 0, \"score_at_start\": 0, \"score_at_end\": 0, \"improvement_in_range\": 0} for aspect in ['relevance', 'technical_quality', 'conciseness']}\n",
    "    \n",
    "    aspects = ['relevance', 'technical_quality', 'conciseness']\n",
    "    results = {}\n",
    "    \n",
    "    for aspect in aspects:\n",
    "        aspect_scores = [s[aspect]['score'] for s in scores]\n",
    "        improvements = [aspect_scores[i+1] - aspect_scores[i] for i in range(len(aspect_scores)-1)]\n",
    "        \n",
    "        window_size = min(3, len(aspect_scores) - 1)\n",
    "        moving_avg = np.convolve(improvements, np.ones(window_size), 'valid') / window_size\n",
    "        \n",
    "        threshold = 0.1 * np.mean(improvements)\n",
    "        above_threshold = [i for i, avg in enumerate(moving_avg) if avg >= threshold]\n",
    "        \n",
    "        if not above_threshold:\n",
    "            optimal_start, optimal_end = 0, 0\n",
    "        else:\n",
    "            optimal_start = above_threshold[0]\n",
    "            optimal_end = above_threshold[-1] + 1\n",
    "        \n",
    "        results[aspect] = {\n",
    "            \"optimal_range_start\": optimal_start,\n",
    "            \"optimal_range_end\": optimal_end,\n",
    "            \"score_at_start\": aspect_scores[optimal_start],\n",
    "            \"score_at_end\": aspect_scores[optimal_end] if optimal_end < len(aspect_scores) else aspect_scores[-1],\n",
    "            \"improvement_in_range\": sum(improvements[optimal_start:optimal_end])\n",
    "        }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def find_optimal_score_range(scores):\n",
    "    if len(scores) < 2:\n",
    "        return {aspect: {\"optimal_range_start\": 0, \"optimal_range_end\": 0, \"highest_score\": 0, \"improvement_in_range\": 0} for aspect in ['relevance', 'technical_quality', 'conciseness']}\n",
    "    \n",
    "    aspects = ['relevance', 'technical_quality', 'conciseness']\n",
    "    results = {}\n",
    "    \n",
    "    for aspect in aspects:\n",
    "        aspect_scores = [s[aspect]['score'] for s in scores]\n",
    "        improvements = [aspect_scores[i+1] - aspect_scores[i] for i in range(len(aspect_scores)-1)]\n",
    "        \n",
    "        highest_score = max(aspect_scores)\n",
    "        highest_score_index = aspect_scores.index(highest_score)\n",
    "        \n",
    "        best_start = 0\n",
    "        best_end = highest_score_index\n",
    "        best_improvement = sum(improvements[:highest_score_index])\n",
    "        \n",
    "        for start in range(highest_score_index):\n",
    "            current_improvement = sum(improvements[start:highest_score_index])\n",
    "            if current_improvement > best_improvement:\n",
    "                best_start = start\n",
    "                best_improvement = current_improvement\n",
    "        \n",
    "        results[aspect] = {\n",
    "            \"optimal_range_start\": best_start,\n",
    "            \"optimal_range_end\": highest_score_index,\n",
    "            \"score_at_start\": aspect_scores[best_start],\n",
    "            \"score_at_end\": highest_score,\n",
    "            \"improvement_in_range\": best_improvement\n",
    "        }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def process_iteration_summaries(model_output, instruction, model):\n",
    "    iteration_scores = [score_summary(summary, f\"Iteration Summary {i+1}\", instruction, model)\n",
    "                        for i, summary in enumerate(model_output[\"iteration_summaries\"])]\n",
    "    return {\n",
    "        \"long_tail_stats\": calculate_long_tail_stats(iteration_scores),\n",
    "        # \"iteration_impact\": analyze_iteration_impact(iteration_scores),\n",
    "        # \"optimal_improvement_range\": find_optimal_improvement_range(iteration_scores),\n",
    "        # \"optimal_score_range\": find_optimal_score_range(iteration_scores)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def quality_scorer(instruction, model_output, model=\"gpt-4o\"):\n",
    "    scores = {\n",
    "        \"iteration_summaries_analysis\": {},\n",
    "        \"accumulated_summary\": {},\n",
    "        \"final_summary\": {}\n",
    "    }\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Process iteration summaries\n",
    "        scores[\"iteration_summaries_analysis\"] = process_iteration_summaries(model_output, instruction, model)\n",
    "\n",
    "        # Score accumulated summary\n",
    "        scores[\"accumulated_summary\"] = score_summary(model_output[\"accumulated_summary\"], \"Accumulated Summary\", instruction, model)\n",
    "\n",
    "        # Score final summary\n",
    "        scores[\"final_summary\"] = score_summary(model_output[\"final_summary\"], \"Final Summary\", instruction, model)\n",
    "\n",
    "        # After calculating all scores\n",
    "        flattened_scores = {}\n",
    "        for key, value in scores.items():\n",
    "            if isinstance(value, dict):\n",
    "                flattened_scores[key] = flatten_dict(value)\n",
    "            else:\n",
    "                flattened_scores[key] = value\n",
    "    \n",
    "        scores = flatten_dict(flattened_scores)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in quality_scorer: {str(e)}\")\n",
    "        scores[\"error\"] = str(e)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a markdown description for the \"Run Evaluation!\" section:\n",
    "\n",
    "## Run Evaluation!\n",
    "\n",
    "In this section, we demonstrate how to run the evaluation of our Chain of Density (CoD) summarization pipeline on ArXiv papers. This process involves using multiple models and assessing their performance using our custom evaluation metrics.\n",
    "\n",
    "1. First, we define a list of models to evaluate:\n",
    "These models represent different versions of Claude, allowing us to compare their performance on our summarization task.\n",
    "\n",
    "2. Next, we set up and run the evaluation:\n",
    "\n",
    "Here's what's happening in this code:\n",
    "\n",
    "- We create a `weave.Evaluation` object, using our previously defined dataset and the `quality_scorer` function.\n",
    "- We iterate through each model in our list.\n",
    "- For each model, we create an `ArxivChainOfDensityPipeline` instance, specifying the model and setting `density_iterations` to 8.\n",
    "- We then run the evaluation asynchronously using `await evaluation.evaluate()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"claude-3-opus-20240229\",\n",
    "    \"claude-3-haiku-20240307\",\n",
    "    \"claude-3-5-sonnet-20240620\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Key new entities/ideas:\n",
      "1. JUDGE-BENCH: Collection of 20 NLP datasets with human annotations\n",
      "2. Spearman's correlation for graded annotations and Cohen's kappa for categorical annotations\n",
      "3. Comparing LLM performance on human-generated vs. machine-generated text\n",
      "\n",
      "New technical summary:\n",
      "This paper presents JUDGE-BENCH, a comprehensive collection of 20 NLP datasets with human annotations, to empirically evaluate the ability of 11 state-of-the-art open-weight and proprietary large language models (LLMs) to replicate human judgments across a diverse range of NLP tasks. The evaluation spans datasets varying in the type of task (e.g., translation, dialogue generation), the property being judged (e.g., coherence, fluency), the type of judgments (categorical or graded), and the expertise of human annotators (experts or non-experts). Spearman's correlation is computed for graded annotations and Cohen's kappa for categorical annotations to quantify the alignment between LLM and human judgments. Key findings include: (1) Each LLM exhibits substantial variance in correlation with human judgments across datasets; (2) GPT-4o ranks first overall, with open models Llama3-70B and Mixtral-8x22B as close seconds; (3) LLMs demonstrate higher correlation with non-expert annotations compared to expert annotations; (4) LLM performance varies across different properties being judged, with no single model excelling at all properties; (5) LLMs align better with human judgments when evaluating human-generated text compared to machine-generated text. The authors conclude that current LLMs are not yet ready to systematically replace human judges and caution against using LLMs for NLP evaluation without calibration against human judgments on each new dataset. JUDGE-BENCH is released as a living benchmark to facilitate future extensions and evaluations as new LLMs emerge.\n",
      "\n",
      "Iteration 2:\n",
      "Key new entities/ideas:\n",
      "1. 11 LLMs evaluated, covering open-weight and proprietary models\n",
      "2. Living benchmark to facilitate future extensions as new LLMs emerge\n",
      "3. JUDGE-BENCH released as open-source\n",
      "\n",
      "New technical summary:\n",
      "This paper introduces JUDGE-BENCH, an open-source living benchmark comprising 20 diverse NLP datasets with human annotations, to rigorously assess the capability of 11 state-of-the-art open-weight and proprietary large language models (LLMs) in replicating human judgments across a wide spectrum of NLP tasks. The evaluation spans datasets varying in task type (e.g., translation, dialogue generation), judged property (e.g., coherence, fluency), judgment type (categorical or graded), and annotator expertise (expert or non-expert). Spearman's correlation for graded annotations and Cohen's kappa for categorical annotations quantify LLM-human judgment alignment. Key findings encompass: (1) substantial per-LLM variance in human correlation across datasets; (2) GPT-4o leading overall, closely followed by Llama3-70B and Mixtral-8x22B; (3) higher LLM correlation with non-expert vs. expert annotations; (4) LLM performance varying across judged properties, no single model excelling at all; (5) better LLM alignment with human judgments on human-generated vs. machine-generated text. The authors conclude current LLMs are not yet reliably fit to replace human judges and caution against employing LLMs for NLP evaluation without per-dataset calibration against human judgments. JUDGE-BENCH, released to facilitate future extensions as new LLMs emerge, holds promise as a valuable resource for the NLP community to track the evolving landscape of LLM-based evaluation.\n",
      "\n",
      "Iteration 3:\n",
      "Key new entities/ideas:\n",
      "1. LLMBar dataset for evaluating instruction-following abilities\n",
      "2. Diminishing gap between open and closed models\n",
      "3. JUDGE-BENCH code base released to ease extension\n",
      "\n",
      "New technical summary:\n",
      "This work presents JUDGE-BENCH, a comprehensive open-source benchmark encompassing 20 diverse NLP datasets with human annotations, to rigorously evaluate the capacity of 11 state-of-the-art open-weight and proprietary large language models (LLMs) to approximate human judges across a broad spectrum of NLP tasks. The evaluation spans datasets varying in task domain (e.g., machine translation, dialogue generation, summarization, instruction following via the LLMBar dataset), assessed property (e.g., coherence, fluency, toxicity), annotation type (categorical or graded), and annotator expertise (expert or non-expert). Spearman's correlation for graded annotations and Cohen's kappa for categorical annotations quantify LLM-human judgment alignment. Key findings include: (1) high per-LLM variance in human correlation across datasets; (2) GPT-4o leading overall, closely trailed by open models Llama3-70B and Mixtral-8x22B, indicating a diminishing gap between open and closed models; (3) stronger LLM correlation with non-expert vs. expert annotations; (4) LLM performance fluctuating across judged properties, no single model dominating all categories; (5) superior LLM alignment with human judgments on human-generated vs. machine-generated text. The authors conclude current LLMs are not yet reliable substitutes for human judges and advise against deploying LLMs for NLP evaluation without per-dataset calibration against human judgments. JUDGE-BENCH, with its released code base to facilitate future extensions as new LLMs emerge, constitutes a valuable resource for the NLP community to track the evolving landscape of LLM-based evaluation. The inclusion of the LLMBar dataset for assessing instruction-following abilities and the narrowing gap between open and closed models represent noteworthy methodological and empirical contributions, highlighting the benchmark's potential to drive impactful research in this rapidly advancing domain.\n",
      "\n",
      "Iteration 4:\n",
      "Key new entities/ideas:\n",
      "1. Krippendorff's  for human inter-rater agreement\n",
      "2. Living benchmark enabling future updates\n",
      "3. Reproducibility concerns addressed by open models\n",
      "\n",
      "New technical summary:\n",
      "This work introduces JUDGE-BENCH, an extensible open-source benchmark covering 20 NLP datasets with human annotations, to thoroughly assess 11 leading open and closed large language models' (LLMs) ability to proxy human judges across diverse tasks. The evaluation spans datasets differing in domain (e.g., translation, dialogue, summarization, LLMBar for instruction following), judged property (coherence, fluency, toxicity, etc.), annotation type (categorical/graded), and annotator expertise. Spearman's  and Cohen's  quantify LLM-human alignment for graded and categorical judgments, respectively. Krippendorff's  estimates human inter-rater agreement, contextualizing task difficulty. \n",
      "\n",
      "Key findings: (1) high intra-LLM variance across datasets; (2) GPT-4o leading, closely followed by Llama3-70B and Mixtral-8x22B, suggesting narrowing open-closed gap and addressing reproducibility concerns; (3) stronger LLM correlation with non-experts vs. experts; (4) fluctuating LLM performance across properties; (5) superior alignment on human vs. machine-generated text. The authors conclude LLMs are not yet reliable human judge surrogates, recommending per-dataset human calibration. JUDGE-BENCH, with its living benchmark design and released codebase, is a valuable resource tracking the evolving LLM evaluation landscape. Methodological and empirical contributions, like LLMBar inclusion and the diminishing open-closed gap, underscore its potential for driving impactful research in this rapidly advancing domain.\n",
      "\n",
      "Iteration 5:\n",
      "Key new entities/ideas:\n",
      "1. Divergent toxicity/safety performance (DICES, Medical-safety)\n",
      "2. Higher scores on human vs. machine-generated text \n",
      "3. JUDGE-BENCH intended as extensible, open-source resource\n",
      "\n",
      "New technical summary:\n",
      "This work introduces JUDGE-BENCH, an extensible open-source benchmark spanning 20 diverse NLP datasets with human annotations, to rigorously assess 11 leading open and closed large language models' (LLMs) ability to proxy human judges. The evaluation covers datasets varying in domain (translation, dialogue, summarization, LLMBar for instruction following), judged property (coherence, fluency, toxicity, etc.), annotation type (categorical/graded), and annotator expertise. Spearman's  and Cohen's  quantify LLM-human alignment for graded and categorical judgments, respectively, while Krippendorff's  estimates task difficulty via human inter-rater agreement.\n",
      "\n",
      "Key methodological contributions include the benchmark's living, open-source design facilitating future extensions and the inclusion of datasets like LLMBar. Empirical findings reveal: (1) high intra-LLM variance across datasets; (2) GPT-4o leading, closely trailed by Llama3-70B and Mixtral-8x22B, indicating a narrowing open-closed gap with reproducibility implications; (3) stronger LLM correlation with non-experts vs. experts; (4) fluctuating LLM performance across evaluated properties; (5) superior alignment on human vs. machine-generated text; (6) divergent toxicity/safety performance on DICES and Medical-safety datasets, with extremely low scores and valid response rates, possibly due to RLHF guardrails. \n",
      "\n",
      "The authors conclude current LLMs are not yet reliable human judge surrogates, advocating per-dataset human calibration. JUDGE-BENCH, with its living benchmark format and released codebase, represents a valuable resource for tracking the rapidly evolving LLM evaluation landscape. The study's methodological and empirical contributions underscore its potential for driving impactful research in this swiftly advancing domain. Findings like the diminishing open-closed gap and identification of challenging toxicity/safety benchmarks highlight promising directions for future work.\n",
      "\n",
      "Iteration 6:\n",
      "Key new entities/ideas:\n",
      "1. Open-source codebase released\n",
      "2. Living benchmark format\n",
      "3. Diminishing open-closed LLM gap\n",
      "\n",
      "New technical summary:\n",
      "This work introduces JUDGE-BENCH, an extensible open-source benchmark spanning 20 diverse NLP datasets with human annotations, to rigorously assess 11 leading open and closed large language models' (LLMs) ability to proxy human judges. The evaluation covers datasets varying in domain (translation, dialogue, summarization, LLMBar for instruction following), judged property (coherence, fluency, toxicity, etc.), annotation type (categorical/graded), and annotator expertise. Spearman's  and Cohen's  quantify LLM-human alignment for graded and categorical judgments, respectively, while Krippendorff's  estimates task difficulty via human inter-rater agreement.\n",
      "\n",
      "Key methodological contributions include the benchmark's living, open-source design facilitating future extensions, the inclusion of datasets like LLMBar, and the release of the study's codebase. Empirical findings reveal: (1) high intra-LLM variance across datasets; (2) GPT-4o leading, closely trailed by Llama3-70B and Mixtral-8x22B, indicating a narrowing open-closed gap with reproducibility implications; (3) stronger LLM correlation with non-experts vs. experts; (4) fluctuating LLM performance across evaluated properties; (5) superior alignment on human vs. machine-generated text; (6) divergent toxicity/safety performance on DICES and Medical-safety datasets, with extremely low scores and valid response rates, possibly due to RLHF guardrails.\n",
      "\n",
      "The authors conclude current LLMs are not yet reliable human judge surrogates, advocating per-dataset human calibration. JUDGE-BENCH, with its living benchmark format and released codebase, represents a valuable resource for tracking the rapidly evolving LLM evaluation landscape. The study's methodological contributions, including the open-source extensible benchmark and identification of the diminishing open-closed LLM performance gap, underscore its potential for driving impactful research in this swiftly advancing domain.\n",
      "\n",
      "Iteration 7:\n",
      "Key new entities/ideas:\n",
      "1. Scalar Quality Metric (SQM) evaluation\n",
      "2. CNN/DailyMail and XSUM datasets\n",
      "3. Greedy decoding for response generation\n",
      "\n",
      "New technical summary:\n",
      "This work introduces JUDGE-BENCH, an open-source, living benchmark spanning 20 diverse NLP datasets with human annotations, to rigorously assess 11 leading open (e.g., Llama3-70B, Mixtral-8x22B) and closed (e.g., GPT-4o) LLMs' ability to proxy human judges. The evaluation methodology encompasses datasets varying in domain (translation, dialogue, summarization, instruction following via LLMBar), judged properties (coherence, fluency, toxicity, etc.), annotation types (categorical/graded), and annotator expertise. Spearman's  and Cohen's  quantify LLM-human alignment for graded and categorical judgments, respectively, while Krippendorff's  estimates task difficulty.\n",
      "\n",
      "Key methodological contributions include the extensible benchmark design, the inclusion of datasets like LLMBar, CNN/DailyMail, XSUM, and the release of the study's codebase. Inference leverages greedy decoding, with open models generating a maximum of 25 new tokens and proprietary models 5 tokens. Scalar Quality Metric (SQM) evaluation is employed for the WMT datasets.\n",
      "\n",
      "Empirical findings, based on 125.22 A100 GPU compute hours, reveal: (1) high intra-LLM variance; (2) GPT-4o leading, closely trailed by Llama3-70B and Mixtral-8x22B, indicating a narrowing open-closed gap; (3) stronger LLM correlation with non-experts vs. experts; (4) fluctuating LLM performance across properties; (5) superior alignment on human vs. machine-generated text; (6) divergent toxicity/safety performance on DICES and Medical-safety datasets, possibly due to RLHF.\n",
      "\n",
      "The authors conclude current LLMs are not yet reliable human judge surrogates, advocating per-dataset human calibration. JUDGE-BENCH's living benchmark format and released codebase represent valuable resources for tracking the rapidly evolving LLM evaluation landscape. The study's methodological contributions underscore its potential for driving impactful research in this swiftly advancing domain.\n",
      "\n",
      "Iteration 8:\n",
      "Key new entities/ideas:\n",
      "1. Invalid response rate handling \n",
      "2. Data leakage considerations\n",
      "3. 14165-14178 and 234-243 page ranges\n",
      "\n",
      "New technical summary:\n",
      "This work introduces JUDGE-BENCH, a comprehensive, open-source benchmark spanning 20 diverse NLP datasets (e.g., WMT, CNN/DailyMail, XSUM, LLMBar) with human annotations, to rigorously assess 11 leading LLMs' (GPT-4o, Llama3-70B, Mixtral-8x22B, etc.) ability to proxy human judges. Datasets vary in task (translation, summarization, instruction following), judged properties (coherence, fluency, toxicity), annotation type (categorical/graded), and annotator expertise. Alignment quantification uses Spearman's , Cohen's , and Krippendorff's .\n",
      "\n",
      "Methodological innovations include the extensible design, inclusion of instruction following datasets like LLMBar (pages 14165-14178), medical safety (pages 234-243), data leakage considerations, and codebase release. Inference employs greedy decoding (max 25 new tokens for open models, 5 for proprietary). Invalid responses are replaced with random values. Scalar Quality Metric (SQM) evaluation is used for WMT.\n",
      "\n",
      "125.22 A100 GPU hours reveal high intra-LLM variance, with GPT-4o narrowly leading Llama3-70B and Mixtral-8x22B. LLMs correlate more strongly with non-experts than experts and performance fluctuates across properties. Alignment is superior on human vs. machine-generated text. Divergent toxicity/safety performance on DICES and Medical-safety datasets is attributed to RLHF.\n",
      "\n",
      "The authors conclude current LLMs are unreliable human judge surrogates without per-dataset calibration. JUDGE-BENCH's living benchmark format and released codebase provide valuable resources for tracking the rapidly evolving LLM evaluation landscape. The study's methodological contributions underscore its potential for driving impactful research in this swiftly advancing domain. Detailed results are presented in pages 1-13 and appendices A-D.\n",
      "\n",
      "Final Summary:\n",
      "JUDGE-BENCH, an extensible, open-source benchmark spanning 20 diverse NLP datasets, rigorously assesses 11 leading LLMs' ability to proxy human judges. Methodological innovations include instruction following datasets (LLMBar, pages 14165-14178), medical safety (pages 234-243), data leakage considerations, and invalid response handling using random value replacement. Alignment quantification employs Spearman's , Cohen's , and Krippendorff's . Inference uses greedy decoding (max 25 new tokens for open models, 5 for proprietary), with Scalar Quality Metric for WMT. The living benchmark format and released codebase drive impactful research in the rapidly evolving LLM evaluation landscape. Results reveal high intra-LLM variance, stronger correlation with non-experts, and divergent toxicity/safety performance attributed to RLHF. The study concludes current LLMs are unreliable human judge surrogates without per-dataset calibration, underscoring the potential for advancing the field through its methodological contributions.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m1\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'quality_scorer'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'iteration_summaries_analysis_long_tail_stats_relevance_mean'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.96875</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'iteration_summaries_analysis_long_tail_stats_relevance_tail_ratio'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.1968503937007875</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'iteration_summaries_analysis_long_tail_stats_technical_quality_mean'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.09375</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'iteration_summaries_analysis_long_tail_stats_technical_quality_tail_ratio'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.099236641221374</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'iteration_summaries_analysis_long_tail_stats_conciseness_mean'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.53125</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'iteration_summaries_analysis_long_tail_stats_conciseness_tail_ratio'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.1327433628318584</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'accumulated_summary_relevance_score'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'accumulated_summary_technical_quality_score'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'accumulated_summary_conciseness_score'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.5</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'final_summary_relevance_score'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'final_summary_technical_quality_score'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.5</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'final_summary_conciseness_score'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.0</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">245.34171319007874</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'quality_scorer'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'iteration_summaries_analysis_long_tail_stats_relevance_mean'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m3.96875\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'iteration_summaries_analysis_long_tail_stats_relevance_tail_ratio'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m1.1968503937007875\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'iteration_summaries_analysis_long_tail_stats_technical_quality_mean'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m4.09375\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'iteration_summaries_analysis_long_tail_stats_technical_quality_tail_ratio'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m1.099236641221374\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'iteration_summaries_analysis_long_tail_stats_conciseness_mean'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m3.53125\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'iteration_summaries_analysis_long_tail_stats_conciseness_tail_ratio'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m1.1327433628318584\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'accumulated_summary_relevance_score'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m3.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'accumulated_summary_technical_quality_score'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m3.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'accumulated_summary_conciseness_score'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m2.5\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'final_summary_relevance_score'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m4.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'final_summary_technical_quality_score'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m3.5\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'final_summary_conciseness_score'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m3.0\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m245.34171319007874\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Key methodologies and contributions:\n",
      "\n",
      "1. JUDGE-BENCH: A comprehensive benchmark of 20 datasets with human annotations across a wide range of NLP tasks and quality dimensions, enabling large-scale evaluation of LLM performance as evaluators.\n",
      "\n",
      "2. Comprehensive evaluation of 11 state-of-the-art open-source and proprietary LLMs, including GPT-4o, LLaMA-3, and Mixtral, to assess their ability to replicate human judgments.\n",
      "\n",
      "3. Analysis of LLM performance variance across datasets and tasks, highlighting limitations in using LLMs as systematic replacements for human judges in NLP evaluations.\n",
      "\n",
      "4. Observation that LLMs generally exhibit higher correlation with non-expert human judgments compared to expert annotations, suggesting potential biases in LLM-based evaluation.\n",
      "\n",
      "5. Comparative analysis of LLM performance on human-generated vs. model-generated text, indicating the need for caution in using LLMs to evaluate NLP system outputs.\n",
      "\n",
      "The systematic large-scale evaluation presented in this work provides valuable insights into the current capabilities and limitations of LLMs as NLP evaluators, informing future research on improving the reliability and generalizability of automated evaluation approaches.\n",
      "\n",
      "Iteration 2:\n",
      "Key methodologies and contributions:\n",
      "\n",
      "1. JUDGE-BENCH: Large-scale NLP evaluation benchmark with 20 datasets, spanning acceptability, reasoning, summarization, dialogue, toxicity, and instruction-following tasks.\n",
      "\n",
      "2. Comprehensive LLM evaluation: 11 state-of-the-art models, including GPT-4o, LLaMA-3, and Mixtral, assessed for alignment with human judgments across diverse datasets and properties.\n",
      "\n",
      "3. Variance analysis: LLMs exhibit significant performance variability across datasets, highlighting limitations in replacing human judges with LLMs for NLP evaluations.\n",
      "\n",
      "4. Non-expert vs. expert comparison: LLMs demonstrate higher correlation with non-expert than expert human annotations, indicating potential biases in LLM-based evaluation.\n",
      "\n",
      "5. Human vs. machine-generated text: LLMs perform better on human-generated text than model-generated outputs, necessitating caution in using them to evaluate NLP systems.\n",
      "\n",
      "This systematic, large-scale evaluation provides crucial insights into the current capabilities and limitations of LLMs as NLP evaluators, informing future research on reliable and generalizable automated evaluation approaches.\n",
      "\n",
      "Iteration 3:\n",
      "Key methodologies and contributions:\n",
      "\n",
      "JUDGE-BENCH: Large-scale NLP evaluation benchmark spanning 20 datasets, properties (acceptability, reasoning, summarization, dialogue, toxicity, instruction-following).\n",
      "\n",
      "Comprehensive LLM evaluation: Assessed alignment of 11 state-of-the-art LLMs, including GPT-4o, LLaMA-3, Mixtral, with human judgments.\n",
      "\n",
      "Variance analysis: LLMs exhibit significant performance variability across datasets, limiting their ability to replace human judges.\n",
      "\n",
      "Non-expert vs. expert comparison: LLMs correlate better with non-expert than expert human annotations, indicating potential biases.\n",
      "\n",
      "Human vs. machine-generated text: LLMs perform better on human-generated text than model-generated outputs, necessitating caution in evaluating NLP systems.\n",
      "\n",
      "This large-scale, systematic evaluation provides crucial insights into current LLM capabilities and limitations as NLP evaluators, informing future research on reliable automated evaluation approaches.\n",
      "\n",
      "Iteration 4:\n",
      "Key Methodologies and Contributions:\n",
      "\n",
      "JUDGE-BENCH: Comprehensive NLP evaluation benchmark spanning 20 datasets, 5 task types (acceptability, reasoning, summarization, dialogue, toxicity).\n",
      "\n",
      "LLM Evaluation: Assessed alignment of 11 LLMs (GPT-4o, LLaMA-3, Mixtral) with human judgments across task-specific properties.\n",
      "\n",
      "Variance Analysis: LLMs exhibit significant performance variability, limiting use as human judge surrogates.\n",
      "\n",
      "Human vs. Machine Comparison: LLMs correlate better with non-expert than expert annotations; perform better on human- vs. model-generated text.\n",
      "\n",
      "This large-scale, systematic evaluation quantifies current LLM capabilities and limitations as NLP evaluators, informing future research on automated, reliable evaluation approaches.\n",
      "\n",
      "Iteration 5:\n",
      "Key Methodologies and Contributions:\n",
      "\n",
      "JUDGE-BENCH benchmark, Graded/Categorical Annotations, Variance Analysis, Human vs. Machine Comparison\n",
      "\n",
      "This work presents JUDGE-BENCH, a comprehensive NLP evaluation benchmark spanning 20 datasets across 5 task types: acceptability, reasoning, summarization, dialogue, and toxicity. The authors systematically evaluated the alignment of 11 LLMs, including GPT-4o, LLaMA-3, and Mixtral, with human judgments across task-specific properties. Their variance analysis revealed significant performance variability across LLMs, limiting their use as human judge surrogates. Notably, the study found LLMs correlate better with non-expert than expert annotations, and perform better on human- vs. model-generated text. This large-scale evaluation quantifies current LLM capabilities and limitations as NLP evaluators, informing future research on automated, reliable evaluation approaches.\n",
      "\n",
      "Iteration 6:\n",
      "Key Methodologies and Contributions:\n",
      "\n",
      "JUDGE-BENCH benchmark, Human-LLM Alignment Evaluation, Categorical/Graded Annotation Analysis, Domain-Specific Task Coverage\n",
      "\n",
      "This work introduces the JUDGE-BENCH benchmark, a comprehensive suite of 20 NLP datasets spanning 5 key task domains: acceptability, reasoning, summarization, dialogue, and toxicity evaluation. The authors conduct a large-scale empirical assessment of 11 state-of-the-art LLMs, including GPT-4o and LLaMA-3, to quantify their ability to replicate human judgments across categorical and graded annotations. Their variance analysis reveals significant performance heterogeneity, suggesting current LLMs exhibit limited capacity to systematically replace human evaluators. Notably, the study finds LLMs correlate better with non-expert than expert annotations, and perform better on human-generated vs. model-generated text. This rigorous, domain-diverse evaluation provides crucial insights into the current limitations of LLMs as NLP assessment tools, informing future research on reliable, scalable evaluation approaches.\n",
      "\n",
      "Iteration 7:\n",
      "JUDGE-BENCH, Categorical/Graded Annotation Analysis, Domain-Specific Performance Heterogeneity\n",
      "\n",
      "This work introduces the JUDGE-BENCH benchmark, a diverse 20-dataset suite spanning acceptability, reasoning, summarization, dialogue, and toxicity evaluation. The authors comprehensively evaluate 11 state-of-the-art LLMs, including GPT-4o and LLaMA-3, on their ability to replicate human judgments across categorical and graded annotations. Variance analysis reveals significant performance heterogeneity, with LLMs correlating better with non-expert than expert annotations, and performing better on human-generated vs. model-generated text. This rigorous, domain-diverse evaluation provides crucial insights into the current limitations of LLMs as NLP assessment tools, informing future research on reliable, scalable evaluation approaches.\n",
      "\n",
      "Iteration 8:\n",
      "Key Methodologies and Contributions:\n",
      "1. JUDGE-BENCH: A 20-dataset benchmark spanning acceptability, reasoning, summarization, dialogue, and toxicity evaluation.\n",
      "2. Comprehensive LLM Evaluation: 11 state-of-the-art models, including GPT-4o and LLaMA-3, assessed on replicating human judgments across categorical and graded annotations.\n",
      "3. Variance Analysis: Significant performance heterogeneity observed, with LLMs correlating better with non-expert than expert annotations, and outperforming on human-generated vs. model-generated text.\n",
      "\n",
      "This rigorous, domain-diverse evaluation provides crucial insights into current limitations of LLMs as reliable, scalable NLP assessment tools, informing future research on effective, reproducible evaluation methodologies. The JUDGE-BENCH benchmark serves as a living resource to track LLM progress in automated judgment tasks.\n",
      "\n",
      "Final Summary:\n",
      "Comprehensive LLM evaluation on JUDGE-BENCH, a 20-dataset benchmark spanning diverse NLP tasks, reveals significant performance heterogeneity. LLMs correlate better with non-expert than expert annotations, and outperform on human-generated vs. model-generated text. This rigorous, domain-diverse assessment provides crucial insights into current limitations of LLMs as scalable NLP evaluation tools, informing future research on effective, reproducible methodologies. The JUDGE-BENCH serves as a living benchmark to track LLM progress in automated judgment tasks.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m1\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'quality_scorer'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'iteration_summaries_analysis_long_tail_stats_relevance_mean'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.84375</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'iteration_summaries_analysis_long_tail_stats_relevance_tail_ratio'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0406504065040652</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'iteration_summaries_analysis_long_tail_stats_technical_quality_mean'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.59375</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'iteration_summaries_analysis_long_tail_stats_technical_quality_tail_ratio'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.1130434782608696</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'iteration_summaries_analysis_long_tail_stats_conciseness_mean'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.5</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'iteration_summaries_analysis_long_tail_stats_conciseness_tail_ratio'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.1428571428571428</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'accumulated_summary_relevance_score'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'accumulated_summary_technical_quality_score'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'accumulated_summary_conciseness_score'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'final_summary_relevance_score'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.5</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'final_summary_technical_quality_score'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.5</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'final_summary_conciseness_score'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.0</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">69.33509612083435</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'quality_scorer'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'iteration_summaries_analysis_long_tail_stats_relevance_mean'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m3.84375\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'iteration_summaries_analysis_long_tail_stats_relevance_tail_ratio'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m1.0406504065040652\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'iteration_summaries_analysis_long_tail_stats_technical_quality_mean'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m3.59375\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'iteration_summaries_analysis_long_tail_stats_technical_quality_tail_ratio'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m1.1130434782608696\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'iteration_summaries_analysis_long_tail_stats_conciseness_mean'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m3.5\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'iteration_summaries_analysis_long_tail_stats_conciseness_tail_ratio'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m1.1428571428571428\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'accumulated_summary_relevance_score'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m4.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'accumulated_summary_technical_quality_score'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m3.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'accumulated_summary_conciseness_score'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m3.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'final_summary_relevance_score'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m2.5\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'final_summary_technical_quality_score'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m2.5\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'final_summary_conciseness_score'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m3.0\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m69.33509612083435\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "New important technical entities/ideas:\n",
      "1. JUDGE-BENCH\n",
      "2. Cohen's  and Spearman's correlation\n",
      "3. Open-weight and proprietary LLMs\n",
      "\n",
      "Summary:\n",
      "\n",
      "This research introduces JUDGE-BENCH, a comprehensive benchmark comprising 20 NLP datasets with human annotations, to evaluate the efficacy of Large Language Models (LLMs) as judges for NLP tasks. The study assesses 11 state-of-the-art open-weight and proprietary LLMs across various quality dimensions and tasks, utilizing Cohen's  for categorical annotations and Spearman's correlation for graded annotations to measure alignment with human judgments.\n",
      "\n",
      "Key methodologies include:\n",
      "1. Dataset curation: Incorporating diverse NLP tasks, annotation types (categorical/graded), and annotator expertise levels.\n",
      "2. Model selection: Evaluating both open-source and proprietary LLMs of varying sizes.\n",
      "3. Prompt engineering: Utilizing original human annotation instructions as model prompts.\n",
      "4. Evaluation protocol: Replacing invalid LLM responses with random values to ensure consistent judgment counts across models.\n",
      "\n",
      "Novel contributions:\n",
      "1. Large-scale empirical study: Comprehensive evaluation across 20 datasets, surpassing previous work in scope and diversity.\n",
      "2. Comparative analysis: Assessing LLM performance against both expert and non-expert human annotators.\n",
      "3. Task-specific insights: Revealing varying LLM efficacy across different NLP tasks and quality dimensions.\n",
      "\n",
      "The study's findings demonstrate significant variability in LLM performance across datasets and quality dimensions, with no single model consistently outperforming others. GPT-4o exhibited the highest overall correlation with human judgments, closely followed by Llama3-70B. Notably, LLMs generally achieved better alignment with non-expert human judgments compared to expert annotations.\n",
      "\n",
      "This research contributes to the ongoing debate on automating NLP evaluation processes, providing empirical evidence that current LLMs are not yet consistently reliable replacements for human judges across all NLP tasks. The study emphasizes the need for task-specific calibration of LLMs against human judgments to establish validity in evaluation scores.\n",
      "\n",
      "The introduction of JUDGE-BENCH as a living benchmark facilitates future updates and extensions, potentially accelerating progress in LLM-based evaluation methodologies. This work has significant implications for the NLP community, offering insights into the current limitations and potential of LLMs as automated evaluators, and providing a foundation for further research in improving LLM-based assessment techniques.\n",
      "\n",
      "Iteration 2:\n",
      "New important technical entities/ideas:\n",
      "1. Zero-shot learning\n",
      "2. Krippendorff's \n",
      "3. Data leakage mitigation\n",
      "\n",
      "Summary:\n",
      "\n",
      "JUDGE-BENCH, a comprehensive NLP evaluation benchmark, employs rigorous methodologies and novel contributions to assess LLM efficacy as automated judges:\n",
      "\n",
      "Methodologies:\n",
      "1. Dataset curation: 20 diverse NLP datasets encompassing varied tasks, annotation types (categorical/graded), and annotator expertise levels, facilitating robust cross-task comparisons.\n",
      "2. Model selection: 11 state-of-the-art open-weight and proprietary LLMs, including GPT-4o, Llama3-70B, and Mixtral-8x22B, ensuring broad coverage of current AI capabilities.\n",
      "3. Prompt engineering: Zero-shot learning approach utilizing original human annotation instructions as model prompts, minimizing potential biases from task-specific fine-tuning.\n",
      "4. Evaluation protocol: \n",
      "   a. Cohen's  for categorical annotations\n",
      "   b. Spearman's correlation for graded annotations\n",
      "   c. Invalid response mitigation: Random value substitution to maintain consistent judgment counts\n",
      "   d. Krippendorff's  calculation for inter-rater agreement on datasets with multiple human annotations\n",
      "5. Data leakage mitigation: Careful tracking and reporting of potential training data overlap between LLMs and evaluation datasets.\n",
      "\n",
      "Novel contributions:\n",
      "1. Large-scale empirical study: Unprecedented scope encompassing 20 datasets, surpassing previous work in diversity and comprehensiveness.\n",
      "2. Comparative analysis framework: \n",
      "   a. LLM performance assessment against both expert and non-expert human annotators\n",
      "   b. Cross-model comparison across varied NLP tasks and quality dimensions\n",
      "3. Task-specific performance insights: Granular analysis revealing varying LLM efficacy across different NLP tasks, properties, and annotation types.\n",
      "4. Living benchmark: JUDGE-BENCH designed for continuous updates, facilitating longitudinal studies on LLM progress in evaluation tasks.\n",
      "\n",
      "Impact:\n",
      "1. Empirical evidence base: Comprehensive data on current LLM limitations in replicating human judgments across diverse NLP tasks.\n",
      "2. Benchmark standardization: JUDGE-BENCH provides a unified framework for assessing future LLM developments in evaluation capabilities.\n",
      "3. Methodological refinement: Insights into effective prompt engineering and evaluation protocols for LLM-based assessments.\n",
      "4. Research direction guidance: Identification of specific areas (e.g., toxicity and safety evaluation) where LLMs significantly underperform, focusing future development efforts.\n",
      "5. Reproducibility enhancement: Emphasis on open-weight models and detailed methodology reporting to address concerns with proprietary LLM evaluations.\n",
      "6. Calibration necessity highlight: Demonstration of the critical need for task-specific LLM calibration against human judgments before deployment in automated evaluations.\n",
      "\n",
      "This research provides a foundational framework for systematic assessment of LLM evaluation capabilities, potentially accelerating progress in automated NLP assessment methodologies while concurrently exposing current limitations and risks associated with premature adoption of LLMs as human judge replacements.\n",
      "\n",
      "Iteration 3:\n",
      "New important technical entities/ideas:\n",
      "1. Greedy decoding\n",
      "2. System prompts\n",
      "3. RLHF guardrails\n",
      "\n",
      "Summary:\n",
      "\n",
      "JUDGE-BENCH, a comprehensive NLP evaluation benchmark, employs rigorous methodologies and novel contributions to assess LLM efficacy as automated judges:\n",
      "\n",
      "Methodologies:\n",
      "1. Dataset curation: 20 diverse NLP datasets encompassing varied tasks, annotation types (categorical/graded), and annotator expertise levels, facilitating robust cross-task comparisons.\n",
      "2. Model selection: 11 state-of-the-art open-weight and proprietary LLMs, including GPT-4o, Llama3-70B, and Mixtral-8x22B, ensuring broad coverage of current AI capabilities.\n",
      "3. Prompt engineering: Zero-shot learning approach utilizing original human annotation instructions as model prompts, minimizing potential biases from task-specific fine-tuning. System prompts were experimented with but showed no improvements.\n",
      "4. Inference procedure: Greedy decoding operationalized by setting temperature parameter to 0 for proprietary models, with a maximum generation of 25 new tokens for open models and 5 for proprietary models.\n",
      "5. Evaluation protocol: \n",
      "   a. Cohen's  for categorical annotations\n",
      "   b. Spearman's correlation for graded annotations\n",
      "   c. Invalid response mitigation: Random value substitution to maintain consistent judgment counts\n",
      "   d. Krippendorff's  calculation for inter-rater agreement on datasets with multiple human annotations\n",
      "6. Data leakage mitigation: Careful tracking and reporting of potential training data overlap between LLMs and evaluation datasets.\n",
      "\n",
      "Novel contributions:\n",
      "1. Large-scale empirical study: Unprecedented scope encompassing 20 datasets, surpassing previous work in diversity and comprehensiveness.\n",
      "2. Comparative analysis framework: \n",
      "   a. LLM performance assessment against both expert and non-expert human annotators\n",
      "   b. Cross-model comparison across varied NLP tasks and quality dimensions\n",
      "3. Task-specific performance insights: Granular analysis revealing varying LLM efficacy across different NLP tasks, properties, and annotation types. Notably, identification of RLHF guardrails' impact on toxicity and safety evaluations, resulting in low model scores and valid response rates.\n",
      "4. Living benchmark: JUDGE-BENCH designed for continuous updates, facilitating longitudinal studies on LLM progress in evaluation tasks.\n",
      "\n",
      "Impact:\n",
      "1. Empirical evidence base: Comprehensive data on current LLM limitations in replicating human judgments across diverse NLP tasks, revealing high variability across models, datasets, and properties judged.\n",
      "2. Benchmark standardization: JUDGE-BENCH provides a unified framework for assessing future LLM developments in evaluation capabilities, with a standardized data schema facilitating integration of additional datasets.\n",
      "3. Methodological refinement: Insights into effective prompt engineering and evaluation protocols for LLM-based assessments, including the limitations of system prompts and the necessity for task-specific calibration.\n",
      "4. Research direction guidance: Identification of specific areas (e.g., toxicity and safety evaluation) where LLMs significantly underperform, focusing future development efforts and highlighting potential RLHF guardrail limitations.\n",
      "5. Reproducibility enhancement: Emphasis on open-weight models and detailed methodology reporting to address concerns with proprietary LLM evaluations. Observed decreasing gap between open and closed models (e.g., GPT-4o and Llama3-70B performance) promising for future evaluation reproducibility.\n",
      "6. Calibration necessity highlight: Demonstration of the critical need for task-specific LLM calibration against human judgments before deployment in automated evaluations, given observed performance variability.\n",
      "7. Human vs. machine-generated text evaluation insights: Observation that all models achieve better alignment with human judgments when evaluating human language compared to machine-generated text, emphasizing caution in automated NLP system output evaluation.\n",
      "\n",
      "This research provides a foundational framework for systematic assessment of LLM evaluation capabilities, potentially accelerating progress in automated NLP assessment methodologies while concurrently exposing current limitations and risks associated with premature adoption of LLMs as human judge replacements. The study's comprehensive approach and granular insights offer a nuanced understanding of LLM evaluation performance across diverse NLP tasks, guiding future research and development efforts in the field.\n",
      "\n",
      "Iteration 4:\n",
      "New important technical entities/ideas:\n",
      "1. Zero-shot learning approach\n",
      "2. Temperature parameter\n",
      "3. Quantitative performance metrics\n",
      "\n",
      "Summary:\n",
      "\n",
      "JUDGE-BENCH, a comprehensive NLP evaluation benchmark, employs rigorous methodologies and novel contributions to assess LLM efficacy as automated judges:\n",
      "\n",
      "Methodologies:\n",
      "1. Dataset curation: 20 diverse NLP datasets encompassing varied tasks, annotation types (categorical/graded), and annotator expertise levels.\n",
      "2. Model selection: 11 state-of-the-art open-weight and proprietary LLMs (GPT-4o, Llama3-70B, Mixtral-8x22B).\n",
      "3. Prompt engineering: Zero-shot learning approach utilizing original human annotation instructions as model prompts. System prompts experimented but ineffective.\n",
      "4. Inference procedure: Greedy decoding (temperature=0) for proprietary models; max 25 tokens (open) / 5 tokens (proprietary) generation.\n",
      "5. Evaluation protocol: \n",
      "   a. Cohen's  (categorical)\n",
      "   b. Spearman's correlation (graded)\n",
      "   c. Invalid response mitigation: Random value substitution\n",
      "   d. Krippendorff's  for inter-rater agreement\n",
      "6. Data leakage mitigation: Tracking/reporting of potential LLM-dataset training overlap.\n",
      "\n",
      "Novel contributions:\n",
      "1. Large-scale empirical study: 20 datasets, surpassing previous work in diversity/comprehensiveness.\n",
      "2. Comparative analysis framework: \n",
      "   a. LLM vs. expert/non-expert human annotators\n",
      "   b. Cross-model comparison across NLP tasks/quality dimensions\n",
      "3. Task-specific performance insights: Granular analysis revealing varying LLM efficacy. RLHF guardrails' impact on toxicity/safety evaluations identified.\n",
      "4. Living benchmark: Designed for continuous updates, facilitating longitudinal LLM progress studies.\n",
      "\n",
      "Impact:\n",
      "1. Empirical evidence base: Comprehensive quantitative performance metrics on LLM limitations in replicating human judgments across diverse NLP tasks.\n",
      "2. Benchmark standardization: Unified framework for assessing future LLM evaluation capabilities, standardized data schema for dataset integration.\n",
      "3. Methodological refinement: Insights into effective prompt engineering, evaluation protocols, limitations of system prompts, necessity for task-specific calibration.\n",
      "4. Research direction guidance: Identification of underperforming areas (e.g., toxicity/safety evaluation), focusing development efforts, highlighting RLHF guardrail limitations.\n",
      "5. Reproducibility enhancement: Emphasis on open-weight models, detailed methodology reporting. Observed decreasing gap between open/closed models (GPT-4o vs. Llama3-70B) promising for evaluation reproducibility.\n",
      "6. Calibration necessity highlight: Demonstration of critical need for task-specific LLM calibration against human judgments pre-deployment.\n",
      "7. Human vs. machine-generated text evaluation insights: Better model alignment with human judgments on human language vs. machine-generated text, emphasizing caution in automated NLP system output evaluation.\n",
      "\n",
      "This research provides a foundational framework for systematic LLM evaluation capability assessment, potentially accelerating automated NLP assessment methodologies while exposing current limitations/risks. The study's comprehensive approach and granular insights offer a nuanced understanding of LLM evaluation performance across diverse NLP tasks, guiding future research/development efforts in the field.\n",
      "\n",
      "Iteration 5:\n",
      "New important technical entities/ideas:\n",
      "1. Greedy decoding operationalization\n",
      "2. A100 GPU utilization\n",
      "3. Valid response rate analysis\n",
      "\n",
      "Summary:\n",
      "\n",
      "JUDGE-BENCH, a comprehensive NLP evaluation benchmark, employs rigorous methodologies and novel contributions to assess LLM efficacy as automated judges:\n",
      "\n",
      "Methodologies:\n",
      "1. Dataset curation: 20 diverse NLP datasets encompassing varied tasks, annotation types (categorical/graded), and annotator expertise levels.\n",
      "2. Model selection: 11 state-of-the-art open-weight and proprietary LLMs (GPT-4o, Llama3-70B, Mixtral-8x22B).\n",
      "3. Prompt engineering: Zero-shot learning approach utilizing original human annotation instructions as model prompts. System prompts experimented but ineffective.\n",
      "4. Inference procedure: Greedy decoding operationalization (temperature=0) for proprietary models; max 25 tokens (open) / 5 tokens (proprietary) generation.\n",
      "5. Evaluation protocol: \n",
      "   a. Cohen's  (categorical)\n",
      "   b. Spearman's correlation (graded)\n",
      "   c. Invalid response mitigation: Random value substitution\n",
      "   d. Krippendorff's  for inter-rater agreement\n",
      "6. Data leakage mitigation: Tracking/reporting of potential LLM-dataset training overlap.\n",
      "7. Computational resources: A100 GPU utilization for 125.22 compute hours.\n",
      "8. Valid response rate analysis: Quantification of model reliability across datasets.\n",
      "\n",
      "Novel contributions:\n",
      "1. Large-scale empirical study: 20 datasets, surpassing previous work in diversity/comprehensiveness.\n",
      "2. Comparative analysis framework: \n",
      "   a. LLM vs. expert/non-expert human annotators\n",
      "   b. Cross-model comparison across NLP tasks/quality dimensions\n",
      "3. Task-specific performance insights: Granular analysis revealing varying LLM efficacy. RLHF guardrails' impact on toxicity/safety evaluations identified.\n",
      "4. Living benchmark: Designed for continuous updates, facilitating longitudinal LLM progress studies.\n",
      "\n",
      "Impact:\n",
      "1. Empirical evidence base: Comprehensive quantitative performance metrics on LLM limitations in replicating human judgments across diverse NLP tasks.\n",
      "2. Benchmark standardization: Unified framework for assessing future LLM evaluation capabilities, standardized data schema for dataset integration.\n",
      "3. Methodological refinement: Insights into effective prompt engineering, evaluation protocols, limitations of system prompts, necessity for task-specific calibration.\n",
      "4. Research direction guidance: Identification of underperforming areas (e.g., toxicity/safety evaluation), focusing development efforts, highlighting RLHF guardrail limitations.\n",
      "5. Reproducibility enhancement: Emphasis on open-weight models, detailed methodology reporting. Observed decreasing gap between open/closed models (GPT-4o vs. Llama3-70B) promising for evaluation reproducibility.\n",
      "6. Calibration necessity highlight: Demonstration of critical need for task-specific LLM calibration against human judgments pre-deployment.\n",
      "7. Human vs. machine-generated text evaluation insights: Better model alignment with human judgments on human language vs. machine-generated text, emphasizing caution in automated NLP system output evaluation.\n",
      "8. Computational resource benchmarking: Establishment of baseline GPU utilization metrics for large-scale LLM evaluation studies.\n",
      "9. Model reliability quantification: Valid response rate analysis provides insights into LLM robustness across diverse NLP tasks, informing model selection and refinement strategies.\n",
      "\n",
      "This research provides a foundational framework for systematic LLM evaluation capability assessment, potentially accelerating automated NLP assessment methodologies while exposing current limitations/risks. The study's comprehensive approach and granular insights offer a nuanced understanding of LLM evaluation performance across diverse NLP tasks, guiding future research/development efforts in the field. The inclusion of computational resource metrics and valid response rate analysis further enhances the benchmark's utility for practical implementation and model comparison.\n",
      "\n",
      "Iteration 6:\n",
      "New important technical entities/ideas:\n",
      "1. Zero-shot learning approach\n",
      "2. RLHF guardrail impact\n",
      "3. Granular cross-model analysis\n",
      "\n",
      "Summary:\n",
      "\n",
      "JUDGE-BENCH: comprehensive NLP evaluation benchmark assessing LLM efficacy as automated judges.\n",
      "\n",
      "Methodologies:\n",
      "1. Dataset curation: 20 diverse NLP datasets (varied tasks, categorical/graded annotations, expert/non-expert annotators)\n",
      "2. Model selection: 11 SOTA open-weight/proprietary LLMs (GPT-4o, Llama3-70B, Mixtral-8x22B)\n",
      "3. Prompt engineering: Zero-shot learning approach utilizing original human annotation instructions\n",
      "4. Inference procedure: Greedy decoding operationalization (temperature=0), max 25/5 tokens (open/proprietary)\n",
      "5. Evaluation protocol:\n",
      "   a. Cohen's  (categorical)\n",
      "   b. Spearman's correlation (graded)\n",
      "   c. Invalid response mitigation: Random value substitution\n",
      "   d. Krippendorff's  (inter-rater agreement)\n",
      "6. Data leakage mitigation: Tracking/reporting LLM-dataset training overlap\n",
      "7. Computational resources: A100 GPU, 125.22 compute hours\n",
      "8. Valid response rate analysis: Model reliability quantification across datasets\n",
      "\n",
      "Novel contributions:\n",
      "1. Large-scale empirical study: 20 datasets, surpassing previous work in diversity/comprehensiveness\n",
      "2. Comparative analysis framework:\n",
      "   a. LLM vs. expert/non-expert human annotators\n",
      "   b. Granular cross-model analysis across NLP tasks/quality dimensions\n",
      "3. Task-specific performance insights: RLHF guardrail impact on toxicity/safety evaluations identified\n",
      "4. Living benchmark: Continuous updates facilitation, longitudinal LLM progress studies\n",
      "\n",
      "Impact:\n",
      "1. Empirical evidence base: Comprehensive quantitative metrics on LLM limitations in replicating human judgments\n",
      "2. Benchmark standardization: Unified framework for future LLM evaluation capability assessment\n",
      "3. Methodological refinement: Prompt engineering insights, evaluation protocols, system prompt limitations, task-specific calibration necessity\n",
      "4. Research direction guidance: Underperforming areas identification (e.g., toxicity/safety evaluation), RLHF guardrail limitations\n",
      "5. Reproducibility enhancement: Open-weight model emphasis, detailed methodology reporting, decreasing open/closed model gap (GPT-4o vs. Llama3-70B)\n",
      "6. Calibration necessity highlight: Task-specific LLM calibration against human judgments pre-deployment\n",
      "7. Human vs. machine-generated text evaluation insights: Better model alignment with human judgments on human language\n",
      "8. Computational resource benchmarking: GPU utilization metrics for large-scale LLM evaluation studies\n",
      "9. Model reliability quantification: Valid response rate analysis informing model selection/refinement strategies\n",
      "\n",
      "JUDGE-BENCH provides a foundational framework for systematic LLM evaluation capability assessment, potentially accelerating automated NLP assessment methodologies while exposing current limitations/risks. The study's comprehensive approach and granular insights offer a nuanced understanding of LLM evaluation performance across diverse NLP tasks, guiding future research/development efforts. Inclusion of computational resource metrics and valid response rate analysis enhances benchmark utility for practical implementation and model comparison.\n",
      "\n",
      "Iteration 7:\n",
      "New important technical entities/ideas:\n",
      "1. Cross-domain generalizability assessment\n",
      "2. Model-specific bias detection\n",
      "3. Multidimensional performance matrix\n",
      "\n",
      "Summary:\n",
      "\n",
      "JUDGE-BENCH: comprehensive NLP evaluation benchmark assessing LLM efficacy as automated judges.\n",
      "\n",
      "Methodologies:\n",
      "1. Dataset curation: 20 diverse NLP datasets (varied tasks, categorical/graded annotations, expert/non-expert annotators)\n",
      "2. Model selection: 11 SOTA open-weight/proprietary LLMs (GPT-4o, Llama3-70B, Mixtral-8x22B)\n",
      "3. Prompt engineering: Zero-shot learning approach utilizing original human annotation instructions\n",
      "4. Inference procedure: Greedy decoding operationalization (temperature=0), max 25/5 tokens (open/proprietary)\n",
      "5. Evaluation protocol:\n",
      "   a. Cohen's  (categorical)\n",
      "   b. Spearman's correlation (graded)\n",
      "   c. Invalid response mitigation: Random value substitution\n",
      "   d. Krippendorff's  (inter-rater agreement)\n",
      "6. Data leakage mitigation: Tracking/reporting LLM-dataset training overlap\n",
      "7. Computational resources: A100 GPU, 125.22 compute hours\n",
      "8. Valid response rate analysis: Model reliability quantification across datasets\n",
      "9. Cross-domain generalizability assessment: Performance evaluation across diverse NLP tasks\n",
      "10. Model-specific bias detection: Identification of systematic deviations from human judgments\n",
      "11. Multidimensional performance matrix: Granular analysis of LLM capabilities across tasks/properties\n",
      "\n",
      "Novel contributions:\n",
      "1. Large-scale empirical study: 20 datasets, surpassing previous work in diversity/comprehensiveness\n",
      "2. Comparative analysis framework:\n",
      "   a. LLM vs. expert/non-expert human annotators\n",
      "   b. Granular cross-model analysis across NLP tasks/quality dimensions\n",
      "3. Task-specific performance insights: RLHF guardrail impact on toxicity/safety evaluations identified\n",
      "4. Living benchmark: Continuous updates facilitation, longitudinal LLM progress studies\n",
      "5. Cross-domain generalizability quantification: Systematic assessment of LLM performance variability\n",
      "6. Model-specific bias identification: Targeted analysis of systematic judgment deviations\n",
      "7. Multidimensional performance characterization: Comprehensive LLM capability mapping\n",
      "\n",
      "Impact:\n",
      "1. Empirical evidence base: Comprehensive quantitative metrics on LLM limitations in replicating human judgments\n",
      "2. Benchmark standardization: Unified framework for future LLM evaluation capability assessment\n",
      "3. Methodological refinement: Prompt engineering insights, evaluation protocols, system prompt limitations, task-specific calibration necessity\n",
      "4. Research direction guidance: Underperforming areas identification (e.g., toxicity/safety evaluation), RLHF guardrail limitations\n",
      "5. Reproducibility enhancement: Open-weight model emphasis, detailed methodology reporting, decreasing open/closed model gap (GPT-4o vs. Llama3-70B)\n",
      "6. Calibration necessity highlight: Task-specific LLM calibration against human judgments pre-deployment\n",
      "7. Human vs. machine-generated text evaluation insights: Better model alignment with human judgments on human language\n",
      "8. Computational resource benchmarking: GPU utilization metrics for large-scale LLM evaluation studies\n",
      "9. Model reliability quantification: Valid response rate analysis informing model selection/refinement strategies\n",
      "10. Cross-domain performance variability elucidation: Identification of task-specific LLM strengths/weaknesses\n",
      "11. Model-specific bias mitigation strategies: Targeted refinement of LLM judgment capabilities\n",
      "12. Multidimensional LLM capability mapping: Granular insights for task-specific model selection/optimization\n",
      "\n",
      "JUDGE-BENCH provides a foundational framework for systematic LLM evaluation capability assessment, potentially accelerating automated NLP assessment methodologies while exposing current limitations/risks. The study's comprehensive approach and granular insights offer a nuanced understanding of LLM evaluation performance across diverse NLP tasks, guiding future research/development efforts. Inclusion of computational resource metrics, valid response rate analysis, cross-domain generalizability assessment, model-specific bias detection, and multidimensional performance characterization enhances benchmark utility for practical implementation, model comparison, and targeted improvement strategies.\n",
      "\n",
      "Iteration 8:\n",
      "New important technical entities/ideas:\n",
      "1. Zero-shot prompt engineering\n",
      "2. RLHF guardrail impact\n",
      "3. Task-specific LLM calibration\n",
      "\n",
      "Summary:\n",
      "\n",
      "JUDGE-BENCH: NLP evaluation benchmark assessing LLM efficacy as automated judges.\n",
      "\n",
      "Key methodologies:\n",
      "1. Dataset curation: 20 diverse NLP datasets (varied tasks, categorical/graded annotations, expert/non-expert annotators)\n",
      "2. Model selection: 11 SOTA open-weight/proprietary LLMs (GPT-4o, Llama3-70B, Mixtral-8x22B)\n",
      "3. Zero-shot prompt engineering: Utilization of original human annotation instructions\n",
      "4. Inference procedure: Greedy decoding (temperature=0), max 25/5 tokens (open/proprietary)\n",
      "5. Evaluation protocol:\n",
      "   a. Cohen's  (categorical)\n",
      "   b. Spearman's correlation (graded)\n",
      "   c. Invalid response mitigation: Random value substitution\n",
      "   d. Krippendorff's  (inter-rater agreement)\n",
      "6. Data leakage mitigation: LLM-dataset training overlap tracking\n",
      "7. Computational resources: A100 GPU, 125.22 compute hours\n",
      "8. Valid response rate analysis: Model reliability quantification\n",
      "9. Cross-domain generalizability assessment: Performance evaluation across NLP tasks\n",
      "10. Model-specific bias detection: Systematic deviation identification\n",
      "11. Multidimensional performance matrix: Granular LLM capability analysis\n",
      "\n",
      "Novel contributions:\n",
      "1. Large-scale empirical study: 20 datasets, surpassing previous work in diversity/comprehensiveness\n",
      "2. Comparative analysis framework:\n",
      "   a. LLM vs. expert/non-expert human annotators\n",
      "   b. Granular cross-model analysis across NLP tasks/quality dimensions\n",
      "3. Task-specific performance insights: RLHF guardrail impact on toxicity/safety evaluations\n",
      "4. Living benchmark: Continuous updates facilitation, longitudinal LLM progress studies\n",
      "5. Cross-domain generalizability quantification: Systematic LLM performance variability assessment\n",
      "6. Model-specific bias identification: Targeted analysis of systematic judgment deviations\n",
      "7. Multidimensional performance characterization: Comprehensive LLM capability mapping\n",
      "\n",
      "Impact:\n",
      "1. Empirical evidence base: Quantitative metrics on LLM limitations in replicating human judgments\n",
      "2. Benchmark standardization: Unified framework for LLM evaluation capability assessment\n",
      "3. Methodological refinement: Prompt engineering insights, evaluation protocols, system prompt limitations, task-specific LLM calibration necessity\n",
      "4. Research direction guidance: Underperforming areas identification (e.g., toxicity/safety evaluation), RLHF guardrail limitations\n",
      "5. Reproducibility enhancement: Open-weight model emphasis, decreasing open/closed model gap (GPT-4o vs. Llama3-70B)\n",
      "6. Human vs. machine-generated text evaluation insights: Better model alignment with human judgments on human language\n",
      "7. Computational resource benchmarking: GPU utilization metrics for large-scale LLM evaluation\n",
      "8. Model reliability quantification: Valid response rate analysis informing model selection/refinement\n",
      "9. Cross-domain performance variability elucidation: Task-specific LLM strengths/weaknesses identification\n",
      "10. Model-specific bias mitigation strategies: Targeted refinement of LLM judgment capabilities\n",
      "11. Multidimensional LLM capability mapping: Granular insights for task-specific model selection/optimization\n",
      "\n",
      "JUDGE-BENCH provides a foundational framework for systematic LLM evaluation capability assessment, potentially accelerating automated NLP assessment methodologies while exposing current limitations/risks. The study's comprehensive approach and granular insights offer a nuanced understanding of LLM evaluation performance across diverse NLP tasks, guiding future research/development efforts. Inclusion of computational resource metrics, valid response rate analysis, cross-domain generalizability assessment, model-specific bias detection, and multidimensional performance characterization enhances benchmark utility for practical implementation, model comparison, and targeted improvement strategies.\n",
      "\n",
      "Final Summary:\n",
      "Key methodologies of JUDGE-BENCH:\n",
      "1. Dataset curation: 20 diverse NLP datasets\n",
      "2. Model selection: 11 SOTA LLMs (GPT-4o, Llama3-70B, Mixtral-8x22B)\n",
      "3. Zero-shot prompt engineering using original annotation instructions\n",
      "4. Evaluation: Cohen's  (categorical), Spearman's correlation (graded), Krippendorff's  (inter-rater agreement)\n",
      "5. Data leakage mitigation and computational resource tracking (125.22 A100 GPU hours)\n",
      "6. Valid response rate analysis, cross-domain generalizability assessment, model-specific bias detection\n",
      "\n",
      "Novel contributions:\n",
      "1. Large-scale empirical study (20 datasets) surpassing previous work in diversity\n",
      "2. Comparative analysis: LLM vs. expert/non-expert annotators, granular cross-model analysis\n",
      "3. Task-specific insights: RLHF guardrail impact on toxicity/safety evaluations\n",
      "4. Living benchmark for longitudinal LLM progress studies\n",
      "5. Cross-domain generalizability quantification\n",
      "6. Model-specific bias identification\n",
      "7. Multidimensional performance characterization\n",
      "\n",
      "Potential impact:\n",
      "1. Empirical evidence base for LLM limitations in replicating human judgments\n",
      "2. Standardized framework for LLM evaluation capability assessment\n",
      "3. Methodological refinement: prompt engineering, evaluation protocols, task-specific calibration\n",
      "4. Identification of underperforming areas (e.g., toxicity/safety evaluation)\n",
      "5. Reproducibility enhancement through open-weight model emphasis\n",
      "6. Insights into human vs. machine-generated text evaluation\n",
      "7. Computational resource benchmarking for large-scale LLM evaluation\n",
      "8. Model reliability quantification via valid response rate analysis\n",
      "9. Elucidation of cross-domain performance variability\n",
      "10. Targeted refinement strategies for model-specific biases\n",
      "11. Granular insights for task-specific model selection/optimization\n",
      "\n",
      "JUDGE-BENCH provides a comprehensive framework for assessing LLM evaluation capabilities, potentially accelerating automated NLP assessment while exposing limitations and risks. The study's multifaceted approach offers nuanced understanding of LLM performance across diverse NLP tasks, guiding future research and development efforts in the field.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m1\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'quality_scorer'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'iteration_summaries_analysis_long_tail_stats_relevance_mean'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.46875</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'iteration_summaries_analysis_long_tail_stats_relevance_tail_ratio'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.118881118881119</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'iteration_summaries_analysis_long_tail_stats_technical_quality_mean'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.46875</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'iteration_summaries_analysis_long_tail_stats_technical_quality_tail_ratio'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.118881118881119</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'iteration_summaries_analysis_long_tail_stats_conciseness_mean'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.5625</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'iteration_summaries_analysis_long_tail_stats_conciseness_tail_ratio'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.1228070175438596</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'accumulated_summary_relevance_score'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.5</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'accumulated_summary_technical_quality_score'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'accumulated_summary_conciseness_score'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'final_summary_relevance_score'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'final_summary_technical_quality_score'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'final_summary_conciseness_score'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.0</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">169.94676089286804</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'quality_scorer'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'iteration_summaries_analysis_long_tail_stats_relevance_mean'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m4.46875\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'iteration_summaries_analysis_long_tail_stats_relevance_tail_ratio'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m1.118881118881119\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'iteration_summaries_analysis_long_tail_stats_technical_quality_mean'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m4.46875\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'iteration_summaries_analysis_long_tail_stats_technical_quality_tail_ratio'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m1.118881118881119\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'iteration_summaries_analysis_long_tail_stats_conciseness_mean'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m3.5625\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'iteration_summaries_analysis_long_tail_stats_conciseness_tail_ratio'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m1.1228070175438596\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'accumulated_summary_relevance_score'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m4.5\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'accumulated_summary_technical_quality_score'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m4.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'accumulated_summary_conciseness_score'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m3.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'final_summary_relevance_score'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m5.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'final_summary_technical_quality_score'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m5.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'final_summary_conciseness_score'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m4.0\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m169.94676089286804\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation = weave.Evaluation(dataset=dataset, scorers=[quality_scorer])\n",
    "for model in models:\n",
    "    arxiv_chain_of_density_pipeline = ArxivChainOfDensityPipeline(model=model, density_iterations=8)\n",
    "    await evaluation.evaluate(arxiv_chain_of_density_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Deeper Analysis of Summary Refinement using Chunking\n",
    "\n",
    "This section introduces an advanced technique to enhance the Chain of Density (CoD) summarization process for ArXiv PDFs. By incorporating a chunking mechanism, we can handle longer documents more effectively and potentially improve the quality of our summaries. Here's an overview of the augmented CoD summarization process:\n",
    "\n",
    "### Augmented Chain of Density Summarization\n",
    "\n",
    "1. **Chunk the Text**: We split the input text into manageable chunks using the `chunk_text` function. This function intelligently handles document structure, including image descriptions.\n",
    "\n",
    "2. **Iterative Chunk Summarization**: For each chunk, we apply the CoD process using `summarize_chunk`. This creates summaries for individual sections of the document.\n",
    "\n",
    "3. **Combine Chunk Summaries**: We use `summarize_chunk_summaries` to integrate information from all chunk summaries into a cohesive whole.\n",
    "\n",
    "4. **Iterative Refinement**: We repeat steps 2-3 for a specified number of iterations (`chunk_iterations`) to progressively refine the summary.\n",
    "\n",
    "5. **Final Density Pass**: After chunk-based summarization, we apply the standard CoD process to further refine and densify the final summary.\n",
    "\n",
    "Key components of this process include:\n",
    "\n",
    "- `chunk_text`: Splits the document into manageable pieces.\n",
    "- `summarize_chunk`: Applies CoD to individual chunks.\n",
    "- `summarize_chunk_summaries`: Combines chunk summaries.\n",
    "- `summarize_chunk_iteration`: Manages the iteration process for chunk summarization.\n",
    "- `iterative_chunk_summarization`: Orchestrates the entire chunk-based summarization process.\n",
    "- `chain_of_density_summarization`: Integrates chunking with the final CoD refinement.\n",
    "\n",
    "This augmented approach allows us to:\n",
    "1. Handle longer documents more effectively.\n",
    "2. Potentially capture more nuanced information from different parts of the paper.\n",
    "3. Provide a more comprehensive summary that considers the entire document structure.\n",
    "\n",
    "The `ArxivChainOfDensityPipeline` class has been updated to incorporate these new features, allowing for easy experimentation with different chunk sizes and iteration counts.\n",
    "\n",
    "To evaluate the effectiveness of this approach, we've also extended our `quality_scorer` function to analyze the chunk-based summaries separately. This gives us insights into how the chunking process affects summary quality at different stages of the pipeline.\n",
    "\n",
    "By using this augmented CoD approach, we aim to create more comprehensive and accurate summaries of ArXiv papers, especially for longer or more complex documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def chunk_text(text, chunk_size):\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "        if len(current_chunk) + len(line) > chunk_size:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = \"\"\n",
    "        \n",
    "        current_chunk += line + \"\\n\"\n",
    "        \n",
    "        if line.startswith(\"[Image Descriptions for page\"):\n",
    "            if current_chunk.strip():\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = \"\"\n",
    "            \n",
    "            image_descriptions = line + \"\\n\"\n",
    "            i += 1\n",
    "            while i < len(lines) and not lines[i].startswith(\"[END OF IMAGE DESCRIPTIONS]\"):\n",
    "                image_descriptions += lines[i] + \"\\n\"\n",
    "                i += 1\n",
    "            if i < len(lines):\n",
    "                image_descriptions += lines[i] + \"\\n\"\n",
    "            \n",
    "            chunks.append(image_descriptions.strip())\n",
    "            current_chunk = \"\"\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    combined_chunks = []\n",
    "    current_combined_chunk = \"\"\n",
    "    for chunk in chunks:\n",
    "        if len(current_combined_chunk) + len(chunk) <= chunk_size:\n",
    "            current_combined_chunk += chunk + \"\\n\\n\"\n",
    "        else:\n",
    "            if current_combined_chunk:\n",
    "                combined_chunks.append(current_combined_chunk.strip())\n",
    "            current_combined_chunk = chunk + \"\\n\\n\"\n",
    "    \n",
    "    if current_combined_chunk:\n",
    "        combined_chunks.append(current_combined_chunk.strip())\n",
    "\n",
    "    return combined_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def summarize_chunk(chunk, instruction, current_summary=\"\", iteration=1, model=\"claude-3-5-sonnet-20240620\"):\n",
    "    prompt = f\"\"\"Current summary:\n",
    "    {current_summary}\n",
    "\n",
    "    New information:\n",
    "    {chunk}\n",
    "\n",
    "    Instruction to focus on: {instruction}\n",
    "\n",
    "    Iteration: {iteration}\n",
    "\n",
    "    Create an extremely dense, highly technical summary that specifically addresses the given instruction. Follow these steps:\n",
    "\n",
    "    1. Identify 3-5 key technical points from the new information that are directly relevant to the instruction, prioritizing:\n",
    "    - Novel methodologies or algorithms related to the instruction\n",
    "    - Specific quantitative results or metrics that address the instruction\n",
    "    - Detailed experimental setups or parameters pertinent to the instruction\n",
    "    - Precise definitions of domain-specific concepts mentioned in the instruction\n",
    "    - Critical limitations or assumptions in the research that affect the instruction\n",
    "\n",
    "    2. Integrate these points with the current summary, ensuring:\n",
    "    - Direct relevance to the instruction at hand\n",
    "    - No redundancy or oversimplification\n",
    "    - Preservation of technical nuances and complexities specific to the instruction\n",
    "    - Inclusion of relevant equations, formulas, or mathematical notations that help address the instruction\n",
    "    - Accurate representation of statistical significance and error margins for instruction-related data\n",
    "\n",
    "    3. Rephrase the combined information to maximize information density while maintaining focus on the instruction:\n",
    "    - Use domain-specific terminology and jargon without simplification, as relevant to the instruction\n",
    "    - Maintain the level of detail expected in a PhD-level discourse on the specific topic of the instruction\n",
    "    - Incorporate precise citations or references where applicable to support the response\n",
    "    - Preserve any conflicting viewpoints or ongoing debates in the field that relate to the instruction\n",
    "\n",
    "    4. With each iteration, aim to increase information density by 30-40% without sacrificing technical accuracy or critical details that address the instruction.\n",
    "\n",
    "    5. Ensure the summary includes instruction-specific:\n",
    "    - Methodological details (e.g., exact algorithms, parameter settings) that are crucial to addressing the instruction\n",
    "    - Precise quantitative results with appropriate units and error bounds that directly relate to the instruction\n",
    "    - Detailed descriptions of novel techniques or approaches that are key to addressing the instruction\n",
    "    - Critical analysis of strengths and limitations in the research as they pertain to the instruction\n",
    "\n",
    "    Produce a summary that is significantly more information-dense and technically precise than the previous one, while remaining laser-focused on addressing the given instruction. Use language appropriate for a highly specialized audience in the field.\"\"\"\n",
    "\n",
    "    response = anthropic_client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=4096,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def summarize_chunk_summaries(instruction, current_summary, chunk_summaries, model=\"claude-3-opus-20240229\"):\n",
    "    return anthropic_client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=4096,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"Given this current summary:\n",
    "\n",
    "    {current_summary}\n",
    "\n",
    "    And these chunk summaries:\n",
    "\n",
    "    {' '.join(chunk_summaries)}\n",
    "\n",
    "    And this instruction to focus on:\n",
    "\n",
    "    {instruction}\n",
    "\n",
    "    Create an extremely dense, final summary that refines the current summary by incorporating key information from the chunk summaries, while specifically addressing the given instruction. Follow these guidelines:\n",
    "\n",
    "    1. Integrate the most relevant and important information from the chunk summaries into the current summary.\n",
    "    2. Ensure all key technical content from both the current summary and chunk summaries that relates to the instruction is retained.\n",
    "    3. Aim to reduce overall length by 30-40% while increasing information density.\n",
    "    4. Prioritize highly specific methodologies, algorithms, metrics, and findings that directly address the instruction.\n",
    "    5. Preserve precise quantitative data, including statistical significance and error margins where applicable and relevant to the instruction.\n",
    "    6. Maintain the use of domain-specific terminology and technical jargon pertinent to the instruction.\n",
    "    7. Use compact phrasing and remove any remaining non-essential information that doesn't directly contribute to addressing the instruction.\n",
    "    8. If relevant to the instruction, include brief mentions of limitations, assumptions, or conflicting viewpoints from across all summaries.\n",
    "    9. Optimize for information density while maintaining coherence for an expert audience, always keeping the focus on the given instruction.\n",
    "\n",
    "    The final summary should be a highly concentrated, technical distillation of all provided summaries that specifically addresses the given instruction, suitable for specialists in the field.\"\"\",\n",
    "                }\n",
    "            ],\n",
    "    ).content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def summarize_chunk_iteration(chunks, instruction, current_summary, iteration, model):\n",
    "    chunk_summaries = []\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        current_summary = summarize_chunk(chunk, instruction, current_summary, iteration, model)\n",
    "        chunk_summaries.append(current_summary)\n",
    "        print(f\"Iteration {iteration}, Chunk {i}:\\n{current_summary}\\n\")\n",
    "    current_summary = summarize_chunk_summaries(instruction, current_summary, chunk_summaries, model)\n",
    "    print(f\"Iteration {iteration}, Final Summary:\\n{current_summary}\\n\")\n",
    "    return current_summary, chunk_summaries\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def iterative_chunk_summarization(chunks, instruction, current_summary, chunk_iterations, model):\n",
    "    chunk_iteration_summaries = []\n",
    "    chunk_summaries = []\n",
    "    for iteration in range(1, chunk_iterations + 1):\n",
    "        current_summary, iteration_chunk_summaries = summarize_chunk_iteration(chunks, instruction, current_summary, iteration, model)\n",
    "        chunk_iteration_summaries.append(current_summary)\n",
    "        chunk_summaries.append(iteration_chunk_summaries)\n",
    "    return current_summary, chunk_iteration_summaries, chunk_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def chain_of_density_summarization(instruction, text, model=\"claude-3-5-sonnet-20240620\", chunk_size=8192, chunk_iterations=2, density_iterations=2):\n",
    "    chunks = chunk_text(text, chunk_size)\n",
    "    print(f\"Number of chunks: {len(chunks)}\")\n",
    "    print(f\"Chunk sizes: {[len(chunk) for chunk in chunks]}\")\n",
    "\n",
    "    current_summary, chunk_iteration_summaries, chunk_summaries = iterative_chunk_summarization(chunks, instruction, \"\", chunk_iterations, model)\n",
    "    current_summary, iteration_summaries = iterative_density_summarization(instruction, current_summary, density_iterations, model)\n",
    "    final_summary_text = final_summary(instruction, current_summary, model)\n",
    "    print(f\"Final Summary:\\n{final_summary_text}\\n\")\n",
    "\n",
    "    return {\n",
    "        \"final_summary\": final_summary_text,\n",
    "        \"accumulated_summary\": current_summary,\n",
    "        \"iteration_summaries\": iteration_summaries,\n",
    "        \"chunk_iteration_summaries\": chunk_iteration_summaries,\n",
    "        \"chunk_summaries\": chunk_summaries\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArxivChainOfDensityPipeline(weave.Model):\n",
    "\n",
    "    model: str = \"claude-3-5-sonnet-20240620\"\n",
    "    chunk_size: int = 20000\n",
    "    chunk_iterations: int = 1\n",
    "    density_iterations: int = 3\n",
    "    use_cache: bool = False\n",
    "    cache: dict = {}\n",
    "\n",
    "    def __init__(self, model: str = \"claude-3-5-sonnet-20240620\", chunk_size: int = 4000, chunk_iterations: int = 1, density_iterations: int = 3, use_cache: bool = False):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_iterations = chunk_iterations\n",
    "        self.density_iterations = density_iterations\n",
    "        self.use_cache = use_cache\n",
    "        if use_cache:\n",
    "            self.cache = {}\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, paper: ArxivPaper, instruction: str) -> dict:\n",
    "\n",
    "        if self.use_cache:\n",
    "            cache_key = (paper.entry_id, instruction)\n",
    "            if cache_key in self.cache:\n",
    "                return self.cache[cache_key]\n",
    "        \n",
    "        extracted_images = extract_images(paper)\n",
    "        cleaned_text = replace_images_with_descriptions(paper, extracted_images)\n",
    "        result = chain_of_density_summarization(instruction, cleaned_text, model=self.model, chunk_size=self.chunk_size, chunk_iterations=self.chunk_iterations, density_iterations=self.density_iterations)\n",
    "        \n",
    "        if self.use_cache:\n",
    "            self.cache[cache_key] = result\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk_summaries(model_output, instruction, model):\n",
    "    scores = {}\n",
    "    for i, chunk_list in enumerate(model_output[\"chunk_summaries\"]):\n",
    "        chunk_summary_scores = []\n",
    "        for j, summary in enumerate(chunk_list):\n",
    "            chunk_summary_score = score_summary(summary, f\"Chunk Summary {i+1}.{j+1}\", instruction, model)\n",
    "            chunk_summary_scores.append(chunk_summary_score)\n",
    "        scores[f\"chunk_summaries_analysis_{i+1}\"] = {\n",
    "            \"long_tail_stats\": calculate_long_tail_stats(chunk_summary_scores),\n",
    "            \"iteration_impact\": analyze_iteration_impact(chunk_summary_scores),\n",
    "            \"optimal_improvement_range\": find_optimal_improvement_range(chunk_summary_scores),\n",
    "            \"optimal_score_range\": find_optimal_score_range(chunk_summary_scores)\n",
    "        }\n",
    "    return scores\n",
    "\n",
    "\n",
    "def process_chunk_iteration_summaries(model_output, instruction, model):\n",
    "    chunk_iteration_scores = [score_summary(summary, f\"Chunk Iteration Summary {i+1}\", instruction, model)\n",
    "                            for i, summary in enumerate(model_output[\"chunk_iteration_summaries\"])]\n",
    "    return {\n",
    "        \"long_tail_stats\": calculate_long_tail_stats(chunk_iteration_scores),\n",
    "        \"iteration_impact\": analyze_iteration_impact(chunk_iteration_scores),\n",
    "        \"optimal_improvement_range\": find_optimal_improvement_range(chunk_iteration_scores),\n",
    "        \"optimal_score_range\": find_optimal_score_range(chunk_iteration_scores)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def quality_scorer(instruction, model_output, model=\"gpt-4o\"):\n",
    "    scores = {\n",
    "        \"chunk_summaries_analysis\": {},\n",
    "        \"chunk_iteration_summaries_analysis\": {},\n",
    "        \"iteration_summaries_analysis\": {},\n",
    "        \"accumulated_summary\": {},\n",
    "        \"final_summary\": {}\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Process chunk summaries\n",
    "        chunk_summaries_scores = process_chunk_summaries(model_output, instruction, model)\n",
    "        scores.update(chunk_summaries_scores)\n",
    "\n",
    "        # Process chunk iteration summaries\n",
    "        scores[\"chunk_iteration_summaries_analysis\"] = process_chunk_iteration_summaries(model_output, instruction, model)\n",
    "\n",
    "        # Process iteration summaries\n",
    "        scores[\"iteration_summaries_analysis\"] = process_iteration_summaries(model_output, instruction, model)\n",
    "\n",
    "        # Score accumulated summary\n",
    "        scores[\"accumulated_summary\"] = score_summary(model_output[\"accumulated_summary\"], \"Accumulated Summary\", instruction, model)\n",
    "\n",
    "        # Score final summary\n",
    "        scores[\"final_summary\"] = score_summary(model_output[\"final_summary\"], \"Final Summary\", instruction, model)\n",
    "\n",
    "        # After calculating all scores\n",
    "        flattened_scores = {}\n",
    "        for key, value in scores.items():\n",
    "            if isinstance(value, dict):\n",
    "                flattened_scores[key] = flatten_dict(value)\n",
    "            else:\n",
    "                flattened_scores[key] = value\n",
    "    \n",
    "        scores = flatten_dict(flattened_scores)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in quality_scorer: {str(e)}\")\n",
    "        scores[\"error\"] = str(e)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation = weave.Evaluation(dataset=dataset, scorers=[quality_scorer])\n",
    "# for model in models:\n",
    "#     arxiv_chain_of_density_pipeline = ArxivChainOfDensityPipeline(model=model)\n",
    "#     await evaluation.evaluate(arxiv_chain_of_density_pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
