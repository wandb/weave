{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36a9f92a",
   "metadata": {},
   "source": [
    "# W&B Production Monitoring Overview\n",
    "In this notebooks you will learn to monitor your production models with W&B through an illustrative example. We will train a model to correctly identify handwritten digits, then monitor a locally deployed version of the model.\n",
    "\n",
    "_Note: To keep the example focused on important code, much of the dataset manipulation, modelling, and other utilities are packaged in local files and imported here_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4c25c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2427bb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daaf18d",
   "metadata": {},
   "source": [
    "## Step 1: Get Data\n",
    "In this example, we will use `keras.datasets.mnist.load_data()` to load in the MNIST dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de553636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_util\n",
    "\n",
    "dataset = model_util.get_dataset()\n",
    "model_util.image_from_array(dataset[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6822a544",
   "metadata": {},
   "source": [
    "## Step 2: Train Model\n",
    "Next we will train a classic NN to predict the digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bf1bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_util.train_model(*dataset, conv_layers=0, epochs=1) # 1 epoch so we can actually see some errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a38ddd",
   "metadata": {},
   "source": [
    "# Step 3: Query Model\n",
    "Now, let's query the model! Normally there is a little pre- and post- processing needed to make a prediction - we will write a short function to handle this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8afe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def simple_predict(image_arr):\n",
    "    # Prepare image for model\n",
    "    tensor = (image_arr.astype(\"float32\")).reshape(1, 28, 28, 1)\n",
    "\n",
    "    # Make the prediction\n",
    "    prediction = model.predict(tensor, verbose=False)\n",
    "\n",
    "    # In this application, we need to reshape the output:\n",
    "    raw_predictions = prediction[0].tolist()\n",
    "    logits = {\n",
    "        str(k): v for k, v in zip(range(10), raw_predictions)\n",
    "    }\n",
    "    \n",
    "    prediction = np.argmax(raw_predictions).tolist()\n",
    "    \n",
    "    return {\"logits\": logits, \"prediction\": prediction}\n",
    "\n",
    "_, _, x_test, y_test = dataset\n",
    "for i in range(10):\n",
    "    image_arr = x_test[i]\n",
    "    truth = y_test[i]\n",
    "    preds = simple_predict(image_arr)\n",
    "    \n",
    "    print(f\"Input: {truth}\")\n",
    "    display(model_util.image_from_array(image_arr))\n",
    "    print(f\"Prediction: {preds['prediction']}\")\n",
    "    print(f\"Logits: {json.dumps(preds['logits'], indent=2)}\")\n",
    "    print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8826acf8",
   "metadata": {},
   "source": [
    "# Step 3a: Save Predictions with W&B Weave using StreamTable\n",
    "With W&B's Weave library, we can stream any data to W&B for storage and further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4975eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this if you are running Weave locally (as opposed to installing from Pypy)\n",
    "import weave\n",
    "weave.use_frontend_devmode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c59bccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from weave.monitoring import StreamTable\n",
    "\n",
    "# Initialize a stream table\n",
    "st = StreamTable(\"timssweeney/demos/mnist_monitoring_example_4\")\n",
    "_, _, x_test, y_test = dataset\n",
    "for i in range(100):\n",
    "    image_arr = x_test[i]\n",
    "    truth = y_test[i].tolist()\n",
    "    preds = simple_predict(image_arr)\n",
    "    \n",
    "    # Log the data\n",
    "    st.log({\n",
    "        **preds,\n",
    "        \"image\": model_util.image_from_array(image_arr),\n",
    "        \"truth\": truth\n",
    "    })\n",
    "\n",
    "# Optional: wait for the logs to finish uploading (nicer for live demos)\n",
    "st.finish()\n",
    "\n",
    "# Show the StreamTable\n",
    "st    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb0cc29",
   "metadata": {},
   "source": [
    "# Step 3a: Save Predictions with W&B Weave using `monitor` decorator\n",
    "The pattern of logging inputs and outputs of a functions is so common, that we provide a decorator which automatically logs the functions i/o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ab5fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from weave.monitoring import monitor\n",
    "import numpy as np\n",
    "\n",
    "@monitor(\n",
    "    entity_name=\"timssweeney\", project_name=\"prodmon\",\n",
    "    # Setting `auto_log=False` will return a PredictionRecord which \n",
    "    # supports additional data being added (in this case the ground truth)\n",
    "    auto_log = False,\n",
    "    # An preprocessor allows the function arguments to be pre-processed before logging.\n",
    "    input_preprocessor = lambda image_arr: {\"image\": model_util.image_from_array(image_arr)}\n",
    ")\n",
    "def monitor_predict(image_arr):\n",
    "    # Prepare image for model\n",
    "    tensor = (image_arr.astype(\"float32\")).reshape(1, 28, 28, 1)\n",
    "\n",
    "    # Make the prediction\n",
    "    prediction = model.predict(tensor, verbose=False)\n",
    "\n",
    "    # In this application, we need to reshape the output:\n",
    "    raw_predictions = prediction[0].tolist()\n",
    "    logits = {\n",
    "        str(k): v for k, v in zip(range(10), raw_predictions)\n",
    "    }\n",
    "    \n",
    "    prediction = np.argmax(raw_predictions).tolist()\n",
    "    \n",
    "    return {\"logits\": logits, \"prediction\": prediction}\n",
    "\n",
    "_, _, x_test, y_test = dataset\n",
    "for i in range(100):\n",
    "    image_arr = x_test[i]\n",
    "    truth = y_test[i].tolist()\n",
    "    preds = monitor_predict(image_arr)\n",
    "    \n",
    "    # Now the output of `monitored_predict` is a `PredictionRecord`\n",
    "    preds.add_data({\"truth\": truth})\n",
    "    preds.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3896993",
   "metadata": {},
   "source": [
    "# Step 4: End to End Example\n",
    "Typically a production application will contain a prediction service that provides predictions to a client. To demonstrate this in a notebook, we will create a `PredictionService` and an `AppUI` (a small interface to allow the user to draw an image, view the prediction, and give feedback on results) that communicate via `predict` and `record_feedback` methods. \n",
    "\n",
    "_Note: this is just for exmaple purposes, your production systems may widely vary in structure_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc97d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import app_util\n",
    "from weave.monitoring import monitor\n",
    "import PIL\n",
    "import numpy as np\n",
    "\n",
    "class PredictionService(app_util.PredictionServiceInterface):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.last_prediction = {}\n",
    "    \n",
    "    @monitor(auto_log = False,  entity_name=\"timssweeney\", project_name=\"prodmon\")\n",
    "    def _raw_predict(self, pil_image: PIL.Image) -> dict:\n",
    "        # Prepare image for model\n",
    "        tensor = (np.array(pil_image.resize((28, 28))).astype(\"float32\") / 255).reshape(1, 28, 28, 1)\n",
    "\n",
    "        # Make the prediction\n",
    "        prediction = self.model.predict(tensor, verbose=False)\n",
    "\n",
    "        # In this application, we need to reshape the output:\n",
    "        raw_predictions = prediction[0].tolist()\n",
    "        logits = {\n",
    "            str(k): v for k, v in zip(range(10), raw_predictions)\n",
    "        }\n",
    "\n",
    "        prediction = np.argmax(raw_predictions).tolist()\n",
    "\n",
    "        return {\"logits\": logits, \"prediction\": prediction}\n",
    "    \n",
    "    def _update_last_prediction(self, prediction) -> None:\n",
    "        if len(self.last_prediction) > 0:\n",
    "            last_pred = self.last_prediction.pop(list(self.last_prediction.keys())[0])\n",
    "            last_pred.finalize()\n",
    "        self.last_prediction[prediction.id] = prediction\n",
    "\n",
    "    \n",
    "    def predict(self, pil_image: PIL.Image) -> app_util.Prediction:\n",
    "        record = self._raw_predict(pil_image)\n",
    "        \n",
    "        # Cache the last prediction for ground_truth recording\n",
    "        self._update_last_prediction(record)\n",
    "        \n",
    "        # Return the prediction\n",
    "        return app_util.Prediction(record.get()['logits'], record.id)\n",
    "    \n",
    "    def record_feedback(self, prediction_id: str, feedback: int) -> None:\n",
    "        if prediction_id not in self.last_prediction:\n",
    "            return\n",
    "\n",
    "        # Get the past prediction\n",
    "        prediction = self.last_prediction.pop(prediction_id)\n",
    "        \n",
    "        # Save the user feedback\n",
    "        prediction.add_data({'user_feedback': feedback})\n",
    "        \n",
    "        # Log the results\n",
    "        prediction.finalize()\n",
    "        \n",
    "app_util.render_app(PredictionService(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ee0f74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
