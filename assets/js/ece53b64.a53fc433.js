"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[5528],{77430:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>d});var r=t(85893),s=t(11151);const o={},a="Instructor",i={id:"guides/integrations/instructor",title:"Instructor",description:"Instructor is a lightweight library that makes it easy to get structured data like JSON from LLMs.",source:"@site/docs/guides/integrations/instructor.md",sourceDirName:"guides/integrations",slug:"/guides/integrations/instructor",permalink:"/guides/integrations/instructor",draft:!1,unlisted:!1,editUrl:"https://github.com/wandb/weave/blob/master/docs/docs/guides/integrations/instructor.md",tags:[],version:"current",lastUpdatedAt:1738860822e3,frontMatter:{},sidebar:"documentationSidebar",previous:{title:"DSPy",permalink:"/guides/integrations/dspy"},next:{title:"Platform & Security",permalink:"/guides/platform/"}},c={},d=[{value:"Tracing",id:"tracing",level:2},{value:"Track your own ops",id:"track-your-own-ops",level:2},{value:"Create a <code>Model</code> for easier experimentation",id:"create-a-model-for-easier-experimentation",level:2},{value:"Serving a Weave Model",id:"serving-a-weave-model",level:2}];function l(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",img:"img",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,s.a)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"instructor",children:"Instructor"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://python.useinstructor.com/",children:"Instructor"})," is a lightweight library that makes it easy to get structured data like JSON from LLMs."]}),"\n",(0,r.jsx)(n.h2,{id:"tracing",children:"Tracing"}),"\n",(0,r.jsx)(n.p,{children:"It\u2019s important to store traces of language model applications in a central location, both during development and in production. These traces can be useful for debugging, and as a dataset that will help you improve your application."}),"\n",(0,r.jsxs)(n.p,{children:["Weave will automatically capture traces for ",(0,r.jsx)(n.a,{href:"https://python.useinstructor.com/",children:"Instructor"}),". To start tracking, calling ",(0,r.jsx)(n.code,{children:'weave.init(project_name="<YOUR-WANDB-PROJECT-NAME>")'})," and use the library as normal."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import instructor\nimport weave\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\n\n# Define your desired output structure\nclass UserInfo(BaseModel):\n    user_name: str\n    age: int\n\n# Initialize Weave\nweave.init(project_name="instructor-test")\n\n# Patch the OpenAI client\nclient = instructor.from_openai(OpenAI())\n\n# Extract structured data from natural language\nuser_info = client.chat.completions.create(\n    model="gpt-3.5-turbo",\n    response_model=UserInfo,\n    messages=[{"role": "user", "content": "John Doe is 30 years old."}],\n)\n'})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsx)(n.tr,{children:(0,r.jsx)(n.th,{children:(0,r.jsx)(n.img,{src:t(75532).Z+"",width:"2880",height:"1512"})})})}),(0,r.jsx)(n.tbody,{children:(0,r.jsx)(n.tr,{children:(0,r.jsx)(n.td,{children:"Weave will now track and log all LLM calls made using Instructor. You can view the traces in the Weave web interface."})})})]}),"\n",(0,r.jsx)(n.h2,{id:"track-your-own-ops",children:"Track your own ops"}),"\n",(0,r.jsxs)(n.p,{children:["Wrapping a function with ",(0,r.jsx)(n.code,{children:"@weave.op"})," starts capturing inputs, outputs and app logic so you can debug how data flows through your app. You can deeply nest ops and build a tree of functions that you want to track. This also starts automatically versioning code as you experiment to capture ad-hoc details that haven't been committed to git."]}),"\n",(0,r.jsxs)(n.p,{children:["Simply create a function decorated with ",(0,r.jsx)(n.a,{href:"/guides/tracking/ops",children:(0,r.jsx)(n.code,{children:"@weave.op"})}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["In the example below, we have the function ",(0,r.jsx)(n.code,{children:"extract_person"})," which is the metric function wrapped with ",(0,r.jsx)(n.code,{children:"@weave.op"}),". This helps us see how intermediate steps, such as OpenAI chat completion call."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import instructor\nimport weave\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\n\n# Define your desired output structure\nclass Person(BaseModel):\n    person_name: str\n    age: int\n\n\n# Initialize Weave\nweave.init(project_name="instructor-test")\n\n# Patch the OpenAI client\nlm_client = instructor.from_openai(OpenAI())\n\n\n# Extract structured data from natural language\n@weave.op()\ndef extract_person(text: str) -> Person:\n    return lm_client.chat.completions.create(\n        model="gpt-3.5-turbo",\n        messages=[\n            {"role": "user", "content": text},\n        ],\n        response_model=Person,\n    )\n\n\nperson = extract_person("My name is John and I am 20 years old")\n'})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsx)(n.tr,{children:(0,r.jsx)(n.th,{children:(0,r.jsx)(n.img,{src:t(51100).Z+"",width:"2880",height:"1514"})})})}),(0,r.jsx)(n.tbody,{children:(0,r.jsx)(n.tr,{children:(0,r.jsxs)(n.td,{children:["Decorating the ",(0,r.jsx)(n.code,{children:"extract_person"})," function with ",(0,r.jsx)(n.code,{children:"@weave.op"})," traces its inputs, outputs, and all internal LM calls made inside the function. Weave also automatically tracks and versions the structured objects generated by Instructor."]})})})]}),"\n",(0,r.jsxs)(n.h2,{id:"create-a-model-for-easier-experimentation",children:["Create a ",(0,r.jsx)(n.code,{children:"Model"})," for easier experimentation"]}),"\n",(0,r.jsxs)(n.p,{children:["Organizing experimentation is difficult when there are many moving pieces. By using the ",(0,r.jsx)(n.a,{href:"../core-types/models",children:(0,r.jsx)(n.code,{children:"Model"})})," class, you can capture and organize the experimental details of your app like your system prompt or the model you're using. This helps organize and compare different iterations of your app."]}),"\n",(0,r.jsxs)(n.p,{children:["In addition to versioning code and capturing inputs/outputs, ",(0,r.jsx)(n.a,{href:"../core-types/models",children:(0,r.jsx)(n.code,{children:"Model"})}),"s capture structured parameters that control your application\u2019s behavior, making it easy to find what parameters worked best. You can also use Weave Models with ",(0,r.jsx)(n.code,{children:"serve"})," (see below), and ",(0,r.jsx)(n.a,{href:"/guides/core-types/evaluations",children:(0,r.jsx)(n.code,{children:"Evaluation"})}),"s."]}),"\n",(0,r.jsxs)(n.p,{children:["In the example below, you can experiment with ",(0,r.jsx)(n.code,{children:"PersonExtractor"}),". Every time you change one of these, you'll get a new ",(0,r.jsx)(n.em,{children:"version"})," of ",(0,r.jsx)(n.code,{children:"PersonExtractor"}),"."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import asyncio\nfrom typing import List, Iterable\n\nimport instructor\nimport weave\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel\n\n\n# Define your desired output structure\nclass Person(BaseModel):\n    person_name: str\n    age: int\n\n\n# Initialize Weave\nweave.init(project_name="instructor-test")\n\n# Patch the OpenAI client\nlm_client = instructor.from_openai(AsyncOpenAI())\n\n\nclass PersonExtractor(weave.Model):\n    openai_model: str\n    max_retries: int\n\n    @weave.op()\n    async def predict(self, text: str) -> List[Person]:\n        model = await lm_client.chat.completions.create(\n            model=self.openai_model,\n            response_model=Iterable[Person],\n            max_retries=self.max_retries,\n            stream=True,\n            messages=[\n                {\n                    "role": "system",\n                    "content": "You are a perfect entity extraction system",\n                },\n                {\n                    "role": "user",\n                    "content": f"Extract `{text}`",\n                },\n            ],\n        )\n        return [m async for m in model]\n\n\nmodel = PersonExtractor(openai_model="gpt-4", max_retries=2)\nasyncio.run(model.predict("John is 30 years old"))\n'})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsx)(n.tr,{children:(0,r.jsx)(n.th,{children:(0,r.jsx)(n.img,{src:t(51952).Z+"",width:"1490",height:"1422"})})})}),(0,r.jsx)(n.tbody,{children:(0,r.jsx)(n.tr,{children:(0,r.jsxs)(n.td,{children:["Tracing and versioning your calls using a ",(0,r.jsx)(n.a,{href:"../core-types/models",children:(0,r.jsx)(n.code,{children:"Model"})})]})})})]}),"\n",(0,r.jsx)(n.h2,{id:"serving-a-weave-model",children:"Serving a Weave Model"}),"\n",(0,r.jsxs)(n.p,{children:["Given a weave reference a ",(0,r.jsx)(n.code,{children:"weave.Model"})," object, you can spin up a fastapi server and ",(0,r.jsx)(n.a,{href:"https://wandb.github.io/weave/guides/tools/serve",children:(0,r.jsx)(n.code,{children:"serve"})})," it."]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsx)(n.tr,{children:(0,r.jsx)(n.th,{children:(0,r.jsx)(n.a,{href:"https://wandb.ai/geekyrakshit/instructor-test/weave/objects/PersonExtractor/versions/xXpMsJvaiTOjKafz1TnHC8wMgH5ZAAwYOaBMvHuLArI",children:(0,r.jsx)(n.img,{src:t(45161).Z+"",width:"2880",height:"1514"})})})})}),(0,r.jsx)(n.tbody,{children:(0,r.jsx)(n.tr,{children:(0,r.jsxs)(n.td,{children:["You can find the weave reference of any ",(0,r.jsx)(n.code,{children:"weave.Model"})," by navigating to the model and copying it from the UI."]})})})]}),"\n",(0,r.jsx)(n.p,{children:"You can serve your model by using the following command in the terminal:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"weave serve weave:///your_entity/project-name/YourModel:<hash>\n"})})]})}function h(e={}){const{wrapper:n}={...(0,s.a)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}},75532:(e,n,t)=>{t.d(n,{Z:()=>r});const r=t.p+"assets/images/instructor_lm_trace-25522be59f6e456773b176ce0b23b2f6.gif"},51100:(e,n,t)=>{t.d(n,{Z:()=>r});const r=t.p+"assets/images/instructor_op_trace-bcceed1c88e0610f8675d059e22e8b0a.png"},45161:(e,n,t)=>{t.d(n,{Z:()=>r});const r=t.p+"assets/images/instructor_serve-569628374dd232dcd6ff3fa1f742f526.png"},51952:(e,n,t)=>{t.d(n,{Z:()=>r});const r=t.p+"assets/images/instructor_weave_model-313d13f30307a5f2dad3063349e55c4f.png"},11151:(e,n,t)=>{t.d(n,{Z:()=>i,a:()=>a});var r=t(67294);const s={},o=r.createContext(s);function a(e){const n=r.useContext(o);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);