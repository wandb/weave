"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[1288],{27471:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>s,metadata:()=>i,toc:()=>c});var a=t(85893),r=t(11151);t(65488),t(85162);const s={},o="Learn Weave with W&B Inference",i={id:"quickstart-inference",title:"Learn Weave with W&B Inference",description:"This guide shows you how to use W&B Weave with W&B Inference. Using W&B Inference, you can build and trace LLM applications using live open-source models without setting up your own infrastructure or managing API keys from multiple providers. Just obtain your W&B API key and use it to interact with all models hosted by W&B Inference.",source:"@site/docs/quickstart-inference.md",sourceDirName:".",slug:"/quickstart-inference",permalink:"/quickstart-inference",draft:!1,unlisted:!1,editUrl:"https://github.com/wandb/weave/blob/master/docs/docs/quickstart-inference.md",tags:[],version:"current",lastUpdatedAt:1758649408e3,frontMatter:{},sidebar:"documentationSidebar",previous:{title:"W&B Weave",permalink:"/"},next:{title:"Quickstart: Track LLM Calls",permalink:"/quickstart"}},l={},c=[{value:"What you&#39;ll learn",id:"what-youll-learn",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Step 1: Trace your first LLM call",id:"step-1-trace-your-first-llm-call",level:2},{value:"Step 2: Build a text summarization application",id:"step-2-build-a-text-summarization-application",level:2},{value:"Step 3: Compare multiple models",id:"step-3-compare-multiple-models",level:2},{value:"Step 4: Evaluate model performance",id:"step-4-evaluate-model-performance",level:2},{value:"Available models",id:"available-models",level:2},{value:"Next steps",id:"next-steps",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2}];function u(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.a)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"learn-weave-with-wb-inference",children:"Learn Weave with W&B Inference"}),"\n",(0,a.jsxs)(n.p,{children:["This guide shows you how to use W&B Weave with ",(0,a.jsx)(n.a,{href:"https://docs.wandb.ai/guides/inference/",children:"W&B Inference"}),". Using W&B Inference, you can build and trace LLM applications using live open-source models without setting up your own infrastructure or managing API keys from multiple providers. Just obtain your W&B API key and use it to interact with ",(0,a.jsx)(n.a,{href:"https://docs.wandb.ai/guides/inference/models/",children:"all models hosted by W&B Inference"}),"."]}),"\n",(0,a.jsx)(n.h2,{id:"what-youll-learn",children:"What you'll learn"}),"\n",(0,a.jsx)(n.p,{children:"In this guide, you'll:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Set up Weave and W&B Inference"}),"\n",(0,a.jsx)(n.li,{children:"Build a basic LLM application with automatic tracing"}),"\n",(0,a.jsx)(n.li,{children:"Compare multiple models"}),"\n",(0,a.jsx)(n.li,{children:"Evaluate model performance on a dataset"}),"\n",(0,a.jsx)(n.li,{children:"View your results in the Weave UI"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsxs)(n.p,{children:["Before you begin, you need a ",(0,a.jsx)(n.a,{href:"https://app.wandb.ai/login?signup=true",children:"W&B account"})," and an API key from ",(0,a.jsx)(n.a,{href:"https://wandb.ai/authorize",children:"https://wandb.ai/authorize"}),"."]}),"\n",(0,a.jsx)(n.p,{children:"Then, in a Python environment running version 3.9 or later, install the required libraries:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"pip install weave openai\n"})}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.code,{children:"openai"})," library is installed because you use the standard ",(0,a.jsx)(n.code,{children:"openai"})," client to interact with W&B Inference, regardless of which hosted model you're actually calling. This allows you to swap between supported models by only changing the slug, and make use of any existing code you have that was written to use the OpenAI API."]}),"\n",(0,a.jsx)(n.h2,{id:"step-1-trace-your-first-llm-call",children:"Step 1: Trace your first LLM call"}),"\n",(0,a.jsx)(n.p,{children:"Start with a basic example that uses Llama 3.1-8B through W&B Inference."}),"\n",(0,a.jsx)(n.p,{children:"When you run this code, Weave:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Traces your LLM call automatically"}),"\n",(0,a.jsx)(n.li,{children:"Logs inputs, outputs, latency, and token usage"}),"\n",(0,a.jsx)(n.li,{children:"Provides a link to view your trace in the Weave UI"}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",metastring:"{5,10,11}",children:'import weave\nimport openai\n\n# Initialize Weave - replace with your-team/your-project\nweave.init("<team-name>/my-first-weave-project")\n\n# Create an OpenAI-compatible client pointing to W&B Inference\nclient = openai.OpenAI(\n    base_url=\'https://api.inference.wandb.ai/v1\',\n    api_key="YOUR_WANDB_API_KEY",  # Replace with your actual API key\n    project="<team-name>/my-first-weave-project",  # Required for usage tracking\n)\n\n# Decorate your function to enable tracing; use the standard OpenAI client\n@weave.op()\ndef ask_llama(question: str) -> str:\n    response = client.chat.completions.create(\n        model="meta-llama/Llama-3.1-8B-Instruct",\n        messages=[\n            {"role": "system", "content": "You are a helpful assistant."},\n            {"role": "user", "content": question}\n        ],\n    )\n    return response.choices[0].message.content\n\n# Call your function - Weave automatically traces everything\nresult = ask_llama("What are the benefits of using W&B Weave for LLM development?")\nprint(result)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"step-2-build-a-text-summarization-application",children:"Step 2: Build a text summarization application"}),"\n",(0,a.jsx)(n.p,{children:"Next, try running this code, which is a basic summarization app that shows how Weave traces nested operations:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",metastring:"{5,9,10}",children:'import weave\nimport openai\n\n# Initialize Weave - replace with your-team/your-project\nweave.init("<team-name>/my-first-weave-project")\n\nclient = openai.OpenAI(\n    base_url=\'https://api.inference.wandb.ai/v1\',\n    api_key="YOUR_WANDB_API_KEY",  # Replace with your actual API key\n    project="<team-name>/my-first-weave-project",  # Required for usage tracking\n)\n\n@weave.op()\ndef extract_key_points(text: str) -> list[str]:\n    """Extract key points from a text."""\n    response = client.chat.completions.create(\n        model="meta-llama/Llama-3.1-8B-Instruct",\n        messages=[\n            {"role": "system", "content": "Extract 3-5 key points from the text. Return each point on a new line."},\n            {"role": "user", "content": text}\n        ],\n    )\n    # Returns response without blank lines\n    return [line for line in response.choices[0].message.content.strip().splitlines() if line.strip()]\n\n@weave.op()\ndef create_summary(key_points: list[str]) -> str:\n    """Create a concise summary based on key points."""\n    points_text = "\\n".join(f"- {point}" for point in key_points)\n    response = client.chat.completions.create(\n        model="meta-llama/Llama-3.1-8B-Instruct",\n        messages=[\n            {"role": "system", "content": "Create a one-sentence summary based on these key points."},\n            {"role": "user", "content": f"Key points:\\n{points_text}"}\n        ],\n    )\n    return response.choices[0].message.content\n\n@weave.op()\ndef summarize_text(text: str) -> dict:\n    """Main summarization pipeline."""\n    key_points = extract_key_points(text)\n    summary = create_summary(key_points)\n    return {\n        "key_points": key_points,\n        "summary": summary\n    }\n\n# Try it with sample text\nsample_text = """\nThe Apollo 11 mission was a historic spaceflight that landed the first humans on the Moon \non July 20, 1969. Commander Neil Armstrong and lunar module pilot Buzz Aldrin descended \nto the lunar surface while Michael Collins remained in orbit. Armstrong became the first \nperson to step onto the Moon, followed by Aldrin 19 minutes later. They spent about \ntwo and a quarter hours together outside the spacecraft, collecting samples and taking photographs.\n"""\n\nresult = summarize_text(sample_text)\nprint("Key Points:", result["key_points"])\nprint("\\nSummary:", result["summary"])\n'})}),"\n",(0,a.jsx)(n.h2,{id:"step-3-compare-multiple-models",children:"Step 3: Compare multiple models"}),"\n",(0,a.jsx)(n.p,{children:"W&B Inference provides access to multiple models. Use the following code to compare the performance of Llama and DeepSeek's respective responses:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",metastring:"{5,9,10}",children:'import weave\nimport openai\n\n# Initialize Weave - replace with your-team/your-project\nweave.init("<team-name>/my-first-weave-project")\n\nclient = openai.OpenAI(\n    base_url=\'https://api.inference.wandb.ai/v1\',\n    api_key="YOUR_WANDB_API_KEY",  # Replace with your actual API key\n    project="<team-name>/my-first-weave-project",  # Required for usage tracking\n)\n\n# Define a Model class to compare different LLMs\nclass InferenceModel(weave.Model):\n    model_name: str\n    \n    @weave.op()\n    def predict(self, question: str) -> str:\n        response = client.chat.completions.create(\n            model=self.model_name,\n            messages=[\n                {"role": "user", "content": question}\n            ],\n        )\n        return response.choices[0].message.content\n\n# Create instances for different models\nllama_model = InferenceModel(model_name="meta-llama/Llama-3.1-8B-Instruct")\ndeepseek_model = InferenceModel(model_name="deepseek-ai/DeepSeek-V3-0324")\n\n# Compare their responses\ntest_question = "Explain quantum computing in one paragraph for a high school student."\n\nprint("Llama 3.1 8B response:")\nprint(llama_model.predict(test_question))\nprint("\\n" + "="*50 + "\\n")\nprint("DeepSeek V3 response:")\nprint(deepseek_model.predict(test_question))\n'})}),"\n",(0,a.jsx)(n.h2,{id:"step-4-evaluate-model-performance",children:"Step 4: Evaluate model performance"}),"\n",(0,a.jsxs)(n.p,{children:["Evaluate how well a model performs on a Q&A task using Weave's built-in ",(0,a.jsx)(n.code,{children:"EvaluationLogger"}),". This provides structured evaluation tracking with automatic aggregation, token usage capture, and rich comparison features in the UI."]}),"\n",(0,a.jsx)(n.p,{children:"Append the following code to the script you used in step 3:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from typing import Optional\nfrom weave import EvaluationLogger\n\n# Create a simple dataset\ndataset = [\n    {"question": "What is 2 + 2?", "expected": "4"},\n    {"question": "What is the capital of France?", "expected": "Paris"},\n    {"question": "Name a primary color", "expected_one_of": ["red", "blue", "yellow"]},\n]\n\n# Define a scorer\n@weave.op()\ndef accuracy_scorer(expected: str, output: str, expected_one_of: Optional[list[str]] = None) -> dict:\n    """Score the accuracy of the model output."""\n    output_clean = output.strip().lower()\n    \n    if expected_one_of:\n        is_correct = any(option.lower() in output_clean for option in expected_one_of)\n    else:\n        is_correct = expected.lower() in output_clean\n    \n    return {"correct": is_correct, "score": 1.0 if is_correct else 0.0}\n\n# Evaluate a model using Weave\'s EvaluationLogger\ndef evaluate_model(model: InferenceModel, dataset: list[dict]):\n    """Run evaluation on a dataset using Weave\'s built-in evaluation framework."""\n    # Initialize EvaluationLogger BEFORE calling the model to capture token usage\n    # This is especially important for W&B Inference to track costs\n    # Convert model name to a valid format (replace non-alphanumeric chars with underscores)\n    safe_model_name = model.model_name.replace("/", "_").replace("-", "_").replace(".", "_")\n    eval_logger = EvaluationLogger(\n        model=safe_model_name,\n        dataset="qa_dataset"\n    )\n    \n    for example in dataset:\n        # Get model prediction\n        output = model.predict(example["question"])\n        \n        # Log the prediction\n        pred_logger = eval_logger.log_prediction(\n            inputs={"question": example["question"]},\n            output=output\n        )\n        \n        # Score the output\n        score = accuracy_scorer(\n            expected=example.get("expected", ""),\n            output=output,\n            expected_one_of=example.get("expected_one_of")\n        )\n        \n        # Log the score\n        pred_logger.log_score(\n            scorer="accuracy",\n            score=score["score"]\n        )\n        \n        # Finish logging for this prediction\n        pred_logger.finish()\n    \n    # Log summary - Weave automatically aggregates the accuracy scores\n    eval_logger.log_summary()\n    print(f"Evaluation complete for {model.model_name} (logged as: {safe_model_name}). View results in the Weave UI.")\n\n# Compare multiple models - a key feature of Weave\'s evaluation framework\nmodels_to_compare = [\n    llama_model,\n    deepseek_model,\n]\n\nfor model in models_to_compare:\n    evaluate_model(model, dataset)\n\n# In the Weave UI, navigate to the Evals tab to compare results across models\n'})}),"\n",(0,a.jsx)(n.p,{children:"Running these examples returns links to the traces in the terminal. Click any link to view traces in the Weave UI."}),"\n",(0,a.jsx)(n.p,{children:"In the Weave UI, you can:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Review a timeline of all your LLM calls"}),"\n",(0,a.jsx)(n.li,{children:"Inspect inputs and outputs for each operation"}),"\n",(0,a.jsx)(n.li,{children:"View token usage and estimated costs (automatically captured by EvaluationLogger)"}),"\n",(0,a.jsx)(n.li,{children:"Analyze latency and performance metrics"}),"\n",(0,a.jsxs)(n.li,{children:["Navigate to the ",(0,a.jsx)(n.strong,{children:"Evals"})," tab to see aggregated evaluation results"]}),"\n",(0,a.jsxs)(n.li,{children:["Use the ",(0,a.jsx)(n.strong,{children:"Compare"})," feature to analyze performance across different models"]}),"\n",(0,a.jsx)(n.li,{children:"Page through specific examples to see how different models performed on the same inputs"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"available-models",children:"Available models"}),"\n",(0,a.jsxs)(n.p,{children:["For a complete list of available models, see the ",(0,a.jsx)(n.a,{href:"https://docs.wandb.ai/guides/inference/models/",children:"Available Models section"})," in the W&B Inference documentation."]}),"\n",(0,a.jsx)(n.h2,{id:"next-steps",children:"Next steps"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Use the Playground"}),": ",(0,a.jsx)(n.a,{href:"/guides/tools/playground#access-the-playground",children:"Try models interactively"})," in the Weave Playground"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Build evaluations"}),": Learn about ",(0,a.jsx)(n.a,{href:"/guides/core-types/evaluations",children:"systematic evaluation"})," of your LLM applications"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Try other integrations"}),": Weave works with ",(0,a.jsx)(n.a,{href:"/guides/integrations/",children:"OpenAI, Anthropic, and many more"})]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,a.jsxs)(t,{children:[(0,a.jsx)("summary",{children:"Authentication errors"}),(0,a.jsx)(n.p,{children:"If you get authentication errors:"}),(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Verify you have a valid W&B account"}),"\n",(0,a.jsxs)(n.li,{children:["Check that you're using the correct API key from ",(0,a.jsx)(n.a,{href:"https://wandb.ai/authorize",children:"wandb.ai/authorize"})]}),"\n",(0,a.jsxs)(n.li,{children:["Ensure your project name follows the format ",(0,a.jsx)(n.code,{children:"team-name/project-name"})]}),"\n"]})]}),"\n",(0,a.jsxs)(t,{children:[(0,a.jsx)("summary",{children:"Rate limit errors"}),(0,a.jsx)(n.p,{children:"W&B Inference has concurrency limits per project. If you hit rate limits:"}),(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Reduce the number of concurrent requests"}),"\n",(0,a.jsx)(n.li,{children:"Add delays between calls"}),"\n",(0,a.jsx)(n.li,{children:"Consider upgrading your plan for higher limits"}),"\n"]}),(0,a.jsxs)(n.p,{children:["For more details, see the ",(0,a.jsx)(n.a,{href:"https://docs.wandb.ai/guides/inference/usage-limits/",children:"limits documentation for W&B Inference"}),"."]})]}),"\n",(0,a.jsxs)(t,{children:[(0,a.jsx)("summary",{children:"Running out of credits"}),(0,a.jsxs)(n.p,{children:["The free tier includes limited credits. See the ",(0,a.jsx)(n.a,{href:"https://docs.wandb.ai/guides/inference/usage-limits/",children:"usage and limits documentation"})," for details."]})]})]})}function d(e={}){const{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(u,{...e})}):u(e)}},85162:(e,n,t)=>{t.r(n),t.d(n,{default:()=>o});t(67294);var a=t(90512);const r={tabItem:"tabItem_Ymn6"};var s=t(85893);function o(e){let{children:n,hidden:t,className:o}=e;return(0,s.jsx)("div",{role:"tabpanel",className:(0,a.Z)(r.tabItem,o),hidden:t,children:n})}},65488:(e,n,t)=>{t.d(n,{Z:()=>m});var a=t(67294),r=t(90512),s=t(12466),o=t(70989),i=t(72389);const l={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var c=t(85893);function u(e){let{className:n,block:t,selectedValue:a,selectValue:o,tabValues:i}=e;const u=[],{blockElementScrollPositionUntilNextRender:d}=(0,s.o5)(),p=e=>{const n=e.currentTarget,t=u.indexOf(n),r=i[t].value;r!==a&&(d(n),o(r))},m=e=>{let n=null;switch(e.key){case"Enter":p(e);break;case"ArrowRight":{const t=u.indexOf(e.currentTarget)+1;n=u[t]??u[0];break}case"ArrowLeft":{const t=u.indexOf(e.currentTarget)-1;n=u[t]??u[u.length-1];break}}n?.focus()};return(0,c.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.Z)("tabs",{"tabs--block":t},n),children:i.map((e=>{let{value:n,label:t,attributes:s}=e;return(0,c.jsx)("li",{role:"tab",tabIndex:a===n?0:-1,"aria-selected":a===n,ref:e=>u.push(e),onKeyDown:m,onClick:p,...s,className:(0,r.Z)("tabs__item",l.tabItem,s?.className,{"tabs__item--active":a===n}),children:t??n},n)}))})}function d(e){let{lazy:n,children:t,selectedValue:r}=e;const s=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=s.find((e=>e.props.value===r));return e?(0,a.cloneElement)(e,{className:"margin-top--md"}):null}return(0,c.jsx)("div",{className:"margin-top--md",children:s.map(((e,n)=>(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==r})))})}function p(e){const n=(0,o.Y)(e);return(0,c.jsxs)("div",{className:(0,r.Z)("tabs-container",l.tabList),children:[(0,c.jsx)(u,{...n,...e}),(0,c.jsx)(d,{...n,...e})]})}function m(e){const n=(0,i.default)();return(0,c.jsx)(p,{...e,children:(0,o.h)(e.children)},String(n))}},70989:(e,n,t)=>{t.d(n,{Y:()=>m,h:()=>c});var a=t(67294),r=t(16550),s=t(20469),o=t(91980),i=t(67392),l=t(20812);function c(e){return a.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function u(e){const{values:n,children:t}=e;return(0,a.useMemo)((()=>{const e=n??function(e){return c(e).map((e=>{let{props:{value:n,label:t,attributes:a,default:r}}=e;return{value:n,label:t,attributes:a,default:r}}))}(t);return function(e){const n=(0,i.l)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function d(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function p(e){let{queryString:n=!1,groupId:t}=e;const s=(0,r.k6)(),i=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,o._X)(i),(0,a.useCallback)((e=>{if(!i)return;const n=new URLSearchParams(s.location.search);n.set(i,e),s.replace({...s.location,search:n.toString()})}),[i,s])]}function m(e){const{defaultValue:n,queryString:t=!1,groupId:r}=e,o=u(e),[i,c]=(0,a.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!d({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const a=t.find((e=>e.default))??t[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:n,tabValues:o}))),[m,h]=p({queryString:t,groupId:r}),[f,g]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[r,s]=(0,l.Nk)(t);return[r,(0,a.useCallback)((e=>{t&&s.set(e)}),[t,s])]}({groupId:r}),y=(()=>{const e=m??f;return d({value:e,tabValues:o})?e:null})();(0,s.Z)((()=>{y&&c(y)}),[y]);return{selectedValue:i,selectValue:(0,a.useCallback)((e=>{if(!d({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);c(e),h(e),g(e)}),[h,g,o]),tabValues:o}}},11151:(e,n,t)=>{t.d(n,{Z:()=>i,a:()=>o});var a=t(67294);const r={},s=a.createContext(r);function o(e){const n=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);