"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[3461],{78996:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>o,contentTitle:()=>i,default:()=>h,frontMatter:()=>t,metadata:()=>l,toc:()=>d});var r=a(85893),s=a(11151);const t={},i="Leaderboards",l={id:"guides/core-types/leaderboards",title:"Leaderboards",description:"Use Weave Leaderboards to evaluate and compare multiple models across multiple metrics and measure accuracy, generation quality, latency, or custom evaluation logic. A leaderboard helps you visualize model performance in a central location, track changes over time, and align on team-wide benchmarks.",source:"@site/docs/guides/core-types/leaderboards.md",sourceDirName:"guides/core-types",slug:"/guides/core-types/leaderboards",permalink:"/guides/core-types/leaderboards",draft:!1,unlisted:!1,editUrl:"https://github.com/wandb/weave/blob/master/docs/docs/guides/core-types/leaderboards.md",tags:[],version:"current",lastUpdatedAt:1752618378e3,frontMatter:{},sidebar:"documentationSidebar",previous:{title:"EvaluationLogger",permalink:"/guides/evaluation/evaluation_logger"},next:{title:"Feedback",permalink:"/guides/tracking/feedback"}},o={},d=[{value:"Create a Leaderboard",id:"create-a-leaderboard",level:2},{value:"UI",id:"ui",level:3},{value:"Add columns",id:"add-columns",level:4},{value:"Python",id:"python",level:3},{value:"End-to-End Python example",id:"end-to-end-python-example",level:2},{value:"View and interpret the Leaderboard",id:"view-and-interpret-the-leaderboard",level:3}];function c(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.a)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"leaderboards",children:"Leaderboards"}),"\n",(0,r.jsxs)(n.p,{children:["Use Weave ",(0,r.jsx)(n.em,{children:"Leaderboards"})," to evaluate and compare multiple models across multiple metrics and measure accuracy, generation quality, latency, or custom evaluation logic. A leaderboard helps you visualize model performance in a central location, track changes over time, and align on team-wide benchmarks."]}),"\n",(0,r.jsx)(n.p,{children:"Leaderboards are ideal for:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Tracking model performance regressions"}),"\n",(0,r.jsx)(n.li,{children:"Coordinating shared evaluation workflows"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"create-a-leaderboard",children:"Create a Leaderboard"}),"\n",(0,r.jsxs)(n.p,{children:["You can create a leaderboard via the ",(0,r.jsx)(n.a,{href:"#ui",children:"Weave UI"})," or ",(0,r.jsx)(n.a,{href:"#python",children:"programmatically"}),"."]}),"\n",(0,r.jsx)(n.h3,{id:"ui",children:"UI"}),"\n",(0,r.jsx)(n.p,{children:"To create and customize leaderboards directly in the Weave UI:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["In the Weave UI, Navigate to the ",(0,r.jsx)(n.strong,{children:"Leaders"})," section. If it's not visible, click ",(0,r.jsx)(n.strong,{children:"More"})," \u2192 ",(0,r.jsx)(n.strong,{children:"Leaders"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["Click ",(0,r.jsx)(n.strong,{children:"+ New Leaderboard"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["In the ",(0,r.jsx)(n.strong,{children:"Leaderboard Title"})," field, enter a descriptive name (e.g., ",(0,r.jsx)(n.code,{children:"summarization-benchmark-v1"}),")."]}),"\n",(0,r.jsx)(n.li,{children:"Optionally, add a description to explain what this leaderboard compares."}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"#add-columns",children:"Add columns"})," to define which evaluations and metrics to display."]}),"\n",(0,r.jsx)(n.li,{children:"Once you're happy with the layout, save and publish your leaderboard to share it with others."}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"add-columns",children:"Add columns"}),"\n",(0,r.jsx)(n.p,{children:"Each column in a leaderboard represents a metric from a specific evaluation. To configure a column, you specify:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Evaluation"}),": Select an evaluation run from the dropdown (must be previously created)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Scorer"}),": Choose a scoring function (e.g., ",(0,r.jsx)(n.code,{children:"jaccard_similarity"}),", ",(0,r.jsx)(n.code,{children:"simple_accuracy"}),") used in that evaluation."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Metric"}),": Choose a summary metric to display (e.g., ",(0,r.jsx)(n.code,{children:"mean"}),", ",(0,r.jsx)(n.code,{children:"true_fraction"}),", etc.)."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["To add more columns, click ",(0,r.jsx)(n.strong,{children:"Add Column"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["To edit a column, click its three-dot menu (",(0,r.jsx)(n.code,{children:"\u22ef"}),") on the right. You can:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Move before / after"})," \u2013 Reorder columns"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Duplicate"})," \u2013 Copy the column definition"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Delete"})," \u2013 Remove the column"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sort ascending"})," \u2013 Set the default sort for the leaderboard (click again to toggle descending)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"python",children:"Python"}),"\n",(0,r.jsx)(n.admonition,{type:"tip",children:(0,r.jsxs)(n.p,{children:["Looking for a complete, runnable code sample? See the ",(0,r.jsx)(n.a,{href:"#end-to-end-python-example",children:"End-to-end Python example"}),"."]})}),"\n",(0,r.jsx)(n.p,{children:"To create and publish a leaderboard:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Define a test dataset. You can use the built-in ",(0,r.jsx)(n.a,{href:"/guides/core-types/datasets",children:(0,r.jsx)(n.code,{children:"Dataset"})}),", or define a list of inputs and targets manually:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'dataset = [\n    {"input": "...", "target": "..."},\n    ...\n]\n'})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Define one or more ",(0,r.jsx)(n.a,{href:"/guides/evaluation/scorers",children:"scorers"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"@weave.op\ndef jaccard_similarity(target: str, output: str) -> float:\n    ...\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Create an ",(0,r.jsx)(n.a,{href:"/guides/core-types/evaluations",children:(0,r.jsx)(n.code,{children:"Evaluation"})}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'evaluation = weave.Evaluation(\n    name="My Eval",\n    dataset=dataset,\n    scorers=[jaccard_similarity],\n)\n'})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Define models to be evaluated:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"@weave.op\ndef my_model(input: str) -> str:\n    ...\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Run the evaluation:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:" async def run_all():\n     await evaluation.evaluate(model_vanilla)\n     await evaluation.evaluate(model_humanlike)\n     await evaluation.evaluate(model_messy)\n\nasyncio.run(run_all())\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Create the leaderboard:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'spec = leaderboard.Leaderboard(\n    name="My Leaderboard",\n    description="Evaluating models on X task",\n    columns=[\n        leaderboard.LeaderboardColumn(\n            evaluation_object_ref=get_ref(evaluation).uri(),\n            scorer_name="jaccard_similarity",\n            summary_metric_path="mean",\n        )\n    ]\n)\n'})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Publish the leaderboard."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"weave.publish(spec)\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Retrieve the results:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"results = leaderboard.get_leaderboard_results(spec, client)\nprint(results)\n"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"end-to-end-python-example",children:"End-to-End Python example"}),"\n",(0,r.jsxs)(n.p,{children:["The following example uses Weave Evaluations and creates a leaderboard to compare three summarization models on a shared dataset using a custom metric. It creates a small benchmark, evaluates each model, scores each model with ",(0,r.jsx)(n.a,{href:"https://www.learndatasci.com/glossary/jaccard-similarity/",children:"Jaccard similarity"}),", and publishes the results to a Weave leaderboard."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import weave\nfrom weave.flow import leaderboard\nfrom weave.trace.ref_util import get_ref\nimport asyncio\n\nclient = weave.init("leaderboard-demo")\n\ndataset = [\n    {\n        "input": "Weave is a tool for building interactive LLM apps. It offers observability, trace inspection, and versioning.",\n        "target": "Weave helps developers build and observe LLM applications."\n    },\n    {\n        "input": "The OpenAI GPT-4o model can process text, audio, and vision inputs, making it a multimodal powerhouse.",\n        "target": "GPT-4o is a multimodal model for text, audio, and images."\n    },\n    {\n        "input": "The W&B team recently added native support for agents and evaluations in Weave.",\n        "target": "W&B added agents and evals to Weave."\n    }\n]\n\n@weave.op\ndef jaccard_similarity(target: str, output: str) -> float:\n    target_tokens = set(target.lower().split())\n    output_tokens = set(output.lower().split())\n    intersection = len(target_tokens & output_tokens)\n    union = len(target_tokens | output_tokens)\n    return intersection / union if union else 0.0\n\nevaluation = weave.Evaluation(\n    name="Summarization Quality",\n    dataset=dataset,\n    scorers=[jaccard_similarity],\n)\n\n@weave.op\ndef model_vanilla(input: str) -> str:\n    return input[:50]\n\n@weave.op\ndef model_humanlike(input: str) -> str:\n    if "Weave" in input:\n        return "Weave helps developers build and observe LLM applications."\n    elif "GPT-4o" in input:\n        return "GPT-4o supports text, audio, and vision input."\n    else:\n        return "W&B added agent support to Weave."\n\n@weave.op\ndef model_messy(input: str) -> str:\n    return "Summarizer summarize models model input text LLMs."\n\nasync def run_all():\n    await evaluation.evaluate(model_vanilla)\n    await evaluation.evaluate(model_humanlike)\n    await evaluation.evaluate(model_messy)\n\nasyncio.run(run_all())\n\nspec = leaderboard.Leaderboard(\n    name="Summarization Model Comparison",\n    description="Evaluate summarizer models using Jaccard similarity on 3 short samples.",\n    columns=[\n        leaderboard.LeaderboardColumn(\n            evaluation_object_ref=get_ref(evaluation).uri(),\n            scorer_name="jaccard_similarity",\n            summary_metric_path="mean",\n        )\n    ]\n)\n\nweave.publish(spec)\n\nresults = leaderboard.get_leaderboard_results(spec, client)\nprint(results)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"view-and-interpret-the-leaderboard",children:"View and interpret the Leaderboard"}),"\n",(0,r.jsx)(n.p,{children:"After the script finishes running, view view the leaderboard:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["In the ",(0,r.jsx)(n.strong,{children:"Weave UI"}),", go to the ",(0,r.jsx)(n.strong,{children:"Leaders"})," tab. If it's not visible, click ",(0,r.jsx)(n.strong,{children:"More"}),", then select ",(0,r.jsx)(n.strong,{children:"Leaders"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["Click on the name of your leaderboard\u2014e.g. ",(0,r.jsx)(n.code,{children:"Summarization Model Comparison"}),"."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["In the leaderboard table, each row represents a given model (",(0,r.jsx)(n.code,{children:"model_humanlike"}),", ",(0,r.jsx)(n.code,{children:"model_vanilla"}),", ",(0,r.jsx)(n.code,{children:"model_messy"}),"). The ",(0,r.jsx)(n.code,{children:"mean"})," column shows the average Jaccard similarity between the model's output and the reference summaries."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"A leaderboard in the Weave UI",src:a(91960).Z+"",width:"1666",height:"712"})}),"\n",(0,r.jsx)(n.p,{children:"For this example:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"model_humanlike"})," performs the best, with ~46% overlap."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"model_vanilla"})," (a naive truncation) gets ~21%."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"model_messy"})," an intentionally bad model, scores ~2%."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.a)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},91960:(e,n,a)=>{a.d(n,{Z:()=>r});const r=a.p+"assets/images/leaderboard-example-fa48aae01ebe16f631e6c7104a8ef81e.png"},11151:(e,n,a)=>{a.d(n,{Z:()=>l,a:()=>i});var r=a(67294);const s={},t=r.createContext(s);function i(e){const n=r.useContext(t);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);