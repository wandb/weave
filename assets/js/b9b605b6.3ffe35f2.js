"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[8419],{45270:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>u,frontMatter:()=>a,metadata:()=>s,toc:()=>d});var t=o(85893),r=o(11151);const a={},i="EvaluationLogger",s={id:"guides/evaluation/evaluation_logger",title:"EvaluationLogger",description:"The EvaluationLogger provides a flexible, incremental way to log evaluation data directly from your Python code. You don't need deep knowledge of Weave's internal data types; simply instantiate a logger and use its methods (logprediction, logscore, log_summary) to record evaluation steps.",source:"@site/docs/guides/evaluation/evaluation_logger.md",sourceDirName:"guides/evaluation",slug:"/guides/evaluation/evaluation_logger",permalink:"/guides/evaluation/evaluation_logger",draft:!1,unlisted:!1,editUrl:"https://github.com/wandb/weave/blob/master/docs/docs/guides/evaluation/evaluation_logger.md",tags:[],version:"current",lastUpdatedAt:1749652482e3,frontMatter:{},sidebar:"documentationSidebar",previous:{title:"Weave Local Scorers",permalink:"/guides/evaluation/weave_local_scorers"},next:{title:"Feedback",permalink:"/guides/tracking/feedback"}},l={},d=[{value:"Basic workflow",id:"basic-workflow",level:2},{value:"Basic example",id:"basic-example",level:2},{value:"Advanced usage",id:"advanced-usage",level:2},{value:"Get outputs before logging",id:"get-outputs-before-logging",level:3},{value:"Log rich media",id:"log-rich-media",level:3},{value:"Log and compare multiple evaluations",id:"log-and-compare-multiple-evaluations",level:3},{value:"Usage tips",id:"usage-tips",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"evaluationlogger",children:(0,t.jsx)(n.code,{children:"EvaluationLogger"})}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"EvaluationLogger"})," provides a flexible, incremental way to log evaluation data directly from your Python code. You don't need deep knowledge of Weave's internal data types; simply instantiate a logger and use its methods (",(0,t.jsx)(n.code,{children:"log_prediction"}),", ",(0,t.jsx)(n.code,{children:"log_score"}),", ",(0,t.jsx)(n.code,{children:"log_summary"}),") to record evaluation steps."]}),"\n",(0,t.jsx)(n.p,{children:"This approach is particularly helpful in complex workflows where the entire dataset or all scorers might not be defined upfront."}),"\n",(0,t.jsxs)(n.p,{children:["In contrast to the standard ",(0,t.jsx)(n.code,{children:"Evaluation"})," object, which requires a predefined ",(0,t.jsx)(n.code,{children:"Dataset"})," and list of ",(0,t.jsx)(n.code,{children:"Scorer"})," objects, the ",(0,t.jsx)(n.code,{children:"EvaluationLogger"})," allows you to log individual predictions and their associated scores incrementally as they become available."]}),"\n",(0,t.jsxs)(n.admonition,{title:"Prefer a more structured evaluation?",type:"info",children:[(0,t.jsxs)(n.p,{children:["If you prefer a more opinionated evaluation framework with predefined datasets and scorers, see ",(0,t.jsx)(n.a,{href:"/guides/core-types/evaluations",children:"Weave's standard Evaluation framework"}),"."]}),(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"EvaluationLogger"})," offers flexibility while the standard framework offers structure and guidance."]})]}),"\n",(0,t.jsx)(n.h2,{id:"basic-workflow",children:"Basic workflow"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"Initialize the logger:"})," Create an instance of ",(0,t.jsx)(n.code,{children:"EvaluationLogger"}),", optionally providing metadata about the ",(0,t.jsx)(n.code,{children:"model"})," and ",(0,t.jsx)(n.code,{children:"dataset"}),". Defaults will be used if omitted.","\n",(0,t.jsx)(n.admonition,{title:"Track token usage and cost",type:"important",children:(0,t.jsxs)(n.p,{children:["To capture token usage and cost for LLM calls (e.g. OpenAI), initialize ",(0,t.jsx)(n.code,{children:"EvaluationLogger"})," before any LLM invocations**.\nIf you call your LLM first and then log predictions afterward, token and cost data are not captured."]})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"Log predictions:"})," Call ",(0,t.jsx)(n.code,{children:"log_prediction"})," for each input/output pair from your system."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"Log scores:"})," Use the returned ",(0,t.jsx)(n.code,{children:"ScoreLogger"})," to ",(0,t.jsx)(n.code,{children:"log_score"})," for the prediction. Multiple scores per prediction are supported."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"Finish prediction:"})," Always call ",(0,t.jsx)(n.code,{children:"finish()"})," after logging scores for a prediction to finalize it."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"Log summary:"})," After all predictions are processed, call ",(0,t.jsx)(n.code,{children:"log_summary"})," to aggregate scores and add optional custom metrics."]}),"\n"]}),"\n",(0,t.jsx)(n.admonition,{type:"important",children:(0,t.jsxs)(n.p,{children:["After calling ",(0,t.jsx)(n.code,{children:"finish()"})," on a prediction, no more scores can be logged for it."]})}),"\n",(0,t.jsxs)(n.p,{children:["For a Python code demonstrating the described workflow, see the ",(0,t.jsx)(n.a,{href:"#basic-example",children:"Basic example"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"basic-example",children:"Basic example"}),"\n",(0,t.jsxs)(n.p,{children:["The following example shows how to use ",(0,t.jsx)(n.code,{children:"EvaluationLogger"})," to log predictions and scores inline with your existing Python code."]}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"user_model"})," model function is defined and applied to a list of inputs. For each example:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["The input and output are logged using ",(0,t.jsx)(n.code,{children:"log_prediction"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["A simple correctness score (",(0,t.jsx)(n.code,{children:"correctness_score"}),") is logged via ",(0,t.jsx)(n.code,{children:"log_score"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"finish()"})," finalizes logging for that prediction.\nFinally, ",(0,t.jsx)(n.code,{children:"log_summary"})," records any aggregate metrics and triggers automatic score summarization in Weave."]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import weave\nfrom openai import OpenAI\nfrom weave import EvaluationLogger \n\nweave.init('my-project')\n\n# Initialize EvaluationLogger BEFORE calling the model to ensure token tracking\neval_logger = EvaluationLogger(\n    model=\"my_model\",\n    dataset=\"my_dataset\"\n)\n\n# Example input data (this can be any data structure you want)\neval_samples = [\n    {'inputs': {'a': 1, 'b': 2}, 'expected': 3},\n    {'inputs': {'a': 2, 'b': 3}, 'expected': 5},\n    {'inputs': {'a': 3, 'b': 4}, 'expected': 7},\n]\n\n# Example model logic using OpenAI\n@weave.op\ndef user_model(a: int, b: int) -> int:\n    oai = OpenAI()\n    response = oai.chat.completions.create(\n        messages=[{\"role\": \"user\", \"content\": f\"What is {a}+{b}?\"}],\n        model=\"gpt-4o-mini\"\n    )\n    # Use the response in some way (here we just return a + b for simplicity)\n    return a + b\n\n# Iterate through examples, predict, and log\nfor sample in eval_samples:\n    inputs = sample[\"inputs\"]\n    model_output = user_model(**inputs) # Pass inputs as kwargs\n\n    # Log the prediction input and output\n    pred_logger = eval_logger.log_prediction(\n        inputs=inputs,\n        output=model_output\n    )\n\n    # Calculate and log a score for this prediction\n    expected = sample[\"expected\"]\n    correctness_score = model_output == expected\n    pred_logger.log_score(\n        scorer=\"correctness\", # Simple string name for the scorer\n        score=correctness_score\n    )\n\n    # Finish logging for this specific prediction\n    pred_logger.finish()\n\n# Log a final summary for the entire evaluation.\n# Weave auto-aggregates the 'correctness' scores logged above.\nsummary_stats = {\"subjective_overall_score\": 0.8}\neval_logger.log_summary(summary_stats)\n\nprint(\"Evaluation logging complete. View results in the Weave UI.\")\n"})}),"\n",(0,t.jsx)(n.h2,{id:"advanced-usage",children:"Advanced usage"}),"\n",(0,t.jsx)(n.h3,{id:"get-outputs-before-logging",children:"Get outputs before logging"}),"\n",(0,t.jsx)(n.p,{children:"You can first compute your model outputs, then separately log predictions and scores. This allows for better separation of evaluation and logging logic."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Initialize EvaluationLogger BEFORE calling the model to ensure token tracking\nev = EvaluationLogger(\n    model="example_model", \n    dataset="example_dataset"\n)\n\n# Model outputs (e.g. OpenAI calls) must happen after logger init for token tracking\noutputs = [your_output_generator(**inputs) for inputs in your_dataset]\npreds = [ev.log_prediction(inputs, output) for inputs, output in zip(your_dataset, outputs)]\nfor pred in preds:\n    pred.log_score(scorer="greater_than_5_scorer", score=output > 5)\n    pred.log_score(scorer="greater_than_7_scorer", score=output > 7)\n    pred.finish()\n\nev.log_summary()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"log-rich-media",children:"Log rich media"}),"\n",(0,t.jsxs)(n.p,{children:["Inputs, outputs, and scores can include rich media such as images, videos, audio, or structured tables. Simply pass a dict or media object into the ",(0,t.jsx)(n.code,{children:"log_prediction"})," or ",(0,t.jsx)(n.code,{children:"log_score"})," methods:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import io\nimport wave\nimport struct\nfrom PIL import Image\nimport random\nfrom typing import Any\nimport weave\n\ndef generate_random_audio_wave_read(duration=2, sample_rate=44100):\n    n_samples = duration * sample_rate\n    amplitude = 32767  # 16-bit max amplitude\n\n    buffer = io.BytesIO()\n\n    # Write wave data to the buffer\n    with wave.open(buffer, \'wb\') as wf:\n        wf.setnchannels(1)\n        wf.setsampwidth(2)  # 16-bit\n        wf.setframerate(sample_rate)\n\n        for _ in range(n_samples):\n            sample = random.randint(-amplitude, amplitude)\n            wf.writeframes(struct.pack(\'<h\', sample))\n\n    # Rewind the buffer to the beginning so we can read from it\n    buffer.seek(0)\n\n    # Return a Wave_read object\n    return wave.open(buffer, \'rb\')\n\nrich_media_dataset = [\n    {\n        \'image\': Image.new(\n            "RGB",\n            (100, 100),\n            color=(\n                random.randint(0, 255),\n                random.randint(0, 255),\n                random.randint(0, 255),\n            ),\n        ),\n        "audio": generate_random_audio_wave_read(),\n    }\n    for _ in range(5)\n]\n\n@weave.op\ndef your_output_generator(image: Image.Image, audio) -> dict[str, Any]:\n    return {\n        "result": random.randint(0, 10),\n        "image": image,\n        "audio": audio,\n    }\n\nev = EvaluationLogger(model="example_model", dataset="example_dataset")\n\nfor inputs in rich_media_dataset:\n    output = your_output_generator(**inputs)\n    pred = ev.log_prediction(inputs, output)\n    pred.log_score(scorer="greater_than_5_scorer", score=output["result"] > 5)\n    pred.log_score(scorer="greater_than_7_scorer", score=output["result"] > 7)\n\nev.log_summary()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"log-and-compare-multiple-evaluations",children:"Log and compare multiple evaluations"}),"\n",(0,t.jsxs)(n.p,{children:["With ",(0,t.jsx)(n.code,{children:"EvaluationLogger"}),", you can log and compare multiple evaluations."]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Run the code sample shown below."}),"\n",(0,t.jsxs)(n.li,{children:["In the Weave UI, navigate to the ",(0,t.jsx)(n.code,{children:"Evals"})," tab."]}),"\n",(0,t.jsx)(n.li,{children:"Select the evals that you want to compare."}),"\n",(0,t.jsxs)(n.li,{children:["Click the ",(0,t.jsx)(n.strong,{children:"Compare"})," button. In the Compare view, you can:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Choose which Evals to add or remove"}),"\n",(0,t.jsx)(n.li,{children:"Choose which metrics to show or hide"}),"\n",(0,t.jsx)(n.li,{children:"Page through specific examples to see how different models performed for the same input on a given dataset"}),"\n"]}),"\n","For more information on comparisons, see ",(0,t.jsx)(n.a,{href:"/guides/tools/comparison",children:"Comparisons"})]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import weave\n\nmodels = [\n    "model1",\n    "model2",\n     {"name": "model3", "metadata": {"coolness": 9001}}\n]\n\nfor model in models:\n    # EvalLogger must be initialized before model calls to capture tokens\n    ev = EvaluationLogger(model=model, dataset="example_dataset")\n    for inputs in your_dataset:\n        output = your_output_generator(**inputs)\n        pred = ev.log_prediction(inputs=inputs, output=output)\n        pred.log_score(scorer="greater_than_3_scorer", score=output > 3)\n        pred.log_score(scorer="greater_than_5_scorer", score=output > 5)\n        pred.log_score(scorer="greater_than_7_scorer", score=output > 7)\n        pred.finish()\n\n    ev.log_summary()\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"The Evals tab",src:o(78926).Z+"",width:"1061",height:"786"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"The Comparison view",src:o(68381).Z+"",width:"1339",height:"1205"})}),"\n",(0,t.jsx)(n.h2,{id:"usage-tips",children:"Usage tips"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Call ",(0,t.jsx)(n.code,{children:"finish()"})," promptly after each prediction."]}),"\n",(0,t.jsxs)(n.li,{children:["Use ",(0,t.jsx)(n.code,{children:"log_summary"})," to capture metrics not tied to single predictions (e.g., overall latency)."]}),"\n",(0,t.jsx)(n.li,{children:"Rich media logging is great for qualitative analysis."}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},68381:(e,n,o)=>{o.d(n,{Z:()=>t});const t=o.p+"assets/images/comparison-61c799d6b7ff9be955cce959f19be593.png"},78926:(e,n,o)=>{o.d(n,{Z:()=>t});const t=o.p+"assets/images/evals_tab-c97e9e1e1d510cf3d22e5ba005a0b825.png"},11151:(e,n,o)=>{o.d(n,{Z:()=>s,a:()=>i});var t=o(67294);const r={},a=t.createContext(r);function i(e){const n=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);