"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[5409],{73185:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var t=o(85893),s=o(11151);const r={},a="Amazon Bedrock",i={id:"guides/integrations/bedrock",title:"Amazon Bedrock",description:"Weave automatically tracks and logs LLM calls made via Amazon Bedrock, AWS's fully managed service that offers foundation models from leading AI companies through a unified API.",source:"@site/docs/guides/integrations/bedrock.md",sourceDirName:"guides/integrations",slug:"/guides/integrations/bedrock",permalink:"/guides/integrations/bedrock",draft:!1,unlisted:!1,editUrl:"https://github.com/wandb/weave/blob/master/docs/docs/guides/integrations/bedrock.md",tags:[],version:"current",lastUpdatedAt:174018181e4,frontMatter:{},sidebar:"documentationSidebar",previous:{title:"Integrations",permalink:"/guides/integrations/"},next:{title:"Anthropic",permalink:"/guides/integrations/anthropic"}},c={},d=[{value:"Traces",id:"traces",level:2},{value:"Wrapping with your own ops",id:"wrapping-with-your-own-ops",level:2},{value:"Create a <code>Model</code> for easier experimentation",id:"create-a-model-for-easier-experimentation",level:2},{value:"Learn more",id:"learn-more",level:2},{value:"Try Bedrock in the Weave Playground",id:"try-bedrock-in-the-weave-playground",level:3},{value:"Report: Compare LLMs on Bedrock for text summarization with Weave",id:"report-compare-llms-on-bedrock-for-text-summarization-with-weave",level:3}];function l(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",img:"img",p:"p",pre:"pre",...(0,s.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"amazon-bedrock",children:"Amazon Bedrock"}),"\n",(0,t.jsx)(n.p,{children:"Weave automatically tracks and logs LLM calls made via Amazon Bedrock, AWS's fully managed service that offers foundation models from leading AI companies through a unified API."}),"\n",(0,t.jsxs)(n.p,{children:["There are multiple ways to log LLM calls to Weave from Amazon Bedrock. You can use ",(0,t.jsx)(n.code,{children:"weave.op"})," to create reusable operations for tracking any calls to a Bedrock model. Optionally, if you're using Anthropic models, you can use Weave\u2019s built-in integration with Anthropic."]}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsxs)(n.p,{children:["For the latest tutorials, visit ",(0,t.jsx)(n.a,{href:"https://wandb.ai/site/partners/aws/",children:"Weights & Biases on Amazon Web Services"}),"."]})}),"\n",(0,t.jsx)(n.h2,{id:"traces",children:"Traces"}),"\n",(0,t.jsx)(n.p,{children:"Weave will automatically capture traces for Bedrock API calls. You can use the Bedrock client as usual after initializing Weave and patching the client:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import weave\nimport boto3\nimport json\nfrom weave.integrations.bedrock.bedrock_sdk import patch_client\n\nweave.init("my_bedrock_app")\n\n# Create and patch the Bedrock client\nclient = boto3.client("bedrock-runtime")\npatch_client(client)\n\n# Use the client as usual\nresponse = client.invoke_model(\n    modelId="anthropic.claude-3-5-sonnet-20240620-v1:0",\n    body=json.dumps({\n        "anthropic_version": "bedrock-2023-05-31",\n        "max_tokens": 100,\n        "messages": [\n            {"role": "user", "content": "What is the capital of France?"}\n        ]\n    }),\n    contentType=\'application/json\',\n    accept=\'application/json\'\n)\nresponse_dict = json.loads(response.get(\'body\').read())\nprint(response_dict["content"][0]["text"])\n'})}),"\n",(0,t.jsxs)(n.p,{children:["of using the ",(0,t.jsx)(n.code,{children:"converse"})," API:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'messages = [{"role": "user", "content": [{"text": "What is the capital of France?"}]}]\n\nresponse = client.converse(\n    modelId="anthropic.claude-3-5-sonnet-20240620-v1:0",\n    system=[{"text": "You are a helpful AI assistant."}],\n    messages=messages,\n    inferenceConfig={"maxTokens": 100},\n)\nprint(response["output"]["message"]["content"][0]["text"])\n\n'})}),"\n",(0,t.jsx)(n.h2,{id:"wrapping-with-your-own-ops",children:"Wrapping with your own ops"}),"\n",(0,t.jsxs)(n.p,{children:["You can create reusable operations using the ",(0,t.jsx)(n.code,{children:"@weave.op()"})," decorator. Here's an example showing both the ",(0,t.jsx)(n.code,{children:"invoke_model"})," and ",(0,t.jsx)(n.code,{children:"converse"})," APIs:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'@weave.op\ndef call_model_invoke(\n    model_id: str,\n    prompt: str,\n    max_tokens: int = 100,\n    temperature: float = 0.7\n) -> dict:\n    body = json.dumps({\n        "anthropic_version": "bedrock-2023-05-31",\n        "max_tokens": max_tokens,\n        "temperature": temperature,\n        "messages": [\n            {"role": "user", "content": prompt}\n        ]\n    })\n\n    response = client.invoke_model(\n        modelId=model_id,\n        body=body,\n        contentType=\'application/json\',\n        accept=\'application/json\'\n    )\n    return json.loads(response.get(\'body\').read())\n\n@weave.op\ndef call_model_converse(\n    model_id: str,\n    messages: str,\n    system_message: str,\n    max_tokens: int = 100,\n) -> dict:\n    response = client.converse(\n        modelId=model_id,\n        system=[{"text": system_message}],\n        messages=messages,\n        inferenceConfig={"maxTokens": max_tokens},\n    )\n    return response\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:o(75754).Z+"",width:"3394",height:"1520"})}),"\n",(0,t.jsxs)(n.h2,{id:"create-a-model-for-easier-experimentation",children:["Create a ",(0,t.jsx)(n.code,{children:"Model"})," for easier experimentation"]}),"\n",(0,t.jsxs)(n.p,{children:["You can create a Weave Model to better organize your experiments and capture parameters. Here's an example using the ",(0,t.jsx)(n.code,{children:"converse"})," API:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class BedrockLLM(weave.Model):\n    model_id: str\n    max_tokens: int = 100\n    system_message: str = "You are a helpful AI assistant."\n\n    @weave.op\n    def predict(self, prompt: str) -> str:\n        "Generate a response using Bedrock\'s converse API"\n        \n        messages = [{\n            "role": "user",\n            "content": [{"text": prompt}]\n        }]\n\n        response = client.converse(\n            modelId=self.model_id,\n            system=[{"text": self.system_message}],\n            messages=messages,\n            inferenceConfig={"maxTokens": self.max_tokens},\n        )\n        return response["output"]["message"]["content"][0]["text"]\n\n# Create and use the model\nmodel = BedrockLLM(\n    model_id="anthropic.claude-3-5-sonnet-20240620-v1:0",\n    max_tokens=100,\n    system_message="You are an expert software engineer that knows a lot of programming. You prefer short answers."\n)\nresult = model.predict("What is the best way to handle errors in Python?")\nprint(result)\n'})}),"\n",(0,t.jsx)(n.p,{children:"This approach allows you to version your experiments and easily track different configurations of your Bedrock-based application."}),"\n",(0,t.jsx)(n.h2,{id:"learn-more",children:"Learn more"}),"\n",(0,t.jsx)(n.p,{children:"Learn more about using Amazon Bedrock with Weave"}),"\n",(0,t.jsx)(n.h3,{id:"try-bedrock-in-the-weave-playground",children:"Try Bedrock in the Weave Playground"}),"\n",(0,t.jsxs)(n.p,{children:["Do you want to experiment with Amazon Bedrock models in the Weave UI without any set up? Try the ",(0,t.jsx)(n.a,{href:"/guides/tools/playground",children:"LLM Playground"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"report-compare-llms-on-bedrock-for-text-summarization-with-weave",children:"Report: Compare LLMs on Bedrock for text summarization with Weave"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.a,{href:"https://wandb.ai/byyoung3/ML_NEWS3/reports/Compare-LLMs-on-Amazon-Bedrock-for-text-summarization-with-W-B-Weave--VmlldzoxMDI1MTIzNw",children:"Compare LLMs on Bedrock for text summarization with Weave"})," report explains how to use Bedrock in combination with Weave to evaluate and compare LLMs for summarization tasks, code samples included."]})]})}function p(e={}){const{wrapper:n}={...(0,s.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},75754:(e,n,o)=>{o.d(n,{Z:()=>t});const t=o.p+"assets/images/bedrock_converse-6f9dce63982bc118ac2fab9b4c96f744.png"},11151:(e,n,o)=>{o.d(n,{Z:()=>i,a:()=>a});var t=o(67294);const s={},r=t.createContext(s);function a(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);