"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[863],{29099:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>r,contentTitle:()=>s,default:()=>u,frontMatter:()=>i,metadata:()=>l,toc:()=>c});var t=a(85893),o=a(11151);const i={},s="Evaluations",l={id:"guides/core-types/evaluations",title:"Evaluations",description:"Evaluation-driven LLM application development helps you systematically improve LLM applications by systematically measuring their behavior using consistent, curated examples.",source:"@site/docs/guides/core-types/evaluations.md",sourceDirName:"guides/core-types",slug:"/guides/core-types/evaluations",permalink:"/guides/core-types/evaluations",draft:!1,unlisted:!1,editUrl:"https://github.com/wandb/weave/blob/master/docs/docs/guides/core-types/evaluations.md",tags:[],version:"current",lastUpdatedAt:1749652482e3,frontMatter:{},sidebar:"documentationSidebar",previous:{title:"Evaluate a RAG App",permalink:"/tutorial-rag"},next:{title:"Datasets",permalink:"/guides/core-types/datasets"}},r={},c=[{value:"1. Create an <code>Evaluation</code> object",id:"1-create-an-evaluation-object",level:2},{value:"(Optional) Custom naming",id:"optional-custom-naming",level:3},{value:"Name the <code>Evaluation</code> object",id:"name-the-evaluation-object",level:4},{value:"Name individual evaluation runs",id:"name-individual-evaluation-runs",level:4},{value:"2. Define a datset of test examples",id:"2-define-a-datset-of-test-examples",level:2},{value:"3. Define scoring functions",id:"3-define-scoring-functions",level:2},{value:"(Optional) Define a custom <code>Scorer</code> class",id:"optional-define-a-custom-scorer-class",level:3},{value:"4. Define a <code>Model</code> to evaluate",id:"4-define-a-model-to-evaluate",level:2},{value:"(Optional) Define a function to evaluate",id:"optional-define-a-function-to-evaluate",level:3},{value:"5. Run the evaluation",id:"5-run-the-evaluation",level:2},{value:"(Optional) Run multiple trials",id:"optional-run-multiple-trials",level:3},{value:"Full evaluation code sample",id:"full-evaluation-code-sample",level:2},{value:"Advanced evaluation usage",id:"advanced-evaluation-usage",level:2},{value:"Format dataset rows before evaluating",id:"format-dataset-rows-before-evaluating",level:3},{value:"Use HuggingFace datasets with evaluations",id:"use-huggingface-datasets-with-evaluations",level:3},{value:"Saved views",id:"saved-views",level:3},{value:"Imperative evaluations (<code>EvaluationLogger</code>)",id:"imperative-evaluations-evaluationlogger",level:3}];function d(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,o.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"evaluations",children:"Evaluations"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.em,{children:"Evaluation-driven LLM application development"})," helps you systematically improve LLM applications by systematically measuring their behavior using consistent, curated examples."]}),"\n",(0,t.jsxs)(n.p,{children:["In Weave, the core of the workflow is the ",(0,t.jsxs)(n.em,{children:[(0,t.jsx)(n.code,{children:"Evaluation"})," object"]}),", which defines:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["A ",(0,t.jsx)(n.a,{href:"../core-types/datasets",children:(0,t.jsx)(n.code,{children:"Dataset"})})," or list of dictionaries for test examples."]}),"\n",(0,t.jsxs)(n.li,{children:["One or more ",(0,t.jsx)(n.a,{href:"/guides/evaluation/scorers",children:"scoring functions"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["Optional configuration like ",(0,t.jsx)(n.a,{href:"#format-dataset-rows-before-evaluating",children:"input preprocessing"}),"."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Once you\u2019ve defined the ",(0,t.jsx)(n.code,{children:"Evaluation"}),", you can run it against a ",(0,t.jsx)(n.a,{href:"/guides/core-types/models",children:(0,t.jsx)(n.code,{children:"Model"})})," object or any custom function containing LLM application logic. Each call to ",(0,t.jsx)(n.code,{children:".evaluate()"})," triggers an ",(0,t.jsx)(n.em,{children:"evaluation run"}),". Think of the ",(0,t.jsx)(n.code,{children:"Evaluation"})," object as a blueprint, and each run as a measurement of how your application performs under that setup."]}),"\n",(0,t.jsx)(n.p,{children:"To get started with evaluations, complete the following steps:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsxs)(n.a,{href:"#1-create-an-evaluation-object",children:["Create an ",(0,t.jsx)(n.code,{children:"Evaluation"})," object"]})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#2-define-a-datset-of-test-examples",children:"Define a dataset of examples"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#3-define-scoring-functions",children:"Define scoring functions"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsxs)(n.a,{href:"#4-define-a-model-to-evaluate",children:["Define a ",(0,t.jsx)(n.code,{children:"Model"})," to evaluate"]})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#5-run-the-evaluation",children:"Run the evaluation"})}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["A complete evaluation code sample can be found ",(0,t.jsx)(n.a,{href:"#full-evaluation-code-sample",children:"here"}),". You can also learn more about ",(0,t.jsx)(n.a,{href:"#advanced-evaluation-usage",children:"advanced evaluation features"})," like ",(0,t.jsx)(n.a,{href:"#saved-views",children:"Saved views"})," and ",(0,t.jsx)(n.a,{href:"#imperative-evaluations-evaluationlogger",children:"Imperative evaluations"}),"."]}),"\n",(0,t.jsxs)(n.h2,{id:"1-create-an-evaluation-object",children:["1. Create an ",(0,t.jsx)(n.code,{children:"Evaluation"})," object"]}),"\n",(0,t.jsxs)(n.p,{children:["Creating an ",(0,t.jsx)(n.code,{children:"Evaluation"})," object is the first step in setting up your evaluation configuration. An ",(0,t.jsx)(n.code,{children:"Evaluation"})," consists of example data, scoring logic, and optional preprocessing. You\u2019ll later use it to run one or more evaluations."]}),"\n",(0,t.jsx)(n.p,{children:"Weave will take each example, pass it through your application and score the output on multiple custom scoring functions. By doing this, you'll have a view of the performance of your application, and a rich UI to drill into individual outputs and scores."}),"\n",(0,t.jsx)(n.h3,{id:"optional-custom-naming",children:"(Optional) Custom naming"}),"\n",(0,t.jsx)(n.p,{children:"There are two types of customizable names in the evaluation flow:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsxs)(n.a,{href:"#name-the-evaluation-object",children:[(0,t.jsx)(n.em,{children:"Evaluation object name"})," (",(0,t.jsx)(n.code,{children:"evaluation_name"}),")"]}),": A persistent label for your configured ",(0,t.jsx)(n.code,{children:"Evaluation"})," object."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsxs)(n.a,{href:"#name-individual-evaluation-runs",children:[(0,t.jsx)(n.em,{children:"Evaluation run display name"})," (",(0,t.jsx)(n.code,{children:'__weave["display_name"]'}),")"]}),": A label for a specific evaluation execution, shown in the UI."]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"name-the-evaluation-object",children:["Name the ",(0,t.jsx)(n.code,{children:"Evaluation"})," object"]}),"\n",(0,t.jsxs)(n.p,{children:["To name the ",(0,t.jsx)(n.code,{children:"Evaluation"})," object itself, pass an ",(0,t.jsx)(n.code,{children:"evaluation_name"})," parameter to the ",(0,t.jsx)(n.code,{children:"Evaluation"})," class. This name helps you identify the Evaluation in code and UI listings."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'evaluation = Evaluation(\n    dataset=examples, scorers=[match_score1], evaluation_name="My Evaluation"\n)\n'})}),"\n",(0,t.jsx)(n.h4,{id:"name-individual-evaluation-runs",children:"Name individual evaluation runs"}),"\n",(0,t.jsxs)(n.p,{children:["To name a specific evaluation run (a call to ",(0,t.jsx)(n.code,{children:"evaluate()"}),"), use the ",(0,t.jsx)(n.code,{children:"__weave"})," dictionary with a ",(0,t.jsx)(n.code,{children:"display_name"}),". This affects what is shown in the UI for that run."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'evaluation = Evaluation(\n    dataset=examples, scorers=[match_score1]\n)\nevaluation.evaluate(model, __weave={"display_name": "My Evaluation Run"})\n'})}),"\n",(0,t.jsx)(n.h2,{id:"2-define-a-datset-of-test-examples",children:"2. Define a datset of test examples"}),"\n",(0,t.jsxs)(n.p,{children:["First, define a ",(0,t.jsx)(n.a,{href:"/guides/core-types/datasets",children:"Dataset"})," object or list of dictionaries with a collection of examples to be evaluated. These examples are often failure cases that you want to test for, these are similar to unit tests in Test-Driven Development (TDD)."]}),"\n",(0,t.jsx)(n.p,{children:"The following examples shows a dataset defined as a list of dictionaries:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'examples = [\n    {"question": "What is the capital of France?", "expected": "Paris"},\n    {"question": "Who wrote \'To Kill a Mockingbird\'?", "expected": "Harper Lee"},\n    {"question": "What is the square root of 64?", "expected": "8"},\n]\n'})}),"\n",(0,t.jsx)(n.h2,{id:"3-define-scoring-functions",children:"3. Define scoring functions"}),"\n",(0,t.jsxs)(n.p,{children:["Then, create one or more ",(0,t.jsx)(n.a,{href:"/guides/evaluation/scorers",children:"scoring functions"}),". These are used to score each example in the ",(0,t.jsx)(n.code,{children:"Dataset"}),". Each scoring function must have a ",(0,t.jsx)(n.code,{children:"output"}),", and return a dictionary with the scores. Optionally, you can include other inputs from your examples."]}),"\n",(0,t.jsxs)(n.p,{children:["Scoring functions need to have a ",(0,t.jsx)(n.code,{children:"output"})," keyword argument, but the other arguments are user defined and are taken from the dataset examples. It will only take the necessary keys by using a dictionary key based on the argument name."]}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsxs)(n.p,{children:["If your scorer expects an ",(0,t.jsx)(n.code,{children:"output"})," argument but isn\u2019t receiving it, check if it might be using the legacy ",(0,t.jsx)(n.code,{children:"model_output"})," key. To fix this, update your scorer function to use output as a keyword argument."]})}),"\n",(0,t.jsxs)(n.p,{children:["The following example scorer function ",(0,t.jsx)(n.code,{children:"match_score1"})," uses the ",(0,t.jsx)(n.code,{children:"expected"})," value from the ",(0,t.jsx)(n.code,{children:"examples"})," dictionary for scoring."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import weave\n\n# Collect your examples\nexamples = [\n    {"question": "What is the capital of France?", "expected": "Paris"},\n    {"question": "Who wrote \'To Kill a Mockingbird\'?", "expected": "Harper Lee"},\n    {"question": "What is the square root of 64?", "expected": "8"},\n]\n\n# Define any custom scoring function\n@weave.op()\ndef match_score1(expected: str, output: dict) -> dict:\n    # Here is where you\'d define the logic to score the model output\n    return {\'match\': expected == output[\'generated_text\']}\n'})}),"\n",(0,t.jsxs)(n.h3,{id:"optional-define-a-custom-scorer-class",children:["(Optional) Define a custom ",(0,t.jsx)(n.code,{children:"Scorer"})," class"]}),"\n",(0,t.jsxs)(n.p,{children:["In some applications we want to create custom ",(0,t.jsx)(n.code,{children:"Scorer"})," classes - where for example a standardized ",(0,t.jsx)(n.code,{children:"LLMJudge"})," class should be created with specific parameters (e.g. chat model, prompt), specific scoring of each row, and specific calculation of an aggregate score."]}),"\n",(0,t.jsxs)(n.p,{children:["See the tutorial on defining a ",(0,t.jsx)(n.code,{children:"Scorer"})," class in ",(0,t.jsx)(n.a,{href:"/tutorial-rag#optional-defining-a-scorer-class",children:"Model-Based Evaluation of RAG applications"})," for more information."]}),"\n",(0,t.jsxs)(n.h2,{id:"4-define-a-model-to-evaluate",children:["4. Define a ",(0,t.jsx)(n.code,{children:"Model"})," to evaluate"]}),"\n",(0,t.jsxs)(n.p,{children:["To evaluate a ",(0,t.jsx)(n.code,{children:"Model"}),", call ",(0,t.jsx)(n.code,{children:"evaluate"})," on it using an ",(0,t.jsx)(n.code,{children:"Evaluation"}),". ",(0,t.jsx)(n.code,{children:"Models"})," are used when you have parameters that you want to experiment with and capture in weave."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from weave import Model, Evaluation\nimport asyncio\n\nclass MyModel(Model):\n    prompt: str\n\n    @weave.op()\n    def predict(self, question: str):\n        # here's where you would add your LLM call and return the output\n        return {'generated_text': 'Hello, ' + self.prompt}\n\nmodel = MyModel(prompt='World')\n\nevaluation = Evaluation(\n    dataset=examples, scorers=[match_score1]\n)\nweave.init('intro-example') # begin tracking results with weave\nasyncio.run(evaluation.evaluate(model))\n"})}),"\n",(0,t.jsxs)(n.p,{children:["This will run ",(0,t.jsx)(n.code,{children:"predict"})," on each example and score the output with each scoring functions."]}),"\n",(0,t.jsx)(n.h3,{id:"optional-define-a-function-to-evaluate",children:"(Optional) Define a function to evaluate"}),"\n",(0,t.jsxs)(n.p,{children:["Alternatively, you can also evaluate a custom function tracked by ",(0,t.jsx)(n.code,{children:"@weave.op()"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"@weave.op\ndef function_to_evaluate(question: str):\n    # here's where you would add your LLM call and return the output\n    return  {'generated_text': 'some response'}\n\nasyncio.run(evaluation.evaluate(function_to_evaluate))\n"})}),"\n",(0,t.jsx)(n.h2,{id:"5-run-the-evaluation",children:"5. Run the evaluation"}),"\n",(0,t.jsxs)(n.p,{children:["To run an evaluation, call ",(0,t.jsx)(n.code,{children:".evaluate()"})," on the object you want to evaluate.\nFor example, assuming an ",(0,t.jsx)(n.code,{children:"Evaluation"})," object called ",(0,t.jsx)(n.code,{children:"evaluation"})," and a ",(0,t.jsx)(n.code,{children:"Model"})," object to evaluate called ",(0,t.jsx)(n.code,{children:"model"}),", the following code instatiates an evaluation run."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"asyncio.run(evaluation.evaluate(model))\n"})}),"\n",(0,t.jsx)(n.admonition,{title:"Evaluation run tips",type:"tip",children:(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"evaluate()"})," method returns a summary of results across all examples. To access the full set of scored rows including outputs and scores, use ",(0,t.jsx)(n.code,{children:"get_eval_results()"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["If you don't provide a ",(0,t.jsx)(n.code,{children:"display_name"})," when calling ",(0,t.jsx)(n.code,{children:".evaluate()"}),", Weave will automatically generate one using the date and a random memorable name. To learn more, see ",(0,t.jsx)(n.a,{href:"#name-individual-evaluation-runs",children:"how to name individual evaluation runs"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["The model passed to ",(0,t.jsx)(n.code,{children:".evaluate"})," must be a ",(0,t.jsx)(n.code,{children:"Model"})," or a function tracked with ",(0,t.jsx)(n.code,{children:"@weave.op"}),". Regular Python functions are not supported unless wrapped with ",(0,t.jsx)(n.code,{children:"@weave.op"}),"."]}),"\n"]})}),"\n",(0,t.jsx)(n.h3,{id:"optional-run-multiple-trials",children:"(Optional) Run multiple trials"}),"\n",(0,t.jsxs)(n.p,{children:["You can set the ",(0,t.jsx)(n.code,{children:"trials"})," parameter on the ",(0,t.jsx)(n.code,{children:"Evaluation"})," object to run each example multiple times."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"evaluation = Evaluation(dataset=examples, scorers=[match_score], trials=3)\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Each example will be passed to the ",(0,t.jsx)(n.code,{children:"model"})," three times, and each run will be scored and displayed independently in Weave."]}),"\n",(0,t.jsx)(n.h2,{id:"full-evaluation-code-sample",children:"Full evaluation code sample"}),"\n",(0,t.jsxs)(n.p,{children:["The following code sample demonstrates a complete evaluation run from start to finish. The ",(0,t.jsx)(n.code,{children:"examples"})," dictionary is used by the ",(0,t.jsx)(n.code,{children:"match_score1"})," and ",(0,t.jsx)(n.code,{children:"match_score2"})," scoring functions to evaluate ",(0,t.jsx)(n.code,{children:"MyModel"})," given the value of ",(0,t.jsx)(n.code,{children:"prompt"}),", as well as a custom function ",(0,t.jsx)(n.code,{children:"function_to_evaluate"}),". The evaluation runs for both the ",(0,t.jsx)(n.code,{children:"Model"})," and the function are invoked via ",(0,t.jsx)(n.code,{children:"asyncio.run(evaluation.evaluate()"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from weave import Evaluation, Model\nimport weave\nimport asyncio\nweave.init(\'intro-example\')\nexamples = [\n    {"question": "What is the capital of France?", "expected": "Paris"},\n    {"question": "Who wrote \'To Kill a Mockingbird\'?", "expected": "Harper Lee"},\n    {"question": "What is the square root of 64?", "expected": "8"},\n]\n\n@weave.op()\ndef match_score1(expected: str, output: dict) -> dict:\n    return {\'match\': expected == output[\'generated_text\']}\n\n@weave.op()\ndef match_score2(expected: dict, output: dict) -> dict:\n    return {\'match\': expected == output[\'generated_text\']}\n\nclass MyModel(Model):\n    prompt: str\n\n    @weave.op()\n    def predict(self, question: str):\n        # here\'s where you would add your LLM call and return the output\n        return {\'generated_text\': \'Hello, \' + question + self.prompt}\n\nmodel = MyModel(prompt=\'World\')\nevaluation = Evaluation(dataset=examples, scorers=[match_score1, match_score2])\n\nasyncio.run(evaluation.evaluate(model))\n\n@weave.op()\ndef function_to_evaluate(question: str):\n    # here\'s where you would add your LLM call and return the output\n    return  {\'generated_text\': \'some response\' + question}\n\nasyncio.run(evaluation.evaluate(function_to_evaluate("What is the capitol of France?")))\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Evals hero",src:a(65259).Z+"",width:"4100",height:"2160"})}),"\n",(0,t.jsx)(n.h2,{id:"advanced-evaluation-usage",children:"Advanced evaluation usage"}),"\n",(0,t.jsx)(n.h3,{id:"format-dataset-rows-before-evaluating",children:"Format dataset rows before evaluating"}),"\n",(0,t.jsx)(n.admonition,{type:"important",children:(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"preprocess_model_input"})," function is only applied to inputs before passing them to the model's prediction function. Scorer functions always receive the original dataset example, without any preprocessing applied."]})}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"preprocess_model_input"})," parameter allows you to transform your dataset examples before they are passed to your evaluation function. This is useful when you need to:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Rename fields to match your model's expected input"}),"\n",(0,t.jsx)(n.li,{children:"Transform data into the correct format"}),"\n",(0,t.jsx)(n.li,{children:"Add or remove fields"}),"\n",(0,t.jsx)(n.li,{children:"Load additional data for each example"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Here's a simple example that shows how to use ",(0,t.jsx)(n.code,{children:"preprocess_model_input"})," to rename fields:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import weave\nfrom weave import Evaluation\nimport asyncio\n\n# Our dataset has "input_text" but our model expects "question"\nexamples = [\n    {"input_text": "What is the capital of France?", "expected": "Paris"},\n    {"input_text": "Who wrote \'To Kill a Mockingbird\'?", "expected": "Harper Lee"},\n    {"input_text": "What is the square root of 64?", "expected": "8"},\n]\n\n@weave.op()\ndef preprocess_example(example):\n    # Rename input_text to question\n    return {\n        "question": example["input_text"]\n    }\n\n@weave.op()\ndef match_score(expected: str, output: dict) -> dict:\n    return {\'match\': expected == output[\'generated_text\']}\n\n@weave.op()\ndef function_to_evaluate(question: str):\n    return {\'generated_text\': f\'Answer to: {question}\'}\n\n# Create evaluation with preprocessing\nevaluation = Evaluation(\n    dataset=examples,\n    scorers=[match_score],\n    preprocess_model_input=preprocess_example\n)\n\n# Run the evaluation\nweave.init(\'preprocessing-example\')\nasyncio.run(evaluation.evaluate(function_to_evaluate))\n'})}),"\n",(0,t.jsxs)(n.p,{children:["In this example, our dataset contains examples with an ",(0,t.jsx)(n.code,{children:"input_text"})," field, but our evaluation function expects a ",(0,t.jsx)(n.code,{children:"question"})," parameter. The ",(0,t.jsx)(n.code,{children:"preprocess_example"})," function transforms each example by renaming the field, allowing the evaluation to work correctly."]}),"\n",(0,t.jsx)(n.p,{children:"The preprocessing function:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Receives the raw example from your dataset"}),"\n",(0,t.jsx)(n.li,{children:"Returns a dictionary with the fields your model expects"}),"\n",(0,t.jsx)(n.li,{children:"Is applied to each example before it's passed to your evaluation function"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This is particularly useful when working with external datasets that may have different field names or structures than what your model expects."}),"\n",(0,t.jsx)(n.h3,{id:"use-huggingface-datasets-with-evaluations",children:"Use HuggingFace datasets with evaluations"}),"\n",(0,t.jsx)(n.p,{children:"We are continuously improving our integrations with third-party services and libraries."}),"\n",(0,t.jsxs)(n.p,{children:["While we work on building more seamless integrations, you can use ",(0,t.jsx)(n.code,{children:"preprocess_model_input"})," as a temporary workaround for using HuggingFace Datasets in Weave evaluations."]}),"\n",(0,t.jsxs)(n.p,{children:["See our ",(0,t.jsx)(n.a,{href:"/reference/gen_notebooks/hf_dataset_evals",children:"Using HuggingFace datasets in evaluations cookbook"})," for the current approach."]}),"\n",(0,t.jsx)(n.h3,{id:"saved-views",children:"Saved views"}),"\n",(0,t.jsxs)(n.p,{children:["You can save your Evals table configurations, filters, and sorts as ",(0,t.jsx)(n.em,{children:"saved views"})," for quick access to your preferred setup. You can configure and access saved views via the UI and the Python SDK. For more information, see ",(0,t.jsx)(n.a,{href:"/guides/tools/saved-views",children:"Saved Views"}),"."]}),"\n",(0,t.jsxs)(n.h3,{id:"imperative-evaluations-evaluationlogger",children:["Imperative evaluations (",(0,t.jsx)(n.code,{children:"EvaluationLogger"}),")"]}),"\n",(0,t.jsxs)(n.p,{children:["If you prefer a more flexible evaluation framework, check out Weave's ",(0,t.jsx)(n.a,{href:"/guides/evaluation/evaluation_logger",children:(0,t.jsx)(n.code,{children:"EvaluationLogger"})}),". The imperative approach offers more flexibility for complex workflows, while the standard evaluation framework provides more structure and guidance."]})]})}function u(e={}){const{wrapper:n}={...(0,o.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},65259:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/evals-hero-9bb44591b72ac8637e7e14bc73db1ba8.png"},11151:(e,n,a)=>{a.d(n,{Z:()=>l,a:()=>s});var t=a(67294);const o={},i=t.createContext(o);function s(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);