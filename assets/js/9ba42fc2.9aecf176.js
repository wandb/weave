"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[4947],{89527:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>d,toc:()=>u});var a=t(85893),i=t(11151),o=t(65488),s=t(85162);const r={},l="Logging media",d={id:"guides/core-types/media",title:"Logging media",description:"W&B Weave supports logging and has dedicated displays for numerous content types such as videos, images, audio files, PDFs, CSVs and HTML.",source:"@site/docs/guides/core-types/media.md",sourceDirName:"guides/core-types",slug:"/guides/core-types/media",permalink:"/guides/core-types/media",draft:!1,unlisted:!1,editUrl:"https://github.com/wandb/weave/blob/master/docs/docs/guides/core-types/media.md",tags:[],version:"current",lastUpdatedAt:1756220019e3,frontMatter:{},sidebar:"documentationSidebar",previous:{title:"Threads",permalink:"/guides/tracking/threads"},next:{title:"Playground",permalink:"/guides/tools/playground"}},c={},u=[{value:"Content API",id:"content-api",level:2},{value:"Usage",id:"usage",level:3},{value:"Type Annotations",id:"type-annotations",level:3},{value:"Direct Initialization",id:"direct-initialization",level:3},{value:"Custom Mimetypes",id:"custom-mimetypes",level:3},{value:"Custom Mimetypes with Type Annotations",id:"custom-mimetypes-with-type-annotations",level:4},{value:"Custom Mimetypes with Direct Initialization",id:"custom-mimetypes-with-direct-initialization",level:4},{value:"Content properties",id:"content-properties",level:3},{value:"Attributes",id:"attributes",level:4},{value:"Utility Methods",id:"utility-methods",level:4},{value:"Initialization Methods",id:"initialization-methods",level:4},{value:"Adding Custom Metadata",id:"adding-custom-metadata",level:3},{value:"Video",id:"video",level:2},{value:"Images",id:"images",level:2},{value:"Resize large images before logging",id:"resize-large-images-before-logging",level:3},{value:"Audio",id:"audio",level:2}];function p(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"logging-media",children:"Logging media"}),"\n",(0,a.jsx)(n.p,{children:"W&B Weave supports logging and has dedicated displays for numerous content types such as videos, images, audio files, PDFs, CSVs and HTML."}),"\n",(0,a.jsx)(n.h2,{id:"content-api",children:"Content API"}),"\n",(0,a.jsxs)(n.p,{children:["The Content API handles media objects in Weave. Instead of using specific modules and classes like ",(0,a.jsx)(n.code,{children:"PIL.Image"})," or ",(0,a.jsx)(n.code,{children:"moviepy.VideoFileClip"}),", using the Content API allows you to import content into Weave as base64 data, file paths, raw bytes, or text."]}),"\n",(0,a.jsx)(n.p,{children:"The Content API introduces special handlers in the web app for media types that don't have legacy API handlers, such as PDF and HTML files. For media with pre-existing support in Weave (such as MP3, MP4, or PNG files), your data will display identically in the web app regardless of which API you use, however, for certain large file types like videos, using the Content API provides significant performance improvements."}),"\n",(0,a.jsx)(n.admonition,{type:"note",children:(0,a.jsx)(n.p,{children:"The Content API is only available in Python."})}),"\n",(0,a.jsx)(n.h3,{id:"usage",children:"Usage"}),"\n",(0,a.jsx)(n.p,{children:"There are two primary ways to use the Content API: type annotations and direct initialization."}),"\n",(0,a.jsx)(n.p,{children:"Type annotations automatically detect the proper constructor to use, while direct initialization provides more fine-grained control and lets you take advantage of runtime features of the Content API in your code."}),"\n",(0,a.jsx)(n.h3,{id:"type-annotations",children:"Type Annotations"}),"\n",(0,a.jsx)(n.p,{children:"The Weave Content API is designed to primarily be used through type annotations, which signal to Weave that traced inputs and outputs should be processed and stored as content blobs."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import weave\nfrom weave import Content\nfrom pathlib import Path\nfrom typing import Annotated\n\n@weave.op\ndef content_annotation(path: Annotated[str, Content]) -> Annotated[bytes, Content]:\n    data = Path(path).read_bytes()\n    return data\n\n# Both input and output will show up as an MP4 file in Weave\n# Input is a string and return value is bytes\nbytes_data = content_annotation('./path/to/your/file.mp4')\n"})}),"\n",(0,a.jsx)(n.h3,{id:"direct-initialization",children:"Direct Initialization"}),"\n",(0,a.jsx)(n.p,{children:"If you want to take advantage of features such as:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Opening a file with a default application (such as a PDF viewer)"}),"\n",(0,a.jsx)(n.li,{children:"Dumping the model to JSON to upload to your own blob storage (such as S3)"}),"\n",(0,a.jsx)(n.li,{children:"Passing custom metadata to associate with the Content blob (such as the model used to generate it)"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"You can initialize content directly from your target type using one of the following methods:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"Content.from_path"})," - Create from a file path"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"Content.from_bytes"})," - Create from raw bytes"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"Content.from_text"})," - Create from text string"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"Content.from_base64"})," - Create from base64-encoded data"]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import weave\nfrom weave import Content\n\n@weave.op\ndef content_initialization(path: str) -> Content:\n    return Content.from_path(path)\n\n# Input shows up as path string and output as PDF file in Weave\ncontent = content_initialization('./path/to/your/file.pdf')\n\ncontent.open()  # Opens the file in your PDF viewer\ncontent.model_dump()  # Dumps the model attributes to JSON\n"})}),"\n",(0,a.jsx)(n.h3,{id:"custom-mimetypes",children:"Custom Mimetypes"}),"\n",(0,a.jsx)(n.p,{children:"Weave can detect most binary mimetypes, but custom mimetypes and text documents such as markdown may not be automatically detected, requiring you to manually specify the mimetype or extension of your file."}),"\n",(0,a.jsx)(n.h4,{id:"custom-mimetypes-with-type-annotations",children:"Custom Mimetypes with Type Annotations"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import weave\nfrom weave import Content\nfrom pathlib import Path\nfrom typing import Annotated, Literal\n\n@weave.op\ndef markdown_content(\n    path: Annotated[str, Content[Literal['md']]]\n) -> Annotated[str, Content[Literal['text/markdown']]]:\n    return Path(path).read_text()\n\nmarkdown_content('path/to/your/document.md')\n"})}),"\n",(0,a.jsx)(n.h4,{id:"custom-mimetypes-with-direct-initialization",children:"Custom Mimetypes with Direct Initialization"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"video_bytes = Path('/path/to/video.mp4').read_bytes()\n\n# Pass an extension such as 'mp4' or '.mp4' to the extension parameter\n# (not available for `from_path`)\ncontent = Content.from_bytes(video_bytes, extension='.mp4')\n\n# Pass a mimetype such as 'video/mp4' to the mimetype parameter\ncontent = Content.from_bytes(video_bytes, mimetype='video/mp4')\n"})}),"\n",(0,a.jsx)(n.h3,{id:"content-properties",children:"Content properties"}),"\n",(0,a.jsxs)(n.p,{children:["For a comprehensive list of class attributes and methods view the ",(0,a.jsx)(n.a,{href:"https://weave-docs.wandb.ai/reference/python-sdk/weave/#class-content",children:"Content reference docs"})]}),"\n",(0,a.jsx)(n.h4,{id:"attributes",children:"Attributes"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Property"}),(0,a.jsx)(n.th,{children:"Type"}),(0,a.jsx)(n.th,{children:"Description"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"data"})}),(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"bytes"})}),(0,a.jsx)(n.td,{children:"Raw binary content"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"metadata"})}),(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"dict[str, Any]"})}),(0,a.jsx)(n.td,{children:"Custom metadata dictionary"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"size"})}),(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"int"})}),(0,a.jsx)(n.td,{children:"Size of content in bytes"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"filename"})}),(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"str"})}),(0,a.jsx)(n.td,{children:"Extracted or provided filename"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"extension"})}),(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"str"})}),(0,a.jsxs)(n.td,{children:["File extension (e.g., ",(0,a.jsx)(n.code,{children:'"jpg"'}),", ",(0,a.jsx)(n.code,{children:'"mp3"'}),")"]})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"mimetype"})}),(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"str"})}),(0,a.jsxs)(n.td,{children:["MIME type (e.g., ",(0,a.jsx)(n.code,{children:'"image/jpeg"'}),")"]})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"path"})}),(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"str | None"})}),(0,a.jsx)(n.td,{children:"Source file path, if applicable"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"digest"})}),(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"str"})}),(0,a.jsx)(n.td,{children:"SHA256 hash of the content"})]})]})]}),"\n",(0,a.jsx)(n.h4,{id:"utility-methods",children:"Utility Methods"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"save(dest: str | Path) -> None"}),": Save content to a file"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"open() -> bool"}),": Open file using system default application (requires the content to have been saved or loaded from a path)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"as_string() -> str"}),": Display the data as a string (bytes are decoded using the encoding attribute)"]}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"initialization-methods",children:"Initialization Methods"}),"\n",(0,a.jsxs)(n.p,{children:["Create ",(0,a.jsx)(n.code,{children:"content"})," object from a file path:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'content = Content.from_path("assets/photo.jpg")\nprint(content.mimetype, content.size)\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Create ",(0,a.jsx)(n.code,{children:"content"})," object from raw bytes:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'content = Content.from_bytes(\n    data_bytes,\n    filename="audio.mp3", \n    mimetype="audio/mpeg"\n)\ncontent.save("output.mp3")\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Create ",(0,a.jsx)(n.code,{children:"content"})," object from text:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'content = Content.from_text("Hello, World!", mimetype="text/plain")\nprint(content.as_string())\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Create ",(0,a.jsx)(n.code,{children:"content"})," object from base64-encoded data:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"content = Content.from_base64(base64_string)\nprint(content.metadata)\n"})}),"\n",(0,a.jsx)(n.h3,{id:"adding-custom-metadata",children:"Adding Custom Metadata"}),"\n",(0,a.jsx)(n.p,{children:"You can attach custom metadata to any Content object:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'content = Content.from_bytes(\n    data,\n    metadata={"resolution": "1920x1080", "model": "dall-e-3" }\n)\nprint(content.metadata["resolution"])\n'})}),"\n",(0,a.jsx)(n.h2,{id:"video",children:"Video"}),"\n",(0,a.jsxs)(n.p,{children:["Weave automatically logs videos using ",(0,a.jsx)(n.a,{href:"https://zulko.github.io/moviepy/",children:(0,a.jsx)(n.code,{children:"moviepy"})}),". This allows you to pass video inputs and outputs to traced functions, and Weave will automatically handle uploading and storing video data."]}),"\n",(0,a.jsx)(n.admonition,{type:"note",children:(0,a.jsx)(n.p,{children:"Video support is currently only available in Python."})}),"\n",(0,a.jsxs)(n.p,{children:["For usage information, see ",(0,a.jsx)(n.a,{href:"../tracking/video",children:"Video Support"}),"."]}),"\n",(0,a.jsx)(n.h2,{id:"images",children:"Images"}),"\n",(0,a.jsxs)(n.p,{children:["Logging type: ",(0,a.jsx)(n.code,{children:"PIL.Image.Image"}),"."]}),"\n",(0,a.jsx)(n.admonition,{type:"important",children:(0,a.jsxs)(n.p,{children:["Base64-encoded image strings (e.g., ",(0,a.jsx)(n.code,{children:"data:image/jpeg;base64,..."}),") are technically supported but discouraged. They can cause performance issues and should only be used if absolutely necessary (e.g., for integration with specific APIs)."]})}),"\n",(0,a.jsx)(n.p,{children:"The following example shows how to log an image generated via the OpenAI DALL-E API:"}),"\n",(0,a.jsxs)(o.Z,{groupId:"programming-language",queryString:!0,children:[(0,a.jsx)(s.default,{value:"python",label:"Python",default:!0,children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import weave\nfrom openai import OpenAI\nimport requests\nfrom PIL import Image\n\nweave.init(\'image-example\')\nclient = OpenAI()\n\n@weave.op\ndef generate_image(prompt: str) -> Image:\n    response = client.images.generate(\n        model="dall-e-3",\n        prompt=prompt,\n        size="1024x1024",\n        quality="standard",\n        n=1,\n    )\n    image_url = response.data[0].url\n    image_response = requests.get(image_url, stream=True)\n    image = Image.open(image_response.raw)\n\n    # return a PIL.Image.Image object to be logged as an image\n    return image\n\ngenerate_image("a cat with a pumpkin hat")\n'})})}),(0,a.jsx)(s.default,{value:"typescript",label:"TypeScript",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"import {OpenAI} from 'openai';\nimport * as weave from 'weave';\n\nasync function main() {\n    const client = await weave.init('image-example');\n    const openai = new OpenAI();\n\n    const generateImage = weave.op(async (prompt: string) => {\n        const response = await openai.images.generate({\n            model: 'dall-e-3',\n            prompt: prompt,\n            size: '1024x1024',\n            quality: 'standard',\n            n: 1,\n        });\n        const imageUrl = response.data[0].url;\n        const imgResponse = await fetch(imageUrl);\n        const data = Buffer.from(await imgResponse.arrayBuffer());\n\n        return weave.weaveImage({data});\n    });\n\n    generateImage('a cat with a pumpkin hat');\n}\n\nmain();\n"})})})]}),"\n",(0,a.jsx)(n.p,{children:"This image is logged to Weave and automatically displayed in the UI."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Screenshot of pumpkin cat trace view",src:t(46026).Z+"",width:"3456",height:"1614"})}),"\n",(0,a.jsx)(n.h3,{id:"resize-large-images-before-logging",children:"Resize large images before logging"}),"\n",(0,a.jsxs)(n.p,{children:["It can be helpful to resize images before logging to reduce UI rendering cost and storage impact. You can use ",(0,a.jsx)(n.code,{children:"postprocess_output"})," in your ",(0,a.jsx)(n.code,{children:"@weave.op"})," to resize an image."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from dataclasses import dataclass\nfrom typing import Any\nfrom PIL import Image\nimport weave\n\nweave.init(\'image-resize-example\')\n\n# Custom output type\n@dataclass\nclass ImageResult:\n    label: str\n    image: Image.Image\n\n# Resize helper\ndef resize_image(image: Image.Image, max_size=(512, 512)) -> Image.Image:\n    image = image.copy()\n    image.thumbnail(max_size, Image.ANTIALIAS)\n    return image\n\n# Postprocess output to resize image before logging\ndef postprocess_output(output: ImageResult) -> ImageResult:\n    resized = resize_image(output.image)\n    return ImageResult(label=output.label, image=resized)\n\n@weave.op(postprocess_output=postprocess_output)\ndef generate_large_image() -> ImageResult:\n    # Create an example image to process (e.g., 2000x2000 red square)\n    img = Image.new("RGB", (2000, 2000), color="red")\n    return ImageResult(label="big red square", image=img)\n\ngenerate_large_image()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"audio",children:"Audio"}),"\n",(0,a.jsxs)(n.p,{children:["Logging type: ",(0,a.jsx)(n.code,{children:"wave.Wave_read"}),"."]}),"\n",(0,a.jsx)(n.p,{children:"The following example shows how to log an audio file using OpenAI's speech generation API."}),"\n",(0,a.jsxs)(o.Z,{groupId:"programming-language",queryString:!0,children:[(0,a.jsx)(s.default,{value:"python",label:"Python",default:!0,children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import weave\nfrom openai import OpenAI\nimport wave\n\nweave.init("audio-example")\nclient = OpenAI()\n\n\n@weave.op\ndef make_audio_file_streaming(text: str) -> wave.Wave_read:\n    with client.audio.speech.with_streaming_response.create(\n        model="tts-1",\n        voice="alloy",\n        input=text,\n        response_format="wav",\n    ) as res:\n        res.stream_to_file("output.wav")\n\n    # return a wave.Wave_read object to be logged as audio\n    return wave.open("output.wav")\n\nmake_audio_file_streaming("Hello, how are you?")\n'})})}),(0,a.jsx)(s.default,{value:"typescript",label:"TypeScript",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"import {OpenAI} from 'openai';\nimport * as weave from 'weave';\n\nasync function main() {\n    await weave.init('audio-example');\n    const openai = new OpenAI();\n\n    const makeAudioFileStreaming = weave.op(async function audio(text: string) {\n        const response = await openai.audio.speech.create({\n            model: 'tts-1',\n            voice: 'alloy',\n            input: text,\n            response_format: 'wav',\n        });\n\n        const chunks: Uint8Array[] = [];\n        for await (const chunk of response.body) {\n            chunks.push(chunk);\n        }\n        return weave.weaveAudio({data: Buffer.concat(chunks)});\n    });\n\n    await makeAudioFileStreaming('Hello, how are you?');\n}\n\nmain();\n"})})})]}),"\n",(0,a.jsx)(n.p,{children:"This audio is logged to Weave and automatically displayed in the UI, along with an audio player. In the audio player, you can view and download the raw audio waveform."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Screenshot of audio trace view",src:t(81325).Z+"",width:"3456",height:"1240"})}),"\n",(0,a.jsx)(n.admonition,{type:"tip",children:(0,a.jsxs)(n.p,{children:["Try our cookbook for ",(0,a.jsx)(n.a,{href:"/reference/gen_notebooks/audio_with_weave",children:"Audio Logging"})," or ",(0,a.jsx)("a",{href:"https://colab.research.google.com/github/wandb/weave/blob/master/docs/./notebooks/audio_with_weave.ipynb",target:"_blank",rel:"noopener noreferrer",class:"navbar__item navbar__link button button--secondary button--med margin-right--sm notebook-cta-button",children:(0,a.jsxs)("div",{children:[(0,a.jsx)("img",{src:"https://upload.wikimedia.org/wikipedia/commons/archive/d/d0/20221103151430%21Google_Colaboratory_SVG_Logo.svg",alt:"Open In Colab",height:"20px"}),(0,a.jsx)("div",{children:"Open in Colab"})]})}),". The cookbook also includes an advanced example of a Real Time Audio API based assistant integrated with Weave."]})})]})}function h(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(p,{...e})}):p(e)}},85162:(e,n,t)=>{t.r(n),t.d(n,{default:()=>s});t(67294);var a=t(90512);const i={tabItem:"tabItem_Ymn6"};var o=t(85893);function s(e){let{children:n,hidden:t,className:s}=e;return(0,o.jsx)("div",{role:"tabpanel",className:(0,a.Z)(i.tabItem,s),hidden:t,children:n})}},65488:(e,n,t)=>{t.d(n,{Z:()=>h});var a=t(67294),i=t(90512),o=t(12466),s=t(70989),r=t(72389);const l={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var d=t(85893);function c(e){let{className:n,block:t,selectedValue:a,selectValue:s,tabValues:r}=e;const c=[],{blockElementScrollPositionUntilNextRender:u}=(0,o.o5)(),p=e=>{const n=e.currentTarget,t=c.indexOf(n),i=r[t].value;i!==a&&(u(n),s(i))},h=e=>{let n=null;switch(e.key){case"Enter":p(e);break;case"ArrowRight":{const t=c.indexOf(e.currentTarget)+1;n=c[t]??c[0];break}case"ArrowLeft":{const t=c.indexOf(e.currentTarget)-1;n=c[t]??c[c.length-1];break}}n?.focus()};return(0,d.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.Z)("tabs",{"tabs--block":t},n),children:r.map((e=>{let{value:n,label:t,attributes:o}=e;return(0,d.jsx)("li",{role:"tab",tabIndex:a===n?0:-1,"aria-selected":a===n,ref:e=>c.push(e),onKeyDown:h,onClick:p,...o,className:(0,i.Z)("tabs__item",l.tabItem,o?.className,{"tabs__item--active":a===n}),children:t??n},n)}))})}function u(e){let{lazy:n,children:t,selectedValue:i}=e;const o=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=o.find((e=>e.props.value===i));return e?(0,a.cloneElement)(e,{className:"margin-top--md"}):null}return(0,d.jsx)("div",{className:"margin-top--md",children:o.map(((e,n)=>(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==i})))})}function p(e){const n=(0,s.Y)(e);return(0,d.jsxs)("div",{className:(0,i.Z)("tabs-container",l.tabList),children:[(0,d.jsx)(c,{...n,...e}),(0,d.jsx)(u,{...n,...e})]})}function h(e){const n=(0,r.default)();return(0,d.jsx)(p,{...e,children:(0,s.h)(e.children)},String(n))}},70989:(e,n,t)=>{t.d(n,{Y:()=>h,h:()=>d});var a=t(67294),i=t(16550),o=t(20469),s=t(91980),r=t(67392),l=t(20812);function d(e){return a.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function c(e){const{values:n,children:t}=e;return(0,a.useMemo)((()=>{const e=n??function(e){return d(e).map((e=>{let{props:{value:n,label:t,attributes:a,default:i}}=e;return{value:n,label:t,attributes:a,default:i}}))}(t);return function(e){const n=(0,r.l)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function u(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function p(e){let{queryString:n=!1,groupId:t}=e;const o=(0,i.k6)(),r=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,s._X)(r),(0,a.useCallback)((e=>{if(!r)return;const n=new URLSearchParams(o.location.search);n.set(r,e),o.replace({...o.location,search:n.toString()})}),[r,o])]}function h(e){const{defaultValue:n,queryString:t=!1,groupId:i}=e,s=c(e),[r,d]=(0,a.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!u({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const a=t.find((e=>e.default))??t[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:n,tabValues:s}))),[h,m]=p({queryString:t,groupId:i}),[g,x]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[i,o]=(0,l.Nk)(t);return[i,(0,a.useCallback)((e=>{t&&o.set(e)}),[t,o])]}({groupId:i}),f=(()=>{const e=h??g;return u({value:e,tabValues:s})?e:null})();(0,o.Z)((()=>{f&&d(f)}),[f]);return{selectedValue:r,selectValue:(0,a.useCallback)((e=>{if(!u({value:e,tabValues:s}))throw new Error(`Can't select invalid tab value=${e}`);d(e),m(e),x(e)}),[m,x,s]),tabValues:s}}},81325:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/audio-trace-f767f4c063dec3089ba868c899eab6d6.png"},46026:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/cat-pumpkin-trace-45be86ef6857c9700e274d3bb75a71fe.png"},11151:(e,n,t)=>{t.d(n,{Z:()=>r,a:()=>s});var a=t(67294);const i={},o=a.createContext(i);function s(e){const n=a.useContext(o);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),a.createElement(o.Provider,{value:n},e.children)}}}]);