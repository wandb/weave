"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[503],{86287:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>g,frontMatter:()=>s,metadata:()=>r,toc:()=>l});var a=t(85893),i=t(11151);const s={},o="Hugging Face Hub",r={id:"guides/integrations/huggingface",title:"Hugging Face Hub",description:"All code samples shown on this page are in Python.",source:"@site/docs/guides/integrations/huggingface.md",sourceDirName:"guides/integrations",slug:"/guides/integrations/huggingface",permalink:"/guides/integrations/huggingface",draft:!1,unlisted:!1,editUrl:"https://github.com/wandb/weave/blob/master/docs/docs/guides/integrations/huggingface.md",tags:[],version:"current",lastUpdatedAt:1749652482e3,frontMatter:{},sidebar:"documentationSidebar",previous:{title:"Groq",permalink:"/guides/integrations/groq"},next:{title:"LiteLLM",permalink:"/guides/integrations/litellm"}},c={},l=[{value:"Overview",id:"overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Basic tracing",id:"basic-tracing",level:2},{value:"Trace a function",id:"trace-a-function",level:2},{value:"Use <code>Model</code>s for experimentation",id:"use-models-for-experimentation",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",...(0,i.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"hugging-face-hub",children:"Hugging Face Hub"}),"\n",(0,a.jsx)(n.admonition,{type:"important",children:(0,a.jsx)(n.p,{children:"All code samples shown on this page are in Python."})}),"\n",(0,a.jsxs)(n.p,{children:["This page explains how to integrate ",(0,a.jsx)(n.a,{href:"https://hf.co/",children:"Hugging Face Hub"})," with W&B Weave to track and analyze your machine learning applications. You'll learn how to log model inferences, monitor function calls, and organize experiments using Weave's tracing and versioning capabilities. By following the examples provided, you can capture valuable insights, debug your applications efficiently, and compare different model configurations\u2014all within the Weave web interface."]}),"\n",(0,a.jsxs)(n.admonition,{title:"Try Hugging Face Hub with Weave in Google Colab",type:"tip",children:[(0,a.jsx)(n.p,{children:"Do you want to experiment with Hugging Face Hub and Weave without any of the set up? You can try the code samples shown here as a Jupyter Notebook on Google Colab."}),(0,a.jsx)("a",{target:"_blank",href:"https://colab.research.google.com/github/wandb/examples/blob/master/weave/docs/quickstart_huggingface.ipynb",children:(0,a.jsx)("img",{src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open In Colab"})})]}),"\n",(0,a.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://hf.co/",children:"Hugging Face Hub"})," is a machine learning platform for creators and collaborators, offering a vast collection of pre-trained models and datasets for various projects."]}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.code,{children:"huggingface_hub"})," Python library provides a unified interface to run inference across multiple services for models hosted on the Hub. You can invoke these models using the ",(0,a.jsx)(n.a,{href:"https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client",children:(0,a.jsx)(n.code,{children:"InferenceClient"})}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["Weave will automatically capture traces for ",(0,a.jsx)(n.a,{href:"https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client",children:(0,a.jsx)(n.code,{children:"InferenceClient"})}),". To start tracking, calling ",(0,a.jsx)(n.code,{children:"weave.init()"})," and use the library as normal."]}),"\n",(0,a.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Before you can use ",(0,a.jsx)(n.code,{children:"huggingface_hub"})," with Weave, you must install the necessary libraries, or upgrade to the latest versions. The following command installs or upgrades ",(0,a.jsx)(n.code,{children:"huggingface_hub"})," and ",(0,a.jsx)(n.code,{children:"weave"})," to the latest version if it's already installed, and reduces installation output."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"pip install -U huggingface_hub weave -qqq\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["To use inference with a model on the Hugging Face Hub, set your ",(0,a.jsx)(n.a,{href:"https://huggingface.co/docs/hub/security-tokens",children:"User Access Token"}),". You can either set the token from your ",(0,a.jsx)(n.a,{href:"https://huggingface.co/settings/tokens",children:"Hugging Face Hub Settings page"})," or programmatically. The following code sample prompts the user to enter their ",(0,a.jsx)(n.code,{children:"HUGGINGFACE_TOKEN"})," and sets the token as an environment variable."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import os\nimport getpass\n\nos.environ["HUGGINGFACE_TOKEN"] = getpass.getpass("Enter your Hugging Face Hub Token: ")\n'})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"basic-tracing",children:"Basic tracing"}),"\n",(0,a.jsx)(n.p,{children:"Storing traces of language model applications in a central location is essential during development and production. These traces help with debugging and serve as valuable datasets for improving your application."}),"\n",(0,a.jsxs)(n.p,{children:["Weave automatically captures traces for the ",(0,a.jsx)(n.a,{href:"https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client",children:(0,a.jsx)(n.code,{children:"InferenceClient"})}),". To start tracking, initialize Weave by calling ",(0,a.jsx)(n.code,{children:"weave.init()"}),", then use the library as usual."]}),"\n",(0,a.jsx)(n.p,{children:"The following example demonstrates how to log inference calls to the Hugging Face Hub using Weave:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import weave\nfrom huggingface_hub import InferenceClient\n\n# Initialize Weave\nweave.init(project_name="quickstart-huggingface")\n\n# Initialize Hugging Face Inference Client\nhuggingface_client = InferenceClient(\n    api_key=os.environ.get("HUGGINGFACE_TOKEN")\n)\n\n# Make a chat completion inference call to the Hugging Face Hub with the Llama-3.2-11B-Vision-Instruct model\nimage_url = "https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg"\nresponse = huggingface_client.chat_completion(\n    model="meta-llama/Llama-3.2-11B-Vision-Instruct",\n    messages=[\n        {\n            "role": "user",\n            "content": [\n                {"type": "image_url", "image_url": {"url": image_url}},\n                {"type": "text", "text": "Describe this image in one sentence."},\n            ],\n        }\n    ],\n    max_tokens=500,\n    seed=42,\n)\n'})}),"\n",(0,a.jsx)(n.p,{children:"After the code shown above runs, Weave tracks and logs all LLM calls made with the Hugging Face Inference Client. You can view these traces in the Weave web interface."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Weave logs each inference call, providing details about inputs, outputs, and metadata.",src:t(88960).Z+"",width:"3452",height:"1866"})}),"\n",(0,a.jsx)(n.p,{children:"Weave logs each inference call, providing details about inputs, outputs, and metadata."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Weave also renders the call as a chat view in the UI, displaying the entire chat history with the model.",src:t(79156).Z+"",width:"3452",height:"1866"})}),"\n",(0,a.jsx)(n.p,{children:"Weave also renders the call as a chat view in the UI, displaying the entire chat history with the model."}),"\n",(0,a.jsx)(n.h2,{id:"trace-a-function",children:"Trace a function"}),"\n",(0,a.jsxs)(n.p,{children:["To gain deeper insights into how data flows through your application, you can use ",(0,a.jsx)(n.code,{children:"@weave.op"})," to track function calls. This captures inputs, outputs, and execution logic, helping with debugging and performance analysis."]}),"\n",(0,a.jsx)(n.p,{children:"By nesting multiple ops, you can build a structured tree of tracked functions. Weave also automatically versions your code, preserving intermediate states as you experiment, even before committing changes to Git."}),"\n",(0,a.jsxs)(n.p,{children:["To start tracking, decorate the functions that you want to track with ",(0,a.jsx)(n.code,{children:"@weave.op"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["In the following example, Weave tracks three functions: ",(0,a.jsx)(n.code,{children:"generate_image"}),", ",(0,a.jsx)(n.code,{children:"check_image_correctness"}),", and ",(0,a.jsx)(n.code,{children:"generate_image_and_check_correctness"}),". These functions generate an image and validate whether it matches a given prompt."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import base64\nfrom PIL import Image\n\n\ndef encode_image(pil_image):\n    import io\n    buffer = io.BytesIO()\n    pil_image.save(buffer, format="JPEG")\n    buffer.seek(0)\n    encoded_image = base64.b64encode(buffer.read()).decode("utf-8")\n    return f"data:image/jpeg;base64,{encoded_image}"\n\n\n@weave.op\ndef generate_image(prompt: str):\n    return huggingface_client.text_to_image(\n        prompt=prompt,\n        model="black-forest-labs/FLUX.1-schnell",\n        num_inference_steps=4,\n    )\n\n\n@weave.op\ndef check_image_correctness(image: Image.Image, image_generation_prompt: str):\n    return huggingface_client.chat_completion(\n        model="meta-llama/Llama-3.2-11B-Vision-Instruct",\n        messages=[\n            {\n                "role": "user",\n                "content": [\n                    {"type": "image_url", "image_url": {"url": encode_image(image)}},\n                    {\n                        "type": "text",\n                        "text": f"Is this image correct for the prompt: {image_generation_prompt}? Answer with only one word: yes or no",\n                    },\n                ],\n            }\n        ],\n        max_tokens=500,\n        seed=42,\n    ).choices[0].message.content\n\n\n@weave.op\ndef generate_image_and_check_correctness(prompt: str):\n    image = generate_image(prompt)\n    return {\n        "image": image,\n        "is_correct": check_image_correctness(image, prompt),\n    }\n\n\nresponse = generate_image_and_check_correctness("A cute puppy")\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Weave now logs all function calls wrapped with ",(0,a.jsx)(n.code,{children:"@weave.op"}),", allowing you to analyze execution details in the Weave UI."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Weave now logs all function calls wrapped with @weave.op, allowing you to analyze execution details in the Weave UI. Weave also captures and visualizes function execution, helping you to understand data flow and logic within your application.",src:t(9861).Z+"",width:"3452",height:"1866"})}),"\n",(0,a.jsx)(n.p,{children:"Weave also captures and visualizes function execution, helping you to understand data flow and logic within your application."}),"\n",(0,a.jsxs)(n.h2,{id:"use-models-for-experimentation",children:["Use ",(0,a.jsx)(n.code,{children:"Model"}),"s for experimentation"]}),"\n",(0,a.jsxs)(n.p,{children:["Managing LLM experiments can be challenging when multiple components are involved. The Weave ",(0,a.jsx)(n.a,{href:"/guides/core-types/models",children:(0,a.jsx)(n.code,{children:"Model"})})," class helps capture and organize experimental details, such as system prompts and model configurations, allowing you to easily compare different iterations."]}),"\n",(0,a.jsxs)(n.p,{children:["In addition to versioning code and capturing inputs/outputs, a ",(0,a.jsx)(n.code,{children:"Model"})," stores structured parameters that control application behavior. This makes it easier to track which configurations produced the best results. You can also integrate a Weave ",(0,a.jsx)(n.code,{children:"Model"})," with Weave ",(0,a.jsx)(n.a,{href:"/guides/tools/serve",children:"Serve"})," and ",(0,a.jsx)(n.a,{href:"/guides/evaluation/scorers",children:"Evaluations"})," for further insights."]}),"\n",(0,a.jsxs)(n.p,{children:["The example below demonstrates defines a ",(0,a.jsx)(n.code,{children:"CityVisitRecommender"})," model for travel recommendations. Each modification to its parameters generates a new version, making experimentation easy."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import rich\n\n\nclass CityVisitRecommender(weave.Model):\n    model: str\n    temperature: float = 0.7\n    max_tokens: int = 500\n    seed: int = 42\n\n    @weave.op()\n    def predict(self, city: str) -> str:\n        return huggingface_client.chat_completion(\n            model=self.model,\n            messages=[\n                {\n                    "role": "system",\n                    "content": "You are a helpful assistant meant to suggest places to visit in a city",\n                },\n                {"role": "user", "content": city},\n            ],\n            max_tokens=self.max_tokens,\n            temperature=self.temperature,\n            seed=self.seed,\n        ).choices[0].message.content\n\n\ncity_visit_recommender = CityVisitRecommender(\n    model="meta-llama/Llama-3.2-11B-Vision-Instruct",\n    temperature=0.7,\n    max_tokens=500,\n    seed=42,\n)\nrich.print(city_visit_recommender.predict("New York City"))\nrich.print(city_visit_recommender.predict("Paris"))\n'})}),"\n",(0,a.jsx)(n.p,{children:"Weave automatically logs models and tracks different versions, making it easy to analyze performance and experiment history."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Weave automatically logs models and tracks different versions, making it easy to analyze performance and experiment history.",src:t(20705).Z+"",width:"3452",height:"1866"})})]})}function g(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},88960:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/trace_call-6bb9e341d9e3ab4298808a55e26d4e04.png"},79156:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/trace_chat-0ea2f6b323feb5469d037cf5a41d682c.png"},20705:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/trace_model-a9dc318ce4002c81682b8a3e3b5f4dbc.png"},9861:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/trace_ops-cabf840c8bbf41ce233c99a840accea8.png"},11151:(e,n,t)=>{t.d(n,{Z:()=>r,a:()=>o});var a=t(67294);const i={},s=a.createContext(i);function o(e){const n=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);