"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[2278],{3654:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>i,toc:()=>c});var o=t(85893),a=t(11151);const r={title:"Introduction to Evaluations"},s=void 0,i={id:"reference/gen_notebooks/Intro_to_Weave_Hello_Eval",title:"Introduction to Evaluations",description:"Open in Colab",source:"@site/docs/reference/gen_notebooks/Intro_to_Weave_Hello_Eval.md",sourceDirName:"reference/gen_notebooks",slug:"/reference/gen_notebooks/Intro_to_Weave_Hello_Eval",permalink:"/reference/gen_notebooks/Intro_to_Weave_Hello_Eval",draft:!1,unlisted:!1,editUrl:"https://github.com/wandb/weave/blob/master/docs/docs/reference/gen_notebooks/Intro_to_Weave_Hello_Eval.md",tags:[],version:"current",lastUpdatedAt:1744151076e3,frontMatter:{title:"Introduction to Evaluations"},sidebar:"notebookSidebar",previous:{title:"Quickstart",permalink:"/reference/gen_notebooks/intro_notebook"},next:{title:"Introduction to Traces",permalink:"/reference/gen_notebooks/Intro_to_Weave_Hello_Trace"}},l={},c=[{value:"\ud83d\udd11 Prerequisites",id:"-prerequisites",level:2},{value:"\ud83d\udc1d Run your first evaluation",id:"-run-your-first-evaluation",level:2},{value:"\ud83d\ude80 Looking for more examples?",id:"-looking-for-more-examples",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,a.a)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(n.admonition,{title:"This is a notebook",type:"tip",children:[(0,o.jsx)("a",{href:"https://colab.research.google.com/github/wandb/weave/blob/master/docs/notebooks/Intro_to_Weave_Hello_Eval.ipynb",target:"_blank",rel:"noopener noreferrer",class:"navbar__item navbar__link button button--secondary button--med margin-right--sm notebook-cta-button",children:(0,o.jsxs)("div",{children:[(0,o.jsx)("img",{src:"https://upload.wikimedia.org/wikipedia/commons/archive/d/d0/20221103151430%21Google_Colaboratory_SVG_Logo.svg",alt:"Open In Colab",height:"20px"}),(0,o.jsx)("div",{children:"Open in Colab"})]})}),(0,o.jsx)("a",{href:"https://github.com/wandb/weave/blob/master/docs/notebooks/Intro_to_Weave_Hello_Eval.ipynb",target:"_blank",rel:"noopener noreferrer",class:"navbar__item navbar__link button button--secondary button--med margin-right--sm notebook-cta-button",children:(0,o.jsxs)("div",{children:[(0,o.jsx)("img",{src:"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg",alt:"View in Github",height:"15px"}),(0,o.jsx)("div",{children:"View in Github"})]})})]}),"\n",(0,o.jsx)("img",{src:"http://wandb.me/logo-im-png",width:"400",alt:"Weights & Biases"}),"\n",(0,o.jsx)(n.p,{children:"Weave is a toolkit for developing AI-powered applications."}),"\n",(0,o.jsx)(n.p,{children:"You can use Weave to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Log and debug language model inputs, outputs, and traces."}),"\n",(0,o.jsx)(n.li,{children:"Build rigorous, apples-to-apples evaluations for language model use cases."}),"\n",(0,o.jsx)(n.li,{children:"Organize all the information generated across the LLM workflow, from experimentation to evaluations to production."}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["This notebook demonstrates how to evaluate a model or function using Weave\u2019s Evaluation API. Evaluation is a core concept in Weave that helps you measure and iterate on your application by running it against a dataset of examples and scoring the outputs using custom-defined functions. You'll define a simple model, create a labeled dataset, track scoring functions with ",(0,o.jsx)(n.code,{children:"@weave.op"}),", and run an evaluation that automatically tracks results in the Weave UI. This forms the foundation for more advanced workflows like LLM fine-tuning, regression testing, or model comparison."]}),"\n",(0,o.jsxs)(n.p,{children:["To get started, complete the prerequisites. Then, define a Weave ",(0,o.jsx)(n.code,{children:"Model"})," with a ",(0,o.jsx)(n.code,{children:"predict"})," method, create a labeled dataset and scoring function, and run an evaluation using ",(0,o.jsx)(n.code,{children:"weave.Evaluation.evaluate()"}),"."]}),"\n",(0,o.jsx)(n.h2,{id:"-prerequisites",children:"\ud83d\udd11 Prerequisites"}),"\n",(0,o.jsx)(n.p,{children:"Before you can run a Weave evaluation, complete the following prerequisites."}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["Install the W&B Weave SDK and log in with your ",(0,o.jsx)(n.a,{href:"https://wandb.ai/settings#api",children:"API key"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:["Install the OpenAI SDK and log in with your ",(0,o.jsx)(n.a,{href:"https://platform.openai.com/api-keys",children:"API key"}),"."]}),"\n",(0,o.jsx)(n.li,{children:"Initialize your W&B project."}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Install dependancies and imports\n!pip install wandb weave openai -q\n\nimport os\nfrom getpass import getpass\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nimport weave\n\n# \ud83d\udd11 Setup your API keys\n# Running this cell will prompt you for your API key with `getpass` and will not echo to the terminal.\n#####\nprint("---")\nprint(\n    "You can find your Weights and Biases API key here: https://wandb.ai/settings#api"\n)\nos.environ["WANDB_API_KEY"] = getpass("Enter your Weights and Biases API key: ")\nprint("---")\nprint("You can generate your OpenAI API key here: https://platform.openai.com/api-keys")\nos.environ["OPENAI_API_KEY"] = getpass("Enter your OpenAI API key: ")\nprint("---")\n#####\n\n# \ud83c\udfe0 Enter your W&B project name\nweave_client = weave.init("MY_PROJECT_NAME")  # \ud83d\udc1d Your W&B project name\n'})}),"\n",(0,o.jsx)(n.h2,{id:"-run-your-first-evaluation",children:"\ud83d\udc1d Run your first evaluation"}),"\n",(0,o.jsxs)(n.p,{children:["The following code sample shows how to evaluate an LLM using Weave\u2019s ",(0,o.jsx)(n.code,{children:"Model"})," and ",(0,o.jsx)(n.code,{children:"Evaluation"})," APIs. First, define a Weave model by subclassing ",(0,o.jsx)(n.code,{children:"weave.Model"}),", specifying the model name and prompt format, and tracking a ",(0,o.jsx)(n.code,{children:"predict"})," method with ",(0,o.jsx)(n.code,{children:"@weave.op"}),". The ",(0,o.jsx)(n.code,{children:"predict"})," method sends a prompt to OpenAI and parses the response into a structured output using a Pydantic schema (",(0,o.jsx)(n.code,{children:"FruitExtract"}),"). Then, create a small evaluation dataset consisting of input sentences and expected targets. Next, define a custom scoring function (also tracked using ",(0,o.jsx)(n.code,{children:"@weave.op"}),") that compares the model\u2019s output to the target label. Finally,  wrap everything in a ",(0,o.jsx)(n.code,{children:"weave.Evaluation"}),", specifying your dataset and scorers, and call ",(0,o.jsx)(n.code,{children:"evaluate()"})," to run the evaluation pipeline asynchronously."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# 1. Construct a Weave model\nclass FruitExtract(BaseModel):\n    fruit: str\n    color: str\n    flavor: str\n\n\nclass ExtractFruitsModel(weave.Model):\n    model_name: str\n    prompt_template: str\n\n    @weave.op()\n    def predict(self, sentence: str) -> dict:\n        client = OpenAI()\n\n        response = client.beta.chat.completions.parse(\n            model=self.model_name,\n            messages=[\n                {\n                    "role": "user",\n                    "content": self.prompt_template.format(sentence=sentence),\n                }\n            ],\n            response_format=FruitExtract,\n        )\n        result = response.choices[0].message.parsed\n        return result\n\n\nmodel = ExtractFruitsModel(\n    name="gpt4o",\n    model_name="gpt-4o",\n    prompt_template=\'Extract fields ("fruit": <str>, "color": <str>, "flavor": <str>) as json, from the following text : {sentence}\',\n)\n\n# 2. Collect some samples\nsentences = [\n    "There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy.",\n    "Pounits are a bright green color and are more savory than sweet.",\n    "Finally, there are fruits called glowls, which have a very sour and bitter taste which is acidic and caustic, and a pale orange tinge to them.",\n]\nlabels = [\n    {"fruit": "neoskizzles", "color": "purple", "flavor": "candy"},\n    {"fruit": "pounits", "color": "green", "flavor": "savory"},\n    {"fruit": "glowls", "color": "orange", "flavor": "sour, bitter"},\n]\nexamples = [\n    {"id": "0", "sentence": sentences[0], "target": labels[0]},\n    {"id": "1", "sentence": sentences[1], "target": labels[1]},\n    {"id": "2", "sentence": sentences[2], "target": labels[2]},\n]\n\n\n# 3. Define a scoring function for your evaluation\n@weave.op()\ndef fruit_name_score(target: dict, output: FruitExtract) -> dict:\n    target_flavors = [f.strip().lower() for f in target["flavor"].split(",")]\n    output_flavors = [f.strip().lower() for f in output.flavor.split(",")]\n    # Check if any target flavor is present in the output flavors\n    matches = any(tf in of for tf in target_flavors for of in output_flavors)\n    return {"correct": matches}\n\n\n# 4. Run your evaluation\nevaluation = weave.Evaluation(\n    name="fruit_eval",\n    dataset=examples,\n    scorers=[fruit_name_score],\n)\nawait evaluation.evaluate(model)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"-looking-for-more-examples",children:"\ud83d\ude80 Looking for more examples?"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Learn how to build an ",(0,o.jsx)(n.a,{href:"https://weave-docs.wandb.ai/tutorial-eval",children:"evlauation pipeline end-to-end"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:["Learn how to evaluate a ",(0,o.jsx)(n.a,{href:"https://weave-docs.wandb.ai/tutorial-rag",children:"RAG application by building"}),"."]}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,a.a)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},11151:(e,n,t)=>{t.d(n,{Z:()=>i,a:()=>s});var o=t(67294);const a={},r=o.createContext(a);function s(e){const n=o.useContext(r);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);