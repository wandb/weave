"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[1061],{87616:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>i,metadata:()=>r,toc:()=>d});var t=a(85893),s=a(11151);const i={},o="LlamaIndex",r={id:"guides/integrations/llamaindex",title:"LlamaIndex",description:"Weave integrates with LlamaIndex, a powerful framework for building LLM-driven applications like RAG systems, chatbots, and agents.",source:"@site/docs/guides/integrations/llamaindex.md",sourceDirName:"guides/integrations",slug:"/guides/integrations/llamaindex",permalink:"/guides/integrations/llamaindex",draft:!1,unlisted:!1,editUrl:"https://github.com/wandb/weave/blob/master/docs/docs/guides/integrations/llamaindex.md",tags:[],version:"current",lastUpdatedAt:1754449325e3,frontMatter:{},sidebar:"documentationSidebar",previous:{title:"LangChain",permalink:"/guides/integrations/langchain"},next:{title:"DSPy",permalink:"/guides/integrations/dspy"}},l={},d=[{value:"Get started",id:"get-started",level:2},{value:"Advanced usage",id:"advanced-usage",level:2},{value:"Synchronous and asynchronous completions",id:"synchronous-and-asynchronous-completions",level:3},{value:"Streaming operations",id:"streaming-operations",level:3},{value:"Chat interface",id:"chat-interface",level:3},{value:"Tool calling",id:"tool-calling",level:3},{value:"Tracking agents",id:"tracking-agents",level:3},{value:"Workflows",id:"workflows",level:3},{value:"RAG pipelines",id:"rag-pipelines",level:3},{value:"Complete example: Tracking agents",id:"complete-example-tracking-agents",level:2}];function c(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"llamaindex",children:"LlamaIndex"}),"\n",(0,t.jsxs)(n.p,{children:["Weave integrates with ",(0,t.jsx)(n.a,{href:"https://docs.llamaindex.ai/en/stable/",children:"LlamaIndex"}),", a powerful framework for building LLM-driven applications like RAG systems, chatbots, and agents."]}),"\n",(0,t.jsxs)(n.p,{children:["Using ",(0,t.jsx)(n.a,{href:"https://docs.llamaindex.ai/en/stable/module_guides/observability/instrumentation/",children:"LlamaIndex\u2019s instrumentation system"}),", the Weave integration automatically captures:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"Execution time"}),": Duration of each operation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"Token usage"}),": Input and output token counts"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"Cost tracking"}),": Estimated costs for API calls"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"Inputs and outputs"}),": Full request and response data"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"Error handling"}),": Detailed error traces and stack traces"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"Nested operations"}),": Complete trace hierarchy showing parent-child relationships"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"Streaming data"}),": Accumulated streaming responses"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"All trace data is viewable in the Weave UI, making it easy to debug and optimize your LlamaIndex applications."}),"\n",(0,t.jsxs)(n.p,{children:["For a basic usage example, see ",(0,t.jsx)(n.a,{href:"#get-started",children:"Get started"}),". For more examples demonstrating advanced usage, see ",(0,t.jsx)(n.a,{href:"#advanced-usage",children:"Advanced usage"})," and the ",(0,t.jsx)(n.a,{href:"#complete-example-tracking-agents",children:"Complete example: Tracking agents"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"get-started",children:"Get started"}),"\n",(0,t.jsxs)(n.p,{children:["To get started, call ",(0,t.jsx)(n.code,{children:"weave.init()"})," in your LLM application. The integration will begin tracing all LlamaIndex operations automatically."]}),"\n",(0,t.jsxs)(n.p,{children:["The example below initializes a Weave project ",(0,t.jsx)(n.code,{children:"llamaindex-demo"}),", sends a prompt to ",(0,t.jsx)(n.code,{children:"gpt-4o-mini"}),", and prints the result."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import weave\nfrom llama_index.llms.openai import OpenAI\n\n# Initialize Weave with your project name\nweave.init("llamaindex-demo")\n\n# All LlamaIndex operations are now automatically traced\nllm = OpenAI(model="gpt-4o-mini")\nresponse = llm.complete("William Shakespeare is ")\nprint(response)\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"LlamaIndex Demo",src:a(75028).Z+"",width:"3442",height:"1974"})}),"\n",(0,t.jsxs)(n.p,{children:["Next, try out the examples in ",(0,t.jsx)(n.a,{href:"#advanced-usage",children:"Advanced usage"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"advanced-usage",children:"Advanced usage"}),"\n",(0,t.jsx)(n.p,{children:"The examples below demonstrate more advanced LlamaIndex features, including streaming, chat, tool calling, and workflows. Each example includes a short description of what it does and how Weave traces it."}),"\n",(0,t.jsx)(n.h3,{id:"synchronous-and-asynchronous-completions",children:"Synchronous and asynchronous completions"}),"\n",(0,t.jsxs)(n.p,{children:["Use these methods to get text completions either synchronously or with ",(0,t.jsx)(n.code,{children:"await"}),". Weave traces both methods."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import weave\nfrom llama_index.llms.openai import OpenAI\n\nweave.init("llamaindex-demo")\n\nllm = OpenAI(model="gpt-4o-mini")\n\n# Synchronous completion\nresponse = llm.complete("William Shakespeare is ")\nprint(response)\n\n# Asynchronous completion\nresponse = await llm.acomplete("William Shakespeare is ")\nprint(response)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"streaming-operations",children:"Streaming operations"}),"\n",(0,t.jsx)(n.p,{children:"LlamaIndex supports streaming token output from completions. Weave captures the token stream for both sync and async modes."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import weave\nfrom llama_index.llms.openai import OpenAI\n\nweave.init("llamaindex-demo")\n\nllm = OpenAI(model="gpt-4o-mini")\n\n# Synchronous streaming\nhandle = llm.stream_complete("William Shakespeare is ")\nfor token in handle:\n    print(token.delta, end="", flush=True)\n\n# Asynchronous streaming\nhandle = await llm.astream_complete("William Shakespeare is ")\nasync for token in handle:\n    print(token.delta, end="", flush=True)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"chat-interface",children:"Chat interface"}),"\n",(0,t.jsx)(n.p,{children:"The chat interface allows for back-and-forth message exchanges. This is useful for building assistants or more interactive experiences. Weave traces all chat interactions."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import weave\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core.llms import ChatMessage\n\nweave.init("llamaindex-demo")\n\nllm = OpenAI(model="gpt-4o-mini")\nmessages = [\n    ChatMessage(role="system", content="You are a helpful assistant."),\n    ChatMessage(role="user", content="Tell me a joke."),\n]\n\n# Synchronous chat\nresponse = llm.chat(messages)\nprint(response)\n\n# Asynchronous chat\nresponse = await llm.achat(messages)\nprint(response)\n\n# Streaming chat\nhandle = llm.stream_chat(messages)\nfor token in handle:\n    print(token.delta, end="", flush=True)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"tool-calling",children:"Tool calling"}),"\n",(0,t.jsx)(n.p,{children:"Tool calling is commonly used when building agents and workflows. Weave automatically traces each tool call and its result."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import weave\nfrom pydantic import BaseModel\nfrom llama_index.core.tools import FunctionTool\nfrom llama_index.llms.openai import OpenAI\n\nweave.init("llamaindex-demo")\n\nclass Song(BaseModel):\n    name: str\n    artist: str\n\ndef generate_song(name: str, artist: str) -> Song:\n    return Song(name=name, artist=artist)\n\ntool = FunctionTool.from_defaults(fn=generate_song)\nllm = OpenAI(model="gpt-4o-mini")\n\nresponse = llm.predict_and_call([tool], "Pick a random song for me")\nprint(response)\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"LlamaIndex Tool Calling",src:a(52442).Z+"",width:"3456",height:"1988"})}),"\n",(0,t.jsx)(n.h3,{id:"tracking-agents",children:"Tracking agents"}),"\n",(0,t.jsx)(n.p,{children:"Agents in LlamaIndex use LLMs and tools to solve tasks. They can ask clarifying questions or call functions repeatedly. Weave captures every step in the flow."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import asyncio\nimport weave\nfrom llama_index.core.agent.workflow import FunctionAgent\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core.memory import ChatMemoryBuffer\n\nweave.init("llamaindex-demo")\n\ndef multiply(a: float, b: float) -> float:\n    return a * b\n\nagent = FunctionAgent(\n    tools=[multiply],\n    llm=OpenAI(model="gpt-4o-mini"),\n    system_prompt="You are a helpful assistant that can multiply two numbers.",\n)\n\nmemory = ChatMemoryBuffer.from_defaults(token_limit=40000)\n\n@weave.op()\nasync def main(query: str):\n    response = await agent.run(query, memory=memory)\n    return str(response)\n\nif __name__ == "__main__":\n    response = asyncio.run(main("What is 1234 * 4567?"))\n    print(response)\n'})}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"@weave.op()"})," decorator wraps the ",(0,t.jsx)(n.code,{children:"main()"})," function so it can be traced in Weave. This helps capture custom async workflows clearly in the trace tree."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"LlamaIndex Agent",src:a(62307).Z+"",width:"3456",height:"1990"})}),"\n",(0,t.jsx)(n.h3,{id:"workflows",children:"Workflows"}),"\n",(0,t.jsx)(n.p,{children:"LlamaIndex workflows use event-based steps to build structured execution logic. Weave traces each step and the data passed between them."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import weave\nfrom llama_index.core.workflow import (\n    StartEvent,\n    StopEvent,\n    Workflow,\n    step,\n    Event,\n)\n\nweave.init("llamaindex-demo")\n\nclass FirstEvent(Event):\n    payload: str\n\nclass SecondEvent(Event):\n    payload: str\n\nclass SimpleWorkflow(Workflow):\n    @step\n    async def step_one(self, ev: StartEvent) -> FirstEvent:\n        return FirstEvent(payload="First step complete")\n\n    @step\n    async def step_two(self, ev: FirstEvent) -> SecondEvent:\n        return SecondEvent(payload="Second step complete")\n\n    @step\n    async def step_three(self, ev: SecondEvent) -> StopEvent:\n        return StopEvent(result="Workflow complete")\n\nworkflow = SimpleWorkflow(timeout=10, verbose=False)\nresult = await workflow.run(first_input="Start the workflow")\nprint(result)\n'})}),"\n",(0,t.jsx)(n.p,{children:"Weave traces each event and transition, which is useful for debugging multi-step flows with conditionals, branches, or loops."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"LlamaIndex Workflow",src:a(15584).Z+"",width:"3456",height:"1988"})}),"\n",(0,t.jsx)(n.h3,{id:"rag-pipelines",children:"RAG pipelines"}),"\n",(0,t.jsx)(n.p,{children:"RAG (Retrieval Augmented Generation) pipelines use LLMs with external context. This example loads documents, builds an index, and queries it. Weave traces the parsing, indexing, and querying stages."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import weave\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.llms.openai import OpenAI\n\nweave.init("llamaindex-demo")\n\n# Load and process documents\ndocuments = SimpleDirectoryReader("data").load_data()\nparser = SentenceSplitter()\nnodes = parser.get_nodes_from_documents(documents)\n\n# Create index and query engine\nindex = VectorStoreIndex(nodes)\nquery_engine = index.as_query_engine()\n\n# Query the documents\nresponse = query_engine.query("What did the author do growing up?")\nprint(response)\n'})}),"\n",(0,t.jsxs)(n.p,{children:["Weave shows separate traces for node parsing, index creation, and querying. To view all steps under a single trace, wrap the logic in a ",(0,t.jsx)(n.code,{children:"@weave.op()"})," function. Note that parsing and indexing are typically one-time setup steps."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.img,{alt:"LlamaIndex RAG",src:a(68757).Z+"",width:"988",height:"450"}),"\n",(0,t.jsx)(n.img,{alt:"LlamaIndex RAG",src:a(90609).Z+"",width:"3456",height:"1998"})]}),"\n",(0,t.jsx)(n.h2,{id:"complete-example-tracking-agents",children:"Complete example: Tracking agents"}),"\n",(0,t.jsx)(n.p,{children:"This end-to-end example shows how to combine a document search tool and a math tool in a single agent. Weave traces the full interaction, including tool calls and the final response."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import weave\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.core.agent.workflow import FunctionAgent\nfrom llama_index.llms.openai import OpenAI\n\nweave.init("llamaindex-demo")\n\n# Create a RAG tool\ndocuments = SimpleDirectoryReader("data").load_data()\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine()\n\ndef multiply(a: float, b: float) -> float:\n    return a * b\n\nasync def search_documents(query: str) -> str:\n    response = await query_engine.aquery(query)\n    return str(response)\n\n# Create an agent with both tools\nagent = FunctionAgent(\n    tools=[multiply, search_documents],\n    llm=OpenAI(model="gpt-4o-mini"),\n    system_prompt="""You are a helpful assistant that can perform calculations\n    and search through documents to answer questions.""",\n)\n\nresponse = await agent.run(\n    "What did the author do in college? Also, what\'s 7 * 8?"\n)\nprint(response)\n'})}),"\n",(0,t.jsx)(n.p,{children:"This combines retrieval and arithmetic tasks. Weave traces the question routing, tool invocations, and agent response timeline."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"LlamaIndex Final Demo",src:a(7226).Z+"",width:"3456",height:"1988"})})]})}function m(e={}){const{wrapper:n}={...(0,s.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},62307:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/agent_trace-ab121894ee9995bae78bcb98f124f056.png"},7226:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/complete-11e1c1b25fbcc6879658cd73fe607daf.png"},75028:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/simple_trace-a83c09667d31d9b75c5781df6ee1fc15.png"},90609:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/single_rag_trace-21b6dffb6b15de06e45af8f3e335949a.png"},68757:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/step_rag_trace-c32444aee5ec8600da87af7347b154b4.png"},52442:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/tool_call_trace-d2f733c0e4f1c3ec2c62a29e26b933c8.png"},15584:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/workflow_trace-16c0047b913bd490a0d135933c607993.png"},11151:(e,n,a)=>{a.d(n,{Z:()=>r,a:()=>o});var t=a(67294);const s={},i=t.createContext(s);function o(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);