"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[25],{72862:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>p,contentTitle:()=>a,default:()=>l,frontMatter:()=>i,metadata:()=>s,toc:()=>c});var r=t(85893),o=t(11151);const i={},a="Send OpenTelemetry Traces",s={id:"guides/tracking/otel",title:"Send OpenTelemetry Traces",description:"Overview",source:"@site/docs/guides/tracking/otel.md",sourceDirName:"guides/tracking",slug:"/guides/tracking/otel",permalink:"/guides/tracking/otel",draft:!1,unlisted:!1,editUrl:"https://github.com/wandb/weave/blob/master/docs/docs/guides/tracking/otel.md",tags:[],version:"current",lastUpdatedAt:1749652482e3,frontMatter:{},sidebar:"documentationSidebar",previous:{title:"Navigate the Trace View",permalink:"/guides/tracking/trace-tree"},next:{title:"Video Support",permalink:"/guides/tracking/video"}},p={},c=[{value:"Overview",id:"overview",level:2},{value:"Endpoint details",id:"endpoint-details",level:2},{value:"Authentication",id:"authentication",level:2},{value:"Required Headers",id:"required-headers",level:2},{value:"Examples:",id:"examples",level:2},{value:"OpenInference Instrumentation:",id:"openinference-instrumentation",level:3},{value:"OpenLLMetry Instrumentation:",id:"openllmetry-instrumentation",level:3},{value:"Without Instrumentation",id:"without-instrumentation",level:3}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.a)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"send-opentelemetry-traces",children:"Send OpenTelemetry Traces"}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"Weave supports ingestion of OpenTelemetry compatible trace data through a dedicated endpoint. This endpoint allows you to send OTLP (OpenTelemetry Protocol) formatted trace data directly to your Weave project."}),"\n",(0,r.jsx)(n.h2,{id:"endpoint-details",children:"Endpoint details"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Path"}),": ",(0,r.jsx)(n.code,{children:"/otel/v1/traces"}),"\n",(0,r.jsx)(n.strong,{children:"Method"}),": POST\n",(0,r.jsx)(n.strong,{children:"Content-Type"}),": ",(0,r.jsx)(n.code,{children:"application/x-protobuf"})]}),"\n",(0,r.jsx)(n.h2,{id:"authentication",children:"Authentication"}),"\n",(0,r.jsx)(n.p,{children:"Standard W&B authentication is used. You must have write permissions to the project where you're sending trace data."}),"\n",(0,r.jsx)(n.h2,{id:"required-headers",children:"Required Headers"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"project_id: <your_entity>/<your_project_name>"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"Authorization=Basic <Base64 Encoding of api:$WANDB_API_KEY>"})}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"examples",children:"Examples:"}),"\n",(0,r.jsx)(n.p,{children:"You must modify the following fields before you can run the code samples below:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"WANDB_API_KEY"}),": You can get this from ",(0,r.jsx)(n.a,{href:"https://wandb.ai/authorize",children:"https://wandb.ai/authorize"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["Entity: You can only log traces to the project under an entity that you have access to. You can find your entity name by visiting your W&N dashboard at [",(0,r.jsx)(n.a,{href:"https://wandb.ai/home",children:"https://wandb.ai/home"}),"],"," and checking the ",(0,r.jsx)(n.strong,{children:"Teams"})," field in the left sidebar."]}),"\n",(0,r.jsx)(n.li,{children:"Project Name: Choose a fun name!"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"OPENAI_API_KEY"}),": You can obtain this from the ",(0,r.jsx)(n.a,{href:"https://platform.openai.com/api-keys",children:"OpenAI dashboard"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"openinference-instrumentation",children:"OpenInference Instrumentation:"}),"\n",(0,r.jsxs)(n.p,{children:["This example shows how to use the OpenAI instrumentation. There are many more available which you can find in the official repository: ",(0,r.jsx)(n.a,{href:"https://github.com/Arize-ai/openinference",children:"https://github.com/Arize-ai/openinference"})]}),"\n",(0,r.jsx)(n.p,{children:"First, install the required dependencies:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install openai openinference-instrumentation-openai opentelemetry-exporter-otlp-proto-http\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Next, paste the following code into a python file such as ",(0,r.jsx)(n.code,{children:"openinference_example.py"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import base64\nimport openai\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk import trace as trace_sdk\nfrom opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n\nOPENAI_API_KEY="YOUR_OPENAI_API_KEY"\nWANDB_BASE_URL = "https://trace.wandb.ai"\nPROJECT_ID = "<your-entity>/<your-project>"\n\nOTEL_EXPORTER_OTLP_ENDPOINT = f"{WANDB_BASE_URL}/otel/v1/traces"\n\n# Can be found at https://wandb.ai/authorize\nWANDB_API_KEY = "<your-wandb-api-key>"\nAUTH = base64.b64encode(f"api:{WANDB_API_KEY}".encode()).decode()\n\nOTEL_EXPORTER_OTLP_HEADERS = {\n    "Authorization": f"Basic {AUTH}",\n    "project_id": PROJECT_ID,\n}\n\ntracer_provider = trace_sdk.TracerProvider()\n\n# Configure the OTLP exporter\nexporter = OTLPSpanExporter(\n    endpoint=OTEL_EXPORTER_OTLP_ENDPOINT,\n    headers=OTEL_EXPORTER_OTLP_HEADERS,\n)\n\n# Add the exporter to the tracer provider\ntracer_provider.add_span_processor(SimpleSpanProcessor(exporter))\n\n# Optionally, print the spans to the console.\ntracer_provider.add_span_processor(SimpleSpanProcessor(ConsoleSpanExporter()))\n\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n\ndef main():\n    client = openai.OpenAI(api_key=OPENAI_API_KEY)\n    response = client.chat.completions.create(\n        model="gpt-3.5-turbo",\n        messages=[{"role": "user", "content": "Describe OTEL in a single sentence."}],\n        max_tokens=20,\n        stream=True,\n        stream_options={"include_usage": True},\n    )\n    for chunk in response:\n        if chunk.choices and (content := chunk.choices[0].delta.content):\n            print(content, end="")\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,r.jsx)(n.p,{children:"Finally, once you have set the fields specified above to their correct values, run the code:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"python openinference_example.py\n"})}),"\n",(0,r.jsx)(n.h3,{id:"openllmetry-instrumentation",children:"OpenLLMetry Instrumentation:"}),"\n",(0,r.jsxs)(n.p,{children:["The following example shows how to use the OpenAI instrumentation. Additional examples are available at ",(0,r.jsx)(n.a,{href:"https://github.com/traceloop/openllmetry/tree/main/packages",children:"https://github.com/traceloop/openllmetry/tree/main/packages"}),"."]}),"\n",(0,r.jsx)(n.p,{children:"First install the required dependencies:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install openai opentelemetry-instrumentation-openai opentelemetry-exporter-otlp-proto-http\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Next, paste the following code into a python file such as ",(0,r.jsx)(n.code,{children:"openllmetry_example.py"}),". Note that this is the same code as above, except the ",(0,r.jsx)(n.code,{children:"OpenAIInstrumentor"})," is imported from ",(0,r.jsx)(n.code,{children:"opentelemetry.instrumentation.openai"})," instead of ",(0,r.jsx)(n.code,{children:"openinference.instrumentation.openai"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import base64\nimport openai\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk import trace as trace_sdk\nfrom opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor\nfrom opentelemetry.instrumentation.openai import OpenAIInstrumentor\n\nOPENAI_API_KEY="YOUR_OPENAI_API_KEY"\nWANDB_BASE_URL = "https://trace.wandb.ai"\nPROJECT_ID = "<your-entity>/<your-project>"\n\nOTEL_EXPORTER_OTLP_ENDPOINT = f"{WANDB_BASE_URL}/otel/v1/traces"\n\n# Can be found at https://wandb.ai/authorize\nWANDB_API_KEY = "<your-wandb-api-key>"\nAUTH = base64.b64encode(f"api:{WANDB_API_KEY}".encode()).decode()\n\nOTEL_EXPORTER_OTLP_HEADERS = {\n    "Authorization": f"Basic {AUTH}",\n    "project_id": PROJECT_ID,\n}\n\ntracer_provider = trace_sdk.TracerProvider()\n\n# Configure the OTLP exporter\nexporter = OTLPSpanExporter(\n    endpoint=OTEL_EXPORTER_OTLP_ENDPOINT,\n    headers=OTEL_EXPORTER_OTLP_HEADERS,\n)\n\n# Add the exporter to the tracer provider\ntracer_provider.add_span_processor(SimpleSpanProcessor(exporter))\n\n# Optionally, print the spans to the console.\ntracer_provider.add_span_processor(SimpleSpanProcessor(ConsoleSpanExporter()))\n\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n\ndef main():\n    client = openai.OpenAI(api_key=OPENAI_API_KEY)\n    response = client.chat.completions.create(\n        model="gpt-3.5-turbo",\n        messages=[{"role": "user", "content": "Describe OTEL in a single sentence."}],\n        max_tokens=20,\n        stream=True,\n        stream_options={"include_usage": True},\n    )\n    for chunk in response:\n        if chunk.choices and (content := chunk.choices[0].delta.content):\n            print(content, end="")\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,r.jsx)(n.p,{children:"Finally, once you have set the fields specified above to their correct values, run the code:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"python openllmetry_example.py\n"})}),"\n",(0,r.jsx)(n.h3,{id:"without-instrumentation",children:"Without Instrumentation"}),"\n",(0,r.jsxs)(n.p,{children:["If you would prefer to use OTEL directly instead of an instrumentation package, you may do so. Span attributes will be parsed according to the OpenTelemetry semantic conventions described at ",(0,r.jsx)(n.a,{href:"https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-spans/",children:"https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-spans/"}),"."]}),"\n",(0,r.jsx)(n.p,{children:"First, install the required dependencies:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install openai opentelemetry-sdk opentelemetry-api opentelemetry-exporter-otlp-proto-http\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Next, paste the following code into a python file such as ",(0,r.jsx)(n.code,{children:"opentelemetry_example.py"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import json\nimport base64\nimport openai\nfrom opentelemetry import trace\nfrom opentelemetry.sdk import trace as trace_sdk\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor\n\nOPENAI_API_KEY = "YOUR_OPENAI_API_KEY"\nWANDB_BASE_URL = "https://trace.wandb.ai"\nPROJECT_ID = "<your-entity>/<your-project>"\n\nOTEL_EXPORTER_OTLP_ENDPOINT = f"{WANDB_BASE_URL}/otel/v1/traces"\n\n# Can be found at https://wandb.ai/authorize\nWANDB_API_KEY = "<your-wandb-api-key>"\nAUTH = base64.b64encode(f"api:{WANDB_API_KEY}".encode()).decode()\n\nOTEL_EXPORTER_OTLP_HEADERS = {\n    "Authorization": f"Basic {AUTH}",\n    "project_id": PROJECT_ID,\n}\n\ntracer_provider = trace_sdk.TracerProvider()\n\n# Configure the OTLP exporter\nexporter = OTLPSpanExporter(\n    endpoint=OTEL_EXPORTER_OTLP_ENDPOINT,\n    headers=OTEL_EXPORTER_OTLP_HEADERS,\n)\n\n# Add the exporter to the tracer provider\ntracer_provider.add_span_processor(SimpleSpanProcessor(exporter))\n\n# Optionally, print the spans to the console.\ntracer_provider.add_span_processor(SimpleSpanProcessor(ConsoleSpanExporter()))\n\ntrace.set_tracer_provider(tracer_provider)\n# Creates a tracer from the global tracer provider\ntracer = trace.get_tracer(__name__)\ntracer.start_span(\'name=standard-span\')\n\ndef my_function():\n    with tracer.start_as_current_span("outer_span") as outer_span:\n        client = openai.OpenAI()\n        input_messages=[{"role": "user", "content": "Describe OTEL in a single sentence."}]\n        # This will only appear in the side panel\n        outer_span.set_attribute("input.value", json.dumps(input_messages))\n        # This follows conventions and will appear in the dashboard\n        outer_span.set_attribute("gen_ai.system", \'openai\')\n        response = client.chat.completions.create(\n            model="gpt-3.5-turbo",\n            messages=input_messages,\n            max_tokens=20,\n            stream=True,\n            stream_options={"include_usage": True},\n        )\n        out = ""\n        for chunk in response:\n            if chunk.choices and (content := chunk.choices[0].delta.content):\n                out += content\n        # This will only appear in the side panel\n        outer_span.set_attribute("output.value", json.dumps({"content": out}))\n\nif __name__ == "__main__":\n    my_function()\n'})}),"\n",(0,r.jsx)(n.p,{children:"Finally, once you have set the fields specified above to their correct values, run the code:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"python opentelemetry_example.py\n"})}),"\n",(0,r.jsxs)(n.p,{children:["The span attribute prefixes ",(0,r.jsx)(n.code,{children:"gen_ai"})," and ",(0,r.jsx)(n.code,{children:"openinference"})," are used to determine which convention to use, if any, when interpreting the trace. If neither key is detected, then all span attributes are visible in the trace view. The full span is available in the side panel when you select a trace."]})]})}function l(e={}){const{wrapper:n}={...(0,o.a)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},11151:(e,n,t)=>{t.d(n,{Z:()=>s,a:()=>a});var r=t(67294);const o={},i=r.createContext(o);function a(e){const n=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);