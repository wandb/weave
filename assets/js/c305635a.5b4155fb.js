"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[6671],{33106:(e,o,n)=>{n.d(o,{ZP:()=>d,d$:()=>i});var t=n(85893);const a="[User Settings](https://docs.wandb.ai/guides/models/app/settings-page/user-settings/#default-team)",l="To find or update your default entity, refer to";const s=function({variant:e="full"}){return"inline"===e?(0,t.jsxs)(t.Fragment,{children:["If not specified, your default entity is used. ",l," ",a," in the W&B Models documentation."]}):(0,t.jsxs)(t.Fragment,{children:["If you don't specify a W&B team when you call `weave.init()`, your default entity is used. ",l," ",a," in the W&B Models documentation."]})},i=[];function r(e){return(0,t.jsx)(t.Fragment,{})}function d(e={}){return(0,t.jsx)(s,{...e,children:(0,t.jsx)(r,{...e})})}},78005:(e,o,n)=>{n.r(o),n.d(o,{assets:()=>d,contentTitle:()=>i,default:()=>u,frontMatter:()=>s,metadata:()=>r,toc:()=>c});var t=n(85893),a=n(11151),l=n(33106);const s={},i="Local Models",r={id:"guides/integrations/local_models",title:"Local Models",description:"Many developers download and run open source models like LLama-3, Mixtral, Gemma, Phi and more locally. There are quite a few ways of running these models locally and Weave supports a few of them out of the box, as long as they support OpenAI SDK compatibility.",source:"@site/docs/guides/integrations/local_models.md",sourceDirName:"guides/integrations",slug:"/guides/integrations/local_models",permalink:"/guides/integrations/local_models",draft:!1,unlisted:!1,editUrl:"https://github.com/wandb/weave/blob/master/docs/docs/guides/integrations/local_models.md",tags:[],version:"current",lastUpdatedAt:1759434814e3,frontMatter:{},sidebar:"documentationSidebar",previous:{title:"Together AI",permalink:"/guides/integrations/together_ai"},next:{title:"OpenAI Agents SDK",permalink:"/guides/integrations/openai_agents"}},d={},c=[{value:"Wrap local model functions with <code>@weave.op()</code>",id:"wrap-local-model-functions-with-weaveop",level:2},...l.d$,{value:"Updating your OpenAI SDK code to use local models",id:"updating-your-openai-sdk-code-to-use-local-models",level:2},{value:"OpenAI SDK supported Local Model runners",id:"openai-sdk-supported-local-model-runners",level:2}];function p(e){const o={a:"a",code:"code",h1:"h1",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",...(0,a.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(o.h1,{id:"local-models",children:"Local Models"}),"\n","\n",(0,t.jsx)(o.p,{children:"Many developers download and run open source models like LLama-3, Mixtral, Gemma, Phi and more locally. There are quite a few ways of running these models locally and Weave supports a few of them out of the box, as long as they support OpenAI SDK compatibility."}),"\n",(0,t.jsxs)(o.h2,{id:"wrap-local-model-functions-with-weaveop",children:["Wrap local model functions with ",(0,t.jsx)(o.code,{children:"@weave.op()"})]}),"\n",(0,t.jsxs)(o.p,{children:["You can integrate Weave with any LLM yourself by initializing Weave with ",(0,t.jsx)(o.code,{children:"weave.init('<your-project-name>')"})," and then wrapping the calls to your LLMs with ",(0,t.jsx)(o.code,{children:"weave.op()"}),". See our guide on ",(0,t.jsx)(o.a,{href:"/guides/tracking/tracing",children:"tracing"})," for more details."]}),"\n",(0,t.jsx)(l.ZP,{}),"\n",(0,t.jsx)(o.h2,{id:"updating-your-openai-sdk-code-to-use-local-models",children:"Updating your OpenAI SDK code to use local models"}),"\n",(0,t.jsx)(o.p,{children:"All of the frameworks of services that support OpenAI SDK compatibility require a few minor changes."}),"\n",(0,t.jsxs)(o.p,{children:["First and most important, is the ",(0,t.jsx)(o.code,{children:"base_url"})," change during the ",(0,t.jsx)(o.code,{children:"openai.OpenAI()"})," initialization."]}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-python",children:'client = openai.OpenAI(\n    base_url="http://localhost:1234",\n)\n'})}),"\n",(0,t.jsxs)(o.p,{children:["In the case of local models, the ",(0,t.jsx)(o.code,{children:"api_key"})," can be any string but it should be overridden, as otherwise OpenAI will try to use it from environment variables and show you an error."]}),"\n",(0,t.jsx)(o.h2,{id:"openai-sdk-supported-local-model-runners",children:"OpenAI SDK supported Local Model runners"}),"\n",(0,t.jsx)(o.p,{children:"Here's a list of apps that allows you to download and run models from Hugging Face on your computer, that support OpenAI SDK compatibility."}),"\n",(0,t.jsxs)(o.ol,{children:["\n",(0,t.jsxs)(o.li,{children:["Nomic ",(0,t.jsx)(o.a,{href:"https://www.nomic.ai/gpt4all",children:"GPT4All"})," - support via Local Server in settings (",(0,t.jsx)(o.a,{href:"https://docs.gpt4all.io/gpt4all_help/faq.html",children:"FAQ"}),")"]}),"\n",(0,t.jsxs)(o.li,{children:[(0,t.jsx)(o.a,{href:"https://lmstudio.ai/",children:"LMStudio"})," - Local Server OpenAI SDK support ",(0,t.jsx)(o.a,{href:"https://lmstudio.ai/docs/local-server",children:"docs"})]}),"\n",(0,t.jsxs)(o.li,{children:[(0,t.jsx)(o.a,{href:"https://ollama.com/",children:"Ollama"})," - ",(0,t.jsx)(o.a,{href:"https://github.com/ollama/ollama/blob/main/docs/openai.md",children:"Experimental Support"})," for OpenAI SDK"]}),"\n",(0,t.jsxs)(o.li,{children:["llama.cpp via ",(0,t.jsx)(o.a,{href:"https://llama-cpp-python.readthedocs.io/en/latest/server/",children:"llama-cpp-python"})," python package"]}),"\n",(0,t.jsxs)(o.li,{children:[(0,t.jsx)(o.a,{href:"https://github.com/Mozilla-Ocho/llamafile#other-example-llamafiles",children:"llamafile"})," - ",(0,t.jsx)(o.code,{children:"http://localhost:8080/v1"})," automatically supports OpenAI SDK on Llamafile run"]}),"\n"]})]})}function u(e={}){const{wrapper:o}={...(0,a.a)(),...e.components};return o?(0,t.jsx)(o,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}},11151:(e,o,n)=>{n.d(o,{Z:()=>i,a:()=>s});var t=n(67294);const a={},l=t.createContext(a);function s(e){const o=t.useContext(l);return t.useMemo((function(){return"function"==typeof e?e(o):{...o,...e}}),[o,e])}function i(e){let o;return o=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),t.createElement(l.Provider,{value:o},e.children)}}}]);