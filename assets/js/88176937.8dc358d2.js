"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[2993],{60802:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>i,metadata:()=>o,toc:()=>c});var t=r(85893),s=r(11151);r(65488),r(85162);const i={},a="Online Evaluation: Guardrails and Monitors",o={id:"guides/evaluation/guardrails_and_monitors",title:"Online Evaluation: Guardrails and Monitors",description:"Feedback",source:"@site/docs/guides/evaluation/guardrails_and_monitors.md",sourceDirName:"guides/evaluation",slug:"/guides/evaluation/guardrails_and_monitors",permalink:"/guides/evaluation/guardrails_and_monitors",draft:!1,unlisted:!1,editUrl:"https://github.com/wandb/weave/blob/master/docs/docs/guides/evaluation/guardrails_and_monitors.md",tags:[],version:"current",lastUpdatedAt:1757710212e3,frontMatter:{},sidebar:"documentationSidebar",previous:{title:"Redacting PII",permalink:"/guides/tracking/redact-pii"},next:{title:"Integrations",permalink:"/guides/integrations/"}},l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Ready-to-Use Scorers",id:"ready-to-use-scorers",level:4},{value:"Guardrails vs. Monitors: When to Use Each",id:"guardrails-vs-monitors-when-to-use-each",level:3},{value:"Using the <code>.call()</code> Method",id:"using-the-call-method",level:3},{value:"Getting Started with Scorers",id:"getting-started-with-scorers",level:2},{value:"Basic Example",id:"basic-example",level:3},{value:"Using Scorers as Guardrails",id:"using-scorers-as-guardrails",level:2},{value:"Using Scorers as monitors",id:"using-scorers-as-monitors",level:2},{value:"Create a monitor",id:"create-a-monitor",level:3},{value:"Example: Create a truthfulness monitor",id:"example-create-a-truthfulness-monitor",level:3},{value:"Prompt variables",id:"prompt-variables",level:3},{value:"AWS Bedrock Guardrails",id:"aws-bedrock-guardrails",level:2},{value:"Implementation Details",id:"implementation-details",level:2},{value:"The Scorer Interface",id:"the-scorer-interface",level:3},{value:"Score Parameters",id:"score-parameters",level:3},{value:"Parameter Matching Rules",id:"parameter-matching-rules",level:4},{value:"Handling Parameter Name Mismatches",id:"handling-parameter-name-mismatches",level:4},{value:"Adding Additional Parameters",id:"adding-additional-parameters",level:4},{value:"Using Scorers: Two Approaches",id:"using-scorers-two-approaches",level:3},{value:"Score Analysis",id:"score-analysis",level:3},{value:"Production Best Practices",id:"production-best-practices",level:2},{value:"1. Set Appropriate Sampling Rates",id:"1-set-appropriate-sampling-rates",level:3},{value:"2. Monitor Multiple Aspects",id:"2-monitor-multiple-aspects",level:3},{value:"3. Analyze and Improve",id:"3-analyze-and-improve",level:3},{value:"4. Access Historical Data",id:"4-access-historical-data",level:3},{value:"5. Initialize Guards Efficiently",id:"5-initialize-guards-efficiently",level:3},{value:"Complete Example",id:"complete-example",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",admonition:"admonition",br:"br",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",mdxAdmonitionTitle:"mdxAdmonitionTitle",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"online-evaluation-guardrails-and-monitors",children:"Online Evaluation: Guardrails and Monitors"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Feedback",src:r(62940).Z+"",width:"1917",height:"945"})}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsx)(n.p,{children:"Building production LLM applications? Two questions likely keep you up at night:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"How do you ensure your LLMs generate safe, appropriate content?"}),"\n",(0,t.jsx)(n.li,{children:"How do you measure and improve output quality over time?"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"W&B Weave's unified scoring system answers both questions through a simple yet powerful framework. Whether you need active safety controls (guardrails) or passive quality monitoring, this guide will show you how to implement robust evaluation systems for your LLM applications."}),"\n",(0,t.jsxs)(n.p,{children:["The foundation of Weave's evaluation system is the ",(0,t.jsx)(n.a,{href:"/guides/evaluation/scorers",children:(0,t.jsx)(n.strong,{children:"Scorer"})})," - a component that evaluates your function's inputs and outputs to measure quality, safety, or any other metric you care about. Scorers are versatile and can be used in two ways:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"As Guardrails"}),": Block or modify unsafe content before it reaches users"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"As Monitors"}),": Track quality metrics over time to identify trends and improvements"]}),"\n"]}),"\n",(0,t.jsx)(n.admonition,{title:"Terminology",type:"note",children:(0,t.jsxs)(n.p,{children:["Throughout this guide, we'll refer to functions decorated with ",(0,t.jsx)(n.code,{children:"@weave.op"}),' as "ops". These are regular Python functions that have been enhanced with Weave\'s tracking capabilities.']})}),"\n",(0,t.jsx)(n.h4,{id:"ready-to-use-scorers",children:"Ready-to-Use Scorers"}),"\n",(0,t.jsxs)(n.p,{children:["While this guide shows you how to create custom scorers, Weave comes with a variety of ",(0,t.jsx)(n.a,{href:"/guides/evaluation/builtin_scorers",children:"predefined scorers"})," that you can use right away, including:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/guides/evaluation/builtin_scorers#hallucinationfreescorer",children:"Hallucination detection"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/guides/evaluation/builtin_scorers#summarizationscorer",children:"Summarization quality"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/guides/evaluation/builtin_scorers#embeddingsimilarityscorer",children:"Embedding similarity"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/guides/evaluation/builtin_scorers#ragas---contextrelevancyscorer",children:"Relevancy evaluation"})}),"\n",(0,t.jsx)(n.li,{children:"And more!"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"guardrails-vs-monitors-when-to-use-each",children:"Guardrails vs. Monitors: When to Use Each"}),"\n",(0,t.jsx)(n.p,{children:"While scorers power both guardrails and monitors, they serve different purposes:"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Aspect"}),(0,t.jsx)(n.th,{children:"Guardrails"}),(0,t.jsx)(n.th,{children:"Monitors"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Purpose"})}),(0,t.jsx)(n.td,{children:"Active intervention to prevent issues"}),(0,t.jsx)(n.td,{children:"Passive observation for analysis"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Timing"})}),(0,t.jsx)(n.td,{children:"Real-time, before output reaches users"}),(0,t.jsx)(n.td,{children:"Can be asynchronous or batched"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Performance"})}),(0,t.jsx)(n.td,{children:"Must be fast (affects response time)"}),(0,t.jsx)(n.td,{children:"Can be slower, run in background"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Sampling"})}),(0,t.jsx)(n.td,{children:"Usually every request"}),(0,t.jsx)(n.td,{children:"Often sampled (e.g., 10% of calls)"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Control Flow"})}),(0,t.jsx)(n.td,{children:"Can block/modify outputs"}),(0,t.jsx)(n.td,{children:"No impact on application flow"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Resource Usage"})}),(0,t.jsx)(n.td,{children:"Must be efficient"}),(0,t.jsx)(n.td,{children:"Can use more resources if needed"})]})]})]}),"\n",(0,t.jsx)(n.p,{children:"For example, a toxicity scorer could be used:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\ud83d\udee1\ufe0f ",(0,t.jsx)(n.strong,{children:"As a Guardrail"}),": Block toxic content immediately"]}),"\n",(0,t.jsxs)(n.li,{children:["\ud83d\udcca ",(0,t.jsx)(n.strong,{children:"As a Monitor"}),": Track toxicity levels over time"]}),"\n"]}),"\n",(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsx)(n.p,{children:"Every scorer result is automatically stored in Weave's database. This means your guardrails double as monitors without any extra work! You can always analyze historical scorer results, regardless of how they were originally used."})}),"\n",(0,t.jsxs)(n.h3,{id:"using-the-call-method",children:["Using the ",(0,t.jsx)(n.code,{children:".call()"})," Method"]}),"\n",(0,t.jsxs)(n.p,{children:["To use scorers with Weave ops, you'll need access to both the operation's result and its tracking information. The ",(0,t.jsx)(n.code,{children:".call()"})," method provides both:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Instead of calling the op directly:\nresult = generate_text(input)  # Primary way to call the op but doesn't give access to the Call object\n\n# Use the .call() method to get both result and Call object:\nresult, call = generate_text.call(input)  # Now you can use the call object with scorers\n"})}),"\n",(0,t.jsxs)(n.admonition,{type:"tip",children:[(0,t.jsxs)(n.mdxAdmonitionTitle,{children:["Why Use ",(0,t.jsx)(n.code,{children:".call()"}),"?"]}),(0,t.jsx)(n.p,{children:"The Call object is essential for associating the score with the call in the database. While you can directly call the scoring function, this would not be associated with the call, and therefore not searchable, filterable, or exportable for later analysis."}),(0,t.jsxs)(n.p,{children:["For more details about Call objects, see the ",(0,t.jsx)(n.a,{href:"/guides/tracking/tracing#getting-a-handle-to-the-call-object-during-execution",children:"Calls guide section on Call objects"}),"."]})]}),"\n",(0,t.jsx)(n.h2,{id:"getting-started-with-scorers",children:"Getting Started with Scorers"}),"\n",(0,t.jsx)(n.h3,{id:"basic-example",children:"Basic Example"}),"\n",(0,t.jsxs)(n.p,{children:["Here's a simple example showing how to use ",(0,t.jsx)(n.code,{children:".call()"})," with a scorer:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import weave\nfrom weave import Scorer\n\nclass LengthScorer(Scorer):\n    @weave.op\n    def score(self, output: str) -> dict:\n        """A simple scorer that checks output length."""\n        return {\n            "length": len(output),\n            "is_short": len(output) < 100\n        }\n\n@weave.op\ndef generate_text(prompt: str) -> str:\n    return "Hello, world!"\n\n# Get both result and Call object\nresult, call = generate_text.call("Say hello")\n\n# Now you can apply scorers\nawait call.apply_scorer(LengthScorer())\n'})}),"\n",(0,t.jsx)(n.h2,{id:"using-scorers-as-guardrails",children:"Using Scorers as Guardrails"}),"\n",(0,t.jsx)(n.p,{children:"Guardrails act as safety checks that run before allowing LLM output to reach users. Here's a practical example:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import weave\nfrom weave import Scorer\n\n@weave.op\ndef generate_text(prompt: str) -> str:\n    """Generate text using an LLM."""\n    # Your LLM generation logic here\n    return "Generated response..."\n\nclass ToxicityScorer(Scorer):\n    @weave.op\n    def score(self, output: str) -> dict:\n        """\n        Evaluate content for toxic language.\n        """\n        # Your toxicity detection logic here\n        return {\n            "flagged": False,  # True if content is toxic\n            "reason": None     # Optional explanation if flagged\n        }\n\nasync def generate_safe_response(prompt: str) -> str:\n    # Get result and Call object\n    result, call = generate_text.call(prompt)\n    \n    # Check safety\n    safety = await call.apply_scorer(ToxicityScorer())\n    if safety.result["flagged"]:\n        return f"I cannot generate that content: {safety.result[\'reason\']}"\n    \n    return result\n'})}),"\n",(0,t.jsxs)(n.admonition,{title:"Scorer Timing",type:"note",children:[(0,t.jsx)(n.p,{children:"When applying scorers:"}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["The main operation (",(0,t.jsx)(n.code,{children:"generate_text"}),") completes and is marked as finished in the UI"]}),"\n",(0,t.jsx)(n.li,{children:"Scorers run asynchronously after the main operation"}),"\n",(0,t.jsx)(n.li,{children:"Scorer results are attached to the call once they complete"}),"\n",(0,t.jsx)(n.li,{children:"You can view scorer results in the UI or query them via the API"}),"\n"]})]}),"\n",(0,t.jsx)(n.h2,{id:"using-scorers-as-monitors",children:"Using Scorers as monitors"}),"\n",(0,t.jsx)(n.admonition,{type:"important",children:(0,t.jsx)(n.p,{children:"This feature is only available in Multi-Tenant (MT) SaaS deployments."})}),"\n",(0,t.jsxs)(n.p,{children:["If you want to track quality metrics without writing scoring logic into your app, you can use ",(0,t.jsx)(n.em,{children:"monitors"}),"."]}),"\n",(0,t.jsx)(n.p,{children:"A monitor is a background process that:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Watches one or more specified functions decorated with ",(0,t.jsx)(n.code,{children:"weave.op"})]}),"\n",(0,t.jsxs)(n.li,{children:["Scores a subset of calls using an ",(0,t.jsx)(n.em,{children:"LLM-as-a-judge"})," scorer, which is an LLM model with a specific prompt tailored to the ops you want to score"]}),"\n",(0,t.jsxs)(n.li,{children:["Runs automatically each time the specified ",(0,t.jsx)(n.code,{children:"weave.op"})," is called, no need to manually call ",(0,t.jsx)(n.code,{children:".apply_scorer()"})]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Monitors are ideal for:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Evaluating and tracking production behavior"}),"\n",(0,t.jsx)(n.li,{children:"Catching regressions or drift"}),"\n",(0,t.jsx)(n.li,{children:"Collecting real-world performance data over time"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Learn how to ",(0,t.jsx)(n.a,{href:"#create-a-monitor",children:"create a monitor in general"})," or try out the ",(0,t.jsx)(n.a,{href:"#example-create-a-truthfulness-monitor",children:"end-to-end example of creating a truthfulness monitor"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"create-a-monitor",children:"Create a monitor"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["From the left menu, select the ",(0,t.jsx)(n.strong,{children:"Monitors"})," tab."]}),"\n",(0,t.jsxs)(n.li,{children:["From the monitors page, click ",(0,t.jsx)(n.strong,{children:"New Monitor"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["In the drawer, configure the monitor:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Name"}),": Valid monitor names must start with a letter or number and can only contain letters, numbers, hyphens, and underscores."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Description"})," ",(0,t.jsx)(n.em,{children:"(optional)"}),": Explain what the monitor does."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Active monitor"})," toggle: Turn the monitor on or off."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Calls to monitor"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Operations"}),": Choose one or more ",(0,t.jsx)(n.code,{children:"@weave.op"}),"s to monitor.","\n",(0,t.jsx)(n.admonition,{type:"important",children:(0,t.jsx)(n.p,{children:"You must log at least one trace for an Op for it to appear in the list of available operations."})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Filter"})," ",(0,t.jsx)(n.em,{children:"(optional)"}),": Narrow down which op columns are eligible for monitoring (e.g., ",(0,t.jsx)(n.code,{children:"max_tokens"})," or ",(0,t.jsx)(n.code,{children:"top_p"}),")"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sampling rate"}),": The percentage of calls to be scored, between 0% and 100% (e.g., 10%)","\n",(0,t.jsxs)(n.admonition,{type:"tip",children:[(0,t.jsx)(n.mdxAdmonitionTitle,{}),(0,t.jsx)(n.p,{children:"A lower sampling rate is useful for controlling costs, as each scoring call has a cost associated with it."})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"LLM-as-a-Judge configuration"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scorer name"}),": Valid scorer names must start with a letter or number and can only contain letters, numbers, hyphens, and underscores."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Judge model"}),": Select the model that will score your ops. Three types of models are available:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/guides/tools/playground#saved-models",children:"Saved models"})}),"\n",(0,t.jsx)(n.li,{children:"Models from providers configured by your W&B admin"}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://docs.wandb.ai/guides/inference/models/",children:"W&B Inference models"})}),"\n"]}),"\n"]}),"\n"]}),"\n","For the selected model, configure the following settings:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Configuration name"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"System prompt"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Response format"})}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scoring prompt"}),": The prompt used by the LLM-as-a-judge to score your ops. \u201cYou can reference ",(0,t.jsx)(n.code,{children:"{output}"}),", individual inputs (like ",(0,t.jsx)(n.code,{children:"{foo}"}),"), and ",(0,t.jsx)(n.code,{children:"{inputs}"})," as a dictionary. ",(0,t.jsx)(n.a,{href:"#prompt-variables",children:"For more information, see prompt variables"}),".\u201d"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["Click ",(0,t.jsx)(n.strong,{children:"Create Monitor"}),". Weave will automatically begin monitoring and scoring calls that match the specified criteria. You can view monitor details in the ",(0,t.jsx)(n.strong,{children:"Monitors"})," tab."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"example-create-a-truthfulness-monitor",children:"Example: Create a truthfulness monitor"}),"\n",(0,t.jsx)(n.p,{children:"In the following example, you'll create:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"weave.op"})," to be monitored, ",(0,t.jsx)(n.code,{children:"generate_statement"}),". This function outputs statements that either returns the input ",(0,t.jsx)(n.code,{children:"ground_truth"})," statement (e.g. ",(0,t.jsx)(n.code,{children:'"The Earth revolves around the Sun."'}),"), or generates a statement that is incorrect based on the ",(0,t.jsx)(n.code,{children:"ground_truth"})," (e.g. ",(0,t.jsx)(n.code,{children:'"The Earth revolves around Saturn."'}),")"]}),"\n",(0,t.jsxs)(n.li,{children:["A monitor, ",(0,t.jsx)(n.code,{children:"truthfulness-monitor"}),", to evaluate the truthfulness of the generated statements."]}),"\n"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Define ",(0,t.jsx)(n.code,{children:"generate_statement"}),":","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:' import weave\n import random\n import openai\n\n # Replace my-team/my-weave-project with your Weave team and project name \n weave.init("my-team/my-weave-project")\n\n client = openai.OpenAI() \n\n @weave.op()\n def generate_statement(ground_truth: str) -> str:\n     if random.random() < 0.5:\n         response = openai.ChatCompletion.create(\n             model="gpt-4.1",\n             messages=[\n                 {\n                     "role": "user",\n                     "content": f"Generate a statement that is incorrect based on this fact: {ground_truth}"\n                 }\n             ]\n         )\n         return response.choices[0].message["content"]\n     else:\n         return ground_truth\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["Execute the code for ",(0,t.jsx)(n.code,{children:"generate_statement"}),"to log a trace. The ",(0,t.jsx)(n.code,{children:"generate_statement"})," op will not appear in the Op dropdown unless it was logged at least once."]}),"\n",(0,t.jsxs)(n.li,{children:["In the Weave UI, navigate to ",(0,t.jsx)(n.strong,{children:"Monitors"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["From the monitors page, click ",(0,t.jsx)(n.strong,{children:"New Monitor"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["Configure the monitor as follows:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Name"}),": ",(0,t.jsx)(n.code,{children:"truthfulness-monitor"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Description"}),":",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.code,{children:"A monitor to evaluate the truthfulness of statements generated by an LLM."})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Active monitor"})," toggle:",(0,t.jsx)(n.br,{}),"\n","Toggle ",(0,t.jsx)(n.strong,{children:"on"})," to begin scoring calls as soon as the monitor is created.\n",(0,t.jsx)(n.img,{alt:"Creating a monitor part 1",src:r(93429).Z+"",width:"994",height:"1330"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Calls to Monitor"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Operations"}),": ",(0,t.jsx)(n.code,{children:"generate_statement"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Filter"})," ",(0,t.jsx)(n.em,{children:"(optional)"}),":  None applied in this example, but could be used to scope monitoring by arguments like ",(0,t.jsx)(n.code,{children:"temperature"})," or ",(0,t.jsx)(n.code,{children:"max_tokens"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sampling rate"}),":",(0,t.jsx)(n.br,{}),"\n","Set to ",(0,t.jsx)(n.code,{children:"100%"})," to score every call.\n",(0,t.jsx)(n.img,{alt:"Creating a monitor part 2",src:r(1225).Z+"",width:"994",height:"1526"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"LLM-as-a-Judge Configuration"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scorer name"}),": ",(0,t.jsx)(n.code,{children:"truthfulness-scorer"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Judge model"}),":",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.code,{children:"o3-mini-2025-01-31"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model settings"}),":"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"LLM ID"}),": ",(0,t.jsx)(n.code,{children:"o3-mini-2025-01-31"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Configuration name"}),": ",(0,t.jsx)(n.code,{children:"truthfulness-scorer-judge-model"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"System prompt"}),":",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.code,{children:"You are an impartial AI judge. Your task is to evaluate the truthfulness of statements."})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Response format"}),": ",(0,t.jsx)(n.code,{children:"json_object"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scoring prompt"}),":","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"Evaluate whether the output statement is accurate based on the input statement.\n\nThis is the input statement: {ground_truth}\n\nThis is the output statement: {output}\n\nThe response should be a JSON object with the following fields:\n- is_true: a boolean stating whether the output statement is true or false based on the input statement.\n- reasoning: your reasoning as to why the statement is true or false.\n"})}),"\n",(0,t.jsx)(n.img,{alt:"Creating a monitor part 3",src:r(28038).Z+"",width:"994",height:"578"})]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["Click ",(0,t.jsx)(n.strong,{children:"Create Monitor"}),". The ",(0,t.jsx)(n.code,{children:"truthfulness-monitor"})," is ready to start monitoring."]}),"\n",(0,t.jsxs)(n.li,{children:["Generate statements for evaluation by the monitor with true and easily verifiable ",(0,t.jsx)(n.code,{children:"ground_truth"})," statements such as ",(0,t.jsx)(n.code,{children:'"Water freezes at 0 degrees Celsius."'}),".","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'generate_statement("The Earth revolves around the Sun.")\ngenerate_statement("Water freezes at 0 degrees Celsius.")\ngenerate_statement("The Great Wall of China was built over several centuries, with construction beginning as early as the 7th century BCE.")\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["In the Weave UI, navigate to the ",(0,t.jsx)(n.strong,{children:"Traces"})," tab."]}),"\n",(0,t.jsxs)(n.li,{children:["From the list of available traces, select any trace for ",(0,t.jsx)(n.strong,{children:"LLMAsAJudgeScorer.score"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["Inspect the trace to see the monitor in action. For this example, the monitor correctly evaluated the ",(0,t.jsx)(n.code,{children:"output"})," (in this instance, equivalent to the ",(0,t.jsx)(n.code,{children:"ground_truth"}),") as ",(0,t.jsx)(n.code,{children:"true"})," and provided sound ",(0,t.jsx)(n.code,{children:"reasoning"}),".\n",(0,t.jsx)(n.img,{alt:"Monitor trace",src:r(63235).Z+"",width:"3024",height:"1428"})]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"prompt-variables",children:"Prompt variables"}),"\n",(0,t.jsx)(n.p,{children:"In scoring prompts, you can reference multiple variables from your op. These values are automatically extracted from your function call when the scorer runs. Consider the following example function:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'@weave.op\ndef my_function(foo: str, bar: str) -> str:\n    return f"{foo} and {bar}"\n'})}),"\n",(0,t.jsx)(n.p,{children:"In this case, the following variables are accessible:"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Variable"}),(0,t.jsx)(n.th,{children:"Description"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"{foo}"})}),(0,t.jsxs)(n.td,{children:["The value of the input argument ",(0,t.jsx)(n.code,{children:"foo"})]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"{bar}"})}),(0,t.jsxs)(n.td,{children:["The value of the input argument ",(0,t.jsx)(n.code,{children:"bar"})]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"{inputs}"})}),(0,t.jsx)(n.td,{children:"A JSON dictionary of all input arguments"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"{output}"})}),(0,t.jsx)(n.td,{children:"The result returned by your op"})]})]})]}),"\n",(0,t.jsx)(n.p,{children:"For example:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"Input foo: {foo}\nInput bar: {bar}\nOutput: {output}\n"})}),"\n",(0,t.jsx)(n.p,{children:"If your op has other arguments, they\u2019ll all be available by name."}),"\n",(0,t.jsx)(n.h2,{id:"aws-bedrock-guardrails",children:"AWS Bedrock Guardrails"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"BedrockGuardrailScorer"})," uses AWS Bedrock's guardrail feature to detect and filter content based on configured policies. It calls the ",(0,t.jsx)(n.code,{children:"apply_guardrail"})," API to apply the guardrail to the content."]}),"\n",(0,t.jsxs)(n.p,{children:["To use the ",(0,t.jsx)(n.code,{children:"BedrockGuardrailScorer"}),", you need the following:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"An AWS account with Bedrock access"}),"\n",(0,t.jsx)(n.li,{children:"An AWS account with access to Bedrock"}),"\n",(0,t.jsx)(n.li,{children:"A configured guardrail in the AWS Bedrock console"}),"\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"boto3"})," Python package"]}),"\n"]}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsxs)(n.p,{children:["You don't need to create your own Bedrock client\u2014Weave creates it for you.  To specify a region, pass the ",(0,t.jsx)(n.code,{children:"bedrock_runtime_kwargs"})," parameter to the scorer."]})}),"\n",(0,t.jsxs)(n.p,{children:["For more details on creating a guardrail, see the ",(0,t.jsx)(n.a,{href:"https://github.com/aws-samples/amazon-bedrock-samples/blob/main/responsible_ai/bedrock-guardrails/guardrails-api.ipynb",children:"Bedrock guardrails notebook"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import weave\nimport boto3\nfrom weave.scorers.bedrock_guardrails import BedrockGuardrailScorer\n\n# Initialize Weave\nweave.init("my_app")\n\n# Create a guardrail scorer\nguardrail_scorer = BedrockGuardrailScorer(\n    guardrail_id="your-guardrail-id",  # Replace "your-guardrail-id" with your guardrail ID\n    guardrail_version="DRAFT",          # Use guardrail_version to use a specific guardrail version\n    source="INPUT",                             # Can be "INPUT" or "OUTPUT"\n    bedrock_runtime_kwargs={"region_name": "us-east-1"}  # AWS region\n)\n\n@weave.op\ndef generate_text(prompt: str) -> str:\n    # Add your text generation logic here\n    return "Generated text..."\n\n# Use the guardrail as a safety check\nasync def generate_safe_text(prompt: str) -> str:\n    result, call = generate_text.call(prompt)\n    \n    # Apply the guardrail\n    score = await call.apply_scorer(guardrail_scorer)\n    \n    # Check if the content passed the guardrail\n    if not score.result.passed:\n        # Use the modified output if available\n        if score.result.metadata.get("modified_output"):\n            return score.result.metadata["modified_output"]\n        return "I cannot generate that content due to content policy restrictions."\n    \n    return result\n'})}),"\n",(0,t.jsx)(n.h2,{id:"implementation-details",children:"Implementation Details"}),"\n",(0,t.jsx)(n.h3,{id:"the-scorer-interface",children:"The Scorer Interface"}),"\n",(0,t.jsxs)(n.p,{children:["A scorer is a class that inherits from ",(0,t.jsx)(n.code,{children:"Scorer"})," and implements a ",(0,t.jsx)(n.code,{children:"score"})," method. The method receives:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"output"}),": The result from your function"]}),"\n",(0,t.jsx)(n.li,{children:"Any input parameters matching your function's parameters"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Here's a comprehensive example:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'@weave.op\ndef generate_styled_text(prompt: str, style: str, temperature: float) -> str:\n    """Generate text in a specific style."""\n    return "Generated text in requested style..."\n\nclass StyleScorer(Scorer):\n    @weave.op\n    def score(self, output: str, prompt: str, style: str) -> dict:\n        """\n        Evaluate if the output matches the requested style.\n        \n        Args:\n            output: The generated text (automatically provided)\n            prompt: Original prompt (matched from function input)\n            style: Requested style (matched from function input)\n        """\n        return {\n            "style_match": 0.9,  # How well it matches requested style\n            "prompt_relevance": 0.8  # How relevant to the prompt\n        }\n\n# Example usage\nasync def generate_and_score():\n    # Generate text with style\n    result, call = generate_styled_text.call(\n        prompt="Write a story",\n        style="noir",\n        temperature=0.7\n    )\n    \n    # Score the result\n    score = await call.apply_scorer(StyleScorer())\n    print(f"Style match score: {score.result[\'style_match\']}")\n'})}),"\n",(0,t.jsx)(n.h3,{id:"score-parameters",children:"Score Parameters"}),"\n",(0,t.jsx)(n.h4,{id:"parameter-matching-rules",children:"Parameter Matching Rules"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"output"})," parameter is special and always contains the function's result"]}),"\n",(0,t.jsx)(n.li,{children:"Other parameters must match the function's parameter names exactly"}),"\n",(0,t.jsx)(n.li,{children:"Scorers can use any subset of the function's parameters"}),"\n",(0,t.jsx)(n.li,{children:"Parameter types should match the function's type hints"}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"handling-parameter-name-mismatches",children:"Handling Parameter Name Mismatches"}),"\n",(0,t.jsx)(n.p,{children:"Sometimes your scorer's parameter names might not match your function's parameter names exactly. For example:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'@weave.op\ndef generate_text(user_input: str):  # Uses \'user_input\'\n    return process(user_input)\n\nclass QualityScorer(Scorer):\n    @weave.op\n    def score(self, output: str, prompt: str):  # Expects \'prompt\'\n        """Evaluate response quality."""\n        return {"quality_score": evaluate_quality(prompt, output)}\n\nresult, call = generate_text.call(user_input="Say hello")\n\n# Map \'prompt\' parameter to \'user_input\'\nscorer = QualityScorer(column_map={"prompt": "user_input"})\nawait call.apply_scorer(scorer)\n'})}),"\n",(0,t.jsxs)(n.p,{children:["Common use cases for ",(0,t.jsx)(n.code,{children:"column_map"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Different naming conventions between functions and scorers"}),"\n",(0,t.jsx)(n.li,{children:"Reusing scorers across different functions"}),"\n",(0,t.jsx)(n.li,{children:"Using third-party scorers with your function names"}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"adding-additional-parameters",children:"Adding Additional Parameters"}),"\n",(0,t.jsxs)(n.p,{children:["Sometimes scorers need extra parameters that aren't part of your function. You can provide these using ",(0,t.jsx)(n.code,{children:"additional_scorer_kwargs"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class ReferenceScorer(Scorer):\n    @weave.op\n    def score(self, output: str, reference_answer: str):\n        """Compare output to a reference answer."""\n        similarity = compute_similarity(output, reference_answer)\n        return {"matches_reference": similarity > 0.8}\n\n# Provide the reference answer as an additional parameter\nawait call.apply_scorer(\n    ReferenceScorer(),\n    additional_scorer_kwargs={\n        "reference_answer": "The Earth orbits around the Sun."\n    }\n)\n'})}),"\n",(0,t.jsx)(n.p,{children:"This is useful when your scorer needs context or configuration that isn't part of the original function call."}),"\n",(0,t.jsx)(n.h3,{id:"using-scorers-two-approaches",children:"Using Scorers: Two Approaches"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"With Weave's Op System"})," (Recommended)"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"result, call = generate_text.call(input)\nscore = await call.apply_scorer(MyScorer())\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Direct Usage"})," (Quick Experiments)"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'scorer = MyScorer()\nscore = scorer.score(output="some text")\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"When to use each:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\ud83d\udc49 Use the op system for production, tracking, and analysis"}),"\n",(0,t.jsx)(n.li,{children:"\ud83d\udc49 Use direct scoring for quick experiments or one-off evaluations"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Tradeoffs of Direct Usage:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u2705 Simpler for quick tests"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 No Op required"}),"\n",(0,t.jsx)(n.li,{children:"\u274c No association with the LLM/Op call"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"score-analysis",children:"Score Analysis"}),"\n",(0,t.jsxs)(n.p,{children:["For detailed information about querying calls and their scorer results, see our ",(0,t.jsx)(n.a,{href:"/guides/evaluation/scorers#score-analysis",children:"Score Analysis Guide"})," and our ",(0,t.jsx)(n.a,{href:"/guides/tracking/tracing#querying-and-exporting-calls",children:"Data Access Guide"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"production-best-practices",children:"Production Best Practices"}),"\n",(0,t.jsx)(n.h3,{id:"1-set-appropriate-sampling-rates",children:"1. Set Appropriate Sampling Rates"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"@weave.op\ndef generate_text(prompt: str) -> str:\n    return generate_response(prompt)\n\nasync def generate_with_sampling(prompt: str) -> str:\n    result, call = generate_text.call(prompt)\n    \n    # Only monitor 10% of calls\n    if random.random() < 0.1:\n        await call.apply_scorer(ToxicityScorer())\n        await call.apply_scorer(QualityScorer())\n    \n    return result\n"})}),"\n",(0,t.jsx)(n.h3,{id:"2-monitor-multiple-aspects",children:"2. Monitor Multiple Aspects"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"async def evaluate_comprehensively(call):\n    await call.apply_scorer(ToxicityScorer())\n    await call.apply_scorer(QualityScorer())\n    await call.apply_scorer(LatencyScorer())\n"})}),"\n",(0,t.jsx)(n.h3,{id:"3-analyze-and-improve",children:"3. Analyze and Improve"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Review trends in the Weave Dashboard"}),"\n",(0,t.jsx)(n.li,{children:"Look for patterns in low-scoring outputs"}),"\n",(0,t.jsx)(n.li,{children:"Use insights to improve your LLM system"}),"\n",(0,t.jsx)(n.li,{children:"Set up alerts for concerning patterns (coming soon)"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"4-access-historical-data",children:"4. Access Historical Data"}),"\n",(0,t.jsx)(n.p,{children:"Scorer results are stored with their associated calls and can be accessed through:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["The Call object's ",(0,t.jsx)(n.code,{children:"feedback"})," field"]}),"\n",(0,t.jsx)(n.li,{children:"The Weave Dashboard"}),"\n",(0,t.jsx)(n.li,{children:"Our query APIs"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"5-initialize-guards-efficiently",children:"5. Initialize Guards Efficiently"}),"\n",(0,t.jsx)(n.p,{children:"For optimal performance, especially with locally-run models, initialize your guards outside of the main function. This pattern is particularly important when:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Your scorers load ML models"}),"\n",(0,t.jsx)(n.li,{children:"You're using local LLMs where latency is critical"}),"\n",(0,t.jsx)(n.li,{children:"Your scorers maintain network connections"}),"\n",(0,t.jsx)(n.li,{children:"You have high-traffic applications"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"See the Complete Example section below for a demonstration of this pattern."}),"\n",(0,t.jsxs)(n.admonition,{title:"Performance Tips",type:"caution",children:[(0,t.jsx)(n.p,{children:"For Guardrails:"}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Keep logic simple and fast"}),"\n",(0,t.jsx)(n.li,{children:"Consider caching common results"}),"\n",(0,t.jsx)(n.li,{children:"Avoid heavy external API calls"}),"\n",(0,t.jsx)(n.li,{children:"Initialize guards outside of your main functions to avoid repeated initialization costs"}),"\n"]}),(0,t.jsx)(n.p,{children:"For Monitors:"}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Use sampling to reduce load"}),"\n",(0,t.jsx)(n.li,{children:"Can use more complex logic"}),"\n",(0,t.jsx)(n.li,{children:"Can make external API calls"}),"\n"]})]}),"\n",(0,t.jsx)(n.h2,{id:"complete-example",children:"Complete Example"}),"\n",(0,t.jsx)(n.p,{children:"Here's a comprehensive example that brings together all the concepts we've covered:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import weave\nfrom weave import Scorer\nimport asyncio\nimport random\nfrom typing import Optional\n\nclass ToxicityScorer(Scorer):\n    def __init__(self):\n        # Initialize any expensive resources here\n        self.model = load_toxicity_model()\n    \n    @weave.op\n    async def score(self, output: str) -> dict:\n        """Check content for toxic language."""\n        try:\n            result = await self.model.evaluate(output)\n            return {\n                "flagged": result.is_toxic,\n                "reason": result.explanation if result.is_toxic else None\n            }\n        except Exception as e:\n            # Log error and default to conservative behavior\n            print(f"Toxicity check failed: {e}")\n            return {"flagged": True, "reason": "Safety check unavailable"}\n\nclass QualityScorer(Scorer):\n    @weave.op\n    async def score(self, output: str, prompt: str) -> dict:\n        """Evaluate response quality and relevance."""\n        return {\n            "coherence": evaluate_coherence(output),\n            "relevance": evaluate_relevance(output, prompt),\n            "grammar": evaluate_grammar(output)\n        }\n\n# Initialize scorers at module level (optional optimization)\ntoxicity_guard = ToxicityScorer()\nquality_monitor = QualityScorer()\nrelevance_monitor = RelevanceScorer()\n\n@weave.op\ndef generate_text(\n    prompt: str,\n    style: Optional[str] = None,\n    temperature: float = 0.7\n) -> str:\n    """Generate an LLM response."""\n    # Your LLM generation logic here\n    return "Generated response..."\n\nasync def generate_safe_response(\n    prompt: str,\n    style: Optional[str] = None,\n    temperature: float = 0.7\n) -> str:\n    """Generate a response with safety checks and quality monitoring."""\n    try:\n        # Generate initial response\n        result, call = generate_text.call(\n            prompt=prompt,\n            style=style,\n            temperature=temperature\n        )\n\n        # Apply safety check (guardrail)\n        safety = await call.apply_scorer(toxicity_guard)\n        if safety.result["flagged"]:\n            return f"I cannot generate that content: {safety.result[\'reason\']}"\n\n        # Sample quality monitoring (10% of requests)\n        if random.random() < 0.1:\n            # Run quality checks in parallel\n            await asyncio.gather(\n                call.apply_scorer(quality_monitor),\n                call.apply_scorer(relevance_monitor)\n            )\n        \n        return result\n\n    except Exception as e:\n        # Log error and return user-friendly message\n        print(f"Generation failed: {e}")\n        return "I\'m sorry, I encountered an error. Please try again."\n\n# Example usage\nasync def main():\n    # Basic usage\n    response = await generate_safe_response("Tell me a story")\n    print(f"Basic response: {response}")\n    \n    # Advanced usage with all parameters\n    response = await generate_safe_response(\n        prompt="Tell me a story",\n        style="noir",\n        temperature=0.8\n    )\n    print(f"Styled response: {response}")\n\n'})}),"\n",(0,t.jsx)(n.p,{children:"This example demonstrates:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Proper scorer initialization and error handling"}),"\n",(0,t.jsx)(n.li,{children:"Combined use of guardrails and monitors"}),"\n",(0,t.jsx)(n.li,{children:"Async operation with parallel scoring"}),"\n",(0,t.jsx)(n.li,{children:"Production-ready error handling and logging"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Explore ",(0,t.jsx)(n.a,{href:"/guides/evaluation/scorers",children:"Available Scorers"})]}),"\n",(0,t.jsxs)(n.li,{children:["Learn about ",(0,t.jsx)(n.a,{href:"/guides/tracking/ops",children:"Weave Ops"})]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},85162:(e,n,r)=>{r.r(n),r.d(n,{default:()=>a});r(67294);var t=r(90512);const s={tabItem:"tabItem_Ymn6"};var i=r(85893);function a(e){let{children:n,hidden:r,className:a}=e;return(0,i.jsx)("div",{role:"tabpanel",className:(0,t.Z)(s.tabItem,a),hidden:r,children:n})}},65488:(e,n,r)=>{r.d(n,{Z:()=>p});var t=r(67294),s=r(90512),i=r(12466),a=r(70989),o=r(72389);const l={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var c=r(85893);function d(e){let{className:n,block:r,selectedValue:t,selectValue:a,tabValues:o}=e;const d=[],{blockElementScrollPositionUntilNextRender:h}=(0,i.o5)(),u=e=>{const n=e.currentTarget,r=d.indexOf(n),s=o[r].value;s!==t&&(h(n),a(s))},p=e=>{let n=null;switch(e.key){case"Enter":u(e);break;case"ArrowRight":{const r=d.indexOf(e.currentTarget)+1;n=d[r]??d[0];break}case"ArrowLeft":{const r=d.indexOf(e.currentTarget)-1;n=d[r]??d[d.length-1];break}}n?.focus()};return(0,c.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.Z)("tabs",{"tabs--block":r},n),children:o.map((e=>{let{value:n,label:r,attributes:i}=e;return(0,c.jsx)("li",{role:"tab",tabIndex:t===n?0:-1,"aria-selected":t===n,ref:e=>d.push(e),onKeyDown:p,onClick:u,...i,className:(0,s.Z)("tabs__item",l.tabItem,i?.className,{"tabs__item--active":t===n}),children:r??n},n)}))})}function h(e){let{lazy:n,children:r,selectedValue:s}=e;const i=(Array.isArray(r)?r:[r]).filter(Boolean);if(n){const e=i.find((e=>e.props.value===s));return e?(0,t.cloneElement)(e,{className:"margin-top--md"}):null}return(0,c.jsx)("div",{className:"margin-top--md",children:i.map(((e,n)=>(0,t.cloneElement)(e,{key:n,hidden:e.props.value!==s})))})}function u(e){const n=(0,a.Y)(e);return(0,c.jsxs)("div",{className:(0,s.Z)("tabs-container",l.tabList),children:[(0,c.jsx)(d,{...n,...e}),(0,c.jsx)(h,{...n,...e})]})}function p(e){const n=(0,o.default)();return(0,c.jsx)(u,{...e,children:(0,a.h)(e.children)},String(n))}},70989:(e,n,r)=>{r.d(n,{Y:()=>p,h:()=>c});var t=r(67294),s=r(16550),i=r(20469),a=r(91980),o=r(67392),l=r(20812);function c(e){return t.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,t.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function d(e){const{values:n,children:r}=e;return(0,t.useMemo)((()=>{const e=n??function(e){return c(e).map((e=>{let{props:{value:n,label:r,attributes:t,default:s}}=e;return{value:n,label:r,attributes:t,default:s}}))}(r);return function(e){const n=(0,o.l)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,r])}function h(e){let{value:n,tabValues:r}=e;return r.some((e=>e.value===n))}function u(e){let{queryString:n=!1,groupId:r}=e;const i=(0,s.k6)(),o=function(e){let{queryString:n=!1,groupId:r}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!r)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return r??null}({queryString:n,groupId:r});return[(0,a._X)(o),(0,t.useCallback)((e=>{if(!o)return;const n=new URLSearchParams(i.location.search);n.set(o,e),i.replace({...i.location,search:n.toString()})}),[o,i])]}function p(e){const{defaultValue:n,queryString:r=!1,groupId:s}=e,a=d(e),[o,c]=(0,t.useState)((()=>function(e){let{defaultValue:n,tabValues:r}=e;if(0===r.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!h({value:n,tabValues:r}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${r.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const t=r.find((e=>e.default))??r[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:a}))),[p,m]=u({queryString:r,groupId:s}),[x,g]=function(e){let{groupId:n}=e;const r=function(e){return e?`docusaurus.tab.${e}`:null}(n),[s,i]=(0,l.Nk)(r);return[s,(0,t.useCallback)((e=>{r&&i.set(e)}),[r,i])]}({groupId:s}),j=(()=>{const e=p??x;return h({value:e,tabValues:a})?e:null})();(0,i.Z)((()=>{j&&c(j)}),[j]);return{selectedValue:o,selectValue:(0,t.useCallback)((e=>{if(!h({value:e,tabValues:a}))throw new Error(`Can't select invalid tab value=${e}`);c(e),m(e),g(e)}),[m,g,a]),tabValues:a}}},63235:(e,n,r)=>{r.d(n,{Z:()=>t});const t=r.p+"assets/images/monitors-4-4cbf6b7392695f16204355fa6deda04f.png"},93429:(e,n,r)=>{r.d(n,{Z:()=>t});const t=r.p+"assets/images/monitors-ui-1-58acd36da5abb718d454efadeb0572ac.png"},1225:(e,n,r)=>{r.d(n,{Z:()=>t});const t=r.p+"assets/images/monitors-ui-2-231cbcf7588ca579e9156336dcec8445.png"},28038:(e,n,r)=>{r.d(n,{Z:()=>t});const t=r.p+"assets/images/monitors-ui-3-1f6d8f05e0a59a3a9bd7866dbca716f0.png"},62940:(e,n,r)=>{r.d(n,{Z:()=>t});const t=r.p+"assets/images/guardrails_scorers-6158fa3f5a8dc7cd376a37cdb9f58794.png"},11151:(e,n,r)=>{r.d(n,{Z:()=>o,a:()=>a});var t=r(67294);const s={},i=t.createContext(s);function a(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);