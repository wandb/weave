"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[863],{29099:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>i,metadata:()=>r,toc:()=>c});var a=t(85893),o=t(11151);const i={},s="Evaluation Overview",r={id:"guides/core-types/evaluations",title:"Evaluation Overview",description:"Evaluation-driven development helps you reliably iterate on an application. The Evaluation class is designed to assess the performance of a Model on a given Dataset or set of examples using scoring functions.",source:"@site/docs/guides/core-types/evaluations.md",sourceDirName:"guides/core-types",slug:"/guides/core-types/evaluations",permalink:"/guides/core-types/evaluations",draft:!1,unlisted:!1,editUrl:"https://github.com/wandb/weave/blob/master/docs/docs/guides/core-types/evaluations.md",tags:[],version:"current",lastUpdatedAt:1745866509e3,frontMatter:{},sidebar:"documentationSidebar",previous:{title:"Evaluate a RAG App",permalink:"/tutorial-rag"},next:{title:"Datasets",permalink:"/guides/core-types/datasets"}},l={},c=[{value:"Create an Evaluation",id:"create-an-evaluation",level:2},{value:"Define an evaluation dataset",id:"define-an-evaluation-dataset",level:3},{value:"Defining scoring functions",id:"defining-scoring-functions",level:3},{value:"Optional: Define a custom <code>Scorer</code> class",id:"optional-define-a-custom-scorer-class",level:3},{value:"Define a Model to evaluate",id:"define-a-model-to-evaluate",level:3},{value:"Custom Naming",id:"custom-naming",level:4},{value:"Define a function to evaluate",id:"define-a-function-to-evaluate",level:3},{value:"Pulling it all together",id:"pulling-it-all-together",level:3},{value:"Advanced evaluation usage",id:"advanced-evaluation-usage",level:2},{value:"Using <code>preprocess_model_input</code> to format dataset rows before evaluating",id:"using-preprocess_model_input-to-format-dataset-rows-before-evaluating",level:3},{value:"Using HuggingFace Datasets with evaluations",id:"using-huggingface-datasets-with-evaluations",level:3},{value:"Saved views",id:"saved-views",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,o.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"evaluation-overview",children:"Evaluation Overview"}),"\n",(0,a.jsxs)(n.p,{children:["Evaluation-driven development helps you reliably iterate on an application. The ",(0,a.jsx)(n.code,{children:"Evaluation"})," class is designed to assess the performance of a ",(0,a.jsx)(n.code,{children:"Model"})," on a given ",(0,a.jsx)(n.code,{children:"Dataset"})," or set of examples using scoring functions."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Evals hero",src:t(65259).Z+"",width:"4100",height:"2160"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import weave\nfrom weave import Evaluation\nimport asyncio\n\n# Collect your examples\nexamples = [\n    {"question": "What is the capital of France?", "expected": "Paris"},\n    {"question": "Who wrote \'To Kill a Mockingbird\'?", "expected": "Harper Lee"},\n    {"question": "What is the square root of 64?", "expected": "8"},\n]\n\n# Define any custom scoring function\n@weave.op()\ndef match_score1(expected: str, output: dict) -> dict:\n    # Here is where you\'d define the logic to score the model output\n    return {\'match\': expected == model_output[\'generated_text\']}\n\n@weave.op()\ndef function_to_evaluate(question: str):\n    # here\'s where you would add your LLM call and return the output\n    return  {\'generated_text\': \'Paris\'}\n\n# Score your examples using scoring functions\nevaluation = Evaluation(\n    dataset=examples, scorers=[match_score1]\n)\n\n# Start tracking the evaluation\nweave.init(\'intro-example\')\n# Run the evaluation\nasyncio.run(evaluation.evaluate(function_to_evaluate))\n'})}),"\n",(0,a.jsx)(n.admonition,{title:"Looking for a less opinionated approach?",type:"info",children:(0,a.jsxs)(n.p,{children:["If you prefer a more flexible evaluation framework, check out Weave's ",(0,a.jsx)(n.a,{href:"/guides/evaluation/evaluation_logger",children:(0,a.jsx)(n.code,{children:"EvaluationLogger"})}),". The imperative approach offers more flexibility for complex workflows, while the standard evaluation framework provides more structure and guidance."]})}),"\n",(0,a.jsx)(n.h2,{id:"create-an-evaluation",children:"Create an Evaluation"}),"\n",(0,a.jsxs)(n.p,{children:["To systematically improve your application, it's helpful to test your changes against a consistent dataset of potential inputs so that you catch regressions and can inspect your apps behaviour under different conditions. Using the ",(0,a.jsx)(n.code,{children:"Evaluation"})," class, you can be sure you're comparing apples-to-apples by keeping track of all of the details that you're experimenting and evaluating with."]}),"\n",(0,a.jsx)(n.p,{children:"Weave will take each example, pass it through your application and score the output on multiple custom scoring functions. By doing this, you'll have a view of the performance of your application, and a rich UI to drill into individual outputs and scores."}),"\n",(0,a.jsx)(n.h3,{id:"define-an-evaluation-dataset",children:"Define an evaluation dataset"}),"\n",(0,a.jsxs)(n.p,{children:["First, define a ",(0,a.jsx)(n.a,{href:"/guides/core-types/datasets",children:"Dataset"})," or list of dictionaries with a collection of examples to be evaluated. These examples are often failure cases that you want to test for, these are similar to unit tests in Test-Driven Development (TDD)."]}),"\n",(0,a.jsx)(n.h3,{id:"defining-scoring-functions",children:"Defining scoring functions"}),"\n",(0,a.jsxs)(n.p,{children:["Then, create a list of scoring functions. These are used to score each example. Each function should have a ",(0,a.jsx)(n.code,{children:"model_output"})," and optionally, other inputs from your examples, and return a dictionary with the scores."]}),"\n",(0,a.jsxs)(n.p,{children:["Scoring functions need to have a ",(0,a.jsx)(n.code,{children:"model_output"})," keyword argument, but the other arguments are user defined and are taken from the dataset examples. It will only take the necessary keys by using a dictionary key based on the argument name."]}),"\n",(0,a.jsxs)(n.p,{children:["This will take ",(0,a.jsx)(n.code,{children:"expected"})," from the dictionary for scoring."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import weave\n\n# Collect your examples\nexamples = [\n    {"question": "What is the capital of France?", "expected": "Paris"},\n    {"question": "Who wrote \'To Kill a Mockingbird\'?", "expected": "Harper Lee"},\n    {"question": "What is the square root of 64?", "expected": "8"},\n]\n\n# Define any custom scoring function\n@weave.op()\ndef match_score1(expected: str, output: dict) -> dict:\n    # Here is where you\'d define the logic to score the model output\n    return {\'match\': expected == model_output[\'generated_text\']}\n'})}),"\n",(0,a.jsxs)(n.h3,{id:"optional-define-a-custom-scorer-class",children:["Optional: Define a custom ",(0,a.jsx)(n.code,{children:"Scorer"})," class"]}),"\n",(0,a.jsxs)(n.p,{children:["In some applications we want to create custom ",(0,a.jsx)(n.code,{children:"Scorer"})," classes - where for example a standardized ",(0,a.jsx)(n.code,{children:"LLMJudge"})," class should be created with specific parameters (e.g. chat model, prompt), specific scoring of each row, and specific calculation of an aggregate score."]}),"\n",(0,a.jsxs)(n.p,{children:["See the tutorial on defining a ",(0,a.jsx)(n.code,{children:"Scorer"})," class in the next chapter on ",(0,a.jsx)(n.a,{href:"/tutorial-rag#optional-defining-a-scorer-class",children:"Model-Based Evaluation of RAG applications"})," for more information."]}),"\n",(0,a.jsx)(n.h3,{id:"define-a-model-to-evaluate",children:"Define a Model to evaluate"}),"\n",(0,a.jsxs)(n.p,{children:["To evaluate a ",(0,a.jsx)(n.code,{children:"Model"}),", call ",(0,a.jsx)(n.code,{children:"evaluate"})," on it using an ",(0,a.jsx)(n.code,{children:"Evaluation"}),". ",(0,a.jsx)(n.code,{children:"Models"})," are used when you have parameters that you want to experiment with and capture in weave."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from weave import Model, Evaluation\nimport asyncio\n\nclass MyModel(Model):\n    prompt: str\n\n    @weave.op()\n    def predict(self, question: str):\n        # here's where you would add your LLM call and return the output\n        return {'generated_text': 'Hello, ' + self.prompt}\n\nmodel = MyModel(prompt='World')\n\nevaluation = Evaluation(\n    dataset=examples, scorers=[match_score1]\n)\nweave.init('intro-example') # begin tracking results with weave\nasyncio.run(evaluation.evaluate(model))\n"})}),"\n",(0,a.jsxs)(n.p,{children:["This will run ",(0,a.jsx)(n.code,{children:"predict"})," on each example and score the output with each scoring functions."]}),"\n",(0,a.jsx)(n.h4,{id:"custom-naming",children:"Custom Naming"}),"\n",(0,a.jsxs)(n.p,{children:["You can change the name of the Evaluation itself by passing a ",(0,a.jsx)(n.code,{children:"name"})," parameter to the ",(0,a.jsx)(n.code,{children:"Evaluation"})," class."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'evaluation = Evaluation(\n    dataset=examples, scorers=[match_score1], name="My Evaluation"\n)\n'})}),"\n",(0,a.jsxs)(n.p,{children:["You can also change the name of individual evaluations by setting the ",(0,a.jsx)(n.code,{children:"display_name"})," key of the ",(0,a.jsx)(n.code,{children:"__weave"})," dictionary."]}),"\n",(0,a.jsx)(n.admonition,{type:"note",children:(0,a.jsxs)(n.p,{children:["Using the ",(0,a.jsx)(n.code,{children:"__weave"})," dictionary sets the call display name which is distinct from the Evaluation object name. In the\nUI, you will see the display name if set, otherwise the Evaluation object name will be used."]})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'evaluation = Evaluation(\n    dataset=examples, scorers=[match_score1]\n)\nevaluation.evaluate(model, __weave={"display_name": "My Evaluation Run"})\n'})}),"\n",(0,a.jsx)(n.h3,{id:"define-a-function-to-evaluate",children:"Define a function to evaluate"}),"\n",(0,a.jsxs)(n.p,{children:["Alternatively, you can also evaluate a function that is wrapped in a ",(0,a.jsx)(n.code,{children:"@weave.op()"}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"@weave.op()\ndef function_to_evaluate(question: str):\n    # here's where you would add your LLM call and return the output\n    return  {'generated_text': 'some response'}\n\nasyncio.run(evaluation.evaluate(function_to_evaluate))\n"})}),"\n",(0,a.jsx)(n.h3,{id:"pulling-it-all-together",children:"Pulling it all together"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from weave import Evaluation, Model\nimport weave\nimport asyncio\nweave.init('intro-example')\nexamples = [\n    {\"question\": \"What is the capital of France?\", \"expected\": \"Paris\"},\n    {\"question\": \"Who wrote 'To Kill a Mockingbird'?\", \"expected\": \"Harper Lee\"},\n    {\"question\": \"What is the square root of 64?\", \"expected\": \"8\"},\n]\n\n@weave.op()\ndef match_score1(expected: str, output: dict) -> dict:\n    return {'match': expected == model_output['generated_text']}\n\n@weave.op()\ndef match_score2(expected: dict, output: dict) -> dict:\n    return {'match': expected == model_output['generated_text']}\n\nclass MyModel(Model):\n    prompt: str\n\n    @weave.op()\n    def predict(self, question: str):\n        # here's where you would add your LLM call and return the output\n        return {'generated_text': 'Hello, ' + question + self.prompt}\n\nmodel = MyModel(prompt='World')\nevaluation = Evaluation(dataset=examples, scorers=[match_score1, match_score2])\n\nasyncio.run(evaluation.evaluate(model))\n\n@weave.op()\ndef function_to_evaluate(question: str):\n    # here's where you would add your LLM call and return the output\n    return  {'generated_text': 'some response' + question}\n\nasyncio.run(evaluation.evaluate(function_to_evaluate))\n"})}),"\n",(0,a.jsx)(n.h2,{id:"advanced-evaluation-usage",children:"Advanced evaluation usage"}),"\n",(0,a.jsxs)(n.h3,{id:"using-preprocess_model_input-to-format-dataset-rows-before-evaluating",children:["Using ",(0,a.jsx)(n.code,{children:"preprocess_model_input"})," to format dataset rows before evaluating"]}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.code,{children:"preprocess_model_input"})," parameter allows you to transform your dataset examples before they are passed to your evaluation function. This is useful when you need to:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Rename fields to match your model's expected input"}),"\n",(0,a.jsx)(n.li,{children:"Transform data into the correct format"}),"\n",(0,a.jsx)(n.li,{children:"Add or remove fields"}),"\n",(0,a.jsx)(n.li,{children:"Load additional data for each example"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["Here's a simple example that shows how to use ",(0,a.jsx)(n.code,{children:"preprocess_model_input"})," to rename fields:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import weave\nfrom weave import Evaluation\nimport asyncio\n\n# Our dataset has "input_text" but our model expects "question"\nexamples = [\n    {"input_text": "What is the capital of France?", "expected": "Paris"},\n    {"input_text": "Who wrote \'To Kill a Mockingbird\'?", "expected": "Harper Lee"},\n    {"input_text": "What is the square root of 64?", "expected": "8"},\n]\n\n@weave.op()\ndef preprocess_example(example):\n    # Rename input_text to question\n    return {\n        "question": example["input_text"]\n    }\n\n@weave.op()\ndef match_score(expected: str, output: dict) -> dict:\n    return {\'match\': expected == model_output[\'generated_text\']}\n\n@weave.op()\ndef function_to_evaluate(question: str):\n    return {\'generated_text\': f\'Answer to: {question}\'}\n\n# Create evaluation with preprocessing\nevaluation = Evaluation(\n    dataset=examples,\n    scorers=[match_score],\n    preprocess_model_input=preprocess_example\n)\n\n# Run the evaluation\nweave.init(\'preprocessing-example\')\nasyncio.run(evaluation.evaluate(function_to_evaluate))\n'})}),"\n",(0,a.jsxs)(n.p,{children:["In this example, our dataset contains examples with an ",(0,a.jsx)(n.code,{children:"input_text"})," field, but our evaluation function expects a ",(0,a.jsx)(n.code,{children:"question"})," parameter. The ",(0,a.jsx)(n.code,{children:"preprocess_example"})," function transforms each example by renaming the field, allowing the evaluation to work correctly."]}),"\n",(0,a.jsx)(n.p,{children:"The preprocessing function:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Receives the raw example from your dataset"}),"\n",(0,a.jsx)(n.li,{children:"Returns a dictionary with the fields your model expects"}),"\n",(0,a.jsx)(n.li,{children:"Is applied to each example before it's passed to your evaluation function"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"This is particularly useful when working with external datasets that may have different field names or structures than what your model expects."}),"\n",(0,a.jsx)(n.h3,{id:"using-huggingface-datasets-with-evaluations",children:"Using HuggingFace Datasets with evaluations"}),"\n",(0,a.jsx)(n.p,{children:"We are continuously improving our integrations with third-party services and libraries."}),"\n",(0,a.jsxs)(n.p,{children:["While we work on building more seamless integrations, you can use ",(0,a.jsx)(n.code,{children:"preprocess_model_input"})," as a temporary workaround for using HuggingFace Datasets in Weave evaluations."]}),"\n",(0,a.jsxs)(n.p,{children:["See our ",(0,a.jsx)(n.a,{href:"/reference/gen_notebooks/hf_dataset_evals",children:"Using HuggingFace Datasets in evaluations cookbook"})," for the current approach."]}),"\n",(0,a.jsx)(n.h2,{id:"saved-views",children:"Saved views"}),"\n",(0,a.jsxs)(n.p,{children:["You can save your Evals table configurations, filters, and sorts as ",(0,a.jsx)(n.em,{children:"saved views"})," for quick access to your preferred setup. You can configure and access saved views via the UI and the Python SDK. For more information, see ",(0,a.jsx)(n.a,{href:"/guides/tools/saved-views",children:"Saved Views"}),"."]})]})}function u(e={}){const{wrapper:n}={...(0,o.a)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},65259:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/evals-hero-9bb44591b72ac8637e7e14bc73db1ba8.png"},11151:(e,n,t)=>{t.d(n,{Z:()=>r,a:()=>s});var a=t(67294);const o={},i=a.createContext(o);function s(e){const n=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),a.createElement(i.Provider,{value:n},e.children)}}}]);