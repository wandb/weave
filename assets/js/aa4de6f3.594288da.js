"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[5577],{2344:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>u,contentTitle:()=>i,default:()=>p,frontMatter:()=>l,metadata:()=>c,toc:()=>d});var a=t(85893),r=t(11151),s=t(65488),o=t(85162);const l={},i="Tutorial: Build an Evaluation pipeline",c={id:"tutorial-eval",title:"Tutorial: Build an Evaluation pipeline",description:"To iterate on an application, we need a way to evaluate if it's improving. To do so, a common practice is to test it against the same set of examples when there is a change. Weave has a first-class way to track evaluations with Model & Evaluation classes. We have built the APIs to make minimal assumptions to allow for the flexibility to support a wide array of use-cases.",source:"@site/docs/tutorial-eval.md",sourceDirName:".",slug:"/tutorial-eval",permalink:"/tutorial-eval",draft:!1,unlisted:!1,editUrl:"https://github.com/wandb/weave/blob/master/docs/docs/tutorial-eval.md",tags:[],version:"current",lastUpdatedAt:1743020786e3,frontMatter:{},sidebar:"documentationSidebar",previous:{title:"Ops",permalink:"/guides/tracking/ops"},next:{title:"Evaluate a RAG App",permalink:"/tutorial-rag"}},u={},d=[{value:"1. Build a <code>Model</code>",id:"1-build-a-model",level:2},{value:"2. Collect some examples",id:"2-collect-some-examples",level:2},{value:"3. Evaluate a <code>Model</code>",id:"3-evaluate-a-model",level:2},{value:"4. Pulling it all together",id:"4-pulling-it-all-together",level:2},{value:"What&#39;s next?",id:"whats-next",level:2}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"tutorial-build-an-evaluation-pipeline",children:"Tutorial: Build an Evaluation pipeline"}),"\n",(0,a.jsxs)(n.p,{children:["To iterate on an application, we need a way to evaluate if it's improving. To do so, a common practice is to test it against the same set of examples when there is a change. Weave has a first-class way to track evaluations with ",(0,a.jsx)(n.code,{children:"Model"})," & ",(0,a.jsx)(n.code,{children:"Evaluation"})," classes. We have built the APIs to make minimal assumptions to allow for the flexibility to support a wide array of use-cases."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Evals hero",src:t(65259).Z+"",width:"4100",height:"2160"})}),"\n",(0,a.jsxs)(n.h2,{id:"1-build-a-model",children:["1. Build a ",(0,a.jsx)(n.code,{children:"Model"})]}),"\n",(0,a.jsxs)(s.Z,{groupId:"programming-language",queryString:!0,children:[(0,a.jsxs)(o.default,{value:"python",label:"Python",default:!0,children:[(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:"Model"}),"s store and version information about your system, such as prompts, temperatures, and more.\nWeave automatically captures when they are used and updates the version when there are changes."]}),(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:"Model"}),"s are declared by subclassing ",(0,a.jsx)(n.code,{children:"Model"})," and implementing a ",(0,a.jsx)(n.code,{children:"predict"})," function definition, which takes one example and returns the response."]}),(0,a.jsx)(n.admonition,{type:"important",children:(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Known Issue"}),": If you are using Google Colab, remove ",(0,a.jsx)(n.code,{children:"async"})," from the following examples."]})}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import json\nimport openai\nimport weave\n\n# highlight-next-line\nclass ExtractFruitsModel(weave.Model):\n    model_name: str\n    prompt_template: str\n\n    # highlight-next-line\n    @weave.op()\n    # highlight-next-line\n    async def predict(self, sentence: str) -> dict:\n        client = openai.AsyncClient()\n\n        response = await client.chat.completions.create(\n            model=self.model_name,\n            messages=[\n                {"role": "user", "content": self.prompt_template.format(sentence=sentence)}\n            ],\n        )\n        result = response.choices[0].message.content\n        if result is None:\n            raise ValueError("No response from model")\n        parsed = json.loads(result)\n        return parsed\n'})}),(0,a.jsxs)(n.p,{children:["You can instantiate ",(0,a.jsx)(n.code,{children:"Model"})," objects as normal like this:"]}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import asyncio\nimport weave\n\nweave.init(\'intro-example\')\n\nmodel = ExtractFruitsModel(model_name=\'gpt-3.5-turbo-1106\',\n                        prompt_template=\'Extract fields ("fruit": <str>, "color": <str>, "flavor": <str>) from the following text, as json: {sentence}\')\nsentence = "There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy."\nprint(asyncio.run(model.predict(sentence)))\n# if you\'re in a Jupyter Notebook, run:\n# await model.predict(sentence)\n'})}),(0,a.jsx)(n.admonition,{type:"note",children:(0,a.jsxs)(n.p,{children:["Checkout the ",(0,a.jsx)(n.a,{href:"/guides/core-types/models",children:"Models"})," guide to learn more."]})})]}),(0,a.jsxs)(o.default,{value:"typescript",label:"TypeScript",children:[(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:"weave.Model"})," is not supported in TypeScript yet.  Instead, you can just wrap your model-like function with ",(0,a.jsx)(n.code,{children:"weave.op"})]}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"// highlight-next-line\nconst model = weave.op(async function myModel({datasetRow}) {\n  const prompt = `Extract fields (\"fruit\": <str>, \"color\": <str>, \"flavor\") from the following text, as json: ${datasetRow.sentence}`;\n  const response = await openaiClient.chat.completions.create({\n    model: 'gpt-3.5-turbo',\n    messages: [{role: 'user', content: prompt}],\n    response_format: {type: 'json_object'},\n  });\n  const result = response?.choices?.[0]?.message?.content;\n  if (result == null) {\n    throw new Error('No response from model');\n  }\n  return JSON.parse(result);\n});\n"})})]})]}),"\n",(0,a.jsx)(n.h2,{id:"2-collect-some-examples",children:"2. Collect some examples"}),"\n",(0,a.jsxs)(s.Z,{groupId:"programming-language",queryString:!0,children:[(0,a.jsx)(o.default,{value:"python",label:"Python",default:!0,children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"sentences = [\n    \"There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy.\",\n    \"Pounits are a bright green color and are more savory than sweet.\",\n    \"Finally, there are fruits called glowls, which have a very sour and bitter taste which is acidic and caustic, and a pale orange tinge to them.\"\n]\nlabels = [\n    {'fruit': 'neoskizzles', 'color': 'purple', 'flavor': 'candy'},\n    {'fruit': 'pounits', 'color': 'bright green', 'flavor': 'savory'},\n    {'fruit': 'glowls', 'color': 'pale orange', 'flavor': 'sour and bitter'}\n]\nexamples = [\n    {'id': '0', 'sentence': sentences[0], 'target': labels[0]},\n    {'id': '1', 'sentence': sentences[1], 'target': labels[1]},\n    {'id': '2', 'sentence': sentences[2], 'target': labels[2]}\n]\n"})})}),(0,a.jsx)(o.default,{value:"typescript",label:"TypeScript",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"const sentences = [\n  'There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy.',\n  'Pounits are a bright green color and are more savory than sweet.',\n  'Finally, there are fruits called glowls, which have a very sour and bitter taste which is acidic and caustic, and a pale orange tinge to them.',\n];\nconst labels = [\n  {fruit: 'neoskizzles', color: 'purple', flavor: 'candy'},\n  {fruit: 'pounits', color: 'bright green', flavor: 'savory'},\n  {fruit: 'glowls', color: 'pale orange', flavor: 'sour and bitter'},\n];\nconst examples = [\n  {id: '0', sentence: sentences[0], target: labels[0]},\n  {id: '1', sentence: sentences[1], target: labels[1]},\n  {id: '2', sentence: sentences[2], target: labels[2]},\n];\nconst dataset = new weave.Dataset({\n  id: 'Fruit Dataset',\n  rows: examples,\n});\n"})})})]}),"\n",(0,a.jsxs)(n.h2,{id:"3-evaluate-a-model",children:["3. Evaluate a ",(0,a.jsx)(n.code,{children:"Model"})]}),"\n",(0,a.jsxs)(s.Z,{groupId:"programming-language",queryString:!0,children:[(0,a.jsxs)(o.default,{value:"python",label:"Python",default:!0,children:[(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:"Evaluation"}),"s assess a ",(0,a.jsx)(n.code,{children:"Model"}),"s performance on a set of examples using a list of specified scoring functions or ",(0,a.jsx)(n.code,{children:"weave.scorer.Scorer"})," classes."]}),(0,a.jsxs)(n.p,{children:["Here, we'll use a default scoring class ",(0,a.jsx)(n.code,{children:"MultiTaskBinaryClassificationF1"})," and we'll also define our own ",(0,a.jsx)(n.code,{children:"fruit_name_score"})," scoring function."]}),(0,a.jsxs)(n.p,{children:["Here ",(0,a.jsx)(n.code,{children:"sentence"})," is passed to the model's predict function, and ",(0,a.jsx)(n.code,{children:"target"})," is used in the scoring function, these are inferred based on the argument names of the ",(0,a.jsx)(n.code,{children:"predict"})," and scoring functions. The ",(0,a.jsx)(n.code,{children:"fruit"})," key needs to be outputted by the model's predict function and must also be existing as a column in the dataset (or outputted by the ",(0,a.jsx)(n.code,{children:"preprocess_model_input"})," function if defined)."]}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import weave\nfrom weave.scorers import MultiTaskBinaryClassificationF1\n\nweave.init('intro-example')\n\n@weave.op()\ndef fruit_name_score(target: dict, output: dict) -> dict:\n    return {'correct': target['fruit'] == output['fruit']}\n\n# highlight-next-line\nevaluation = weave.Evaluation(\n    # highlight-next-line\n    dataset=examples,\n    # highlight-next-line\n    scorers=[\n        # highlight-next-line\n        MultiTaskBinaryClassificationF1(class_names=[\"fruit\", \"color\", \"flavor\"]),\n        # highlight-next-line\n        fruit_name_score\n    # highlight-next-line\n    ],\n# highlight-next-line\n)\n# highlight-next-line\nprint(asyncio.run(evaluation.evaluate(model)))\n# if you're in a Jupyter Notebook, run:\n# await evaluation.evaluate(model)\n"})})]}),(0,a.jsxs)(o.default,{value:"typescript",label:"TypeScript",children:[(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:"Evaluation"}),"s assess a model's performance on a set of examples using a list of specified scoring functions."]}),(0,a.jsx)(n.p,{children:"For this example, we'll define a few simple scoring functions."}),(0,a.jsxs)(n.p,{children:["Here, ",(0,a.jsx)(n.code,{children:"sentence"})," is passed to the model and ",(0,a.jsx)(n.code,{children:"..."})," is used in the scoring function. These are defined..."]}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"import * as weave from 'weave';\nimport {OpenAI} from 'openai';\n\nconst client = await weave.init('intro-example');\nconst openaiClient = weave.wrapOpenAI(new OpenAI());\n\nconst fruitNameScorer = weave.op(\n  ({modelOutput, datasetRow}) => datasetRow.target.fruit == modelOutput.fruit,\n  {name: 'fruitNameScore'}\n);\n\nconst evaluation = new weave.Evaluation({\n  dataset: ds,\n  scorers: [fruitNameScorer],\n});\n\nconst results = await evaluation.evaluate(model);\nconsole.log(JSON.stringify(results, null, 2));\n"})})]})]}),"\n",(0,a.jsxs)(n.p,{children:["In some applications we want to create custom ",(0,a.jsx)(n.code,{children:"Scorer"})," classes - where for example a standardized ",(0,a.jsx)(n.code,{children:"LLMJudge"})," class should be created with specific parameters (e.g. chat model, prompt), specific scoring of each row, and specific calculation of an aggregate score. See the tutorial on defining a ",(0,a.jsx)(n.code,{children:"Scorer"})," class in the next chapter on ",(0,a.jsx)(n.a,{href:"/tutorial-rag#optional-defining-a-scorer-class",children:"Model-Based Evaluation of RAG applications"})," for more information."]}),"\n",(0,a.jsx)(n.h2,{id:"4-pulling-it-all-together",children:"4. Pulling it all together"}),"\n",(0,a.jsxs)(s.Z,{groupId:"programming-language",queryString:!0,children:[(0,a.jsx)(o.default,{value:"python",label:"Python",default:!0,children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import json\nimport asyncio\n# highlight-next-line\nimport weave\n# highlight-next-line\nfrom weave.scorers import MultiTaskBinaryClassificationF1\nimport openai\n\n# We create a model class with one predict function.\n# All inputs, predictions and parameters are automatically captured for easy inspection.\n\n# highlight-next-line\nclass ExtractFruitsModel(weave.Model):\n    model_name: str\n    prompt_template: str\n\n    # highlight-next-line\n    @weave.op()\n    # highlight-next-line\n    async def predict(self, sentence: str) -> dict:\n        client = openai.AsyncClient()\n\n        response = await client.chat.completions.create(\n            model=self.model_name,\n            messages=[\n                {\"role\": \"user\", \"content\": self.prompt_template.format(sentence=sentence)}\n            ],\n            response_format={ \"type\": \"json_object\" }\n        )\n        result = response.choices[0].message.content\n        if result is None:\n            raise ValueError(\"No response from model\")\n        parsed = json.loads(result)\n        return parsed\n\n# We call init to begin capturing data in the project, intro-example.\nweave.init('intro-example')\n\n# We create our model with our system prompt.\nmodel = ExtractFruitsModel(name='gpt4',\n                        model_name='gpt-4-0125-preview',\n                        prompt_template='Extract fields (\"fruit\": <str>, \"color\": <str>, \"flavor\") from the following text, as json: {sentence}')\nsentences = [\"There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy.\",\n\"Pounits are a bright green color and are more savory than sweet.\",\n\"Finally, there are fruits called glowls, which have a very sour and bitter taste which is acidic and caustic, and a pale orange tinge to them.\"]\nlabels = [\n    {'fruit': 'neoskizzles', 'color': 'purple', 'flavor': 'candy'},\n    {'fruit': 'pounits', 'color': 'bright green', 'flavor': 'savory'},\n    {'fruit': 'glowls', 'color': 'pale orange', 'flavor': 'sour and bitter'}\n]\nexamples = [\n    {'id': '0', 'sentence': sentences[0], 'target': labels[0]},\n    {'id': '1', 'sentence': sentences[1], 'target': labels[1]},\n    {'id': '2', 'sentence': sentences[2], 'target': labels[2]}\n]\n# If you have already published the Dataset, you can run:\n# dataset = weave.ref('example_labels').get()\n\n# We define a scoring function to compare our model predictions with a ground truth label.\n@weave.op()\ndef fruit_name_score(target: dict, output: dict) -> dict:\n    return {'correct': target['fruit'] == output['fruit']}\n\n# Finally, we run an evaluation of this model.\n# This will generate a prediction for each input example, and then score it with each scoring function.\n# highlight-next-line\nevaluation = weave.Evaluation(\n    # highlight-next-line\n    name='fruit_eval',\n    # highlight-next-line\n    dataset=examples, scorers=[MultiTaskBinaryClassificationF1(class_names=[\"fruit\", \"color\", \"flavor\"]), fruit_name_score],\n# highlight-next-line\n)\nprint(asyncio.run(evaluation.evaluate(model)))\n# if you're in a Jupyter Notebook, run:\n# await evaluation.evaluate(model)\n"})})}),(0,a.jsx)(o.default,{value:"typescript",label:"TypeScript",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"import {OpenAI} from 'openai';\nimport 'source-map-support/register';\nimport * as weave from 'weave';\n\nconst sentences = [\n  'There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy.',\n  'Pounits are a bright green color and are more savory than sweet.',\n  'Finally, there are fruits called glowls, which have a very sour and bitter taste which is acidic and caustic, and a pale orange tinge to them.',\n  'There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy.',\n];\nconst labels = [\n  {fruit: 'neoskizzles', color: 'purple', flavor: 'candy'},\n  {fruit: 'pounits', color: 'bright green', flavor: 'savory'},\n  {fruit: 'glowls', color: 'pale orange', flavor: 'sour and bitter'},\n];\nconst examples = [\n  {id: '0', sentence: sentences[0], target: labels[0]},\n  {id: '1', sentence: sentences[1], target: labels[1]},\n  {id: '2', sentence: sentences[2], target: labels[2]},\n];\nconst dataset = new weave.Dataset({\n  id: 'Fruit Dataset',\n  rows: examples,\n});\n\nconst openaiClient = weave.wrapOpenAI(new OpenAI());\n\nconst model = weave.op(async function myModel({datasetRow}) {\n  const prompt = `Extract fields (\"fruit\": <str>, \"color\": <str>, \"flavor\") from the following text, as json: ${datasetRow.sentence}`;\n  const response = await openaiClient.chat.completions.create({\n    model: 'gpt-3.5-turbo',\n    messages: [{role: 'user', content: prompt}],\n    response_format: {type: 'json_object'},\n  });\n  const result = response?.choices?.[0]?.message?.content;\n  if (result == null) {\n    throw new Error('No response from model');\n  }\n  return JSON.parse(result);\n});\n\nconst fruitNameScorer = weave.op(\n  ({modelOutput, datasetRow}) => datasetRow.target.fruit == modelOutput.fruit,\n  {name: 'fruitNameScore'}\n);\n\nasync function main() {\n  await weave.init('examples');\n  const evaluation = new weave.Evaluation({\n    dataset,\n    scorers: [fruitNameScorer],\n  });\n\n  const results = await evaluation.evaluate({model});\n  console.log(JSON.stringify(results, null, 2));\n}\n\nmain();\n\n"})})})]}),"\n",(0,a.jsx)(n.h2,{id:"whats-next",children:"What's next?"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Follow the ",(0,a.jsx)(n.a,{href:"/tutorial-rag",children:"Model-Based Evaluation of RAG applications"})," to evaluate a RAG app using an LLM judge."]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},85162:(e,n,t)=>{t.r(n),t.d(n,{default:()=>o});t(67294);var a=t(90512);const r={tabItem:"tabItem_Ymn6"};var s=t(85893);function o(e){let{children:n,hidden:t,className:o}=e;return(0,s.jsx)("div",{role:"tabpanel",className:(0,a.Z)(r.tabItem,o),hidden:t,children:n})}},65488:(e,n,t)=>{t.d(n,{Z:()=>p});var a=t(67294),r=t(90512),s=t(12466),o=t(70989),l=t(72389);const i={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var c=t(85893);function u(e){let{className:n,block:t,selectedValue:a,selectValue:o,tabValues:l}=e;const u=[],{blockElementScrollPositionUntilNextRender:d}=(0,s.o5)(),h=e=>{const n=e.currentTarget,t=u.indexOf(n),r=l[t].value;r!==a&&(d(n),o(r))},p=e=>{let n=null;switch(e.key){case"Enter":h(e);break;case"ArrowRight":{const t=u.indexOf(e.currentTarget)+1;n=u[t]??u[0];break}case"ArrowLeft":{const t=u.indexOf(e.currentTarget)-1;n=u[t]??u[u.length-1];break}}n?.focus()};return(0,c.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.Z)("tabs",{"tabs--block":t},n),children:l.map((e=>{let{value:n,label:t,attributes:s}=e;return(0,c.jsx)("li",{role:"tab",tabIndex:a===n?0:-1,"aria-selected":a===n,ref:e=>u.push(e),onKeyDown:p,onClick:h,...s,className:(0,r.Z)("tabs__item",i.tabItem,s?.className,{"tabs__item--active":a===n}),children:t??n},n)}))})}function d(e){let{lazy:n,children:t,selectedValue:r}=e;const s=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=s.find((e=>e.props.value===r));return e?(0,a.cloneElement)(e,{className:"margin-top--md"}):null}return(0,c.jsx)("div",{className:"margin-top--md",children:s.map(((e,n)=>(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==r})))})}function h(e){const n=(0,o.Y)(e);return(0,c.jsxs)("div",{className:(0,r.Z)("tabs-container",i.tabList),children:[(0,c.jsx)(u,{...n,...e}),(0,c.jsx)(d,{...n,...e})]})}function p(e){const n=(0,l.default)();return(0,c.jsx)(h,{...e,children:(0,o.h)(e.children)},String(n))}},70989:(e,n,t)=>{t.d(n,{Y:()=>p,h:()=>c});var a=t(67294),r=t(16550),s=t(20469),o=t(91980),l=t(67392),i=t(20812);function c(e){return a.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function u(e){const{values:n,children:t}=e;return(0,a.useMemo)((()=>{const e=n??function(e){return c(e).map((e=>{let{props:{value:n,label:t,attributes:a,default:r}}=e;return{value:n,label:t,attributes:a,default:r}}))}(t);return function(e){const n=(0,l.l)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function d(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function h(e){let{queryString:n=!1,groupId:t}=e;const s=(0,r.k6)(),l=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,o._X)(l),(0,a.useCallback)((e=>{if(!l)return;const n=new URLSearchParams(s.location.search);n.set(l,e),s.replace({...s.location,search:n.toString()})}),[l,s])]}function p(e){const{defaultValue:n,queryString:t=!1,groupId:r}=e,o=u(e),[l,c]=(0,a.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!d({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const a=t.find((e=>e.default))??t[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:n,tabValues:o}))),[p,m]=h({queryString:t,groupId:r}),[f,g]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[r,s]=(0,i.Nk)(t);return[r,(0,a.useCallback)((e=>{t&&s.set(e)}),[t,s])]}({groupId:r}),v=(()=>{const e=p??f;return d({value:e,tabValues:o})?e:null})();(0,s.Z)((()=>{v&&c(v)}),[v]);return{selectedValue:l,selectValue:(0,a.useCallback)((e=>{if(!d({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);c(e),m(e),g(e)}),[m,g,o]),tabValues:o}}},65259:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/evals-hero-9bb44591b72ac8637e7e14bc73db1ba8.png"},11151:(e,n,t)=>{t.d(n,{Z:()=>l,a:()=>o});var a=t(67294);const r={},s=a.createContext(r);function o(e){const n=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);