"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9546],{68173:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>_,frontMatter:()=>i,metadata:()=>o,toc:()=>d});var s=t(85893),a=t(11151);const i={title:"Log Audio With Weave"},r="How to use Weave with Audio Data: An OpenAI Example",o={id:"reference/gen_notebooks/audio_with_weave",title:"Log Audio With Weave",description:"Open in Colab",source:"@site/docs/reference/gen_notebooks/audio_with_weave.md",sourceDirName:"reference/gen_notebooks",slug:"/reference/gen_notebooks/audio_with_weave",permalink:"/reference/gen_notebooks/audio_with_weave",draft:!1,unlisted:!1,editUrl:"https://github.com/wandb/weave/blob/master/docs/docs/reference/gen_notebooks/audio_with_weave.md",tags:[],version:"current",lastUpdatedAt:1744733412e3,frontMatter:{title:"Log Audio With Weave"},sidebar:"notebookSidebar",previous:{title:"Weave with W&B Models",permalink:"/reference/gen_notebooks/Models_and_Weave_Integration_Demo"},next:{title:"Chain of Density Summarization",permalink:"/reference/gen_notebooks/chain_of_density"}},l={},d=[{value:"Setup",id:"setup",level:2},{value:"Audio Streaming and Storage Example",id:"audio-streaming-and-storage-example",level:2},{value:"Testing",id:"testing",level:2},{value:"Requirements Setup",id:"requirements-setup",level:2},{value:"Microphone Configuration",id:"microphone-configuration",level:2},{value:"OpenAI Realtime API Schema Implementation",id:"openai-realtime-api-schema-implementation",level:2},{value:"Audio Stream Writer (To Disk and In Memory)",id:"audio-stream-writer-to-disk-and-in-memory",level:2},{value:"Realtime Audio Model",id:"realtime-audio-model",level:2},{value:"Audio recorder",id:"audio-recorder",level:2},{value:"Main Thread (Run me!)",id:"main-thread-run-me",level:2}];function p(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.a)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(n.admonition,{title:"This is a notebook",type:"tip",children:[(0,s.jsx)("a",{href:"https://colab.research.google.com/github/wandb/weave/blob/master/docs/./notebooks/audio_with_weave.ipynb",target:"_blank",rel:"noopener noreferrer",class:"navbar__item navbar__link button button--secondary button--med margin-right--sm notebook-cta-button",children:(0,s.jsxs)("div",{children:[(0,s.jsx)("img",{src:"https://upload.wikimedia.org/wikipedia/commons/archive/d/d0/20221103151430%21Google_Colaboratory_SVG_Logo.svg",alt:"Open In Colab",height:"20px"}),(0,s.jsx)("div",{children:"Open in Colab"})]})}),(0,s.jsx)("a",{href:"https://github.com/wandb/weave/blob/master/docs/./notebooks/audio_with_weave.ipynb",target:"_blank",rel:"noopener noreferrer",class:"navbar__item navbar__link button button--secondary button--med margin-right--sm notebook-cta-button",children:(0,s.jsxs)("div",{children:[(0,s.jsx)("img",{src:"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg",alt:"View in Github",height:"15px"}),(0,s.jsx)("div",{children:"View in Github"})]})})]}),"\n",(0,s.jsx)(n.h1,{id:"how-to-use-weave-with-audio-data-an-openai-example",children:"How to use Weave with Audio Data: An OpenAI Example"}),"\n",(0,s.jsx)(n.p,{children:"This demo uses the OpenAI chat completions API with GPT 4o Audio Preview to generate audio responses to text prompts and track these in Weave."}),"\n",(0,s.jsx)("img",{src:"https://i.imgur.com/OUfsZ2x.png"}),"\n",(0,s.jsxs)(n.p,{children:["For the advanced use case, we leverage the OpenAI Realtime API to stream audio in realtime. Click the following thumbnail to view the video demonstration, or click ",(0,s.jsx)(n.a,{href:"https://www.youtube.com/watch?v=lnnd73xDElw",children:"here"}),"."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://www.youtube.com/watch?v=lnnd73xDElw",title:"Everything Is AWESOME",children:(0,s.jsx)(n.img,{src:"https://img.youtube.com/vi/lnnd73xDElw/0.jpg",alt:"Everything Is AWESOME"})})}),"\n",(0,s.jsx)(n.h2,{id:"setup",children:"Setup"}),"\n",(0,s.jsxs)(n.p,{children:["Start by installing the OpenAI (",(0,s.jsx)(n.code,{children:"openai"}),") and Weave (",(0,s.jsx)(n.code,{children:"weave"}),") dependencies, as well as API key management dependencey ",(0,s.jsx)(n.code,{children:"set-env"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"%%capture\n!pip install openai\n!pip install weave\n!pip install set-env-colab-kaggle-dotenv -q # for env var\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"%%capture\n# Temporary workaround to fix bug in openai:\n# TypeError: Client.__init__() got an unexpected keyword argument 'proxies'\n# See https://community.openai.com/t/error-with-openai-1-56-0-client-init-got-an-unexpected-keyword-argument-proxies/1040332/15\n!pip install \"httpx<0.28\"\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Next, load the required API keys for OpenAI and Weave. Here, we use set_env which is compatible with google colab's secret keys manager, and is an alternative to colab's specific ",(0,s.jsx)(n.code,{children:"google.colab.userdata"}),". See: ",(0,s.jsx)(n.a,{href:"https://pypi.org/project/set-env-colab-kaggle-dotenv/",children:"here"})," for usage instructions."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Set environment variables.\nfrom set_env import set_env\n\n_ = set_env("OPENAI_API_KEY")\n_ = set_env("WANDB_API_KEY")\n'})}),"\n",(0,s.jsx)(n.p,{children:"And finally import the required libraries."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import base64\nimport os\nimport time\nimport wave\n\nimport numpy as np\nfrom IPython.display import display\nfrom openai import OpenAI\n\nimport weave\n"})}),"\n",(0,s.jsx)(n.h2,{id:"audio-streaming-and-storage-example",children:"Audio Streaming and Storage Example"}),"\n",(0,s.jsx)(n.p,{children:"Now we will setup a call to OpenAI's completions endpoint with audio modality enabled. First create the OpenAI client and initiate a Weave project."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))\nweave.init("openai-audio-chat")\n'})}),"\n",(0,s.jsx)(n.p,{children:"Now we will define our OpenAI completions request and add our Weave decorator (op)."}),"\n",(0,s.jsxs)(n.p,{children:["Here, we define the function ",(0,s.jsx)(n.code,{children:"prompt_endpont_and_log_trace"}),". This function has three primary steps:"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["We make a completion object using the ",(0,s.jsx)(n.code,{children:"GPT 4o Audio Preview"})," model that supports text and audio inputs and outputs."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"We prompt the model to count to 13 slowly with varying accents."}),"\n",(0,s.jsx)(n.li,{children:'We set the completion to "stream".'}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"We open a new output file to which the streamed data is writen chunk by chunk."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"We return an open file handler to the audio file so Weave logs the audio data in the trace."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'SAMPLE_RATE = 22050\n\n\n@weave.op()\ndef prompt_endpoint_and_log_trace(system_prompt=None, user_prompt=None):\n    if not system_prompt:\n        system_prompt = "You\'re the fastest counter in the world"\n    if not user_prompt:\n        user_prompt = "Count to 13 super super slow, enunciate each number with a dramatic flair, changing up accents as you go along. British, French, German, Spanish, etc."\n    # Request from the OpenAI API with audio modality\n    completion = client.chat.completions.create(\n        model="gpt-4o-audio-preview",\n        modalities=["text", "audio"],\n        audio={"voice": "fable", "format": "pcm16"},\n        stream=True,\n        messages=[\n            {"role": "system", "content": system_prompt},\n            {"role": "user", "content": user_prompt},\n        ],\n    )\n\n    # Open a wave file for writing\n    with wave.open("./output.wav", "wb") as wav_file:\n        wav_file.setnchannels(1)  # Mono\n        wav_file.setsampwidth(2)  # 16-bit\n        wav_file.setframerate(SAMPLE_RATE)  # Sample rate (adjust if needed)\n\n        # Write chunks as they are streamed in from the API\n        for chunk in completion:\n            if (\n                hasattr(chunk, "choices")\n                and chunk.choices is not None\n                and len(chunk.choices) > 0\n            ):\n                if (\n                    hasattr(chunk.choices[0].delta, "audio")\n                    and chunk.choices[0].delta.audio.get("data") is not None\n                ):\n                    # Decode the base64 audio data\n                    audio_data = base64.b64decode(\n                        chunk.choices[0].delta.audio.get("data")\n                    )\n\n                    # Write the current chunk to the wave file\n                    wav_file.writeframes(audio_data)\n\n    # Return the file to Weave op\n    return wave.open("output.wav", "rb")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"testing",children:"Testing"}),"\n",(0,s.jsx)(n.p,{children:'Run the following cell. The system and user prompt will be stored in a Weave trace as well as the output audio.\nAfter running the cell, click the link next to the "\ud83c\udf69" emoji to view your trace.'}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from IPython.display import Audio, display\n\n# Call the function to write the audio stream\nprompt_endpoint_and_log_trace(\n    system_prompt="You\'re the fastest counter in the world",\n    user_prompt="Count to 13 super super slow, enunciate each number with a dramatic flair, changing up accents as you go along. British, French, German, Spanish, etc.",\n)\n\n# Display the updated audio stream\ndisplay(Audio("output.wav", rate=SAMPLE_RATE, autoplay=True))\n'})}),"\n",(0,s.jsx)(n.h1,{id:"advanced-usage-realtime-audio-api-with-weave",children:"Advanced Usage: Realtime Audio API with Weave"}),"\n",(0,s.jsx)("img",{src:"https://i.imgur.com/ZiW3IVu.png"}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsxs)(n.p,{children:[(0,s.jsx)("summary",{children:" (Advanced) Realtime Audio API with Weave "}),"\nOpenAI's realtime API is a highly functional and reliable conversational API for building realtime audio and text assistants."]}),(0,s.jsx)(n.p,{children:"Please note:"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Review the cells in ",(0,s.jsx)(n.a,{href:"#microphone-configuration",children:"Microphone Configuration"})]}),"\n",(0,s.jsxs)(n.li,{children:["Due to limitations of the Google Colab execution environment, ",(0,s.jsx)(n.strong,{children:"this must be run on your host machine"})," as a Jupyter Notebook. This cannot be ran in the browser.","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["On MacOS you will need to install ",(0,s.jsx)(n.code,{children:"portaudio"})," via Brew (see ",(0,s.jsx)(n.a,{href:"https://formulae.brew.sh/formula/portaudio",children:"here"}),") for Pyaudio to function."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{children:"OpenAI's Python SDK does not yet provide Realtime API support. We implement the complete OAI Realtime API schema in Pydantic for greater legibility, and may deprecate once official support is released."}),"\n",(0,s.jsxs)(n.li,{children:["The ",(0,s.jsx)(n.code,{children:"enable_audio_playback"})," toggle will cause playback of assistant outputted audio. Please note that ",(0,s.jsx)(n.strong,{children:"headphones are required if this is enabled"}),", as echo detection requires a highly complex implementation."]}),"\n"]}),(0,s.jsx)(n.h2,{id:"requirements-setup",children:"Requirements Setup"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"%%capture\n!pip install numpy==2.0\n!pip install weave\n!pip install pyaudio # On mac, you may need to install portaudio first with `brew install portaudio`\n!pip install websocket-client\n!pip install set-env-colab-kaggle-dotenv -q # for env var\n!pip install resampy\n"})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import base64\nimport io\nimport json\nimport os\nimport threading\nimport time\nimport wave\nfrom typing import Optional\n\nimport numpy as np\nimport pyaudio\nimport resampy\nimport websocket\nfrom set_env import set_env\n\nimport weave\n"})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Set environment variables.\n# See: https://pypi.org/project/set-env-colab-kaggle-dotenv/ for usage instructions.\n_ = set_env("OPENAI_API_KEY")\n_ = set_env("WANDB_API_KEY")\n'})}),(0,s.jsx)(n.h2,{id:"microphone-configuration",children:"Microphone Configuration"}),(0,s.jsxs)(n.p,{children:["Run the following cell to find all available audio devices. Then, populate the ",(0,s.jsx)(n.code,{children:"INPUT_DEVICE_INDEX"})," and the ",(0,s.jsx)(n.code,{children:"OUTPUT_DEVICE_INDEX"})," based on the devices listed. Your input device will have at least 1 input channels, and your output device will have at least 1 output channels."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Get device list from pyaudio so we can configure the next cell\np = pyaudio.PyAudio()\ndevices_data = {i: p.get_device_info_by_index(i) for i in range(p.get_device_count())}\nfor i, device in devices_data.items():\n    print(\n        f\"Found device @{i}: {device['name']} with sample rate: {device['defaultSampleRate']} and input channels: {device['maxInputChannels']} and output channels: {device['maxOutputChannels']}\"\n    )\n"})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'INPUT_DEVICE_INDEX = 3  # @param                                                 # Choose based on device list above. Make sure device has > 0 input channels.\nOUTPUT_DEVICE_INDEX = 12  # @param                                                # Chose based on device list above. Make sure device has > 0 output channels.\nenable_audio_playback = True  # @param {type:"boolean"}                           # Toggle on assistant audio playback. Requires headphones.\n\n# Audio recording and streaming parameters\nINPUT_DEVICE_CHANNELS = devices_data[INPUT_DEVICE_INDEX][\n    "maxInputChannels"\n]  # From device list above\nSAMPLE_RATE = int(\n    devices_data[INPUT_DEVICE_INDEX]["defaultSampleRate"]\n)  # From device list above\nCHUNK = int(SAMPLE_RATE / 10)  # Samples per frame\nSAMPLE_WIDTH = p.get_sample_size(pyaudio.paInt16)  # Samples per frame for the format\nCHUNK_DURATION = 0.3  # Seconds of audio per chunk sent to OAI API\nOAI_SAMPLE_RATE = (\n    24000  # OAI Sample Rate is 24kHz, we need this to play or save assistant audio\n)\nOUTPUT_DEVICE_CHANNELS = 1  # Set to 1 for mono output\n'})}),(0,s.jsx)(n.h2,{id:"openai-realtime-api-schema-implementation",children:"OpenAI Realtime API Schema Implementation"}),(0,s.jsx)(n.p,{children:"The OpenAI Python SDK does not yet provide Realtime API support. We implement the complete OAI Realtime API schema in Pydantic for greater legibility, and may deprecate once official support is released."}),(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:" Pydantic Schema for OpenAI Realtime API (OpenAI's SDK lacks Realtime API support) "}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from enum import Enum\nfrom typing import Any, Literal, Optional, Union\n\nfrom pydantic import BaseModel, Field, ValidationError\n\n\nclass BaseEvent(BaseModel):\n    type: Union["ClientEventTypes", "ServerEventTypes"]\n    event_id: Optional[str] = None  # Add event_id as an optional field for all events\n\n    # def model_dump_json(self, *args, **kwargs):\n    #     # Only include non-None fields\n    #     return super().model_dump_json(*args, exclude_none=True, **kwargs)\n\n\nclass ChatMessage(BaseModel):\n    role: Literal["user", "assistant"]\n    content: str\n    timestamp: float\n\n\n""" CLIENT EVENTS """\n\n\nclass ClientEventTypes(str, Enum):\n    SESSION_UPDATE = "session.update"\n    CONVERSATION_ITEM_CREATE = "conversation.item.create"\n    CONVERSATION_ITEM_TRUNCATE = "conversation.item.truncate"\n    CONVERSATION_ITEM_DELETE = "conversation.item.delete"\n    RESPONSE_CREATE = "response.create"\n    RESPONSE_CANCEL = "response.cancel"\n    INPUT_AUDIO_BUFFER_APPEND = "input_audio_buffer.append"\n    INPUT_AUDIO_BUFFER_COMMIT = "input_audio_buffer.commit"\n    INPUT_AUDIO_BUFFER_CLEAR = "input_audio_buffer.clear"\n    ERROR = "error"\n\n\n#### Session Update\nclass TurnDetection(BaseModel):\n    type: Literal["server_vad"]\n    threshold: float = Field(..., ge=0.0, le=1.0)\n    prefix_padding_ms: int\n    silence_duration_ms: int\n\n\nclass InputAudioTranscription(BaseModel):\n    model: Optional[str] = None\n\n\nclass ToolParameterProperty(BaseModel):\n    type: str\n\n\nclass ToolParameter(BaseModel):\n    type: str\n    properties: dict[str, ToolParameterProperty]\n    required: list[str]\n\n\nclass Tool(BaseModel):\n    type: Literal["function", "code_interpreter", "file_search"]\n    name: Optional[str] = None\n    description: Optional[str] = None\n    parameters: Optional[ToolParameter] = None\n\n\nclass Session(BaseModel):\n    modalities: Optional[list[str]] = None\n    instructions: Optional[str] = None\n    voice: Optional[str] = None\n    input_audio_format: Optional[str] = None\n    output_audio_format: Optional[str] = None\n    input_audio_transcription: Optional[InputAudioTranscription] = None\n    turn_detection: Optional[TurnDetection] = None\n    tools: Optional[list[Tool]] = None\n    tool_choice: Optional[str] = None\n    temperature: Optional[float] = None\n    max_output_tokens: Optional[int] = None\n\n\nclass SessionUpdate(BaseEvent):\n    type: Literal[ClientEventTypes.SESSION_UPDATE] = ClientEventTypes.SESSION_UPDATE\n    session: Session\n\n\n#### Audio Buffers\nclass InputAudioBufferAppend(BaseEvent):\n    type: Literal[ClientEventTypes.INPUT_AUDIO_BUFFER_APPEND] = (\n        ClientEventTypes.INPUT_AUDIO_BUFFER_APPEND\n    )\n    audio: str\n\n\nclass InputAudioBufferCommit(BaseEvent):\n    type: Literal[ClientEventTypes.INPUT_AUDIO_BUFFER_COMMIT] = (\n        ClientEventTypes.INPUT_AUDIO_BUFFER_COMMIT\n    )\n\n\nclass InputAudioBufferClear(BaseEvent):\n    type: Literal[ClientEventTypes.INPUT_AUDIO_BUFFER_CLEAR] = (\n        ClientEventTypes.INPUT_AUDIO_BUFFER_CLEAR\n    )\n\n\n#### Messages\nclass MessageContent(BaseModel):\n    type: Literal["input_audio"]\n    audio: str\n\n\nclass ConversationItemContent(BaseModel):\n    type: Literal["input_text", "input_audio", "text", "audio"]\n    text: Optional[str] = None\n    audio: Optional[str] = None\n    transcript: Optional[str] = None\n\n\nclass FunctionCallContent(BaseModel):\n    call_id: str\n    name: str\n    arguments: str\n\n\nclass FunctionCallOutputContent(BaseModel):\n    output: str\n\n\nclass ConversationItem(BaseModel):\n    id: Optional[str] = None\n    type: Literal["message", "function_call", "function_call_output"]\n    status: Optional[Literal["completed", "in_progress", "incomplete"]] = None\n    role: Literal["user", "assistant", "system"]\n    content: list[\n        Union[ConversationItemContent, FunctionCallContent, FunctionCallOutputContent]\n    ]\n    call_id: Optional[str] = None\n    name: Optional[str] = None\n    arguments: Optional[str] = None\n    output: Optional[str] = None\n\n\nclass ConversationItemCreate(BaseEvent):\n    type: Literal[ClientEventTypes.CONVERSATION_ITEM_CREATE] = (\n        ClientEventTypes.CONVERSATION_ITEM_CREATE\n    )\n    item: ConversationItem\n\n\nclass ConversationItemTruncate(BaseEvent):\n    type: Literal[ClientEventTypes.CONVERSATION_ITEM_TRUNCATE] = (\n        ClientEventTypes.CONVERSATION_ITEM_TRUNCATE\n    )\n    item_id: str\n    content_index: int\n    audio_end_ms: int\n\n\nclass ConversationItemDelete(BaseEvent):\n    type: Literal[ClientEventTypes.CONVERSATION_ITEM_DELETE] = (\n        ClientEventTypes.CONVERSATION_ITEM_DELETE\n    )\n    item_id: str\n\n\n#### Responses\nclass ResponseCreate(BaseEvent):\n    type: Literal[ClientEventTypes.RESPONSE_CREATE] = ClientEventTypes.RESPONSE_CREATE\n\n\nclass ResponseCancel(BaseEvent):\n    type: Literal[ClientEventTypes.RESPONSE_CANCEL] = ClientEventTypes.RESPONSE_CANCEL\n\n\n# Update the Event union to include all event types\nClientEvent = Union[\n    SessionUpdate,\n    InputAudioBufferAppend,\n    InputAudioBufferCommit,\n    InputAudioBufferClear,\n    ConversationItemCreate,\n    ConversationItemTruncate,\n    ConversationItemDelete,\n    ResponseCreate,\n    ResponseCancel,\n]\n\n""" SERVER EVENTS """\n\n\nclass ServerEventTypes(str, Enum):\n    ERROR = "error"\n    RESPONSE_AUDIO_TRANSCRIPT_DONE = "response.audio_transcript.done"\n    RESPONSE_AUDIO_TRANSCRIPT_DELTA = "response.audio_transcript.delta"\n    RESPONSE_AUDIO_DELTA = "response.audio.delta"\n    SESSION_CREATED = "session.created"\n    SESSION_UPDATED = "session.updated"\n    CONVERSATION_CREATED = "conversation.created"\n    INPUT_AUDIO_BUFFER_COMMITTED = "input_audio_buffer.committed"\n    INPUT_AUDIO_BUFFER_CLEARED = "input_audio_buffer.cleared"\n    INPUT_AUDIO_BUFFER_SPEECH_STARTED = "input_audio_buffer.speech_started"\n    INPUT_AUDIO_BUFFER_SPEECH_STOPPED = "input_audio_buffer.speech_stopped"\n    CONVERSATION_ITEM_CREATED = "conversation.item.created"\n    CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_COMPLETED = (\n        "conversation.item.input_audio_transcription.completed"\n    )\n    CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_FAILED = (\n        "conversation.item.input_audio_transcription.failed"\n    )\n    CONVERSATION_ITEM_TRUNCATED = "conversation.item.truncated"\n    CONVERSATION_ITEM_DELETED = "conversation.item.deleted"\n    RESPONSE_CREATED = "response.created"\n    RESPONSE_DONE = "response.done"\n    RESPONSE_OUTPUT_ITEM_ADDED = "response.output_item.added"\n    RESPONSE_OUTPUT_ITEM_DONE = "response.output_item.done"\n    RESPONSE_CONTENT_PART_ADDED = "response.content_part.added"\n    RESPONSE_CONTENT_PART_DONE = "response.content_part.done"\n    RESPONSE_TEXT_DELTA = "response.text.delta"\n    RESPONSE_TEXT_DONE = "response.text.done"\n    RESPONSE_AUDIO_DONE = "response.audio.done"\n    RESPONSE_FUNCTION_CALL_ARGUMENTS_DELTA = "response.function_call_arguments.delta"\n    RESPONSE_FUNCTION_CALL_ARGUMENTS_DONE = "response.function_call_arguments.done"\n    RATE_LIMITS_UPDATED = "rate_limits.updated"\n\n\n#### Errors\nclass ErrorDetails(BaseModel):\n    type: Optional[str] = None\n    code: Optional[str] = None\n    message: Optional[str] = None\n    param: Optional[str] = None\n\n\nclass ErrorEvent(BaseEvent):\n    type: Literal[ServerEventTypes.ERROR] = ServerEventTypes.ERROR\n    error: ErrorDetails\n\n\n#### Session\nclass SessionCreated(BaseEvent):\n    type: Literal[ServerEventTypes.SESSION_CREATED] = ServerEventTypes.SESSION_CREATED\n    session: Session\n\n\nclass SessionUpdated(BaseEvent):\n    type: Literal[ServerEventTypes.SESSION_UPDATED] = ServerEventTypes.SESSION_UPDATED\n    session: Session\n\n\n#### Conversation\nclass Conversation(BaseModel):\n    id: str\n    object: Literal["realtime.conversation"]\n\n\nclass ConversationCreated(BaseEvent):\n    type: Literal[ServerEventTypes.CONVERSATION_CREATED] = (\n        ServerEventTypes.CONVERSATION_CREATED\n    )\n    conversation: Conversation\n\n\nclass ConversationItemCreated(BaseEvent):\n    type: Literal[ServerEventTypes.CONVERSATION_ITEM_CREATED] = (\n        ServerEventTypes.CONVERSATION_ITEM_CREATED\n    )\n    previous_item_id: Optional[str] = None\n    item: ConversationItem\n\n\nclass ConversationItemInputAudioTranscriptionCompleted(BaseEvent):\n    type: Literal[\n        ServerEventTypes.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_COMPLETED\n    ] = ServerEventTypes.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_COMPLETED\n    item_id: str\n    content_index: int\n    transcript: str\n\n\nclass ConversationItemInputAudioTranscriptionFailed(BaseEvent):\n    type: Literal[\n        ServerEventTypes.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_FAILED\n    ] = ServerEventTypes.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_FAILED\n    item_id: str\n    content_index: int\n    error: dict[str, Any]\n\n\nclass ConversationItemTruncated(BaseEvent):\n    type: Literal[ServerEventTypes.CONVERSATION_ITEM_TRUNCATED] = (\n        ServerEventTypes.CONVERSATION_ITEM_TRUNCATED\n    )\n    item_id: str\n    content_index: int\n    audio_end_ms: int\n\n\nclass ConversationItemDeleted(BaseEvent):\n    type: Literal[ServerEventTypes.CONVERSATION_ITEM_DELETED] = (\n        ServerEventTypes.CONVERSATION_ITEM_DELETED\n    )\n    item_id: str\n\n\n#### Response\nclass ResponseUsage(BaseModel):\n    total_tokens: int\n    input_tokens: int\n    output_tokens: int\n    input_token_details: Optional[dict[str, int]] = None\n    output_token_details: Optional[dict[str, int]] = None\n\n\nclass ResponseOutput(BaseModel):\n    id: str\n    object: Literal["realtime.item"]\n    type: str\n    status: str\n    role: str\n    content: list[dict[str, Any]]\n\n\nclass ResponseContentPart(BaseModel):\n    type: str\n    text: Optional[str] = None\n\n\nclass ResponseOutputItemContent(BaseModel):\n    type: str\n    text: Optional[str] = None\n\n\nclass ResponseStatusDetails(BaseModel):\n    type: str\n    reason: str\n\n\nclass ResponseOutputItem(BaseModel):\n    id: str\n    object: Literal["realtime.item"]\n    type: str\n    status: str\n    role: str\n    content: list[ResponseOutputItemContent]\n\n\nclass Response(BaseModel):\n    id: str\n    object: Literal["realtime.response"]\n    status: str\n    status_details: Optional[ResponseStatusDetails] = None\n    output: list[ResponseOutput]\n    usage: Optional[ResponseUsage]\n\n\nclass ResponseCreated(BaseEvent):\n    type: Literal[ServerEventTypes.RESPONSE_CREATED] = ServerEventTypes.RESPONSE_CREATED\n    response: Response\n\n\nclass ResponseDone(BaseEvent):\n    type: Literal[ServerEventTypes.RESPONSE_DONE] = ServerEventTypes.RESPONSE_DONE\n    response: Response\n\n\nclass ResponseOutputItemAdded(BaseEvent):\n    type: Literal[ServerEventTypes.RESPONSE_OUTPUT_ITEM_ADDED] = (\n        ServerEventTypes.RESPONSE_OUTPUT_ITEM_ADDED\n    )\n    response_id: str\n    output_index: int\n    item: ResponseOutputItem\n\n\nclass ResponseOutputItemDone(BaseEvent):\n    type: Literal[ServerEventTypes.RESPONSE_OUTPUT_ITEM_DONE] = (\n        ServerEventTypes.RESPONSE_OUTPUT_ITEM_DONE\n    )\n    response_id: str\n    output_index: int\n    item: ResponseOutputItem\n\n\nclass ResponseContentPartAdded(BaseEvent):\n    type: Literal[ServerEventTypes.RESPONSE_CONTENT_PART_ADDED] = (\n        ServerEventTypes.RESPONSE_CONTENT_PART_ADDED\n    )\n    response_id: str\n    item_id: str\n    output_index: int\n    content_index: int\n    part: ResponseContentPart\n\n\nclass ResponseContentPartDone(BaseEvent):\n    type: Literal[ServerEventTypes.RESPONSE_CONTENT_PART_DONE] = (\n        ServerEventTypes.RESPONSE_CONTENT_PART_DONE\n    )\n    response_id: str\n    item_id: str\n    output_index: int\n    content_index: int\n    part: ResponseContentPart\n\n\n#### Response Text\nclass ResponseTextDelta(BaseEvent):\n    type: Literal[ServerEventTypes.RESPONSE_TEXT_DELTA] = (\n        ServerEventTypes.RESPONSE_TEXT_DELTA\n    )\n    response_id: str\n    item_id: str\n    output_index: int\n    content_index: int\n    delta: str\n\n\nclass ResponseTextDone(BaseEvent):\n    type: Literal[ServerEventTypes.RESPONSE_TEXT_DONE] = (\n        ServerEventTypes.RESPONSE_TEXT_DONE\n    )\n    response_id: str\n    item_id: str\n    output_index: int\n    content_index: int\n    text: str\n\n\n#### Response Audio\nclass ResponseAudioTranscriptDone(BaseEvent):\n    type: Literal[ServerEventTypes.RESPONSE_AUDIO_TRANSCRIPT_DONE] = (\n        ServerEventTypes.RESPONSE_AUDIO_TRANSCRIPT_DONE\n    )\n    transcript: str\n\n\nclass ResponseAudioTranscriptDelta(BaseEvent):\n    type: Literal[ServerEventTypes.RESPONSE_AUDIO_TRANSCRIPT_DELTA] = (\n        ServerEventTypes.RESPONSE_AUDIO_TRANSCRIPT_DELTA\n    )\n    delta: str\n\n\nclass ResponseAudioDelta(BaseEvent):\n    type: Literal[ServerEventTypes.RESPONSE_AUDIO_DELTA] = (\n        ServerEventTypes.RESPONSE_AUDIO_DELTA\n    )\n    response_id: str\n    item_id: str\n    delta: str\n\n\nclass ResponseAudioDone(BaseEvent):\n    type: Literal[ServerEventTypes.RESPONSE_AUDIO_DONE] = (\n        ServerEventTypes.RESPONSE_AUDIO_DONE\n    )\n    response_id: str\n    item_id: str\n    output_index: int\n    content_index: int\n\n\nclass InputAudioBufferCommitted(BaseEvent):\n    type: Literal[ServerEventTypes.INPUT_AUDIO_BUFFER_COMMITTED] = (\n        ServerEventTypes.INPUT_AUDIO_BUFFER_COMMITTED\n    )\n    previous_item_id: Optional[str] = None\n    item_id: Optional[str] = None\n    event_id: Optional[str] = None\n\n\nclass InputAudioBufferCleared(BaseEvent):\n    type: Literal[ServerEventTypes.INPUT_AUDIO_BUFFER_CLEARED] = (\n        ServerEventTypes.INPUT_AUDIO_BUFFER_CLEARED\n    )\n\n\nclass InputAudioBufferSpeechStarted(BaseEvent):\n    type: Literal[ServerEventTypes.INPUT_AUDIO_BUFFER_SPEECH_STARTED] = (\n        ServerEventTypes.INPUT_AUDIO_BUFFER_SPEECH_STARTED\n    )\n    audio_start_ms: int\n    item_id: str\n\n\nclass InputAudioBufferSpeechStopped(BaseEvent):\n    type: Literal[ServerEventTypes.INPUT_AUDIO_BUFFER_SPEECH_STOPPED] = (\n        ServerEventTypes.INPUT_AUDIO_BUFFER_SPEECH_STOPPED\n    )\n    audio_end_ms: int\n    item_id: str\n\n\n#### Function Calls\nclass ResponseFunctionCallArgumentsDelta(BaseEvent):\n    type: Literal[ServerEventTypes.RESPONSE_FUNCTION_CALL_ARGUMENTS_DELTA] = (\n        ServerEventTypes.RESPONSE_FUNCTION_CALL_ARGUMENTS_DELTA\n    )\n    response_id: str\n    item_id: str\n    output_index: int\n    call_id: str\n    delta: str\n\n\nclass ResponseFunctionCallArgumentsDone(BaseEvent):\n    type: Literal[ServerEventTypes.RESPONSE_FUNCTION_CALL_ARGUMENTS_DONE] = (\n        ServerEventTypes.RESPONSE_FUNCTION_CALL_ARGUMENTS_DONE\n    )\n    response_id: str\n    item_id: str\n    output_index: int\n    call_id: str\n    arguments: str\n\n\n#### Rate Limits\nclass RateLimit(BaseModel):\n    name: str\n    limit: int\n    remaining: int\n    reset_seconds: float\n\n\nclass RateLimitsUpdated(BaseEvent):\n    type: Literal[ServerEventTypes.RATE_LIMITS_UPDATED] = (\n        ServerEventTypes.RATE_LIMITS_UPDATED\n    )\n    rate_limits: list[RateLimit]\n\n\nServerEvent = Union[\n    ErrorEvent,\n    ConversationCreated,\n    ResponseAudioTranscriptDone,\n    ResponseAudioTranscriptDelta,\n    ResponseAudioDelta,\n    ResponseCreated,\n    ResponseDone,\n    ResponseOutputItemAdded,\n    ResponseOutputItemDone,\n    ResponseContentPartAdded,\n    ResponseContentPartDone,\n    ResponseTextDelta,\n    ResponseTextDone,\n    ResponseAudioDone,\n    ConversationItemInputAudioTranscriptionCompleted,\n    SessionCreated,\n    SessionUpdated,\n    InputAudioBufferCleared,\n    InputAudioBufferSpeechStarted,\n    InputAudioBufferSpeechStopped,\n    ConversationItemCreated,\n    ConversationItemInputAudioTranscriptionFailed,\n    ConversationItemTruncated,\n    ConversationItemDeleted,\n    RateLimitsUpdated,\n]\n\nEVENT_TYPE_TO_MODEL = {\n    ServerEventTypes.ERROR: ErrorEvent,\n    ServerEventTypes.RESPONSE_AUDIO_TRANSCRIPT_DONE: ResponseAudioTranscriptDone,\n    ServerEventTypes.RESPONSE_AUDIO_TRANSCRIPT_DELTA: ResponseAudioTranscriptDelta,\n    ServerEventTypes.RESPONSE_AUDIO_DELTA: ResponseAudioDelta,\n    ServerEventTypes.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_COMPLETED: ConversationItemInputAudioTranscriptionCompleted,\n    ServerEventTypes.SESSION_CREATED: SessionCreated,\n    ServerEventTypes.SESSION_UPDATED: SessionUpdated,\n    ServerEventTypes.CONVERSATION_CREATED: ConversationCreated,\n    ServerEventTypes.INPUT_AUDIO_BUFFER_COMMITTED: InputAudioBufferCommitted,\n    ServerEventTypes.INPUT_AUDIO_BUFFER_CLEARED: InputAudioBufferCleared,\n    ServerEventTypes.INPUT_AUDIO_BUFFER_SPEECH_STARTED: InputAudioBufferSpeechStarted,\n    ServerEventTypes.INPUT_AUDIO_BUFFER_SPEECH_STOPPED: InputAudioBufferSpeechStopped,\n    ServerEventTypes.CONVERSATION_ITEM_CREATED: ConversationItemCreated,\n    ServerEventTypes.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_FAILED: ConversationItemInputAudioTranscriptionFailed,\n    ServerEventTypes.CONVERSATION_ITEM_TRUNCATED: ConversationItemTruncated,\n    ServerEventTypes.CONVERSATION_ITEM_DELETED: ConversationItemDeleted,\n    ServerEventTypes.RESPONSE_CREATED: ResponseCreated,\n    ServerEventTypes.RESPONSE_DONE: ResponseDone,\n    ServerEventTypes.RESPONSE_OUTPUT_ITEM_ADDED: ResponseOutputItemAdded,\n    ServerEventTypes.RESPONSE_OUTPUT_ITEM_DONE: ResponseOutputItemDone,\n    ServerEventTypes.RESPONSE_CONTENT_PART_ADDED: ResponseContentPartAdded,\n    ServerEventTypes.RESPONSE_CONTENT_PART_DONE: ResponseContentPartDone,\n    ServerEventTypes.RESPONSE_TEXT_DELTA: ResponseTextDelta,\n    ServerEventTypes.RESPONSE_TEXT_DONE: ResponseTextDone,\n    ServerEventTypes.RESPONSE_AUDIO_DONE: ResponseAudioDone,\n    ServerEventTypes.RATE_LIMITS_UPDATED: RateLimitsUpdated,\n}\n\n\ndef parse_server_event(event_data: dict) -> ServerEvent:\n    event_type = event_data.get("type")\n    if not event_type:\n        raise ValueError("Event data is missing \'type\' field")\n\n    model_class = EVENT_TYPE_TO_MODEL.get(event_type)\n    if not model_class:\n        raise ValueError(f"Unknown event type: {event_type}")\n\n    try:\n        return model_class(**event_data)\n    except ValidationError as e:\n        raise ValueError(f"Failed to parse event of type {event_type}: {str(e)}")\n'})})]}),(0,s.jsx)(n.h2,{id:"audio-stream-writer-to-disk-and-in-memory",children:"Audio Stream Writer (To Disk and In Memory)"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class StreamingWavWriter:\n    """Writes audio integer or byte array chunks to a WAV file."""\n\n    wav_file = None\n    buffer = None\n    in_memory = False\n\n    def __init__(\n        self,\n        filename=None,\n        channels=INPUT_DEVICE_CHANNELS,\n        sample_width=SAMPLE_WIDTH,\n        framerate=SAMPLE_RATE,\n    ):\n        self.in_memory = filename is None\n        if self.in_memory:\n            self.buffer = io.BytesIO()\n            self.wav_file = wave.open(self.buffer, "wb")\n        else:\n            self.wav_file = wave.open(filename, "wb")\n\n        self.wav_file.setnchannels(channels)\n        self.wav_file.setsampwidth(sample_width)\n        self.wav_file.setframerate(framerate)\n\n    def append_int16_chunk(self, int16_data):\n        if int16_data is not None:\n            self.wav_file.writeframes(\n                int16_data.tobytes()\n                if isinstance(int16_data, np.ndarray)\n                else int16_data\n            )\n\n    def close(self):\n        self.wav_file.close()\n\n    def get_wav_buffer(self):\n        assert self.in_memory, "Buffer only available if stream is in memory."\n        return self.buffer\n'})}),(0,s.jsx)(n.h2,{id:"realtime-audio-model",children:"Realtime Audio Model"}),(0,s.jsx)(n.p,{children:"The realtime (RT) audio model uses a websocket to send events to OpenAI's Realtime audio API. This works as follows:"}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"init:"})," We initialize local buffers (input audio) and streams (assistant playback stream, user audio disk writer stream) and open a connection to the Realtime API."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"receive_messages_thread"}),": A thread handles receiving messages from the API. Four primary event types are handled: - RESPONSE_AUDIO_TRANSCRIPT_DONE:"]}),"\n",(0,s.jsx)(n.p,{children:"The server indicates the assistant's response is completed and provides the transcript."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_COMPLETED:"}),"\n",(0,s.jsx)(n.p,{children:"The server indicates the user's audio has been transcribed, and sends the transcript of the user's audio. We log the transcript to Weave and print it for the user."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"RESPONSE_AUDIO_DELTA:"}),"\n",(0,s.jsx)(n.p,{children:"The server sends a new chunk of assistant response audio. We append this to the ongoing response data via the response ID, and add this to the output stream for playback."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"RESPONSE_DONE:"}),"\n",(0,s.jsx)(n.p,{children:"The server indicates completion of an assistant response. We get all audio chunks associated with the response, as well as the transcript, and log these in Weave."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["3.",(0,s.jsx)(n.strong,{children:"send_audio"}),": A handler appends user audio chunks to a buffer, and sends chunks of audio when the audio buffer reaches a certain size."]}),"\n"]}),"\n"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class RTAudioModel(weave.Model):\n    """Model class for realtime e2e audio OpenAI model interaction with Whisper user transcription for logging."""\n\n    realtime_model_name: str = "gpt-4o-realtime-preview-2024-10-01"  # realtime e2e audio only model interaction\n\n    stop_event: Optional[threading.Event] = threading.Event()  # Event to stop the model\n    ws: Optional[websocket.WebSocket] = None  # Websocket for OpenAI communications\n\n    user_wav_writer: Optional[StreamingWavWriter] = (\n        None  # Stream for writing user output to file\n    )\n    input_audio_buffer: Optional[np.ndarray] = None  # Buffer for user audio chunks\n    assistant_outputs: dict[str, StreamingWavWriter] = (\n        None  # Assistant outputs aggregated to send to weave\n    )\n    playback_stream: Optional[pyaudio.Stream] = (\n        None  # Playback stream for playing assistant responses\n    )\n\n    def __init__(self):\n        super().__init__()\n        self.stop_event.clear()\n        self.user_wav_writer = StreamingWavWriter(\n            filename="user_audio.wav", framerate=SAMPLE_RATE\n        )\n        self.input_audio_buffer = np.array([], dtype=np.int16)\n        self.ws = websocket.WebSocket()\n        self.assistant_outputs = {}\n\n        # Open the assistant audio playback stream if enabled\n        if enable_audio_playback:\n            self.playback_stream = pyaudio.PyAudio().open(\n                format=pyaudio.paInt16,\n                channels=OUTPUT_DEVICE_CHANNELS,\n                rate=OAI_SAMPLE_RATE,\n                output=True,\n                output_device_index=OUTPUT_DEVICE_INDEX,\n            )\n\n        # Connect Websocket\n        try:\n            self.ws.connect(\n                f"wss://api.openai.com/v1/realtime?model={self.realtime_model_name}",\n                header={\n                    "Authorization": f"Bearer {os.environ.get(\'OPENAI_API_KEY\')}",\n                    "OpenAI-Beta": "realtime=v1",\n                },\n            )\n\n            # Send config msg\n            config_event = SessionUpdate(\n                session=Session(\n                    modalities=["text", "audio"],  # modalities to use\n                    input_audio_transcription=InputAudioTranscription(\n                        model="whisper-1"\n                    ),  # whisper-1 for transcription\n                    turn_detection=TurnDetection(\n                        type="server_vad",\n                        threshold=0.3,\n                        prefix_padding_ms=300,\n                        silence_duration_ms=600,\n                    ),  # server VAD to detect silence\n                )\n            )\n            self.ws.send(config_event.model_dump_json(exclude_none=True))\n            self.log_ws_message(config_event.model_dump_json(exclude_none=True), "Sent")\n\n            # Start listener\n            websocket_thread = threading.Thread(target=self.receive_messages_thread)\n            websocket_thread.daemon = True\n            websocket_thread.start()\n\n        except Exception as e:\n            print(f"Error connecting to WebSocket: {e}")\n\n    ##### Weave Integration and Message Handlers #####\n    def handle_assistant_response_audio_delta(self, data: ResponseAudioDelta):\n        if data.response_id not in self.assistant_outputs:\n            self.assistant_outputs[data.response_id] = StreamingWavWriter(\n                framerate=OAI_SAMPLE_RATE\n            )\n\n        data_bytes = base64.b64decode(data.delta)\n        self.assistant_outputs[data.response_id].append_int16_chunk(data_bytes)\n\n        if enable_audio_playback:\n            self.playback_stream.write(data_bytes)\n\n        return {"assistant_audio": data_bytes}\n\n    @weave.op()\n    def handle_assistant_response_done(self, data: ResponseDone):\n        wave_file_stream = self.assistant_outputs[data.response.id]\n        wave_file_stream.close()\n        wave_file_stream.buffer.seek(0)\n        weave_payload = {\n            "assistant_audio": wave.open(wave_file_stream.get_wav_buffer(), "rb"),\n            "assistant_transcript": data.response.output[0]\n            .content[0]\n            .get("transcript", "Transcript Unavailable."),\n        }\n        return weave_payload\n\n    @weave.op()\n    def handle_user_transcription_done(\n        self, data: ConversationItemInputAudioTranscriptionCompleted\n    ):\n        return {"user_transcript": data.transcript}\n\n    ##### Message Receiver and Sender #####\n    def receive_messages_thread(self):\n        while not self.stop_event.is_set():\n            try:\n                data = json.loads(self.ws.recv())\n                self.log_ws_message(json.dumps(data, indent=2))\n\n                parsed_event = parse_server_event(data)\n\n                if parsed_event.type == ServerEventTypes.RESPONSE_AUDIO_TRANSCRIPT_DONE:\n                    print("Assistant: ", parsed_event.transcript)\n                elif (\n                    parsed_event.type\n                    == ServerEventTypes.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_COMPLETED\n                ):\n                    print("User: ", parsed_event.transcript)\n                    self.handle_user_transcription_done(parsed_event)\n                elif parsed_event.type == ServerEventTypes.RESPONSE_AUDIO_DELTA:\n                    self.handle_assistant_response_audio_delta(parsed_event)\n                elif parsed_event.type == ServerEventTypes.RESPONSE_DONE:\n                    self.handle_assistant_response_done(parsed_event)\n                elif parsed_event.type == ServerEventTypes.ERROR:\n                    print(\n                        f"\\nError from server: {parsed_event.error.model_dump_json(exclude_none=True)}"\n                    )\n            except websocket.WebSocketConnectionClosedException:\n                print("\\nWebSocket connection closed")\n                break\n            except json.JSONDecodeError:\n                continue\n            except Exception as e:\n                print(f"\\nError in receive_messages: {e}")\n                break\n\n    def send_audio(self, audio_chunk):\n        if self.ws and self.ws.connected:\n            self.input_audio_buffer = np.append(\n                self.input_audio_buffer, np.frombuffer(audio_chunk, dtype=np.int16)\n            )\n            if len(self.input_audio_buffer) >= SAMPLE_RATE * CHUNK_DURATION:\n                try:\n                    # Resample audio to OAI sample rate\n                    resampled_audio = (\n                        resampy.resample(\n                            self.input_audio_buffer, SAMPLE_RATE, OAI_SAMPLE_RATE\n                        )\n                        if SAMPLE_RATE != OAI_SAMPLE_RATE\n                        else self.input_audio_buffer\n                    )\n\n                    # Send audio chunk to OAI API\n                    audio_event = InputAudioBufferAppend(\n                        audio=base64.b64encode(\n                            resampled_audio.astype(np.int16).tobytes()\n                        ).decode("utf-8")  # Convert audio array to b64 bytes\n                    )\n                    self.ws.send(audio_event.model_dump_json(exclude_none=True))\n                    self.log_ws_message(\n                        audio_event.model_dump_json(exclude_none=True), "Sent"\n                    )\n                finally:\n                    self.user_wav_writer.append_int16_chunk(self.input_audio_buffer)\n\n                    # Clear the audio buffer\n                    self.input_audio_buffer = np.array([], dtype=np.int16)\n        else:\n            print("Error sending audio: websocket not initialized.")\n\n    ##### General Utility Functions #####\n    def log_ws_message(self, message, direction="Received"):\n        with open("websocket_log.txt", "a") as log_file:\n            log_file.write(\n                f"{time.strftime(\'%Y-%m-%d %H:%M:%S\')} - {direction}: {message}\\n"\n            )\n\n    def stop(self):\n        self.stop_event.set()\n\n        if self.ws:\n            self.ws.close()\n\n        self.user_wav_writer.close()\n'})}),(0,s.jsx)(n.h2,{id:"audio-recorder",children:"Audio recorder"}),(0,s.jsxs)(n.p,{children:["We use a pyaudio input stream with a handler linked to the ",(0,s.jsx)(n.code,{children:"send_audio"})," method of the RTAudio model. The stream is returned to the main thread so it can be safely exited upon program completion."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Audio capture stream\ndef record_audio(realtime_model: RTAudioModel) -> pyaudio.Stream:\n    """Setup a Pyaudio input stream and use the RTAudioModel as a callback for streaming data."""\n\n    def audio_callback(in_data, frame_count, time_info, status):\n        realtime_model.send_audio(in_data)\n        return (None, pyaudio.paContinue)\n\n    p = pyaudio.PyAudio()\n    stream = p.open(\n        format=pyaudio.paInt16,\n        channels=INPUT_DEVICE_CHANNELS,\n        rate=SAMPLE_RATE,\n        input=True,\n        input_device_index=INPUT_DEVICE_INDEX,\n        frames_per_buffer=CHUNK,\n        stream_callback=audio_callback,\n    )\n    stream.start_stream()\n\n    print("Recording started. Please begin speaking to your personal assistant...")\n    return stream\n'})}),(0,s.jsx)(n.h2,{id:"main-thread-run-me",children:"Main Thread (Run me!)"}),(0,s.jsx)(n.p,{children:"The main thread initiates a Realtime Audio Model with Weave integrated. Next, a reccording is opened and we wait for a keyboard interrupt from the user."}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'weave.init(project_name="realtime-oai-audio-testing")\n\nrealtime_model = RTAudioModel()\n\nif realtime_model.ws and realtime_model.ws.connected:\n    recording_stream: pyaudio.Stream = record_audio(realtime_model)\n\n    try:\n        while not realtime_model.stop_event.is_set():\n            time.sleep(1)\n    except KeyboardInterrupt:\n        pass\n    except Exception as e:\n        print(f"Error in main loop: {e}")\n        import traceback\n\n        traceback.print_exc()\n    finally:\n        print("Exiting...")\n        realtime_model.stop()\n        if recording_stream and recording_stream.is_active():\n            recording_stream.stop_stream()\n            recording_stream.close()\nelse:\n    print(\n        "WebSocket connection failed. Please check your API key and internet connection."\n    )\n'})})]})]})}function _(e={}){const{wrapper:n}={...(0,a.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(p,{...e})}):p(e)}},11151:(e,n,t)=>{t.d(n,{Z:()=>o,a:()=>r});var s=t(67294);const a={},i=s.createContext(a);function r(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);