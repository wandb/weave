"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[229],{1603:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>c});var a=n(85893),s=n(11151);const r={},i="MistralAI",o={id:"guides/integrations/mistral",title:"MistralAI",description:"Weave automatically tracks and logs LLM calls made via the MistralAI Python library.",source:"@site/docs/guides/integrations/mistral.md",sourceDirName:"guides/integrations",slug:"/guides/integrations/mistral",permalink:"/guides/integrations/mistral",draft:!1,unlisted:!1,editUrl:"https://github.com/wandb/weave/blob/master/docs/docs/guides/integrations/mistral.md",tags:[],version:"current",lastUpdatedAt:1749652482e3,frontMatter:{},sidebar:"documentationSidebar",previous:{title:"Microsoft Azure",permalink:"/guides/integrations/azure"},next:{title:"NVIDIA NIM",permalink:"/guides/integrations/nvidia_nim"}},l={},c=[{value:"Traces",id:"traces",level:2},{value:"Wrapping with your own ops",id:"wrapping-with-your-own-ops",level:2},{value:"Create a <code>Model</code> for easier experimentation",id:"create-a-model-for-easier-experimentation",level:2}];function d(e){const t={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",img:"img",p:"p",pre:"pre",...(0,s.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.h1,{id:"mistralai",children:"MistralAI"}),"\n",(0,a.jsx)("a",{target:"_blank",href:"https://colab.research.google.com/github/wandb/examples/blob/master/weave/docs/quickstart_mistral.ipynb",children:(0,a.jsx)("img",{src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open In Colab"})}),"\n",(0,a.jsxs)(t.p,{children:["Weave automatically tracks and logs LLM calls made via the ",(0,a.jsx)(t.a,{href:"https://github.com/mistralai/client-python",children:"MistralAI Python library"}),"."]}),"\n",(0,a.jsxs)(t.blockquote,{children:["\n",(0,a.jsxs)(t.p,{children:["We support the new Mistral v1.0 SDK, check the migration guide ",(0,a.jsx)(t.a,{href:"https://github.com/mistralai/client-python/blob/main/MIGRATION.md",children:"here"})]}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"traces",children:"Traces"}),"\n",(0,a.jsx)(t.p,{children:"It\u2019s important to store traces of LLM applications in a central database, both during development and in production. You\u2019ll use these traces for debugging, and as a dataset that will help you improve your application."}),"\n",(0,a.jsxs)(t.p,{children:["Weave will automatically capture traces for ",(0,a.jsx)(t.a,{href:"https://github.com/mistralai/client-python",children:"mistralai"}),". You can use the library as usual, start by calling ",(0,a.jsx)(t.code,{children:"weave.init()"}),":"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'import weave\nweave.init("cheese_recommender")\n\n# then use mistralai library as usual\nimport os\nfrom mistralai import Mistral\n\napi_key = os.environ["MISTRAL_API_KEY"]\nmodel = "mistral-large-latest"\n\nclient = Mistral(api_key=api_key)\n\nmessages = [\n    {\n        "role": "user",\n        "content": "What is the best French cheese?",\n    },\n]\n\nchat_response = client.chat.complete(\n    model=model,\n    messages=messages,\n)\n'})}),"\n",(0,a.jsx)(t.p,{children:"Weave will now track and log all LLM calls made through the MistralAI library. You can view the traces in the Weave web interface."}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.a,{href:"https://wandb.ai/capecape/mistralai_project/weave/calls",children:(0,a.jsx)(t.img,{alt:"mistral_trace.png",src:n(76734).Z+"",width:"3024",height:"1468"})})}),"\n",(0,a.jsx)(t.h2,{id:"wrapping-with-your-own-ops",children:"Wrapping with your own ops"}),"\n",(0,a.jsxs)(t.p,{children:["Weave ops make results ",(0,a.jsx)(t.em,{children:"reproducible"})," by automatically versioning code as you experiment, and they capture their inputs and outputs. Simply create a function decorated with ",(0,a.jsx)(t.a,{href:"/guides/tracking/ops",children:(0,a.jsx)(t.code,{children:"@weave.op()"})})," that calls into ",(0,a.jsx)(t.a,{href:"https://docs.mistral.ai/capabilities/completion/",children:(0,a.jsx)(t.code,{children:"mistralai.client.MistralClient.chat()"})})," and Weave will track the inputs and outputs for you. Let's see how we can do this for our cheese recommender:"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'# highlight-next-line\n@weave.op()\ndef cheese_recommender(region:str, model:str) -> str:\n    "Recommend the best cheese in a given region"\n    \n    messages = [\n        {\n            "role": "user",\n            "content": f"What is the best cheese in {region}?",\n        },\n    ]\n\n    chat_response = client.chat.complete(\n        model=model,\n        messages=messages,\n    )\n    return chat_response.choices[0].message.content\n\ncheese_recommender(region="France", model="mistral-large-latest")\ncheese_recommender(region="Spain", model="mistral-large-latest")\ncheese_recommender(region="Netherlands", model="mistral-large-latest")\n'})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.a,{href:"https://wandb.ai/capecape/mistralai_project/weave/calls",children:(0,a.jsx)(t.img,{alt:"mistral_ops.png",src:n(69365).Z+"",width:"2877",height:"1080"})})}),"\n",(0,a.jsxs)(t.h2,{id:"create-a-model-for-easier-experimentation",children:["Create a ",(0,a.jsx)(t.code,{children:"Model"})," for easier experimentation"]}),"\n",(0,a.jsxs)(t.p,{children:["Organizing experimentation is difficult when there are many moving pieces. By using the ",(0,a.jsx)(t.a,{href:"/guides/core-types/models",children:(0,a.jsx)(t.code,{children:"Model"})})," class, you can capture and organize the experimental details of your app like your system prompt or the model you're using. This helps organize and compare different iterations of your app."]}),"\n",(0,a.jsxs)(t.p,{children:["In addition to versioning code and capturing inputs/outputs, ",(0,a.jsx)(t.a,{href:"/guides/core-types/models",children:(0,a.jsx)(t.code,{children:"Model"})}),"s capture structured parameters that control your application\u2019s behavior, making it easy to find what parameters worked best. You can also use Weave Models with ",(0,a.jsx)(t.code,{children:"serve"}),", and ",(0,a.jsx)(t.a,{href:"/guides/core-types/evaluations",children:(0,a.jsx)(t.code,{children:"Evaluation"})}),"s."]}),"\n",(0,a.jsxs)(t.p,{children:["In the example below, you can experiment with ",(0,a.jsx)(t.code,{children:"model"})," and ",(0,a.jsx)(t.code,{children:"country"}),". Every time you change one of these, you'll get a new ",(0,a.jsx)(t.em,{children:"version"})," of ",(0,a.jsx)(t.code,{children:"CheeseRecommender"}),"."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'import weave\nfrom mistralai import Mistral\n\nweave.init("mistralai_project")\n\nclass CheeseRecommender(weave.Model): # Change to `weave.Model`\n    model: str\n    temperature: float\n\n    @weave.op()\n    def predict(self, region:str) -> str: # Change to `predict`\n        "Recommend the best cheese in a given region"\n        \n        client = Mistral(api_key=api_key)\n\n        messages = [\n            {\n                "role": "user",\n                "content": f"What is the best cheese in {region}?",\n            },\n        ]\n\n        chat_response = client.chat.complete(\n            model=model,\n            messages=messages,\n            temperature=self.temperature\n        )\n        return chat_response.choices[0].message.content\n\ncheese_model = CheeseRecommender(\n    model="mistral-medium-latest",\n    temperature=0.0\n    )\nresult = cheese_model.predict(region="France")\nprint(result)\n'})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.a,{href:"https://wandb.ai/capecape/mistralai_project/weave/models",children:(0,a.jsx)(t.img,{alt:"mistral_model.png",src:n(84801).Z+"",width:"3010",height:"1536"})})})]})}function h(e={}){const{wrapper:t}={...(0,s.a)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},84801:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/mistral_model-0a89ef6374318020e9da24e9d74f0d52.png"},69365:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/mistral_ops-8e251fbc90ce13b865c2140b60445abf.png"},76734:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/mistral_trace-3fa22a88515d57264ee6099e52aa206a.png"},11151:(e,t,n)=>{n.d(t,{Z:()=>o,a:()=>i});var a=n(67294);const s={},r=a.createContext(s);function i(e){const t=a.useContext(r);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),a.createElement(r.Provider,{value:t},e.children)}}}]);