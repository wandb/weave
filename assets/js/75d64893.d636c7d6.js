"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[5059],{32913:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>r,contentTitle:()=>s,default:()=>h,frontMatter:()=>i,metadata:()=>d,toc:()=>l});var o=t(85893),a=t(11151);const i={title:"Weave with W&B Models"},s="Use Weave with W&B Models",d={id:"reference/gen_notebooks/Models_and_Weave_Integration_Demo",title:"Weave with W&B Models",description:"Open in Colab",source:"@site/docs/reference/gen_notebooks/Models_and_Weave_Integration_Demo.md",sourceDirName:"reference/gen_notebooks",slug:"/reference/gen_notebooks/Models_and_Weave_Integration_Demo",permalink:"/reference/gen_notebooks/Models_and_Weave_Integration_Demo",draft:!1,unlisted:!1,editUrl:"https://github.com/wandb/weave/blob/master/docs/docs/reference/gen_notebooks/Models_and_Weave_Integration_Demo.md",tags:[],version:"current",lastUpdatedAt:1743620285e3,frontMatter:{title:"Weave with W&B Models"},sidebar:"notebookSidebar",previous:{title:"Intro_to_Weave_Hello_Trace",permalink:"/reference/gen_notebooks/Intro_to_Weave_Hello_Trace"},next:{title:"Log Audio With Weave",permalink:"/reference/gen_notebooks/audio_with_weave"}},r={},l=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Download <code>ChatModel</code> from Models Registry and implement <code>UnslothLoRAChatModel</code>",id:"download-chatmodel-from-models-registry-and-implement-unslothlorachatmodel",level:2},{value:"Integrate the new <code>ChatModel</code> version into <code>RagModel</code>",id:"integrate-the-new-chatmodel-version-into-ragmodel",level:2},{value:"Run a <code>weave.Evaluation</code>",id:"run-a-weaveevaluation",level:2},{value:"Save the new RAG Model to the Registry",id:"save-the-new-rag-model-to-the-registry",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.a)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(n.admonition,{title:"This is a notebook",type:"tip",children:[(0,o.jsx)("a",{href:"https://colab.research.google.com/github/wandb/weave/blob/master/docs/./notebooks/Models_and_Weave_Integration_Demo.ipynb",target:"_blank",rel:"noopener noreferrer",class:"navbar__item navbar__link button button--secondary button--med margin-right--sm notebook-cta-button",children:(0,o.jsxs)("div",{children:[(0,o.jsx)("img",{src:"https://upload.wikimedia.org/wikipedia/commons/archive/d/d0/20221103151430%21Google_Colaboratory_SVG_Logo.svg",alt:"Open In Colab",height:"20px"}),(0,o.jsx)("div",{children:"Open in Colab"})]})}),(0,o.jsx)("a",{href:"https://github.com/wandb/weave/blob/master/docs/./notebooks/Models_and_Weave_Integration_Demo.ipynb",target:"_blank",rel:"noopener noreferrer",class:"navbar__item navbar__link button button--secondary button--med margin-right--sm notebook-cta-button",children:(0,o.jsxs)("div",{children:[(0,o.jsx)("img",{src:"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg",alt:"View in Github",height:"15px"}),(0,o.jsx)("div",{children:"View in Github"})]})})]}),"\n",(0,o.jsx)(n.h1,{id:"use-weave-with-wb-models",children:"Use Weave with W&B Models"}),"\n",(0,o.jsxs)(n.p,{children:["This notebook demonstrates how to use W&B Weave with ",(0,o.jsx)(n.a,{href:"https://docs.wandb.ai/guides/",children:"W&B Models"})," using the scenario of two different teams working on an end-to-end implementation of a Retrieval-Augmented Generation (RAG) application, from fine-tuning the model to building an app around the model. Specifically, the Model Team fine-tunes a new Chat Model (Llama 3.2),and saves it to the ",(0,o.jsx)(n.a,{href:"https://docs.wandb.ai/guides/registry/",children:"W&B Models Registry"}),". Then, the App Team retrieves the fine-tuned Chat Model from the Registry, and uses Weave to create and evaluate a RAG chatbot application"]}),"\n",(0,o.jsx)(n.p,{children:"The guide walks you through the following steps, which are the same steps that the teams in the described scenario would follow:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["Downloading a fine-tuned Llama 3.2 model registered in ",(0,o.jsx)(n.a,{href:"https://docs.wandb.ai/guides/registry/",children:"W&B Models Registry"})]}),"\n",(0,o.jsx)(n.li,{children:"Implementing a RAG application using the fine-tuned Llama 3.2 model"}),"\n",(0,o.jsx)(n.li,{children:"Tracking and evaluating the RAG application using Weave"}),"\n",(0,o.jsx)(n.li,{children:"Registering the improved RAG app to Registry"}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["Find the public workspace for both W&B Models and W&B Weave ",(0,o.jsx)(n.a,{href:"https://wandb.ai/wandb-smle/weave-cookboook-demo/weave/evaluations",children:"here"}),"."]}),"\n",(0,o.jsx)("img",{src:"https://github.com/NiWaRe/agent-dev-collection/blob/master/screenshots/weave_models_workflow.jpeg?raw=true",alt:"Weights & Biases"}),"\n",(0,o.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsx)(n.p,{children:"First, install the necessary libraries, set up API keys, log in to W&B, and create a new W&B project."}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["Install ",(0,o.jsx)(n.code,{children:"weave"}),", ",(0,o.jsx)(n.code,{children:"pandas"}),", ",(0,o.jsx)(n.code,{children:"unsloth"}),", ",(0,o.jsx)(n.code,{children:"wandb"}),", ",(0,o.jsx)(n.code,{children:"litellm"}),", ",(0,o.jsx)(n.code,{children:"pydantic"}),", ",(0,o.jsx)(n.code,{children:"torch"}),", and ",(0,o.jsx)(n.code,{children:"faiss-gpu"})," using ",(0,o.jsx)(n.code,{children:"pip"}),"."]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"%%capture\n!pip install weave wandb pandas pydantic litellm faiss-gpu\n"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'%%capture\n!pip install unsloth\n# Also get the latest nightly Unsloth!\n!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"\n'})}),"\n",(0,o.jsxs)(n.ol,{start:"2",children:["\n",(0,o.jsx)(n.li,{children:"Add the necessary API keys from your environment."}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import os\nfrom google.colab import userdata\n\nos.environ[\"WANDB_API_KEY\"] = userdata.get('WANDB_API_KEY')  # W&B Models and Weave\nos.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY') # OpenAI - for retrieval embeddings\nos.environ[\"GEMINI_API_KEY\"] = userdata.get('GEMINI_API_KEY') # Gemini - for the base chat model\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"3",children:["\n",(0,o.jsx)(n.li,{children:"Log in to W&B, and create a new project."}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import wandb\nimport weave\nimport pandas as pd\n\nwandb.login()\n\nPROJECT = "weave-cookboook-demo"\nENTITY = "wandb-smle"\n\nweave.init(ENTITY+"/"+PROJECT)\n'})}),"\n",(0,o.jsxs)(n.h2,{id:"download-chatmodel-from-models-registry-and-implement-unslothlorachatmodel",children:["Download ",(0,o.jsx)(n.code,{children:"ChatModel"})," from Models Registry and implement ",(0,o.jsx)(n.code,{children:"UnslothLoRAChatModel"})]}),"\n",(0,o.jsxs)(n.p,{children:["In our scenario, the Llama-3.2 model has already been fine-tuned by the Model Team using the ",(0,o.jsx)(n.code,{children:"unsloth"})," library for performance optimization, and is ",(0,o.jsx)(n.a,{href:"https://wandb.ai/wandb-smle/weave-cookboook-demo/weave/object-versions?filter=%7B%22objectName%22%3A%22RagModel%22%7D&peekPath=%2Fwandb-smle%2Fweave-rag-experiments%2Fobjects%2FChatModelRag%2Fversions%2F2mhdPb667uoFlXStXtZ0MuYoxPaiAXj3KyLS1kYRi84%3F%26",children:"available in the W&B Models Registry"}),". In this step, we'll retrieve the fine-tuned ",(0,o.jsx)(n.a,{href:"https://wandb.ai/wandb-smle/weave-cookboook-demo/weave/object-versions?filter=%7B%22objectName%22%3A%22RagModel%22%7D&peekPath=%2Fwandb-smle%2Fweave-rag-experiments%2Fobjects%2FChatModelRag%2Fversions%2F2mhdPb667uoFlXStXtZ0MuYoxPaiAXj3KyLS1kYRi84%3F%26",children:(0,o.jsx)(n.code,{children:"ChatModel"})})," from the Registry and convert it into a ",(0,o.jsx)(n.code,{children:"weave.Model"})," to make it compatible with the ",(0,o.jsx)(n.a,{href:"https://wandb.ai/wandb-smle/weave-cookboook-demo/weave/object-versions?filter=%7B%22objectName%22%3A%22RagModel%22%7D&peekPath=%2Fwandb-smle%2Fweave-cookboook-demo%2Fobjects%2FRagModel%2Fversions%2FcqRaGKcxutBWXyM0fCGTR1Yk2mISLsNari4wlGTwERo%3F%26",children:(0,o.jsx)(n.code,{children:"RagModel"})}),"."]}),"\n",(0,o.jsx)(n.admonition,{type:"important",children:(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"RagModel"})," referenced below is a top-level ",(0,o.jsx)(n.code,{children:"weave.Model"})," that can be considered a complete RAG Application. It contains a ",(0,o.jsx)(n.code,{children:"ChatModel"}),", vector database, and a prompt. The ",(0,o.jsx)(n.code,{children:"ChatModel"})," is also a ",(0,o.jsx)(n.code,{children:"weave.Model"}),", which contains code to download an artifact from the W&B Registry. ",(0,o.jsx)(n.code,{children:"ChatModel"})," can be changed modularly to support any kind of other LLM chat model as part of the ",(0,o.jsx)(n.code,{children:"RagModel"}),". For more information, ",(0,o.jsx)(n.a,{href:"https://wandb.ai/wandb-smle/weave-cookboook-demo/weave/evaluations?peekPath=%2Fwandb-smle%2Fweave-cookboook-demo%2Fobjects%2FRagModel%2Fversions%2Fx7MzcgHDrGXYHHDQ9BA8N89qDwcGkdSdpxH30ubm8ZM%3F%26",children:"view the model in Weave"}),"."]})}),"\n",(0,o.jsxs)(n.p,{children:["To load the ",(0,o.jsx)(n.code,{children:"ChatModel"}),", ",(0,o.jsx)(n.code,{children:"unsloth.FastLanguageModel"})," or ",(0,o.jsx)(n.code,{children:"peft.AutoPeftModelForCausalLM"})," with adapters are used, enabling efficient integration into the app. After downloading the model from the Registry, you can set up the initialization and prediction logic by using the ",(0,o.jsx)(n.code,{children:"model_post_init"})," method. The required code for this step is available in the ",(0,o.jsx)(n.strong,{children:"Use"})," tab of the Registry and can be copied directly into your implementation"]}),"\n",(0,o.jsxs)(n.p,{children:["The code below defines the ",(0,o.jsx)(n.code,{children:"UnslothLoRAChatModel"})," class to manage, initialize, and use the fine-tuned Llama-3.2 model retrieved from the W&B Models Registry. ",(0,o.jsx)(n.code,{children:"UnslothLoRAChatModel"})," uses ",(0,o.jsx)(n.code,{children:"unsloth.FastLanguageModel"})," for optimized inference. The ",(0,o.jsx)(n.code,{children:"model_post_init"})," method handles downloading and setting up the model, while the ",(0,o.jsx)(n.code,{children:"predict"})," method processes user queries and generates responses. To adapt the code for your use case, update the ",(0,o.jsx)(n.code,{children:"MODEL_REG_URL"})," with the correct Registry path for your fine-tuned model and adjust parameters like ",(0,o.jsx)(n.code,{children:"max_seq_length"})," or ",(0,o.jsx)(n.code,{children:"dtype"})," based on your hardware or requirements."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import weave\nfrom pydantic import PrivateAttr\nfrom typing import Any, List, Dict, Optional\nfrom unsloth import FastLanguageModel\nimport torch\n\nclass UnslothLoRAChatModel(weave.Model):\n    """\n    We define an extra ChatModel class to be able store and version more parameters than just the model name.\n    Especially, relevant if we consider fine-tuning (locally or aaS) because of specific parameters.\n    """\n    chat_model: str\n    cm_temperature: float\n    cm_max_new_tokens: int\n    cm_quantize: bool\n    inference_batch_size: int\n    dtype: Any\n    device: str\n    _model: Any = PrivateAttr()\n    _tokenizer: Any = PrivateAttr()\n\n    def model_post_init(self, __context):\n      # we can simply paste this from the "Use" tab from the registry\n      run = wandb.init(project=PROJECT, job_type="model_download")\n      artifact = run.use_artifact(f"{self.chat_model}")\n      model_path = artifact.download()\n\n      # unsloth version (enable native 2x faster inference)\n      self._model, self._tokenizer = FastLanguageModel.from_pretrained(\n          model_name = model_path,\n          max_seq_length = self.cm_max_new_tokens,\n          dtype = self.dtype,\n          load_in_4bit = self.cm_quantize,\n      )\n      FastLanguageModel.for_inference(self._model)\n\n    @weave.op()\n    async def predict(self, query: List[str]) -> dict:\n      # add_generation_prompt = true - Must add for generation\n      input_ids = self._tokenizer.apply_chat_template(\n          query, tokenize = True, add_generation_prompt = True, return_tensors = "pt",\n      ).to("cuda")\n\n      output_ids = self._model.generate(\n          input_ids = input_ids, max_new_tokens = 64, use_cache = True, temperature = 1.5, min_p = 0.1\n      )\n\n      decoded_outputs = self._tokenizer.batch_decode(\n          output_ids[0][input_ids.shape[1]:], skip_special_tokens=True\n      )\n\n      return \'\'.join(decoded_outputs).strip()\n'})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'MODEL_REG_URL = "wandb32/wandb-registry-RAG Chat Models/Finetuned Llama-3.2:v3"\n\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None          # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True   # Use 4bit quantization to reduce memory usage. Can be False.\n\nnew_chat_model = UnslothLoRAChatModel(\n    name = "UnslothLoRAChatModelRag",\n    chat_model = MODEL_REG_URL,\n    cm_temperature = 1.0,\n    cm_max_new_tokens = max_seq_length,\n    cm_quantize = load_in_4bit,\n    inference_batch_size = max_seq_length,\n    dtype = dtype,\n    device = "auto",\n)\n'})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'await new_chat_model.predict([{"role": "user", "content": "What is the capital of Germany?"}])\n'})}),"\n",(0,o.jsxs)(n.h2,{id:"integrate-the-new-chatmodel-version-into-ragmodel",children:["Integrate the new ",(0,o.jsx)(n.code,{children:"ChatModel"})," version into ",(0,o.jsx)(n.code,{children:"RagModel"})]}),"\n",(0,o.jsxs)(n.p,{children:["Building a RAG application from a fine-tuned chat model improves conversational AI by using tailored components without having to rebuild the entire pipeline. In this step, we retrieve the existing ",(0,o.jsx)(n.code,{children:"RagModel"})," from our Weave project and update its ",(0,o.jsx)(n.code,{children:"ChatModel"})," to use the newly fine-tuned model. This seamless swap means that other components like the vector database (VDB) and prompts remain untouched, preserving the application's overall structure while improving performance."]}),"\n",(0,o.jsxs)(n.p,{children:["The code below retrieves the ",(0,o.jsx)(n.code,{children:"RagModel"})," object using a reference from the Weave project. The ",(0,o.jsx)(n.code,{children:"chat_model"})," attribute of the ",(0,o.jsx)(n.code,{children:"RagModel"})," is then updated to use the new ",(0,o.jsx)(n.code,{children:"UnslothLoRAChatModel"})," instance created in the previous step. After this, the updated ",(0,o.jsx)(n.code,{children:"RagModel"})," is published to create a new version. Finally, the updated ",(0,o.jsx)(n.code,{children:"RagModel"})," is used to run a sample prediction query, verifying that the new chat model is being used."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'RagModel = weave.ref("weave:///wandb-smle/weave-cookboook-demo/object/RagModel:cqRaGKcxutBWXyM0fCGTR1Yk2mISLsNari4wlGTwERo").get()\n'})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"RagModel.chat_model.chat_model\n"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'await RagModel.predict("When was the first conference on climate change?")\n'})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# MAGIC: exchange chat_model and publish new version (no need to worry about other RAG components)\nRagModel.chat_model = new_chat_model\n"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"RagModel.chat_model.chat_model\n"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# first publish new version so that in prediction we reference new version\nPUB_REFERENCE = weave.publish(RagModel, "RagModel")\n'})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'await RagModel.predict("When was the first conference on climate change?")\n'})}),"\n",(0,o.jsxs)(n.h2,{id:"run-a-weaveevaluation",children:["Run a ",(0,o.jsx)(n.code,{children:"weave.Evaluation"})]}),"\n",(0,o.jsxs)(n.p,{children:["In the next step, we evaluate the performance of our updated ",(0,o.jsx)(n.code,{children:"RagModel"})," using an existing ",(0,o.jsx)(n.code,{children:"weave.Evaluation"}),". This process ensures that the new fine-tuned chat model is performing as expected within the RAG application. To streamline integration and enable collaboration between the Models and Apps teams, we log evaluation results for both the model's W&B run and as part of the Weave workspace."]}),"\n",(0,o.jsx)(n.p,{children:"In Models:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["The evaluation summary is logged to the W&B run used to download the fine-tuned chat model. This includes summary metrics and graphs displayed in a ",(0,o.jsx)(n.a,{href:"https://wandb.ai/wandb-smle/weave-cookboook-demo/workspace?nw=eglm8z7o9",children:"workspace view"})," for analysis."]}),"\n",(0,o.jsx)(n.li,{children:"The evaluation trace ID is added to the run's configuration, linking directly to the Weave page for easier traceability by the Model Team."}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"In Weave:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["The artifact or registry link for the ",(0,o.jsx)(n.code,{children:"ChatModel"})," is stored as an input to the ",(0,o.jsx)(n.code,{children:"RagModel"}),"."]}),"\n",(0,o.jsx)(n.li,{children:"The W&B run ID is saved as an extra column in the evaluation traces for better context."}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["The code below demonstrates how to retrieve an evaluation object, execute the evaluation using the updated ",(0,o.jsx)(n.code,{children:"RagModel"}),", and log the results to both W&B and Weave. Ensure that the evaluation reference (",(0,o.jsx)(n.code,{children:"WEAVE_EVAL"}),") matches your project setup."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# MAGIC: we can simply get an evaluation with a eval dataset and scorers and use them\nWEAVE_EVAL = "weave:///wandb-smle/weave-cookboook-demo/object/climate_rag_eval:ntRX6qn3Tx6w3UEVZXdhIh1BWGh7uXcQpOQnIuvnSgo"\nclimate_rag_eval = weave.ref(WEAVE_EVAL).get()\n'})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"with weave.attributes({'wandb-run-id': wandb.run.id}):\n  # use .call attribute to retrieve both the result and the call in order to save eval trace to Models\n  summary, call = await climate_rag_eval.evaluate.call(climate_rag_eval, RagModel)\n"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# log to models\nwandb.run.log(pd.json_normalize(summary, sep=\'/\').to_dict(orient="records")[0])\nwandb.run.config.update({"weave_url": f"https://wandb.ai/wandb-smle/weave-cookboook-demo/r/call/{call.id}"})\nwandb.run.finish()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"save-the-new-rag-model-to-the-registry",children:"Save the new RAG Model to the Registry"}),"\n",(0,o.jsxs)(n.p,{children:["To make the updated ",(0,o.jsx)(n.code,{children:"RagModel"})," available for future use by both the Models and Apps teams, we push it to the W&B Models Registry as a reference artifact."]}),"\n",(0,o.jsxs)(n.p,{children:["The code below retrieves the ",(0,o.jsx)(n.code,{children:"weave"})," object version and name for the updated ",(0,o.jsx)(n.code,{children:"RagModel"})," and uses them to create reference links. A new artifact is then created in W&B with metadata containing the model's Weave URL. This artifact is logged to the W&B Registry and linked to a designated registry path."]}),"\n",(0,o.jsxs)(n.p,{children:["Before running the code, ensure the ",(0,o.jsx)(n.code,{children:"ENTITY"})," and ",(0,o.jsx)(n.code,{children:"PROJECT"})," variables match your W&B setup, and the target registry path is correctly specified. This process finalizes the workflow by publishing the new ",(0,o.jsx)(n.code,{children:"RagModel"})," to the W&B ecosystem for easy collaboration and reuse."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"MODELS_OBJECT_VERSION = PUB_REFERENCE.digest # weave object version\nMODELS_OBJECT_NAME = PUB_REFERENCE.name # weave object name\n"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'models_url = f"https://wandb.ai/{ENTITY}/{PROJECT}/weave/objects/{MODELS_OBJECT_NAME}/versions/{MODELS_OBJECT_VERSION}"\nmodels_link = f"weave:///{ENTITY}/{PROJECT}/object/{MODELS_OBJECT_NAME}:{MODELS_OBJECT_VERSION}"\n\nwith wandb.init(project=PROJECT, entity=ENTITY) as run:\n  # create new Artifact\n  artifact_model = wandb.Artifact(\n      name = "RagModel", type = "model", description="Models Link from RagModel in Weave", metadata={\'url\':models_url}\n  )\n  artifact_model.add_reference(models_link, name = "model", checksum = False)\n\n  # log new artifact\n  run.log_artifact(artifact_model, aliases=[MODELS_OBJECT_VERSION])\n\n  # link to registry\n  run.link_artifact(\n    artifact_model,\n    target_path="wandb32/wandb-registry-RAG Models/RAG Model"\n  )\n'})})]})}function h(e={}){const{wrapper:n}={...(0,a.a)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},11151:(e,n,t)=>{t.d(n,{Z:()=>d,a:()=>s});var o=t(67294);const a={},i=o.createContext(a);function s(e){const n=o.useContext(i);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);