"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[4305],{99608:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>s,contentTitle:()=>o,default:()=>p,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var t=i(85893),a=i(11151);const r={},o="Verdict",l={id:"guides/integrations/verdict",title:"Verdict",description:"Weave is designed to make tracking and logging all calls made through the Verdict Python library effortless.",source:"@site/docs/guides/integrations/verdict.md",sourceDirName:"guides/integrations",slug:"/guides/integrations/verdict",permalink:"/guides/integrations/verdict",draft:!1,unlisted:!1,editUrl:"https://github.com/wandb/weave/blob/master/docs/docs/guides/integrations/verdict.md",tags:[],version:"current",lastUpdatedAt:1755191267e3,frontMatter:{},sidebar:"documentationSidebar",previous:{title:"AutoGen",permalink:"/guides/integrations/autogen"},next:{title:"TypeScript SDK: Third-Party Integration Guide",permalink:"/guides/integrations/js"}},s={},c=[{value:"Getting Started",id:"getting-started",level:2},{value:"Tracking Call Metadata",id:"tracking-call-metadata",level:2},{value:"Traces",id:"traces",level:2},{value:"Pipeline Tracing Example",id:"pipeline-tracing-example",level:2},{value:"Configuration",id:"configuration",level:2},{value:"Custom Tracers and Weave",id:"custom-tracers-and-weave",level:2},{value:"Models and Evaluations",id:"models-and-evaluations",level:2},{value:"Evaluations",id:"evaluations",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Performance Monitoring",id:"performance-monitoring",level:3},{value:"Error Handling",id:"error-handling",level:3}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,a.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"verdict",children:"Verdict"}),"\n",(0,t.jsx)("a",{target:"_blank",href:"https://github.com/wandb/examples/blob/master/weave/docs/quickstart_verdict.ipynb",children:(0,t.jsx)("img",{src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open In Colab"})}),"\n",(0,t.jsxs)(n.p,{children:["Weave is designed to make tracking and logging all calls made through the ",(0,t.jsx)(n.a,{href:"https://verdict.haizelabs.com/docs/",children:"Verdict Python library"})," effortless."]}),"\n",(0,t.jsx)(n.p,{children:"When working with AI evaluation pipelines, debugging is crucial. Whether a pipeline step fails, outputs are unexpected, or nested operations create confusion, pinpointing issues can be challenging. Verdict applications often consist of multiple pipeline steps, judges, and transformations, making it essential to understand the inner workings of your evaluation workflows."}),"\n",(0,t.jsxs)(n.p,{children:["Weave simplifies this process by automatically capturing traces for your ",(0,t.jsx)(n.a,{href:"https://verdict.readthedocs.io/",children:"Verdict"})," applications. This enables you to monitor and analyze your pipeline's performance, making it easier to debug and optimize your AI evaluation workflows."]}),"\n",(0,t.jsx)(n.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,t.jsxs)(n.p,{children:["To get started, simply call ",(0,t.jsx)(n.code,{children:"weave.init(project=...)"})," at the beginning of your script. Use the ",(0,t.jsx)(n.code,{children:"project"})," argument to log to a specific W&B Team name with ",(0,t.jsx)(n.code,{children:"team-name/project-name"})," or do ",(0,t.jsx)(n.code,{children:"project-name"})," to log to your default team/entity."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import weave\nfrom verdict import Pipeline\nfrom verdict.common.judge import JudgeUnit\nfrom verdict.schema import Schema\n\n# Initialize Weave with your project name\n# highlight-next-line\nweave.init("verdict_demo")\n\n# Create a simple evaluation pipeline\npipeline = Pipeline()\npipeline = pipeline >> JudgeUnit().prompt("Rate the quality of this text: {source.text}")\n\n# Create sample data\ndata = Schema.of(text="This is a sample text for evaluation.")\n\n# Run the pipeline - this will be automatically traced\noutput = pipeline.run(data)\n\nprint(output)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"tracking-call-metadata",children:"Tracking Call Metadata"}),"\n",(0,t.jsxs)(n.p,{children:["To track metadata from your Verdict pipeline calls, you can use the ",(0,t.jsx)(n.a,{href:"https://weave-docs.wandb.ai/reference/python-sdk/weave/#function-attributes",children:(0,t.jsx)(n.code,{children:"weave.attributes"})})," context manager. This context manager allows you to set custom metadata for a specific block of code, such as a pipeline run or evaluation batch."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import weave\nfrom verdict import Pipeline\nfrom verdict.common.judge import JudgeUnit\nfrom verdict.schema import Schema\n\n# Initialize Weave with your project name\n# highlight-next-line\nweave.init("verdict_demo")\n\npipeline = Pipeline()\npipeline = pipeline >> JudgeUnit().prompt("Evaluate sentiment: {source.text}")\n\ndata = Schema.of(text="I love this product!")\n\n# highlight-next-line\nwith weave.attributes({"evaluation_type": "sentiment", "batch_id": "batch_001"}):\n    output = pipeline.run(data)\n\nprint(output)\n'})}),"\n",(0,t.jsx)(n.p,{children:"Weave automatically tracks the metadata against the trace of the Verdict pipeline call. You can view the metadata in the Weave web interface."}),"\n",(0,t.jsx)(n.h2,{id:"traces",children:"Traces"}),"\n",(0,t.jsx)(n.p,{children:"Storing traces of AI evaluation pipelines in a central database is crucial during both development and production. These traces are essential for debugging and improving your evaluation workflows by providing a valuable dataset."}),"\n",(0,t.jsx)(n.p,{children:"Weave automatically captures traces for your Verdict applications. It will track and log all calls made through the Verdict library, including:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Pipeline execution steps"}),"\n",(0,t.jsx)(n.li,{children:"Judge unit evaluations"}),"\n",(0,t.jsx)(n.li,{children:"Layer transformations"}),"\n",(0,t.jsx)(n.li,{children:"Pooling operations"}),"\n",(0,t.jsx)(n.li,{children:"Custom units and transformations"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"You can view the traces in the Weave web interface, showing the hierarchical structure of your pipeline execution."}),"\n",(0,t.jsx)(n.h2,{id:"pipeline-tracing-example",children:"Pipeline Tracing Example"}),"\n",(0,t.jsx)(n.p,{children:"Here's a more complex example showing how Weave traces nested pipeline operations:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import weave\nfrom verdict import Pipeline, Layer\nfrom verdict.common.judge import JudgeUnit\nfrom verdict.transform import MeanPoolUnit\nfrom verdict.schema import Schema\n\n# Initialize Weave with your project name\n# highlight-next-line\nweave.init("verdict_demo")\n\n# Create a complex pipeline with multiple steps\npipeline = Pipeline()\npipeline = pipeline >> Layer([\n    JudgeUnit().prompt("Rate coherence: {source.text}"),\n    JudgeUnit().prompt("Rate relevance: {source.text}"),\n    JudgeUnit().prompt("Rate accuracy: {source.text}")\n], 3)\npipeline = pipeline >> MeanPoolUnit()\n\n# Sample data\ndata = Schema.of(text="This is a comprehensive evaluation of text quality across multiple dimensions.")\n\n# Run the pipeline - all operations will be traced\nresult = pipeline.run(data)\n\nprint(f"Average score: {result}")\n'})}),"\n",(0,t.jsx)(n.p,{children:"This will create a detailed trace showing:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The main Pipeline execution"}),"\n",(0,t.jsx)(n.li,{children:"Each JudgeUnit evaluation within the Layer"}),"\n",(0,t.jsx)(n.li,{children:"The MeanPoolUnit aggregation step"}),"\n",(0,t.jsx)(n.li,{children:"Timing information for each operation"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"configuration",children:"Configuration"}),"\n",(0,t.jsxs)(n.p,{children:["Upon calling ",(0,t.jsx)(n.code,{children:"weave.init()"}),", tracing is automatically enabled for Verdict pipelines. The integration works by patching the ",(0,t.jsx)(n.code,{children:"Pipeline.__init__"})," method to inject a ",(0,t.jsx)(n.code,{children:"VerdictTracer"})," that forwards all trace data to Weave."]}),"\n",(0,t.jsx)(n.p,{children:"No additional configuration is needed - Weave will automatically:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Capture all pipeline operations"}),"\n",(0,t.jsx)(n.li,{children:"Track execution timing"}),"\n",(0,t.jsx)(n.li,{children:"Log inputs and outputs"}),"\n",(0,t.jsx)(n.li,{children:"Maintain trace hierarchy"}),"\n",(0,t.jsx)(n.li,{children:"Handle concurrent pipeline execution"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"custom-tracers-and-weave",children:"Custom Tracers and Weave"}),"\n",(0,t.jsxs)(n.p,{children:["If you're using custom Verdict tracers in your application, Weave's ",(0,t.jsx)(n.code,{children:"VerdictTracer"})," can work alongside them:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import weave\nfrom verdict import Pipeline\nfrom verdict.common.judge import JudgeUnit\nfrom verdict.util.tracing import ConsoleTracer\nfrom verdict.schema import Schema\n\n# Initialize Weave with your project name\n# highlight-next-line\nweave.init("verdict_demo")\n\n# You can still use Verdict\'s built-in tracers\nconsole_tracer = ConsoleTracer()\n\n# Create pipeline with both Weave (automatic) and Console tracing\npipeline = Pipeline(tracer=[console_tracer])  # Weave tracer is added automatically\npipeline = pipeline >> JudgeUnit().prompt("Evaluate: {source.text}")\n\ndata = Schema.of(text="Sample evaluation text")\n\n# This will trace to both Weave and console\nresult = pipeline.run(data)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"models-and-evaluations",children:"Models and Evaluations"}),"\n",(0,t.jsxs)(n.p,{children:["Organizing and evaluating AI systems with multiple pipeline components can be challenging. Using the ",(0,t.jsx)(n.a,{href:"/guides/core-types/models",children:(0,t.jsx)(n.code,{children:"weave.Model"})}),", you can capture and organize experimental details like prompts, pipeline configurations, and evaluation parameters, making it easier to compare different iterations."]}),"\n",(0,t.jsxs)(n.p,{children:["The following example demonstrates wrapping a Verdict pipeline in a ",(0,t.jsx)(n.code,{children:"WeaveModel"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import asyncio\nimport weave\nfrom verdict import Pipeline\nfrom verdict.common.judge import JudgeUnit\nfrom verdict.schema import Schema\n\n# Initialize Weave with your project name\n# highlight-next-line\nweave.init("verdict_demo")\n\n# highlight-next-line\nclass TextQualityEvaluator(weave.Model):\n    judge_prompt: str\n    pipeline_name: str\n\n# highlight-next-line\n    @weave.op()\n    async def predict(self, text: str) -> dict:\n        pipeline = Pipeline(name=self.pipeline_name)\n        pipeline = pipeline >> JudgeUnit().prompt(self.judge_prompt)\n        \n        data = Schema.of(text=text)\n        result = pipeline.run(data)\n        \n        return {\n            "text": text,\n            "quality_score": result.score if hasattr(result, \'score\') else result,\n            "evaluation_prompt": self.judge_prompt\n        }\n\nmodel = TextQualityEvaluator(\n    judge_prompt="Rate the quality of this text on a scale of 1-10: {source.text}",\n    pipeline_name="text_quality_evaluator"\n)\n\ntext = "This is a well-written and informative piece of content that provides clear value to readers."\n\nprediction = asyncio.run(model.predict(text))\n\n# if you\'re in a Jupyter Notebook, run:\n# prediction = await model.predict(text)\n\nprint(prediction)\n'})}),"\n",(0,t.jsx)(n.p,{children:"This code creates a model that can be visualized in the Weave UI, showing both the pipeline structure and the evaluation results."}),"\n",(0,t.jsx)(n.h3,{id:"evaluations",children:"Evaluations"}),"\n",(0,t.jsxs)(n.p,{children:["Evaluations help you measure the performance of your evaluation pipelines themselves. By using the ",(0,t.jsx)(n.a,{href:"/guides/core-types/evaluations",children:(0,t.jsx)(n.code,{children:"weave.Evaluation"})})," class, you can capture how well your Verdict pipelines perform on specific tasks or datasets:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import asyncio\nimport weave\nfrom verdict import Pipeline\nfrom verdict.common.judge import JudgeUnit\nfrom verdict.schema import Schema\n\n# Initialize Weave\n# highlight-next-line\nweave.init("verdict_demo")\n\n# Create evaluation model\nclass SentimentEvaluator(weave.Model):\n    @weave.op()\n    async def predict(self, text: str) -> dict:\n        pipeline = Pipeline()\n        pipeline = pipeline >> JudgeUnit().prompt(\n            "Classify sentiment as positive, negative, or neutral: {source.text}"\n        )\n        \n        data = Schema.of(text=text)\n        result = pipeline.run(data)\n        \n        return {"sentiment": result}\n\n# Test data\ntexts = [\n    "I love this product, it\'s amazing!",\n    "This is terrible, worst purchase ever.",\n    "The weather is okay today."\n]\nlabels = ["positive", "negative", "neutral"]\n\nexamples = [\n    {"id": str(i), "text": texts[i], "target": labels[i]}\n    for i in range(len(texts))\n]\n\n# Scoring function\n@weave.op()\ndef sentiment_accuracy(target: str, output: dict) -> dict:\n    predicted = output.get("sentiment", "").lower()\n    return {"correct": target.lower() in predicted}\n\nmodel = SentimentEvaluator()\n\nevaluation = weave.Evaluation(\n    dataset=examples,\n    scorers=[sentiment_accuracy],\n)\n\nscores = asyncio.run(evaluation.evaluate(model))\n# if you\'re in a Jupyter Notebook, run:\n# scores = await evaluation.evaluate(model)\n\nprint(scores)\n'})}),"\n",(0,t.jsx)(n.p,{children:"This creates an evaluation trace that shows how your Verdict pipeline performs across different test cases."}),"\n",(0,t.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,t.jsx)(n.h3,{id:"performance-monitoring",children:"Performance Monitoring"}),"\n",(0,t.jsx)(n.p,{children:"Weave automatically captures timing information for all pipeline operations. You can use this to identify performance bottlenecks:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import weave\nfrom verdict import Pipeline, Layer\nfrom verdict.common.judge import JudgeUnit\nfrom verdict.schema import Schema\n\n# highlight-next-line\nweave.init("verdict_demo")\n\n# Create a pipeline that might have performance variations\npipeline = Pipeline()\npipeline = pipeline >> Layer([\n    JudgeUnit().prompt("Quick evaluation: {source.text}"),\n    JudgeUnit().prompt("Detailed analysis: {source.text}"),  # This might be slower\n], 2)\n\ndata = Schema.of(text="Sample text for performance testing")\n\n# Run multiple times to see timing patterns\nfor i in range(3):\n    with weave.attributes({"run_number": i}):\n        result = pipeline.run(data)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"error-handling",children:"Error Handling"}),"\n",(0,t.jsx)(n.p,{children:"Weave automatically captures exceptions that occur during pipeline execution:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import weave\nfrom verdict import Pipeline\nfrom verdict.common.judge import JudgeUnit\nfrom verdict.schema import Schema\n\n# highlight-next-line\nweave.init("verdict_demo")\n\npipeline = Pipeline()\npipeline = pipeline >> JudgeUnit().prompt("Process: {source.invalid_field}")  # This will cause an error\n\ndata = Schema.of(text="Sample text")\n\ntry:\n    result = pipeline.run(data)\nexcept Exception as e:\n    print(f"Pipeline failed: {e}")\n    # Error details are captured in Weave trace\n'})}),"\n",(0,t.jsx)(n.p,{children:"By integrating Weave with Verdict, you get comprehensive observability into your AI evaluation pipelines, making it easier to debug, optimize, and understand your evaluation workflows."})]})}function p(e={}){const{wrapper:n}={...(0,a.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},11151:(e,n,i)=>{i.d(n,{Z:()=>l,a:()=>o});var t=i(67294);const a={},r=t.createContext(a);function o(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);