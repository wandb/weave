"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[5409],{33106:(e,n,t)=>{t.d(n,{ZP:()=>c,d$:()=>i});var o=t(85893);const s="[User Settings](https://docs.wandb.ai/guides/models/app/settings-page/user-settings/#default-team)",r="To find or update your default entity, refer to";const a=function({variant:e="full"}){return"inline"===e?(0,o.jsxs)(o.Fragment,{children:["If not specified, your default entity is used. ",r," ",s," in the W&B Models documentation."]}):(0,o.jsxs)(o.Fragment,{children:["If you don't specify a W&B team when you call `weave.init()`, your default entity is used. ",r," ",s," in the W&B Models documentation."]})},i=[];function d(e){return(0,o.jsx)(o.Fragment,{})}function c(e={}){return(0,o.jsx)(a,{...e,children:(0,o.jsx)(d,{...e})})}},73185:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>m,frontMatter:()=>a,metadata:()=>d,toc:()=>l});var o=t(85893),s=t(11151),r=t(33106);const a={},i="Amazon Bedrock",d={id:"guides/integrations/bedrock",title:"Amazon Bedrock",description:"Weave automatically tracks and logs LLM calls made via Amazon Bedrock, AWS's fully managed service that offers foundation models from leading AI companies through a unified API.",source:"@site/docs/guides/integrations/bedrock.md",sourceDirName:"guides/integrations",slug:"/guides/integrations/bedrock",permalink:"/guides/integrations/bedrock",draft:!1,unlisted:!1,editUrl:"https://github.com/wandb/weave/blob/master/docs/docs/guides/integrations/bedrock.md",tags:[],version:"current",lastUpdatedAt:1759013133e3,frontMatter:{},sidebar:"documentationSidebar",previous:{title:"Integrations",permalink:"/guides/integrations/"},next:{title:"Anthropic",permalink:"/guides/integrations/anthropic"}},c={},l=[{value:"Traces",id:"traces",level:2},...r.d$,{value:"Wrapping with your own ops",id:"wrapping-with-your-own-ops",level:2},{value:"Create a <code>Model</code> for easier experimentation",id:"create-a-model-for-easier-experimentation",level:2},{value:"Learn more",id:"learn-more",level:2},{value:"Try Bedrock in the Weave Playground",id:"try-bedrock-in-the-weave-playground",level:3},{value:"Report: Compare LLMs on Bedrock for text summarization with Weave",id:"report-compare-llms-on-bedrock-for-text-summarization-with-weave",level:3}];function p(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",img:"img",p:"p",pre:"pre",...(0,s.a)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"amazon-bedrock",children:"Amazon Bedrock"}),"\n","\n",(0,o.jsx)(n.p,{children:"Weave automatically tracks and logs LLM calls made via Amazon Bedrock, AWS's fully managed service that offers foundation models from leading AI companies through a unified API."}),"\n",(0,o.jsxs)(n.p,{children:["There are multiple ways to log LLM calls to Weave from Amazon Bedrock. You can use ",(0,o.jsx)(n.code,{children:"weave.op"})," to create reusable operations for tracking any calls to a Bedrock model. Optionally, if you're using Anthropic models, you can use Weave\u2019s built-in integration with Anthropic."]}),"\n",(0,o.jsx)(n.admonition,{type:"tip",children:(0,o.jsxs)(n.p,{children:["For the latest tutorials, visit ",(0,o.jsx)(n.a,{href:"https://wandb.ai/site/partners/aws/",children:"Weights & Biases on Amazon Web Services"}),"."]})}),"\n",(0,o.jsx)(n.h2,{id:"traces",children:"Traces"}),"\n",(0,o.jsx)(n.p,{children:"Weave will automatically capture traces for Bedrock API calls after you initialize Weave and patch the client."}),"\n",(0,o.jsx)(n.p,{children:"To use the Bedrock API:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import weave\nimport boto3\nimport json\nfrom weave.integrations.bedrock.bedrock_sdk import patch_client\n\nweave.init("my_bedrock_app")\n\n# Create and patch the Bedrock client\nclient = boto3.client("bedrock-runtime")\npatch_client(client)\n\n# Use the client as usual\nresponse = client.invoke_model(\n    modelId="anthropic.claude-3-5-sonnet-20240620-v1:0",\n    body=json.dumps({\n        "anthropic_version": "bedrock-2023-05-31",\n        "max_tokens": 100,\n        "messages": [\n            {"role": "user", "content": "What is the capital of France?"}\n        ]\n    }),\n    contentType=\'application/json\',\n    accept=\'application/json\'\n)\nresponse_dict = json.loads(response.get(\'body\').read())\nprint(response_dict["content"][0]["text"])\n'})}),"\n",(0,o.jsx)(r.ZP,{}),"\n",(0,o.jsxs)(n.p,{children:["To use the ",(0,o.jsx)(n.code,{children:"converse"})," API:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'messages = [{"role": "user", "content": [{"text": "What is the capital of France?"}]}]\n\nresponse = client.converse(\n    modelId="anthropic.claude-3-5-sonnet-20240620-v1:0",\n    system=[{"text": "You are a helpful AI assistant."}],\n    messages=messages,\n    inferenceConfig={"maxTokens": 100},\n)\nprint(response["output"]["message"]["content"][0]["text"])\n\n'})}),"\n",(0,o.jsx)(n.h2,{id:"wrapping-with-your-own-ops",children:"Wrapping with your own ops"}),"\n",(0,o.jsxs)(n.p,{children:["You can create reusable operations using the ",(0,o.jsx)(n.code,{children:"@weave.op()"})," decorator. Here's an example showing both the ",(0,o.jsx)(n.code,{children:"invoke_model"})," and ",(0,o.jsx)(n.code,{children:"converse"})," APIs:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'@weave.op\ndef call_model_invoke(\n    model_id: str,\n    prompt: str,\n    max_tokens: int = 100,\n    temperature: float = 0.7\n) -> dict:\n    body = json.dumps({\n        "anthropic_version": "bedrock-2023-05-31",\n        "max_tokens": max_tokens,\n        "temperature": temperature,\n        "messages": [\n            {"role": "user", "content": prompt}\n        ]\n    })\n\n    response = client.invoke_model(\n        modelId=model_id,\n        body=body,\n        contentType=\'application/json\',\n        accept=\'application/json\'\n    )\n    return json.loads(response.get(\'body\').read())\n\n@weave.op\ndef call_model_converse(\n    model_id: str,\n    messages: str,\n    system_message: str,\n    max_tokens: int = 100,\n) -> dict:\n    response = client.converse(\n        modelId=model_id,\n        system=[{"text": system_message}],\n        messages=messages,\n        inferenceConfig={"maxTokens": max_tokens},\n    )\n    return response\n'})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{src:t(75754).Z+"",width:"3394",height:"1520"})}),"\n",(0,o.jsxs)(n.h2,{id:"create-a-model-for-easier-experimentation",children:["Create a ",(0,o.jsx)(n.code,{children:"Model"})," for easier experimentation"]}),"\n",(0,o.jsxs)(n.p,{children:["You can create a Weave Model to better organize your experiments and capture parameters. Here's an example using the ",(0,o.jsx)(n.code,{children:"converse"})," API:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class BedrockLLM(weave.Model):\n    model_id: str\n    max_tokens: int = 100\n    system_message: str = "You are a helpful AI assistant."\n\n    @weave.op\n    def predict(self, prompt: str) -> str:\n        "Generate a response using Bedrock\'s converse API"\n        \n        messages = [{\n            "role": "user",\n            "content": [{"text": prompt}]\n        }]\n\n        response = client.converse(\n            modelId=self.model_id,\n            system=[{"text": self.system_message}],\n            messages=messages,\n            inferenceConfig={"maxTokens": self.max_tokens},\n        )\n        return response["output"]["message"]["content"][0]["text"]\n\n# Create and use the model\nmodel = BedrockLLM(\n    model_id="anthropic.claude-3-5-sonnet-20240620-v1:0",\n    max_tokens=100,\n    system_message="You are an expert software engineer that knows a lot of programming. You prefer short answers."\n)\nresult = model.predict("What is the best way to handle errors in Python?")\nprint(result)\n'})}),"\n",(0,o.jsx)(n.p,{children:"This approach allows you to version your experiments and easily track different configurations of your Bedrock-based application."}),"\n",(0,o.jsx)(n.h2,{id:"learn-more",children:"Learn more"}),"\n",(0,o.jsx)(n.p,{children:"Learn more about using Amazon Bedrock with Weave"}),"\n",(0,o.jsx)(n.h3,{id:"try-bedrock-in-the-weave-playground",children:"Try Bedrock in the Weave Playground"}),"\n",(0,o.jsxs)(n.p,{children:["Do you want to experiment with Amazon Bedrock models in the Weave UI without any set up? Try the ",(0,o.jsx)(n.a,{href:"/guides/tools/playground",children:"LLM Playground"}),"."]}),"\n",(0,o.jsx)(n.h3,{id:"report-compare-llms-on-bedrock-for-text-summarization-with-weave",children:"Report: Compare LLMs on Bedrock for text summarization with Weave"}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.a,{href:"https://wandb.ai/byyoung3/ML_NEWS3/reports/Compare-LLMs-on-Amazon-Bedrock-for-text-summarization-with-W-B-Weave--VmlldzoxMDI1MTIzNw",children:"Compare LLMs on Bedrock for text summarization with Weave"})," report explains how to use Bedrock in combination with Weave to evaluate and compare LLMs for summarization tasks, code samples included."]})]})}function m(e={}){const{wrapper:n}={...(0,s.a)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(p,{...e})}):p(e)}},75754:(e,n,t)=>{t.d(n,{Z:()=>o});const o=t.p+"assets/images/bedrock_converse-6f57502f8cc170b7857f9a0863574697.png"},11151:(e,n,t)=>{t.d(n,{Z:()=>i,a:()=>a});var o=t(67294);const s={},r=o.createContext(s);function a(e){const n=o.useContext(r);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);