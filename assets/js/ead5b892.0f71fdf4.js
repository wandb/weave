"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[4267],{19336:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>m,frontMatter:()=>s,metadata:()=>r,toc:()=>l});var a=t(85893),o=t(11151);const s={},i="AutoGen",r={id:"guides/integrations/autogen",title:"AutoGen",description:"AutoGen is a framework from Microsoft for building AI agents and applications. It simplifies the creation of complex multi-agent systems, offering components for conversational AI (AgentChat), core multi-agent functionalities (Core), and integrations with external services (Extensions). AutoGen also provides a Studio for no-code agent prototyping. For more details, visit the official AutoGen documentation.",source:"@site/docs/guides/integrations/autogen.md",sourceDirName:"guides/integrations",slug:"/guides/integrations/autogen",permalink:"/guides/integrations/autogen",draft:!1,unlisted:!1,editUrl:"https://github.com/wandb/weave/blob/master/docs/docs/guides/integrations/autogen.md",tags:[],version:"current",lastUpdatedAt:175768229e4,frontMatter:{},sidebar:"documentationSidebar",previous:{title:"Agno",permalink:"/guides/integrations/agno"},next:{title:"Verdict",permalink:"/guides/integrations/verdict"}},c={},l=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Basic Setup",id:"basic-setup",level:2},{value:"Tracing a Simple Model Client",id:"tracing-a-simple-model-client",level:2},{value:"Tracing a client create call",id:"tracing-a-client-create-call",level:3},{value:"Tracing a client create call with streaming",id:"tracing-a-client-create-call-with-streaming",level:3},{value:"Weave records cached calls",id:"weave-records-cached-calls",level:3},{value:"Tracing an Agent with Tool Calls",id:"tracing-an-agent-with-tool-calls",level:2},{value:"Tracing a GroupChat - RoundRobin",id:"tracing-a-groupchat---roundrobin",level:2},{value:"Tracing Memory",id:"tracing-memory",level:2},{value:"Tracing RAG Workflows",id:"tracing-rag-workflows",level:2},{value:"Tracing Agent Runtimes",id:"tracing-agent-runtimes",level:2},{value:"Tracing Workflows (Sequential)",id:"tracing-workflows-sequential",level:2},{value:"Tracing Code Executor",id:"tracing-code-executor",level:2},{value:"Learn More",id:"learn-more",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"autogen",children:"AutoGen"}),"\n",(0,a.jsxs)(n.p,{children:["AutoGen is a framework from Microsoft for building AI agents and applications. It simplifies the creation of complex multi-agent systems, offering components for conversational AI (",(0,a.jsx)(n.code,{children:"AgentChat"}),"), core multi-agent functionalities (",(0,a.jsx)(n.code,{children:"Core"}),"), and integrations with external services (",(0,a.jsx)(n.code,{children:"Extensions"}),"). AutoGen also provides a ",(0,a.jsx)(n.code,{children:"Studio"})," for no-code agent prototyping. For more details, visit the ",(0,a.jsx)(n.a,{href:"https://microsoft.github.io/autogen/stable//index.html",children:"official AutoGen documentation"}),"."]}),"\n",(0,a.jsx)(n.admonition,{type:"note",children:(0,a.jsxs)(n.p,{children:["This guide assumes you have a basic understanding of ",(0,a.jsx)(n.a,{href:"https://microsoft.github.io/autogen/stable/index.html",children:"AutoGen"}),"."]})}),"\n",(0,a.jsxs)(n.p,{children:["Weave integrates with ",(0,a.jsx)(n.a,{href:"https://microsoft.github.io/autogen/stable/index.html",children:"AutoGen"})," to help you trace and visualize the execution of your multi-agent applications. By simply initializing Weave, you can automatically track interactions within ",(0,a.jsx)(n.code,{children:"autogen_agentchat"}),", ",(0,a.jsx)(n.code,{children:"autogen_core"}),", and ",(0,a.jsx)(n.code,{children:"autogen_ext"}),". This guide will walk you through various examples of how to use Weave with AutoGen."]}),"\n",(0,a.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsx)(n.p,{children:"Before you begin, ensure you have AutoGen and Weave installed. You'll also need any SDKs for the LLM providers you intend to use (e.g., OpenAI, Anthropic)."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'pip install autogen_agentchat "autogen_ext[openai,anthropic]" weave \n'})}),"\n",(0,a.jsx)(n.p,{children:"Set up your API keys as environment variables:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import os\n\nos.environ["OPENAI_API_KEY"] = "<your-openai-api-key>"\nos.environ["ANTHROPIC_API_KEY"] = "<your-anthropic-api-key>"\n'})}),"\n",(0,a.jsx)(n.h2,{id:"basic-setup",children:"Basic Setup"}),"\n",(0,a.jsx)(n.p,{children:"Initialize Weave at the beginning of your script to start capturing traces."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# highlight-next-line\nimport weave\n# highlight-next-line\nweave.init("autogen-demo")\n'})}),"\n",(0,a.jsx)(n.h2,{id:"tracing-a-simple-model-client",children:"Tracing a Simple Model Client"}),"\n",(0,a.jsx)(n.p,{children:"Weave can trace calls made directly to model clients within AutoGen."}),"\n",(0,a.jsx)(n.h3,{id:"tracing-a-client-create-call",children:"Tracing a client create call"}),"\n",(0,a.jsxs)(n.p,{children:["This example demonstrates tracing a single call to an ",(0,a.jsx)(n.code,{children:"OpenAIChatCompletionClient"}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import asyncio\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n# from autogen_ext.models.anthropic import AnthropicChatCompletionClient\n\nasync def simple_client_call(model_name = "gpt-4o"):\n    model_client = OpenAIChatCompletionClient(\n        model=model_name,\n    )\n    # Alternatively, you can use Anthropic or other model clients\n    # model_client = AnthropicChatCompletionClient(\n        # model="claude-3-haiku-20240307"\n    # )\n    response = await model_client.create(\n        [UserMessage(content="Hello, how are you?", source="user")]\n    )\n    print(response)\n\nasyncio.run(simple_client_call())\n\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://wandb.ai/parambharat/autogen-demo/weave/traces?view=traces_default&peekPath=%2Fparambharat%2Fautogen-demo%2Fcalls%2F0196ee09-8dcf-7b72-8cdc-7699608cd6ef%3FhideTraceTree%3D0",children:(0,a.jsx)(n.img,{alt:"autogen-simple-client.png",src:t(108).Z+"",width:"3016",height:"1574"})})}),"\n",(0,a.jsx)(n.h3,{id:"tracing-a-client-create-call-with-streaming",children:"Tracing a client create call with streaming"}),"\n",(0,a.jsx)(n.p,{children:"Weave also supports tracing streamed responses."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'\nasync def simple_client_call_stream(model_name = "gpt-4o"):\n    openai_model_client = OpenAIChatCompletionClient(model=model_name)\n    async for item in openai_model_client.create_stream(\n        [UserMessage(content="Hello, how are you?", source="user")]\n    ):\n      print(item, flush=True, end="")\n\nasyncio.run(simple_client_call_stream())\n\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://wandb.ai/parambharat/autogen-demo/weave/traces?view=traces_default&peekPath=%2Fparambharat%2Fautogen-demo%2Fcalls%2F0196ee0e-24be-7523-b15e-04f87c03ac68%3FhideTraceTree%3D0",children:(0,a.jsx)(n.img,{alt:"autogen-streaming-client.png",src:t(82367).Z+"",width:"3016",height:"1590"})})}),"\n",(0,a.jsx)(n.h3,{id:"weave-records-cached-calls",children:"Weave records cached calls"}),"\n",(0,a.jsxs)(n.p,{children:["AutoGen's ",(0,a.jsx)(n.code,{children:"ChatCompletionCache"})," can be used, and Weave will trace these interactions, showing whether a response came from the cache or a new call."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'\nfrom autogen_ext.models.cache import ChatCompletionCache\n\nasync def run_cache_client(model_name = "gpt-4o"):\n      openai_model_client = OpenAIChatCompletionClient(model=model_name)\n      cache_client = ChatCompletionCache(openai_model_client,)\n\n      response = await cache_client.create(\n          [UserMessage(content="Hello, how are you?", source="user")]\n      )\n      print(response)  # Should print response from OpenAI\n      response = await cache_client.create(\n          [UserMessage(content="Hello, how are you?", source="user")]\n      )\n      print(response)  # Should print cached response\n\nasyncio.run(run_cache_client())\n\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://wandb.ai/parambharat/autogen-demo/weave/traces?view=traces_default&peekPath=%2Fparambharat%2Fautogen-demo%2Fcalls%2F0196ee11-fded-72c2-baaa-7c0ba2a7cd3b%3FhideTraceTree%3D0",children:(0,a.jsx)(n.img,{alt:"autogen-cached-client.png",src:t(51550).Z+"",width:"3018",height:"1596"})})}),"\n",(0,a.jsx)(n.h2,{id:"tracing-an-agent-with-tool-calls",children:"Tracing an Agent with Tool Calls"}),"\n",(0,a.jsx)(n.p,{children:"Weave traces agents and their tool usage, providing visibility into how agents select and execute tools."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from autogen_agentchat.agents import AssistantAgent\n\nasync def get_weather(city: str) -> str:\n    return f"The weather in {city} is 73 degrees and Sunny."\n\nasync def run_agent_with_tools(model_name = "gpt-4o"):\n    model_client = OpenAIChatCompletionClient(model=model_name)\n\n    agent = AssistantAgent(\n        name="weather_agent",\n        model_client=model_client,\n        tools=[get_weather],\n        system_message="You are a helpful assistant.",\n        reflect_on_tool_use=True,\n    )\n    # For streaming output to console:\n    # await Console(agent.run_stream(task="What is the weather in New York?"))\n    res = await agent.run(task="What is the weather in New York?")\n    print(res)\n    await model_client.close()\n\nasyncio.run(run_agent_with_tools())\n\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://wandb.ai/parambharat/autogen-demo/weave/traces?view=traces_default&peekPath=%2Fparambharat%2Fautogen-demo%2Fcalls%2F0196ee13-e5ca-72a1-b7b6-4b263fad89e3%3FhideTraceTree%3D0",children:(0,a.jsx)(n.img,{alt:"autogen-agent-tools.png",src:t(68835).Z+"",width:"3018",height:"1590"})})}),"\n",(0,a.jsx)(n.h2,{id:"tracing-a-groupchat---roundrobin",children:"Tracing a GroupChat - RoundRobin"}),"\n",(0,a.jsxs)(n.p,{children:["Interactions within group chats, such as ",(0,a.jsx)(n.code,{children:"RoundRobinGroupChat"}),", are traced by Weave, allowing you to follow the conversation flow between agents."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\n\n# we add this weave op here because we want to trace the entire group chat\n# it\'s completely optional but highly recommended to use it\n\n# highlight-next-line\n@weave.op\nasync def run_round_robin_group_chat(model_name="gpt-4o"):\n    model_client = OpenAIChatCompletionClient(model=model_name)\n\n    primary_agent = AssistantAgent(\n        "primary",\n        model_client=model_client,\n        system_message="You are a helpful AI assistant.",\n    )\n\n    critic_agent = AssistantAgent(\n        "critic",\n        model_client=model_client,\n        system_message="Provide constructive feedback. Respond with \'APPROVE\' to when your feedbacks are addressed.",\n    )\n\n    text_termination = TextMentionTermination("APPROVE")\n\n    team = RoundRobinGroupChat(\n        [primary_agent, critic_agent], termination_condition=text_termination\n    )\n    await team.reset()\n    # For streaming output to console:\n    # await Console(team.run_stream(task="Write a short poem about the fall season."))\n    result = await team.run(task="Write a short poem about the fall season.")\n    print(result)\n    await model_client.close()\n\n\nasyncio.run(run_round_robin_group_chat())\n\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://wandb.ai/parambharat/autogen-demo/weave/traces?filter=%7B%22opVersionRefs%22%3A%5B%22weave%3A%2F%2F%2Fparambharat%2Fautogen-demo%2Fop%2Frun_round_robin_group_chat%3A*%22%5D%7D&peekPath=%2Fparambharat%2Fautogen-demo%2Fcalls%2F0196f16c-26ce-7b32-8f0c-2366d29038a3%3FdescendentCallId%3D0196f16c-26ce-7b32-8f0c-2366d29038a3%26hideTraceTree%3D0",children:(0,a.jsx)(n.img,{alt:"round_robin_group_chat.png",src:t(17437).Z+"",width:"3018",height:"1588"})})}),"\n",(0,a.jsx)(n.h2,{id:"tracing-memory",children:"Tracing Memory"}),"\n",(0,a.jsxs)(n.p,{children:["AutoGen's memory components can be traced with Weave. You can use ",(0,a.jsx)(n.code,{children:"@weave.op()"})," to group memory operations under a single trace for better readability."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'\nfrom autogen_core.memory import ListMemory, MemoryContent, MemoryMimeType\n\n# We add this weave op here because we want to trace \n# the memory add calls along with the memory get calls under a single trace\n# it\'s completely optional but highly recommended to use it\n\n# highlight-next-line\n@weave.op\nasync def run_memory_agent(model_name="gpt-4o"):\n    user_memory = ListMemory()\n\n    await user_memory.add(\n        MemoryContent(\n            content="The weather should be in metric units",\n            mime_type=MemoryMimeType.TEXT,\n        )\n    )\n\n    await user_memory.add(\n        MemoryContent(\n            content="Meal recipe must be vegan", mime_type=MemoryMimeType.TEXT\n        )\n    )\n\n    async def get_weather(city: str, units: str = "imperial") -> str:\n        if units == "imperial":\n            return f"The weather in {city} is 73 \xb0F and Sunny."\n        elif units == "metric":\n            return f"The weather in {city} is 23 \xb0C and Sunny."\n        else:\n            return f"Sorry, I don\'t know the weather in {city}."\n\n    model_client = OpenAIChatCompletionClient(model=model_name)\n    assistant_agent = AssistantAgent(\n        name="assistant_agent",\n        model_client=model_client,\n        tools=[get_weather],\n        memory=[user_memory],\n    )\n\n    # For streaming output to console:\n    # stream = assistant_agent.run_stream(task="What is the weather in New York?")\n    # await Console(stream)\n    result = await assistant_agent.run(task="What is the weather in New York?")\n    print(result)\n    await model_client.close()\n\n\nasyncio.run(run_memory_agent())\n\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://wandb.ai/parambharat/autogen-demo/weave/traces?view=traces_default&peekPath=%2Fparambharat%2Fautogen-demo%2Fcalls%2F0196ee18-28b6-7063-90df-77aaedf88dc9%3FhideTraceTree%3D0",children:(0,a.jsx)(n.img,{alt:"autogen-memory.png",src:t(39634).Z+"",width:"3020",height:"1586"})})}),"\n",(0,a.jsx)(n.h2,{id:"tracing-rag-workflows",children:"Tracing RAG Workflows"}),"\n",(0,a.jsxs)(n.p,{children:["Retrieval Augmented Generation (RAG) workflows, including document indexing and retrieval with memory systems like ",(0,a.jsx)(n.code,{children:"ChromaDBVectorMemory"}),", are traceable. Decorating the RAG process with ",(0,a.jsx)(n.code,{children:"@weave.op()"})," helps in visualizing the entire flow."]}),"\n",(0,a.jsx)(n.admonition,{type:"note",children:(0,a.jsxs)(n.p,{children:["The RAG example requires ",(0,a.jsx)(n.code,{children:"chromadb"}),". Install it with ",(0,a.jsx)(n.code,{children:"pip install chromadb"}),"."]})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# highlight-next-line\n# !pip install -q chromadb \n# Ensure chromadb is installed in your environment: `pip install chromadb`\n\nimport re\nfrom typing import List\nimport os\nfrom pathlib import Path\n\nimport aiofiles\nimport httpx\n\nfrom autogen_core.memory import Memory, MemoryContent, MemoryMimeType\nfrom autogen_ext.memory.chromadb import (\n    ChromaDBVectorMemory,\n    PersistentChromaDBVectorMemoryConfig,\n)\n\nclass SimpleDocumentIndexer:\n    def __init__(self, memory: Memory, chunk_size: int = 1500) -> None:\n        self.memory = memory\n        self.chunk_size = chunk_size\n\n    async def _fetch_content(self, source: str) -> str:\n        if source.startswith(("http://", "https://")):\n            async with httpx.AsyncClient() as client:\n                response = await client.get(source)\n                return response.text\n        else:\n            async with aiofiles.open(source, "r", encoding="utf-8") as f:\n                return await f.read()\n\n    def _strip_html(self, text: str) -> str:\n        text = re.sub(r"<[^>]*>", " ", text)\n        text = re.sub(r"\\\\s+", " ", text)\n        return text.strip()\n\n    def _split_text(self, text: str) -> List[str]:\n        chunks: list[str] = []\n        for i in range(0, len(text), self.chunk_size):\n            chunk = text[i : i + self.chunk_size]\n            chunks.append(chunk.strip())\n        return chunks\n\n    async def index_documents(self, sources: List[str]) -> int:\n        total_chunks = 0\n        for source in sources:\n            try:\n                content = await self._fetch_content(source)\n                if "<" in content and ">" in content:\n                    content = self._strip_html(content)\n                chunks = self._split_text(content)\n                for i, chunk in enumerate(chunks):\n                    await self.memory.add(\n                        MemoryContent(\n                            content=chunk,\n                            mime_type=MemoryMimeType.TEXT,\n                            metadata={"source": source, "chunk_index": i},\n                        )\n                    )\n                total_chunks += len(chunks)\n            except Exception as e:\n                print(f"Error indexing {source}: {str(e)}")\n        return total_chunks\n\n# highlight-next-line\n@weave.op\nasync def run_rag_agent(model_name="gpt-4o"):\n    rag_memory = ChromaDBVectorMemory(\n        config=PersistentChromaDBVectorMemoryConfig(\n            collection_name="autogen_docs",\n            persistence_path=os.path.join(str(Path.home()), ".chromadb_autogen_weave"),\n            k=3,\n            score_threshold=0.4,\n        )\n    )\n    # await rag_memory.clear() # Uncomment to clear existing memory if needed\n\n    async def index_autogen_docs() -> None:\n        indexer = SimpleDocumentIndexer(memory=rag_memory)\n        sources = [\n            "https://raw.githubusercontent.com/microsoft/autogen/main/README.md",\n            "https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/agents.html",\n        ]\n        chunks: int = await indexer.index_documents(sources)\n        print(f"Indexed {chunks} chunks from {len(sources)} AutoGen documents")\n    \n    # Only index if the collection is empty or you want to re-index\n    # For demo purposes, we might index it each time or check if already indexed.\n    # This example will try to index each run. Consider adding a check.\n    await index_autogen_docs()\n\n    model_client = OpenAIChatCompletionClient(model=model_name)\n    rag_assistant = AssistantAgent(\n        name="rag_assistant",\n        model_client=model_client,\n        memory=[rag_memory],\n    )\n    \n    # For streaming output to console:\n    # stream = rag_assistant.run_stream(task="What is AgentChat?")\n    # await Console(stream)\n    result = await rag_assistant.run(task="What is AgentChat?")\n    print(result)\n\n    await rag_memory.close()\n    await model_client.close()\n\nasyncio.run(run_rag_agent())\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://wandb.ai/parambharat/autogen-demo/weave/traces?view=traces_default&peekPath=%2Fparambharat%2Fautogen-demo%2Fcalls%2F0196ee1b-bac5-7b80-8be7-6e6ea7d1d63d%3FhideTraceTree%3D0",children:(0,a.jsx)(n.img,{alt:"autogen-rag.png",src:t(762).Z+"",width:"3022",height:"1596"})})}),"\n",(0,a.jsx)(n.h2,{id:"tracing-agent-runtimes",children:"Tracing Agent Runtimes"}),"\n",(0,a.jsxs)(n.p,{children:["Weave can trace operations within AutoGen's agent runtimes, like ",(0,a.jsx)(n.code,{children:"SingleThreadedAgentRuntime"}),". Using ",(0,a.jsx)(n.code,{children:"@weave.op()"})," around the runtime execution function can group related traces."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from dataclasses import dataclass\nfrom typing import Callable\n\nfrom autogen_core import (\n    DefaultTopicId,\n    MessageContext,\n    RoutedAgent,\n    default_subscription,\n    message_handler,\n    AgentId,\n    SingleThreadedAgentRuntime\n)\n\n@dataclass\nclass Message:\n    content: int\n\n@default_subscription\nclass Modifier(RoutedAgent):\n    def __init__(self, modify_val: Callable[[int], int]) -> None:\n        super().__init__("A modifier agent.")\n        self._modify_val = modify_val\n\n    @message_handler\n    async def handle_message(self, message: Message, ctx: MessageContext) -> None:\n        val = self._modify_val(message.content)\n        print(f"{\'-\'*80}\\\\nModifier:\\\\nModified {message.content} to {val}")\n        await self.publish_message(Message(content=val), DefaultTopicId())\n\n@default_subscription\nclass Checker(RoutedAgent):\n    def __init__(self, run_until: Callable[[int], bool]) -> None:\n        super().__init__("A checker agent.")\n        self._run_until = run_until\n\n    @message_handler\n    async def handle_message(self, message: Message, ctx: MessageContext) -> None:\n        if not self._run_until(message.content):\n            print(f"{\'-\'*80}\\\\nChecker:\\\\n{message.content} passed the check, continue.")\n            await self.publish_message(\n                Message(content=message.content), DefaultTopicId()\n            )\n        else:\n            print(f"{\'-\'*80}\\\\nChecker:\\\\n{message.content} failed the check, stopping.")\n\n# we add this weave op here because we want to trace \n# the entire agent runtime call under a single trace\n# it\'s completely optional but highly recommended to use it\n\n# highlight-next-line\n@weave.op\nasync def run_agent_runtime() -> None:\n    runtime = SingleThreadedAgentRuntime()\n\n    await Modifier.register(\n        runtime,\n        "modifier",\n        lambda: Modifier(modify_val=lambda x: x - 1),\n    )\n\n    await Checker.register(\n        runtime,\n        "checker",\n        lambda: Checker(run_until=lambda x: x <= 1),\n    )\n\n    runtime.start()\n    await runtime.send_message(Message(content=3), AgentId("checker", "default"))\n    await runtime.stop_when_idle()\n\nasyncio.run(run_agent_runtime())\n\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://wandb.ai/parambharat/autogen-demo/weave/traces?view=traces_default&peekPath=%2Fparambharat%2Fautogen-demo%2Fcalls%2F0196ee1d-6246-7f11-afb1-3a1874f79023%3FhideTraceTree%3D0",children:(0,a.jsx)(n.img,{alt:"autogen-runtime.png",src:t(52130).Z+"",width:"3022",height:"1592"})})}),"\n",(0,a.jsx)(n.h2,{id:"tracing-workflows-sequential",children:"Tracing Workflows (Sequential)"}),"\n",(0,a.jsxs)(n.p,{children:["Complex agent workflows, defining sequences of agent interactions, can be traced. ",(0,a.jsx)(n.code,{children:"@weave.op()"})," can be used to provide a high-level trace for the entire workflow."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'\nfrom autogen_core import TopicId, type_subscription\nfrom autogen_core.models import ChatCompletionClient, SystemMessage, UserMessage\n\n@dataclass\nclass WorkflowMessage:\n    content: str\n\nconcept_extractor_topic_type = "ConceptExtractorAgent"\nwriter_topic_type = "WriterAgent"\nformat_proof_topic_type = "FormatProofAgent"\nuser_topic_type = "User"\n\n@type_subscription(topic_type=concept_extractor_topic_type)\nclass ConceptExtractorAgent(RoutedAgent):\n    def __init__(self, model_client: ChatCompletionClient) -> None:\n        super().__init__("A concept extractor agent.")\n        self._system_message = SystemMessage(\n            content=(\n                "You are a marketing analyst. Given a product description, identify:\\n"\n                "- Key features\\n"\n                "- Target audience\\n"\n                "- Unique selling points\\n\\n"\n            )\n        )\n        self._model_client = model_client\n\n    @message_handler\n    async def handle_user_description(self, message: WorkflowMessage, ctx: MessageContext) -> None:\n        prompt = f"Product description: {message.content}"\n        llm_result = await self._model_client.create(\n            messages=[self._system_message, UserMessage(content=prompt, source=self.id.key)],\n            cancellation_token=ctx.cancellation_token,\n        )\n        response = llm_result.content\n        assert isinstance(response, str)\n        print(f"{\'-\'*80}\\\\n{self.id.type}:\\\\n{response}")\n        await self.publish_message(\n            WorkflowMessage(response), topic_id=TopicId(writer_topic_type, source=self.id.key)\n        )\n\n@type_subscription(topic_type=writer_topic_type)\nclass WriterAgent(RoutedAgent):\n    def __init__(self, model_client: ChatCompletionClient) -> None:\n        super().__init__("A writer agent.")\n        self._system_message = SystemMessage(\n            content=(\n                "You are a marketing copywriter. Given a block of text describing features, audience, and USPs, "\n                "compose a compelling marketing copy (like a newsletter section) that highlights these points. "\n                "Output should be short (around 150 words), output just the copy as a single text block."\n            )\n        )\n        self._model_client = model_client\n    \n    @message_handler\n    async def handle_intermediate_text(self, message: WorkflowMessage, ctx: MessageContext) -> None:\n        prompt = f"Below is the info about the product:\\\\n\\\\n{message.content}"\n        llm_result = await self._model_client.create(\n            messages=[self._system_message, UserMessage(content=prompt, source=self.id.key)],\n            cancellation_token=ctx.cancellation_token,\n        )\n        response = llm_result.content\n        assert isinstance(response, str)\n        print(f"{\'-\'*80}\\\\n{self.id.type}:\\\\n{response}")\n        await self.publish_message(\n            WorkflowMessage(response), topic_id=TopicId(format_proof_topic_type, source=self.id.key)\n        )\n\n@type_subscription(topic_type=format_proof_topic_type)\nclass FormatProofAgent(RoutedAgent):\n    def __init__(self, model_client: ChatCompletionClient) -> None:\n        super().__init__("A format & proof agent.")\n        self._system_message = SystemMessage(\n            content=(\n                "You are an editor. Given the draft copy, correct grammar, improve clarity, ensure consistent tone, "\n                "give format and make it polished. Output the final improved copy as a single text block."\n            )\n        )\n        self._model_client = model_client\n\n    @message_handler\n    async def handle_intermediate_text(self, message: WorkflowMessage, ctx: MessageContext) -> None:\n        prompt = f"Draft copy:\\\\n{message.content}."\n        llm_result = await self._model_client.create(\n            messages=[self._system_message, UserMessage(content=prompt, source=self.id.key)],\n            cancellation_token=ctx.cancellation_token,\n        )\n        response = llm_result.content\n        assert isinstance(response, str)\n        print(f"{\'-\'*80}\\\\n{self.id.type}:\\\\n{response}")\n        await self.publish_message(\n            WorkflowMessage(response), topic_id=TopicId(user_topic_type, source=self.id.key)\n        )\n\n@type_subscription(topic_type=user_topic_type)\nclass UserAgent(RoutedAgent):\n    def __init__(self) -> None:\n        super().__init__("A user agent that outputs the final copy to the user.")\n\n    @message_handler\n    async def handle_final_copy(self, message: WorkflowMessage, ctx: MessageContext) -> None:\n        print(f"\\\\n{\'-\'*80}\\\\n{self.id.type} received final copy:\\\\n{message.content}")\n\n# we add this weave op here because we want to trace \n# the entire agent workflow under a single trace\n# it\'s completely optional but highly recommended to use it\n\n# highlight-next-line\n@weave.op(call_display_name="Sequential Agent Workflow")\nasync def run_agent_workflow(model_name="gpt-4o"):\n    model_client = OpenAIChatCompletionClient(model=model_name)\n    runtime = SingleThreadedAgentRuntime()\n\n    await ConceptExtractorAgent.register(runtime, type=concept_extractor_topic_type, factory=lambda: ConceptExtractorAgent(model_client=model_client))\n    await WriterAgent.register(runtime, type=writer_topic_type, factory=lambda: WriterAgent(model_client=model_client))\n    await FormatProofAgent.register(runtime, type=format_proof_topic_type, factory=lambda: FormatProofAgent(model_client=model_client))\n    await UserAgent.register(runtime, type=user_topic_type, factory=lambda: UserAgent())\n\n    runtime.start()\n    await runtime.publish_message(\n        WorkflowMessage(\n            content="An eco-friendly stainless steel water bottle that keeps drinks cold for 24 hours"\n        ),\n        topic_id=TopicId(concept_extractor_topic_type, source="default"),\n    )\n    await runtime.stop_when_idle()\n    await model_client.close()\n\nasyncio.run(run_agent_workflow())\n\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://wandb.ai/parambharat/autogen-demo/weave/traces?view=traces_default&peekPath=%2Fparambharat%2Fautogen-demo%2Fcalls%2F0196ee1f-dd53-73f2-9119-2a44da92c5ae%3FhideTraceTree%3D0",children:(0,a.jsx)(n.img,{alt:"autogen-sequential-workflow.png",src:t(62086).Z+"",width:"3022",height:"1704"})})}),"\n",(0,a.jsx)(n.h2,{id:"tracing-code-executor",children:"Tracing Code Executor"}),"\n",(0,a.jsx)(n.admonition,{title:"Docker Required",type:"warning",children:(0,a.jsx)(n.p,{children:"This example involves code execution using Docker and may not work in all environments (e.g., Colab directly). Ensure Docker is running locally if you try this."})}),"\n",(0,a.jsx)(n.p,{children:"Weave traces the generation and execution of code by AutoGen agents."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'\nimport tempfile\nfrom autogen_core import DefaultTopicId\nfrom autogen_core.code_executor import CodeBlock, CodeExecutor\nfrom autogen_core.models import (\n    AssistantMessage,\n    ChatCompletionClient,\n    LLMMessage,\n    SystemMessage,\n    UserMessage,\n)\nfrom autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor\n\n\n@dataclass\nclass CodeGenMessage:\n    content: str\n\n@default_subscription\nclass Assistant(RoutedAgent):\n    def __init__(self, model_client: ChatCompletionClient) -> None:\n        super().__init__("An assistant agent.")\n        self._model_client = model_client\n        self._chat_history: List[LLMMessage] = [\n           SystemMessage(\n                content="""Write Python script in markdown block, and it will be executed.\nAlways save figures to file in the current directory. Do not use plt.show(). All code required to complete this task must be contained within a single response.""",\n            )\n        ]\n\n    @message_handler\n    async def handle_message(self, message: CodeGenMessage, ctx: MessageContext) -> None:\n        self._chat_history.append(UserMessage(content=message.content, source="user"))\n        result = await self._model_client.create(self._chat_history)\n        print(f"\\\\n{\'-\'*80}\\\\nAssistant:\\\\n{result.content}")\n        self._chat_history.append(AssistantMessage(content=result.content, source="assistant"))\n        await self.publish_message(CodeGenMessage(content=result.content), DefaultTopicId())\n\ndef extract_markdown_code_blocks(markdown_text: str) -> List[CodeBlock]:\n    pattern = re.compile(r"```(?:\\\\s*([\\\\w\\\\+\\\\-]+))?\\\\n([\\\\s\\\\S]*?)```")\n    matches = pattern.findall(markdown_text)\n    code_blocks: List[CodeBlock] = []\n    for match in matches:\n        language = match[0].strip() if match[0] else ""\n        code_content = match[1]\n        code_blocks.append(CodeBlock(code=code_content, language=language))\n    return code_blocks\n\n@default_subscription\nclass Executor(RoutedAgent):\n    def __init__(self, code_executor: CodeExecutor) -> None:\n        super().__init__("An executor agent.")\n        self._code_executor = code_executor\n\n    @message_handler\n    async def handle_message(self, message: CodeGenMessage, ctx: MessageContext) -> None:\n        code_blocks = extract_markdown_code_blocks(message.content)\n        if code_blocks:\n            result = await self._code_executor.execute_code_blocks(\n                code_blocks, cancellation_token=ctx.cancellation_token\n            )\n            print(f"\\\\n{\'-\'*80}\\\\nExecutor:\\\\n{result.output}")\n            await self.publish_message(CodeGenMessage(content=result.output), DefaultTopicId())\n\n# we add this weave op here because we want to trace \n# the entire code gen workflow under a single trace\n# it\'s completely optional but highly recommended to use it\n\n# highlight-next-line\n@weave.op(call_display_name="CodeGen Agent Workflow")\nasync def run_codegen(model_name="gpt-4o"): # Updated model\n    work_dir = tempfile.mkdtemp()\n    runtime = SingleThreadedAgentRuntime()\n\n    # Ensure Docker is running for this example\n    try:\n        async with DockerCommandLineCodeExecutor(work_dir=work_dir) as executor:\n            model_client = OpenAIChatCompletionClient(model=model_name)\n            await Assistant.register(runtime, "assistant", lambda: Assistant(model_client=model_client))\n            await Executor.register(runtime, "executor", lambda: Executor(executor))\n\n            runtime.start()\n            await runtime.publish_message(\n                CodeGenMessage(content="Create a plot of NVDA vs TSLA stock returns YTD from 2024-01-01."),\n                DefaultTopicId(),\n            )\n            await runtime.stop_when_idle()\n            await model_client.close()\n    except Exception as e:\n        print(f"Could not run Docker code executor example: {e}")\n        print("Please ensure Docker is installed and running.")\n    finally:\n        import shutil\n        shutil.rmtree(work_dir)\n\n\nasyncio.run(run_codegen())\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://wandb.ai/parambharat/autogen-demo/weave/traces?view=traces_default&peekPath=%2Fparambharat%2Fautogen-demo%2Fcalls%2F0196f173-21c2-7540-9dc7-fbab0b94ce0e%3FhideTraceTree%3D0",children:(0,a.jsx)(n.img,{alt:"autogen-codegen.png",src:t(23971).Z+"",width:"3018",height:"1592"})})}),"\n",(0,a.jsx)(n.h2,{id:"learn-more",children:"Learn More"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Weave"}),":","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"/guides/tracking/tracing",children:"Tracing Guide"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"/guides/tracking/ops",children:"Op Decorator"})}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"AutoGen"}),":","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://microsoft.github.io/autogen/stable//index.html",children:"Official Documentation"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://github.com/microsoft/autogen",children:"AutoGen GitHub"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"This guide provides a starting point for integrating Weave with AutoGen. Explore the Weave UI to see detailed traces of your agent interactions, model calls, and tool usage."})]})}function m(e={}){const{wrapper:n}={...(0,o.a)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},68835:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/autogen-agent-tools-84e460d4a5ee87812e718a6ad2bff159.png"},51550:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/autogen-cached-client-7ebf351d84a977db3c69735c97253874.png"},23971:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/autogen-codegen-5a807241c9e81764704cd300c0a93459.png"},39634:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/autogen-memory-69eea05d6dd7e44d5e218855fedebb00.png"},762:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/autogen-rag-a9e8ad719c7d335f4fdfb3a19212f2dc.png"},52130:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/autogen-runtime-ad43a1cd31e76ec8a2b7aace93bb3673.png"},62086:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/autogen-sequential-workflow-360839e445d5c406b2767573904f255f.png"},108:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/autogen-simple-client-157ce934e4e75b99af0196a12464462d.png"},82367:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/autogen-streaming-client-e4a9277b6a89b6aaf4ffd53e7f410ed8.png"},17437:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/round_robin_group_chat-0251569c3db6fdd27be90b602a2b92da.png"},11151:(e,n,t)=>{t.d(n,{Z:()=>r,a:()=>i});var a=t(67294);const o={},s=a.createContext(o);function i(e){const n=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);