{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <!-- docusaurus_head_meta::start\n",
    "\n",
    "## title: Log Audio With Weave\n",
    "\n",
    "docusaurus_head_meta::end -->\n",
    "\n",
    "<!--- @wandbcode{feedback-colab} -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLKCxvuewpXp"
   },
   "source": [
    "# How to use Weave with Audio Data: An OpenAI Example\n",
    "\n",
    "This demo uses the OpenAI chat completions API with GPT 4o Audio Preview to generate audio responses to text prompts and track these in Weave.\n",
    "\n",
    "<img src=\"https://i.imgur.com/OUfsZ2x.png\"></img>\n",
    "\n",
    "For the advanced use case, we leverage the OpenAI Realtime API to stream audio in realtime. Click the following thumbnail to view the video demonstration, or click [here](https://www.youtube.com/watch?v=lnnd73xDElw).\n",
    "\n",
    "[![Everything Is AWESOME](https://img.youtube.com/vi/lnnd73xDElw/0.jpg)](https://www.youtube.com/watch?v=lnnd73xDElw \"Everything Is AWESOME\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5_ISqCHw-if"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Start by installing the OpenAI (`openai`) and Weave (`weave`) dependencies, as well as API key management dependencey `set-env`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2Y2XINQTjm4q"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install openai\n",
    "!pip install weave\n",
    "!pip install set-env-colab-kaggle-dotenv -q # for env var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Temporary workaround to fix bug in openai:\n",
    "# TypeError: Client.__init__() got an unexpected keyword argument 'proxies'\n",
    "# See https://community.openai.com/t/error-with-openai-1-56-0-client-init-got-an-unexpected-keyword-argument-proxies/1040332/15\n",
    "!pip install \"httpx<0.28\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSsInJkXxTUi"
   },
   "source": [
    "Next, load the required API keys for OpenAI and Weave. Here, we use set_env which is compatible with google colab's secret keys manager, and is an alternative to colab's specific `google.colab.userdata`. See: [here](https://pypi.org/project/set-env-colab-kaggle-dotenv/) for usage instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fjEhdPz-klhq",
    "outputId": "0f0067fb-e9de-46e5-ede0-64b3db79ef3e"
   },
   "outputs": [],
   "source": [
    "# Set environment variables.\n",
    "from set_env import set_env\n",
    "\n",
    "_ = set_env(\"OPENAI_API_KEY\")\n",
    "_ = set_env(\"WANDB_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXjZLdpexbJO"
   },
   "source": [
    "And finally import the required libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sjsUjX2Gxasp"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import time\n",
    "import wave\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from openai import OpenAI\n",
    "\n",
    "import weave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clLjkW05xdYq"
   },
   "source": [
    "## Audio Streaming and Storage Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5sFlW3-xj62"
   },
   "source": [
    "Now we will setup a call to OpenAI's completions endpoint with audio modality enabled. First create the OpenAI client and initiate a Weave project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VZVBhFzRxjKp",
    "outputId": "7447b59b-0733-42e8-c103-0712a578d9b6"
   },
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "weave.init(\"openai-audio-chat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9gg1LkNzOIZ"
   },
   "source": [
    "Now we will define our OpenAI completions request and add our Weave decorator (op).\n",
    "\n",
    "Here, we define the function `prompt_endpont_and_log_trace`. This function has three primary steps:\n",
    "\n",
    "1. We make a completion object using the `GPT 4o Audio Preview` model that supports text and audio inputs and outputs.\n",
    "\n",
    "   - We prompt the model to count to 13 slowly with varying accents.\n",
    "   - We set the completion to \"stream\".\n",
    "\n",
    "2. We open a new output file to which the streamed data is writen chunk by chunk.\n",
    "\n",
    "3. We return an open file handler to the audio file so Weave logs the audio data in the trace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XGjJIbOYjkf5"
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 22050\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def prompt_endpoint_and_log_trace(system_prompt=None, user_prompt=None):\n",
    "    if not system_prompt:\n",
    "        system_prompt = \"You're the fastest counter in the world\"\n",
    "    if not user_prompt:\n",
    "        user_prompt = \"Count to 13 super super slow, enunciate each number with a dramatic flair, changing up accents as you go along. British, French, German, Spanish, etc.\"\n",
    "    # Request from the OpenAI API with audio modality\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-audio-preview\",\n",
    "        modalities=[\"text\", \"audio\"],\n",
    "        audio={\"voice\": \"fable\", \"format\": \"pcm16\"},\n",
    "        stream=True,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Open a wave file for writing\n",
    "    with wave.open(\"./output.wav\", \"wb\") as wav_file:\n",
    "        wav_file.setnchannels(1)  # Mono\n",
    "        wav_file.setsampwidth(2)  # 16-bit\n",
    "        wav_file.setframerate(SAMPLE_RATE)  # Sample rate (adjust if needed)\n",
    "\n",
    "        # Write chunks as they are streamed in from the API\n",
    "        for chunk in completion:\n",
    "            if (\n",
    "                hasattr(chunk, \"choices\")\n",
    "                and chunk.choices is not None\n",
    "                and len(chunk.choices) > 0\n",
    "            ):\n",
    "                if (\n",
    "                    hasattr(chunk.choices[0].delta, \"audio\")\n",
    "                    and chunk.choices[0].delta.audio.get(\"data\") is not None\n",
    "                ):\n",
    "                    # Decode the base64 audio data\n",
    "                    audio_data = base64.b64decode(\n",
    "                        chunk.choices[0].delta.audio.get(\"data\")\n",
    "                    )\n",
    "\n",
    "                    # Write the current chunk to the wave file\n",
    "                    wav_file.writeframes(audio_data)\n",
    "\n",
    "    # Return the file to Weave op\n",
    "    return wave.open(\"output.wav\", \"rb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiidpfzXz7X0"
   },
   "source": [
    "## Testing\n",
    "\n",
    "Run the following cell. The system and user prompt will be stored in a Weave trace as well as the output audio.\n",
    "After running the cell, click the link next to the \"üç©\" emoji to view your trace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "xnpZASeJoPQn",
    "outputId": "25d9a956-3367-4668-ca91-718f6ccb3feb"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "# Call the function to write the audio stream\n",
    "prompt_endpoint_and_log_trace(\n",
    "    system_prompt=\"You're the fastest counter in the world\",\n",
    "    user_prompt=\"Count to 13 super super slow, enunciate each number with a dramatic flair, changing up accents as you go along. British, French, German, Spanish, etc.\",\n",
    ")\n",
    "\n",
    "# Display the updated audio stream\n",
    "display(Audio(\"output.wav\", rate=SAMPLE_RATE, autoplay=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7zY5fho4hOG"
   },
   "source": [
    "# Advanced Usage: Realtime Audio API with Weave\n",
    "\n",
    "<img src=\"https://i.imgur.com/ZiW3IVu.png\"/>\n",
    "<details>\n",
    "<summary> (Advanced) Realtime Audio API with Weave </summary>\n",
    "OpenAI's realtime API is a highly functional and reliable conversational API for building realtime audio and text assistants.\n",
    "\n",
    "Please note:\n",
    "\n",
    "- Review the cells in [Microphone Configuration](#microphone-configuration)\n",
    "- Due to limitations of the Google Colab execution environment, **this must be run on your host machine** as a Jupyter Notebook. This cannot be ran in the browser.\n",
    "  - On MacOS you will need to install `portaudio` via Brew (see [here](https://formulae.brew.sh/formula/portaudio)) for Pyaudio to function.\n",
    "- OpenAI's Python SDK does not yet provide Realtime API support. We implement the complete OAI Realtime API schema in Pydantic for greater legibility, and may deprecate once official support is released.\n",
    "- The `enable_audio_playback` toggle will cause playback of assistant outputted audio. Please note that **headphones are required if this is enabled**, as echo detection requires a highly complex implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKY6F0d06gRh"
   },
   "source": [
    "## Requirements Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "A9SvfhFH6fGL"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install numpy==2.0\n",
    "!pip install weave\n",
    "!pip install pyaudio # On mac, you may need to install portaudio first with `brew install portaudio`\n",
    "!pip install websocket-client\n",
    "!pip install set-env-colab-kaggle-dotenv -q # for env var\n",
    "!pip install resampy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dhe4PsRx6miw"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "import wave\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pyaudio\n",
    "import resampy\n",
    "import websocket\n",
    "from set_env import set_env\n",
    "\n",
    "import weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_aXNWTV6n36"
   },
   "outputs": [],
   "source": [
    "# Set environment variables.\n",
    "# See: https://pypi.org/project/set-env-colab-kaggle-dotenv/ for usage instructions.\n",
    "_ = set_env(\"OPENAI_API_KEY\")\n",
    "_ = set_env(\"WANDB_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "detJ21276p31"
   },
   "source": [
    "## Microphone Configuration\n",
    "\n",
    "Run the following cell to find all available audio devices. Then, populate the `INPUT_DEVICE_INDEX` and the `OUTPUT_DEVICE_INDEX` based on the devices listed. Your input device will have at least 1 input channels, and your output device will have at least 1 output channels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "uIDGPQr06t71",
    "outputId": "87ebaaf0-15b0-4ce7-c807-6fc375c860fc"
   },
   "outputs": [],
   "source": [
    "# Get device list from pyaudio so we can configure the next cell\n",
    "p = pyaudio.PyAudio()\n",
    "devices_data = {i: p.get_device_info_by_index(i) for i in range(p.get_device_count())}\n",
    "for i, device in devices_data.items():\n",
    "    print(\n",
    "        f\"Found device @{i}: {device['name']} with sample rate: {device['defaultSampleRate']} and input channels: {device['maxInputChannels']} and output channels: {device['maxOutputChannels']}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "BoJDJUf76vjg",
    "outputId": "aba922db-bdcf-421b-dec4-84e7fc2ffd50"
   },
   "outputs": [],
   "source": [
    "INPUT_DEVICE_INDEX = 3  # @param                                                 # Choose based on device list above. Make sure device has > 0 input channels.\n",
    "OUTPUT_DEVICE_INDEX = 12  # @param                                                # Chose based on device list above. Make sure device has > 0 output channels.\n",
    "enable_audio_playback = True  # @param {type:\"boolean\"}                           # Toggle on assistant audio playback. Requires headphones.\n",
    "\n",
    "# Audio recording and streaming parameters\n",
    "INPUT_DEVICE_CHANNELS = devices_data[INPUT_DEVICE_INDEX][\n",
    "    \"maxInputChannels\"\n",
    "]  # From device list above\n",
    "SAMPLE_RATE = int(\n",
    "    devices_data[INPUT_DEVICE_INDEX][\"defaultSampleRate\"]\n",
    ")  # From device list above\n",
    "CHUNK = int(SAMPLE_RATE / 10)  # Samples per frame\n",
    "SAMPLE_WIDTH = p.get_sample_size(pyaudio.paInt16)  # Samples per frame for the format\n",
    "CHUNK_DURATION = 0.3  # Seconds of audio per chunk sent to OAI API\n",
    "OAI_SAMPLE_RATE = (\n",
    "    24000  # OAI Sample Rate is 24kHz, we need this to play or save assistant audio\n",
    ")\n",
    "OUTPUT_DEVICE_CHANNELS = 1  # Set to 1 for mono output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Realtime API Schema Implementation\n",
    "\n",
    "The OpenAI Python SDK does not yet provide Realtime API support. We implement the complete OAI Realtime API schema in Pydantic for greater legibility, and may deprecate once official support is released.\n",
    "\n",
    "<details>\n",
    "<summary> Pydantic Schema for OpenAI Realtime API (OpenAI's SDK lacks Realtime API support) </summary>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MqmBWnso6YjS"
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import Any, Literal, Optional, Union\n",
    "\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "\n",
    "class BaseEvent(BaseModel):\n",
    "    type: Union[\"ClientEventTypes\", \"ServerEventTypes\"]\n",
    "    event_id: Optional[str] = None  # Add event_id as an optional field for all events\n",
    "\n",
    "    # def model_dump_json(self, *args, **kwargs):\n",
    "    #     # Only include non-None fields\n",
    "    #     return super().model_dump_json(*args, exclude_none=True, **kwargs)\n",
    "\n",
    "\n",
    "class ChatMessage(BaseModel):\n",
    "    role: Literal[\"user\", \"assistant\"]\n",
    "    content: str\n",
    "    timestamp: float\n",
    "\n",
    "\n",
    "\"\"\" CLIENT EVENTS \"\"\"\n",
    "\n",
    "\n",
    "class ClientEventTypes(str, Enum):\n",
    "    SESSION_UPDATE = \"session.update\"\n",
    "    CONVERSATION_ITEM_CREATE = \"conversation.item.create\"\n",
    "    CONVERSATION_ITEM_TRUNCATE = \"conversation.item.truncate\"\n",
    "    CONVERSATION_ITEM_DELETE = \"conversation.item.delete\"\n",
    "    RESPONSE_CREATE = \"response.create\"\n",
    "    RESPONSE_CANCEL = \"response.cancel\"\n",
    "    INPUT_AUDIO_BUFFER_APPEND = \"input_audio_buffer.append\"\n",
    "    INPUT_AUDIO_BUFFER_COMMIT = \"input_audio_buffer.commit\"\n",
    "    INPUT_AUDIO_BUFFER_CLEAR = \"input_audio_buffer.clear\"\n",
    "    ERROR = \"error\"\n",
    "\n",
    "\n",
    "#### Session Update\n",
    "class TurnDetection(BaseModel):\n",
    "    type: Literal[\"server_vad\"]\n",
    "    threshold: float = Field(..., ge=0.0, le=1.0)\n",
    "    prefix_padding_ms: int\n",
    "    silence_duration_ms: int\n",
    "\n",
    "\n",
    "class InputAudioTranscription(BaseModel):\n",
    "    model: Optional[str] = None\n",
    "\n",
    "\n",
    "class ToolParameterProperty(BaseModel):\n",
    "    type: str\n",
    "\n",
    "\n",
    "class ToolParameter(BaseModel):\n",
    "    type: str\n",
    "    properties: dict[str, ToolParameterProperty]\n",
    "    required: list[str]\n",
    "\n",
    "\n",
    "class Tool(BaseModel):\n",
    "    type: Literal[\"function\", \"code_interpreter\", \"file_search\"]\n",
    "    name: Optional[str] = None\n",
    "    description: Optional[str] = None\n",
    "    parameters: Optional[ToolParameter] = None\n",
    "\n",
    "\n",
    "class Session(BaseModel):\n",
    "    modalities: Optional[list[str]] = None\n",
    "    instructions: Optional[str] = None\n",
    "    voice: Optional[str] = None\n",
    "    input_audio_format: Optional[str] = None\n",
    "    output_audio_format: Optional[str] = None\n",
    "    input_audio_transcription: Optional[InputAudioTranscription] = None\n",
    "    turn_detection: Optional[TurnDetection] = None\n",
    "    tools: Optional[list[Tool]] = None\n",
    "    tool_choice: Optional[str] = None\n",
    "    temperature: Optional[float] = None\n",
    "    max_output_tokens: Optional[int] = None\n",
    "\n",
    "\n",
    "class SessionUpdate(BaseEvent):\n",
    "    type: Literal[ClientEventTypes.SESSION_UPDATE] = ClientEventTypes.SESSION_UPDATE\n",
    "    session: Session\n",
    "\n",
    "\n",
    "#### Audio Buffers\n",
    "class InputAudioBufferAppend(BaseEvent):\n",
    "    type: Literal[ClientEventTypes.INPUT_AUDIO_BUFFER_APPEND] = (\n",
    "        ClientEventTypes.INPUT_AUDIO_BUFFER_APPEND\n",
    "    )\n",
    "    audio: str\n",
    "\n",
    "\n",
    "class InputAudioBufferCommit(BaseEvent):\n",
    "    type: Literal[ClientEventTypes.INPUT_AUDIO_BUFFER_COMMIT] = (\n",
    "        ClientEventTypes.INPUT_AUDIO_BUFFER_COMMIT\n",
    "    )\n",
    "\n",
    "\n",
    "class InputAudioBufferClear(BaseEvent):\n",
    "    type: Literal[ClientEventTypes.INPUT_AUDIO_BUFFER_CLEAR] = (\n",
    "        ClientEventTypes.INPUT_AUDIO_BUFFER_CLEAR\n",
    "    )\n",
    "\n",
    "\n",
    "#### Messages\n",
    "class MessageContent(BaseModel):\n",
    "    type: Literal[\"input_audio\"]\n",
    "    audio: str\n",
    "\n",
    "\n",
    "class ConversationItemContent(BaseModel):\n",
    "    type: Literal[\"input_text\", \"input_audio\", \"text\", \"audio\"]\n",
    "    text: Optional[str] = None\n",
    "    audio: Optional[str] = None\n",
    "    transcript: Optional[str] = None\n",
    "\n",
    "\n",
    "class FunctionCallContent(BaseModel):\n",
    "    call_id: str\n",
    "    name: str\n",
    "    arguments: str\n",
    "\n",
    "\n",
    "class FunctionCallOutputContent(BaseModel):\n",
    "    output: str\n",
    "\n",
    "\n",
    "class ConversationItem(BaseModel):\n",
    "    id: Optional[str] = None\n",
    "    type: Literal[\"message\", \"function_call\", \"function_call_output\"]\n",
    "    status: Optional[Literal[\"completed\", \"in_progress\", \"incomplete\"]] = None\n",
    "    role: Literal[\"user\", \"assistant\", \"system\"]\n",
    "    content: list[\n",
    "        Union[ConversationItemContent, FunctionCallContent, FunctionCallOutputContent]\n",
    "    ]\n",
    "    call_id: Optional[str] = None\n",
    "    name: Optional[str] = None\n",
    "    arguments: Optional[str] = None\n",
    "    output: Optional[str] = None\n",
    "\n",
    "\n",
    "class ConversationItemCreate(BaseEvent):\n",
    "    type: Literal[ClientEventTypes.CONVERSATION_ITEM_CREATE] = (\n",
    "        ClientEventTypes.CONVERSATION_ITEM_CREATE\n",
    "    )\n",
    "    item: ConversationItem\n",
    "\n",
    "\n",
    "class ConversationItemTruncate(BaseEvent):\n",
    "    type: Literal[ClientEventTypes.CONVERSATION_ITEM_TRUNCATE] = (\n",
    "        ClientEventTypes.CONVERSATION_ITEM_TRUNCATE\n",
    "    )\n",
    "    item_id: str\n",
    "    content_index: int\n",
    "    audio_end_ms: int\n",
    "\n",
    "\n",
    "class ConversationItemDelete(BaseEvent):\n",
    "    type: Literal[ClientEventTypes.CONVERSATION_ITEM_DELETE] = (\n",
    "        ClientEventTypes.CONVERSATION_ITEM_DELETE\n",
    "    )\n",
    "    item_id: str\n",
    "\n",
    "\n",
    "#### Responses\n",
    "class ResponseCreate(BaseEvent):\n",
    "    type: Literal[ClientEventTypes.RESPONSE_CREATE] = ClientEventTypes.RESPONSE_CREATE\n",
    "\n",
    "\n",
    "class ResponseCancel(BaseEvent):\n",
    "    type: Literal[ClientEventTypes.RESPONSE_CANCEL] = ClientEventTypes.RESPONSE_CANCEL\n",
    "\n",
    "\n",
    "# Update the Event union to include all event types\n",
    "ClientEvent = Union[\n",
    "    SessionUpdate,\n",
    "    InputAudioBufferAppend,\n",
    "    InputAudioBufferCommit,\n",
    "    InputAudioBufferClear,\n",
    "    ConversationItemCreate,\n",
    "    ConversationItemTruncate,\n",
    "    ConversationItemDelete,\n",
    "    ResponseCreate,\n",
    "    ResponseCancel,\n",
    "]\n",
    "\n",
    "\"\"\" SERVER EVENTS \"\"\"\n",
    "\n",
    "\n",
    "class ServerEventTypes(str, Enum):\n",
    "    ERROR = \"error\"\n",
    "    RESPONSE_AUDIO_TRANSCRIPT_DONE = \"response.audio_transcript.done\"\n",
    "    RESPONSE_AUDIO_TRANSCRIPT_DELTA = \"response.audio_transcript.delta\"\n",
    "    RESPONSE_AUDIO_DELTA = \"response.audio.delta\"\n",
    "    SESSION_CREATED = \"session.created\"\n",
    "    SESSION_UPDATED = \"session.updated\"\n",
    "    CONVERSATION_CREATED = \"conversation.created\"\n",
    "    INPUT_AUDIO_BUFFER_COMMITTED = \"input_audio_buffer.committed\"\n",
    "    INPUT_AUDIO_BUFFER_CLEARED = \"input_audio_buffer.cleared\"\n",
    "    INPUT_AUDIO_BUFFER_SPEECH_STARTED = \"input_audio_buffer.speech_started\"\n",
    "    INPUT_AUDIO_BUFFER_SPEECH_STOPPED = \"input_audio_buffer.speech_stopped\"\n",
    "    CONVERSATION_ITEM_CREATED = \"conversation.item.created\"\n",
    "    CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_COMPLETED = (\n",
    "        \"conversation.item.input_audio_transcription.completed\"\n",
    "    )\n",
    "    CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_FAILED = (\n",
    "        \"conversation.item.input_audio_transcription.failed\"\n",
    "    )\n",
    "    CONVERSATION_ITEM_TRUNCATED = \"conversation.item.truncated\"\n",
    "    CONVERSATION_ITEM_DELETED = \"conversation.item.deleted\"\n",
    "    RESPONSE_CREATED = \"response.created\"\n",
    "    RESPONSE_DONE = \"response.done\"\n",
    "    RESPONSE_OUTPUT_ITEM_ADDED = \"response.output_item.added\"\n",
    "    RESPONSE_OUTPUT_ITEM_DONE = \"response.output_item.done\"\n",
    "    RESPONSE_CONTENT_PART_ADDED = \"response.content_part.added\"\n",
    "    RESPONSE_CONTENT_PART_DONE = \"response.content_part.done\"\n",
    "    RESPONSE_TEXT_DELTA = \"response.text.delta\"\n",
    "    RESPONSE_TEXT_DONE = \"response.text.done\"\n",
    "    RESPONSE_AUDIO_DONE = \"response.audio.done\"\n",
    "    RESPONSE_FUNCTION_CALL_ARGUMENTS_DELTA = \"response.function_call_arguments.delta\"\n",
    "    RESPONSE_FUNCTION_CALL_ARGUMENTS_DONE = \"response.function_call_arguments.done\"\n",
    "    RATE_LIMITS_UPDATED = \"rate_limits.updated\"\n",
    "\n",
    "\n",
    "#### Errors\n",
    "class ErrorDetails(BaseModel):\n",
    "    type: Optional[str] = None\n",
    "    code: Optional[str] = None\n",
    "    message: Optional[str] = None\n",
    "    param: Optional[str] = None\n",
    "\n",
    "\n",
    "class ErrorEvent(BaseEvent):\n",
    "    type: Literal[ServerEventTypes.ERROR] = ServerEventTypes.ERROR\n",
    "    error: ErrorDetails\n",
    "\n",
    "\n",
    "#### Session\n",
    "class SessionCreated(BaseEvent):\n",
    "    type: Literal[ServerEventTypes.SESSION_CREATED] = ServerEventTypes.SESSION_CREATED\n",
    "    session: Session\n",
    "\n",
    "\n",
    "class SessionUpdated(BaseEvent):\n",
    "    type: Literal[ServerEventTypes.SESSION_UPDATED] = ServerEventTypes.SESSION_UPDATED\n",
    "    session: Session\n",
    "\n",
    "\n",
    "#### Conversation\n",
    "class Conversation(BaseModel):\n",
    "    id: str\n",
    "    object: Literal[\"realtime.conversation\"]\n",
    "\n",
    "\n",
    "class ConversationCreated(BaseEvent):\n",
    "    type: Literal[ServerEventTypes.CONVERSATION_CREATED] = (\n",
    "        ServerEventTypes.CONVERSATION_CREATED\n",
    "    )\n",
    "    conversation: Conversation\n",
    "\n",
    "\n",
    "class ConversationItemCreated(BaseEvent):\n",
    "    type: Literal[ServerEventTypes.CONVERSATION_ITEM_CREATED] = (\n",
    "        ServerEventTypes.CONVERSATION_ITEM_CREATED\n",
    "    )\n",
    "    previous_item_id: Optional[str] = None\n",
    "    item: ConversationItem\n",
    "\n",
    "\n",
    "class ConversationItemInputAudioTranscriptionCompleted(BaseEvent):\n",
    "    type: Literal[\n",
    "        ServerEventTypes.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_COMPLETED\n",
    "    ] = ServerEventTypes.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_COMPLETED\n",
    "    item_id: str\n",
    "    content_index: int\n",
    "    transcript: str\n",
    "\n",
    "\n",
    "class ConversationItemInputAudioTranscriptionFailed(BaseEvent):\n",
    "    type: Literal[\n",
    "        ServerEventTypes.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_FAILED\n",
    "    ] = ServerEventTypes.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_FAILED\n",
    "    item_id: str\n",
    "    content_index: int\n",
    "    error: dict[str, Any]\n",
    "\n",
    "\n",
    "class ConversationItemTruncated(BaseEvent):\n",
    "    type: Literal[ServerEventTypes.CONVERSATION_ITEM_TRUNCATED] = (\n",
    "        ServerEventTypes.CONVERSATION_ITEM_TRUNCATED\n",
    "    )\n",
    "    item_id: str\n",
    "    content_index: int\n",
    "    audio_end_ms: int\n",
    "\n",
    "\n",
    "class ConversationItemDeleted(BaseEvent):\n",
    "    type: Literal[ServerEventTypes.CONVERSATION_ITEM_DELETED] = (\n",
    "        ServerEventTypes.CONVERSATION_ITEM_DELETED\n",
    "    )\n",
    "    item_id: str\n",
    "\n",
    "\n",
    "#### Response\n",
    "class ResponseUsage(BaseModel):\n",
    "    total_tokens: int\n",
    "    input_tokens: int\n",
    "    output_tokens: int\n",
    "    input_token_details: Optional[dict[str, int]] = None\n",
    "    output_token_details: Optional[dict[str, int]] = None\n",
    "\n",
    "\n",
    "class ResponseOutput(BaseModel):\n",
    "    id: str\n",
    "    object: Literal[\"realtime.item\"]\n",
    "    type: str\n",
    "    status: str\n",
    "    role: str\n",
    "    content: list[dict[str, Any]]\n",
    "\n",
    "\n",
    "class ResponseContentPart(BaseModel):\n",
    "    type: str\n",
    "    text: Optional[str] = None\n",
    "\n",
    "\n",
    "class ResponseOutputItemContent(BaseModel):\n",
    "    type: str\n",
    "    text: Optional[str] = None\n",
    "\n",
    "\n",
    "class ResponseStatusDetails(BaseModel):\n",
    "    type: str\n",
    "    reason: str\n",
    "\n",
    "\n",
    "class ResponseOutputItem(BaseModel):\n",
    "    id: str\n",
    "    object: Literal[\"realtime.item\"]\n",
    "    type: str\n",
    "    status: str\n",
    "    role: str\n",
    "    content: list[ResponseOutputItemContent]\n",
    "\n",
    "\n",
    "class Response(BaseModel):\n",
    "    id: str\n",
    "    object: Literal[\"realtime.response\"]\n",
    "    status: str\n",
    "    status_details: Optional[ResponseStatusDetails] = None\n",
    "    output: list[ResponseOutput]\n",
    "    usage: Optional[ResponseUsage]\n",
    "\n",
    "\n",
    "class ResponseCreated(BaseEvent):\n",
    "    type: Literal[ServerEventTypes.RESPONSE_CREATED] = ServerEventTypes.RESPONSE_CREATED\n",
    "    response: Response\n",
    "\n",
    "\n",
    "class ResponseDone(BaseEvent):\n",
    "    type: Literal[ServerEventTypes.RESPONSE_DONE] = ServerEventTypes.RESPONSE_DONE\n",
    "    response: Response\n",
    "\n",
    "\n",
    "class ResponseOutputItemAdded(BaseEvent):\n",
    "    type: Literal[ServerEventTypes.RESPONSE_OUTPUT_ITEM_ADDED] = (\n",
    "        ServerEventTypes.RESPONSE_OUTPUT_ITEM_ADDED\n",
    "    )\n",
    "    response_id: str\n",
    "    output_index: int\n",
    "    item: ResponseOutputItem\n",
    "\n",
    "\n",
    "class ResponseOutputItemDone(BaseEvent):\n",
    "    type: Literal[ServerEventTypes.RESPONSE_OUTPUT_ITEM_DONE] = (\n",
    "        ServerEventTypes.RESPONSE_OUTPUT_ITEM_DONE\n",
    "    )\n",
    "    response_id: str\n",
    "    output_index: int\n",
    "    item: ResponseOutputItem\n",
    "\n",
    "\n",
    "class ResponseContentPartAdded(BaseEvent):\n",
    "    type: Literal[ServerEventTypes.RESPONSE_CONTENT_PART_ADDED] = (\n",
    "        ServerEventTypes.RESPONSE_CONTENT_PART_ADDED\n",
    "    )\n",
    "    response_id: str\n",
    "    item_id: str\n",
    "    output_index: int\n",
    "    content_index: int\n",
    "    part: ResponseContentPart\n",
    "\n",
    "\n",
    "class ResponseContentPartDone(BaseEvent):\n",
    "    type: Literal[ServerEventTypes.RESPONSE_CONTENT_PART_DONE] = (\n",
    "        ServerEventTypes.RESPONSE_CONTENT_PART_DONE\n",
    "    )\n",
    "    response_id: str\n",
    "    item_id: str\n",
    "    output_index: int\n",
    "    content_index: int\n",
    "    part: ResponseContentPart\n",
    "\n",
    "\n",
    "#### Response Text\n",
    "class ResponseTextDelta(BaseEvent):\n",
    "    type: Literal[ServerEventTypes.RESPONSE_TEXT_DELTA] = (\n",
    "        ServerEventTypes.RESPONSE_TEXT_DELTA\n",
    "    )\n",
    "    response_id: str\n",
    "    item_id: str\n",
    "    output_index: int\n",
    "    content_index: int\n",
    "    delta: str\n",
    "\n",
    "\n",
    "class ResponseTextDone(BaseEvent):\n",
    "    type: Literal[ServerEventTypes.RESPONSE_TEXT_DONE] = (\n",
    "        ServerEventTypes.RESPONSE_TEXT_DONE\n",
    "    )\n",
    "    response_id: str\n",
    "    item_id: str\n",
    "    output_index: int\n",
    "    content_index: int\n",
    "    text: str\n",
    "\n",
    "\n",
    "#### Response Audio\n",
    "class ResponseAudioTranscriptDone(BaseEvent):\n",
    "    type: Literal[ServerEventTypes.RESPONSE_AUDIO_TRANSCRIPT_DONE] = (\n",
    "        ServerEventTypes.RESPONSE_AUDIO_TRANSCRIPT_DONE\n",
    "    )\n",
    "    transcript: str\n",
    "\n",
    "\n",
    "class ResponseAudioTranscriptDelta(BaseEvent):\n",
    "    type: Literal[ServerEventTypes.RESPONSE_AUDIO_TRANSCRIPT_DELTA] = (\n",
    "        ServerEventTypes.RESPONSE_AUDIO_TRANSCRIPT_DELTA\n",
    "    )\n",
    "    delta: str\n",
    "\n",
    "\n",
    "class ResponseAudioDelta(BaseEvent):\n",
    "    type: Literal[ServerEventTypes.RESPONSE_AUDIO_DELTA] = (\n",
    "        ServerEventTypes.RESPONSE_AUDIO_DELTA\n",
    "    )\n",
    "    response_id: str\n",
    "    item_id: str\n",
    "    delta: str\n",
    "\n",
    "\n",
    "class ResponseAudioDone(BaseEvent):\n",
    "    type: Literal[ServerEventTypes.RESPONSE_AUDIO_DONE] = (\n",
    "        ServerEventTypes.RESPONSE_AUDIO_DONE\n",
    "    )\n",
    "    response_id: str\n",
    "    item_id: str\n",
    "    output_index: int\n",
    "    content_index: int\n",
    "\n",
    "\n",
    "class InputAudioBufferCommitted(BaseEvent):\n",
    "    type: Literal[ServerEventTypes.INPUT_AUDIO_BUFFER_COMMITTED] = (\n",
    "        ServerEventTypes.INPUT_AUDIO_BUFFER_COMMITTED\n",
    "    )\n",
    "    previous_item_id: Optional[str] = None\n",
    "    item_id: Optional[str] = None\n",
    "    event_id: Optional[str] = None\n",
    "\n",
    "\n",
    "class InputAudioBufferCleared(BaseEvent):\n",
    "    type: Literal[ServerEventTypes.INPUT_AUDIO_BUFFER_CLEARED] = (\n",
    "        ServerEventTypes.INPUT_AUDIO_BUFFER_CLEARED\n",
    "    )\n",
    "\n",
    "\n",
    "class InputAudioBufferSpeechStarted(BaseEvent):\n",
    "    type: Literal[ServerEventTypes.INPUT_AUDIO_BUFFER_SPEECH_STARTED] = (\n",
    "        ServerEventTypes.INPUT_AUDIO_BUFFER_SPEECH_STARTED\n",
    "    )\n",
    "    audio_start_ms: int\n",
    "    item_id: str\n",
    "\n",
    "\n",
    "class InputAudioBufferSpeechStopped(BaseEvent):\n",
    "    type: Literal[ServerEventTypes.INPUT_AUDIO_BUFFER_SPEECH_STOPPED] = (\n",
    "        ServerEventTypes.INPUT_AUDIO_BUFFER_SPEECH_STOPPED\n",
    "    )\n",
    "    audio_end_ms: int\n",
    "    item_id: str\n",
    "\n",
    "\n",
    "#### Function Calls\n",
    "class ResponseFunctionCallArgumentsDelta(BaseEvent):\n",
    "    type: Literal[ServerEventTypes.RESPONSE_FUNCTION_CALL_ARGUMENTS_DELTA] = (\n",
    "        ServerEventTypes.RESPONSE_FUNCTION_CALL_ARGUMENTS_DELTA\n",
    "    )\n",
    "    response_id: str\n",
    "    item_id: str\n",
    "    output_index: int\n",
    "    call_id: str\n",
    "    delta: str\n",
    "\n",
    "\n",
    "class ResponseFunctionCallArgumentsDone(BaseEvent):\n",
    "    type: Literal[ServerEventTypes.RESPONSE_FUNCTION_CALL_ARGUMENTS_DONE] = (\n",
    "        ServerEventTypes.RESPONSE_FUNCTION_CALL_ARGUMENTS_DONE\n",
    "    )\n",
    "    response_id: str\n",
    "    item_id: str\n",
    "    output_index: int\n",
    "    call_id: str\n",
    "    arguments: str\n",
    "\n",
    "\n",
    "#### Rate Limits\n",
    "class RateLimit(BaseModel):\n",
    "    name: str\n",
    "    limit: int\n",
    "    remaining: int\n",
    "    reset_seconds: float\n",
    "\n",
    "\n",
    "class RateLimitsUpdated(BaseEvent):\n",
    "    type: Literal[ServerEventTypes.RATE_LIMITS_UPDATED] = (\n",
    "        ServerEventTypes.RATE_LIMITS_UPDATED\n",
    "    )\n",
    "    rate_limits: list[RateLimit]\n",
    "\n",
    "\n",
    "ServerEvent = Union[\n",
    "    ErrorEvent,\n",
    "    ConversationCreated,\n",
    "    ResponseAudioTranscriptDone,\n",
    "    ResponseAudioTranscriptDelta,\n",
    "    ResponseAudioDelta,\n",
    "    ResponseCreated,\n",
    "    ResponseDone,\n",
    "    ResponseOutputItemAdded,\n",
    "    ResponseOutputItemDone,\n",
    "    ResponseContentPartAdded,\n",
    "    ResponseContentPartDone,\n",
    "    ResponseTextDelta,\n",
    "    ResponseTextDone,\n",
    "    ResponseAudioDone,\n",
    "    ConversationItemInputAudioTranscriptionCompleted,\n",
    "    SessionCreated,\n",
    "    SessionUpdated,\n",
    "    InputAudioBufferCleared,\n",
    "    InputAudioBufferSpeechStarted,\n",
    "    InputAudioBufferSpeechStopped,\n",
    "    ConversationItemCreated,\n",
    "    ConversationItemInputAudioTranscriptionFailed,\n",
    "    ConversationItemTruncated,\n",
    "    ConversationItemDeleted,\n",
    "    RateLimitsUpdated,\n",
    "]\n",
    "\n",
    "EVENT_TYPE_TO_MODEL = {\n",
    "    ServerEventTypes.ERROR: ErrorEvent,\n",
    "    ServerEventTypes.RESPONSE_AUDIO_TRANSCRIPT_DONE: ResponseAudioTranscriptDone,\n",
    "    ServerEventTypes.RESPONSE_AUDIO_TRANSCRIPT_DELTA: ResponseAudioTranscriptDelta,\n",
    "    ServerEventTypes.RESPONSE_AUDIO_DELTA: ResponseAudioDelta,\n",
    "    ServerEventTypes.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_COMPLETED: ConversationItemInputAudioTranscriptionCompleted,\n",
    "    ServerEventTypes.SESSION_CREATED: SessionCreated,\n",
    "    ServerEventTypes.SESSION_UPDATED: SessionUpdated,\n",
    "    ServerEventTypes.CONVERSATION_CREATED: ConversationCreated,\n",
    "    ServerEventTypes.INPUT_AUDIO_BUFFER_COMMITTED: InputAudioBufferCommitted,\n",
    "    ServerEventTypes.INPUT_AUDIO_BUFFER_CLEARED: InputAudioBufferCleared,\n",
    "    ServerEventTypes.INPUT_AUDIO_BUFFER_SPEECH_STARTED: InputAudioBufferSpeechStarted,\n",
    "    ServerEventTypes.INPUT_AUDIO_BUFFER_SPEECH_STOPPED: InputAudioBufferSpeechStopped,\n",
    "    ServerEventTypes.CONVERSATION_ITEM_CREATED: ConversationItemCreated,\n",
    "    ServerEventTypes.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_FAILED: ConversationItemInputAudioTranscriptionFailed,\n",
    "    ServerEventTypes.CONVERSATION_ITEM_TRUNCATED: ConversationItemTruncated,\n",
    "    ServerEventTypes.CONVERSATION_ITEM_DELETED: ConversationItemDeleted,\n",
    "    ServerEventTypes.RESPONSE_CREATED: ResponseCreated,\n",
    "    ServerEventTypes.RESPONSE_DONE: ResponseDone,\n",
    "    ServerEventTypes.RESPONSE_OUTPUT_ITEM_ADDED: ResponseOutputItemAdded,\n",
    "    ServerEventTypes.RESPONSE_OUTPUT_ITEM_DONE: ResponseOutputItemDone,\n",
    "    ServerEventTypes.RESPONSE_CONTENT_PART_ADDED: ResponseContentPartAdded,\n",
    "    ServerEventTypes.RESPONSE_CONTENT_PART_DONE: ResponseContentPartDone,\n",
    "    ServerEventTypes.RESPONSE_TEXT_DELTA: ResponseTextDelta,\n",
    "    ServerEventTypes.RESPONSE_TEXT_DONE: ResponseTextDone,\n",
    "    ServerEventTypes.RESPONSE_AUDIO_DONE: ResponseAudioDone,\n",
    "    ServerEventTypes.RATE_LIMITS_UPDATED: RateLimitsUpdated,\n",
    "}\n",
    "\n",
    "\n",
    "def parse_server_event(event_data: dict) -> ServerEvent:\n",
    "    event_type = event_data.get(\"type\")\n",
    "    if not event_type:\n",
    "        raise ValueError(\"Event data is missing 'type' field\")\n",
    "\n",
    "    model_class = EVENT_TYPE_TO_MODEL.get(event_type)\n",
    "    if not model_class:\n",
    "        raise ValueError(f\"Unknown event type: {event_type}\")\n",
    "\n",
    "    try:\n",
    "        return model_class(**event_data)\n",
    "    except ValidationError as e:\n",
    "        raise ValueError(f\"Failed to parse event of type {event_type}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Stream Writer (To Disk and In Memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RNmGNIrv64x3"
   },
   "outputs": [],
   "source": [
    "class StreamingWavWriter:\n",
    "    \"\"\"Writes audio integer or byte array chunks to a WAV file.\"\"\"\n",
    "\n",
    "    wav_file = None\n",
    "    buffer = None\n",
    "    in_memory = False\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        filename=None,\n",
    "        channels=INPUT_DEVICE_CHANNELS,\n",
    "        sample_width=SAMPLE_WIDTH,\n",
    "        framerate=SAMPLE_RATE,\n",
    "    ):\n",
    "        self.in_memory = filename is None\n",
    "        if self.in_memory:\n",
    "            self.buffer = io.BytesIO()\n",
    "            self.wav_file = wave.open(self.buffer, \"wb\")\n",
    "        else:\n",
    "            self.wav_file = wave.open(filename, \"wb\")\n",
    "\n",
    "        self.wav_file.setnchannels(channels)\n",
    "        self.wav_file.setsampwidth(sample_width)\n",
    "        self.wav_file.setframerate(framerate)\n",
    "\n",
    "    def append_int16_chunk(self, int16_data):\n",
    "        if int16_data is not None:\n",
    "            self.wav_file.writeframes(\n",
    "                int16_data.tobytes()\n",
    "                if isinstance(int16_data, np.ndarray)\n",
    "                else int16_data\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        self.wav_file.close()\n",
    "\n",
    "    def get_wav_buffer(self):\n",
    "        assert self.in_memory, \"Buffer only available if stream is in memory.\"\n",
    "        return self.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1VyirmR_7BMW"
   },
   "source": [
    "## Realtime Audio Model\n",
    "\n",
    "The realtime (RT) audio model uses a websocket to send events to OpenAI's Realtime audio API. This works as follows:\n",
    "\n",
    "1.  **init:** We initialize local buffers (input audio) and streams (assistant playback stream, user audio disk writer stream) and open a connection to the Realtime API.\n",
    "2.  **receive_messages_thread**: A thread handles receiving messages from the API. Four primary event types are handled: - RESPONSE_AUDIO_TRANSCRIPT_DONE:\n",
    "\n",
    "            The server indicates the assistant's response is completed and provides the transcript.\n",
    "\n",
    "        - CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_COMPLETED:\n",
    "\n",
    "            The server indicates the user's audio has been transcribed, and sends the transcript of the user's audio. We log the transcript to Weave and print it for the user.\n",
    "\n",
    "        - RESPONSE_AUDIO_DELTA:\n",
    "\n",
    "            The server sends a new chunk of assistant response audio. We append this to the ongoing response data via the response ID, and add this to the output stream for playback.\n",
    "\n",
    "        - RESPONSE_DONE:\n",
    "\n",
    "            The server indicates completion of an assistant response. We get all audio chunks associated with the response, as well as the transcript, and log these in Weave.\n",
    "\n",
    "    3.**send_audio**: A handler appends user audio chunks to a buffer, and sends chunks of audio when the audio buffer reaches a certain size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_AdjnYZG7Fq8"
   },
   "outputs": [],
   "source": [
    "class RTAudioModel(weave.Model):\n",
    "    \"\"\"Model class for realtime e2e audio OpenAI model interaction with Whisper user transcription for logging.\"\"\"\n",
    "\n",
    "    realtime_model_name: str = \"gpt-4o-realtime-preview-2024-10-01\"  # realtime e2e audio only model interaction\n",
    "\n",
    "    stop_event: Optional[threading.Event] = threading.Event()  # Event to stop the model\n",
    "    ws: Optional[websocket.WebSocket] = None  # Websocket for OpenAI communications\n",
    "\n",
    "    user_wav_writer: Optional[StreamingWavWriter] = (\n",
    "        None  # Stream for writing user output to file\n",
    "    )\n",
    "    input_audio_buffer: Optional[np.ndarray] = None  # Buffer for user audio chunks\n",
    "    assistant_outputs: dict[str, StreamingWavWriter] = (\n",
    "        None  # Assistant outputs aggregated to send to weave\n",
    "    )\n",
    "    playback_stream: Optional[pyaudio.Stream] = (\n",
    "        None  # Playback stream for playing assistant responses\n",
    "    )\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stop_event.clear()\n",
    "        self.user_wav_writer = StreamingWavWriter(\n",
    "            filename=\"user_audio.wav\", framerate=SAMPLE_RATE\n",
    "        )\n",
    "        self.input_audio_buffer = np.array([], dtype=np.int16)\n",
    "        self.ws = websocket.WebSocket()\n",
    "        self.assistant_outputs = {}\n",
    "\n",
    "        # Open the assistant audio playback stream if enabled\n",
    "        if enable_audio_playback:\n",
    "            self.playback_stream = pyaudio.PyAudio().open(\n",
    "                format=pyaudio.paInt16,\n",
    "                channels=OUTPUT_DEVICE_CHANNELS,\n",
    "                rate=OAI_SAMPLE_RATE,\n",
    "                output=True,\n",
    "                output_device_index=OUTPUT_DEVICE_INDEX,\n",
    "            )\n",
    "\n",
    "        # Connect Websocket\n",
    "        try:\n",
    "            self.ws.connect(\n",
    "                f\"wss://api.openai.com/v1/realtime?model={self.realtime_model_name}\",\n",
    "                header={\n",
    "                    \"Authorization\": f\"Bearer {os.environ.get('OPENAI_API_KEY')}\",\n",
    "                    \"OpenAI-Beta\": \"realtime=v1\",\n",
    "                },\n",
    "            )\n",
    "\n",
    "            # Send config msg\n",
    "            config_event = SessionUpdate(\n",
    "                session=Session(\n",
    "                    modalities=[\"text\", \"audio\"],  # modalities to use\n",
    "                    input_audio_transcription=InputAudioTranscription(\n",
    "                        model=\"whisper-1\"\n",
    "                    ),  # whisper-1 for transcription\n",
    "                    turn_detection=TurnDetection(\n",
    "                        type=\"server_vad\",\n",
    "                        threshold=0.3,\n",
    "                        prefix_padding_ms=300,\n",
    "                        silence_duration_ms=600,\n",
    "                    ),  # server VAD to detect silence\n",
    "                )\n",
    "            )\n",
    "            self.ws.send(config_event.model_dump_json(exclude_none=True))\n",
    "            self.log_ws_message(config_event.model_dump_json(exclude_none=True), \"Sent\")\n",
    "\n",
    "            # Start listener\n",
    "            websocket_thread = threading.Thread(target=self.receive_messages_thread)\n",
    "            websocket_thread.daemon = True\n",
    "            websocket_thread.start()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to WebSocket: {e}\")\n",
    "\n",
    "    ##### Weave Integration and Message Handlers #####\n",
    "    def handle_assistant_response_audio_delta(self, data: ResponseAudioDelta):\n",
    "        if data.response_id not in self.assistant_outputs:\n",
    "            self.assistant_outputs[data.response_id] = StreamingWavWriter(\n",
    "                framerate=OAI_SAMPLE_RATE\n",
    "            )\n",
    "\n",
    "        data_bytes = base64.b64decode(data.delta)\n",
    "        self.assistant_outputs[data.response_id].append_int16_chunk(data_bytes)\n",
    "\n",
    "        if enable_audio_playback:\n",
    "            self.playback_stream.write(data_bytes)\n",
    "\n",
    "        return {\"assistant_audio\": data_bytes}\n",
    "\n",
    "    @weave.op()\n",
    "    def handle_assistant_response_done(self, data: ResponseDone):\n",
    "        wave_file_stream = self.assistant_outputs[data.response.id]\n",
    "        wave_file_stream.close()\n",
    "        wave_file_stream.buffer.seek(0)\n",
    "        weave_payload = {\n",
    "            \"assistant_audio\": wave.open(wave_file_stream.get_wav_buffer(), \"rb\"),\n",
    "            \"assistant_transcript\": data.response.output[0]\n",
    "            .content[0]\n",
    "            .get(\"transcript\", \"Transcript Unavailable.\"),\n",
    "        }\n",
    "        return weave_payload\n",
    "\n",
    "    @weave.op()\n",
    "    def handle_user_transcription_done(\n",
    "        self, data: ConversationItemInputAudioTranscriptionCompleted\n",
    "    ):\n",
    "        return {\"user_transcript\": data.transcript}\n",
    "\n",
    "    ##### Message Receiver and Sender #####\n",
    "    def receive_messages_thread(self):\n",
    "        while not self.stop_event.is_set():\n",
    "            try:\n",
    "                data = json.loads(self.ws.recv())\n",
    "                self.log_ws_message(json.dumps(data, indent=2))\n",
    "\n",
    "                parsed_event = parse_server_event(data)\n",
    "\n",
    "                if parsed_event.type == ServerEventTypes.RESPONSE_AUDIO_TRANSCRIPT_DONE:\n",
    "                    print(\"Assistant: \", parsed_event.transcript)\n",
    "                elif (\n",
    "                    parsed_event.type\n",
    "                    == ServerEventTypes.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_COMPLETED\n",
    "                ):\n",
    "                    print(\"User: \", parsed_event.transcript)\n",
    "                    self.handle_user_transcription_done(parsed_event)\n",
    "                elif parsed_event.type == ServerEventTypes.RESPONSE_AUDIO_DELTA:\n",
    "                    self.handle_assistant_response_audio_delta(parsed_event)\n",
    "                elif parsed_event.type == ServerEventTypes.RESPONSE_DONE:\n",
    "                    self.handle_assistant_response_done(parsed_event)\n",
    "                elif parsed_event.type == ServerEventTypes.ERROR:\n",
    "                    print(\n",
    "                        f\"\\nError from server: {parsed_event.error.model_dump_json(exclude_none=True)}\"\n",
    "                    )\n",
    "            except websocket.WebSocketConnectionClosedException:\n",
    "                print(\"\\nWebSocket connection closed\")\n",
    "                break\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError in receive_messages: {e}\")\n",
    "                break\n",
    "\n",
    "    def send_audio(self, audio_chunk):\n",
    "        if self.ws and self.ws.connected:\n",
    "            self.input_audio_buffer = np.append(\n",
    "                self.input_audio_buffer, np.frombuffer(audio_chunk, dtype=np.int16)\n",
    "            )\n",
    "            if len(self.input_audio_buffer) >= SAMPLE_RATE * CHUNK_DURATION:\n",
    "                try:\n",
    "                    # Resample audio to OAI sample rate\n",
    "                    resampled_audio = (\n",
    "                        resampy.resample(\n",
    "                            self.input_audio_buffer, SAMPLE_RATE, OAI_SAMPLE_RATE\n",
    "                        )\n",
    "                        if SAMPLE_RATE != OAI_SAMPLE_RATE\n",
    "                        else self.input_audio_buffer\n",
    "                    )\n",
    "\n",
    "                    # Send audio chunk to OAI API\n",
    "                    audio_event = InputAudioBufferAppend(\n",
    "                        audio=base64.b64encode(\n",
    "                            resampled_audio.astype(np.int16).tobytes()\n",
    "                        ).decode(\"utf-8\")  # Convert audio array to b64 bytes\n",
    "                    )\n",
    "                    self.ws.send(audio_event.model_dump_json(exclude_none=True))\n",
    "                    self.log_ws_message(\n",
    "                        audio_event.model_dump_json(exclude_none=True), \"Sent\"\n",
    "                    )\n",
    "                finally:\n",
    "                    self.user_wav_writer.append_int16_chunk(self.input_audio_buffer)\n",
    "\n",
    "                    # Clear the audio buffer\n",
    "                    self.input_audio_buffer = np.array([], dtype=np.int16)\n",
    "        else:\n",
    "            print(\"Error sending audio: websocket not initialized.\")\n",
    "\n",
    "    ##### General Utility Functions #####\n",
    "    def log_ws_message(self, message, direction=\"Received\"):\n",
    "        with open(\"websocket_log.txt\", \"a\") as log_file:\n",
    "            log_file.write(\n",
    "                f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - {direction}: {message}\\n\"\n",
    "            )\n",
    "\n",
    "    def stop(self):\n",
    "        self.stop_event.set()\n",
    "\n",
    "        if self.ws:\n",
    "            self.ws.close()\n",
    "\n",
    "        self.user_wav_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5fatsR_gty-J"
   },
   "source": [
    "## Audio recorder\n",
    "\n",
    "We use a pyaudio input stream with a handler linked to the `send_audio` method of the RTAudio model. The stream is returned to the main thread so it can be safely exited upon program completion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rXTXD6Jb7KVD"
   },
   "outputs": [],
   "source": [
    "# Audio capture stream\n",
    "def record_audio(realtime_model: RTAudioModel) -> pyaudio.Stream:\n",
    "    \"\"\"Setup a Pyaudio input stream and use the RTAudioModel as a callback for streaming data.\"\"\"\n",
    "\n",
    "    def audio_callback(in_data, frame_count, time_info, status):\n",
    "        realtime_model.send_audio(in_data)\n",
    "        return (None, pyaudio.paContinue)\n",
    "\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(\n",
    "        format=pyaudio.paInt16,\n",
    "        channels=INPUT_DEVICE_CHANNELS,\n",
    "        rate=SAMPLE_RATE,\n",
    "        input=True,\n",
    "        input_device_index=INPUT_DEVICE_INDEX,\n",
    "        frames_per_buffer=CHUNK,\n",
    "        stream_callback=audio_callback,\n",
    "    )\n",
    "    stream.start_stream()\n",
    "\n",
    "    print(\"Recording started. Please begin speaking to your personal assistant...\")\n",
    "    return stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Zsqlvfq7L2p"
   },
   "source": [
    "## Main Thread (Run me!)\n",
    "\n",
    "The main thread initiates a Realtime Audio Model with Weave integrated. Next, a reccording is opened and we wait for a keyboard interrupt from the user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wJzK9c7Q7M-0"
   },
   "outputs": [],
   "source": [
    "weave.init(project_name=\"realtime-oai-audio-testing\")\n",
    "\n",
    "realtime_model = RTAudioModel()\n",
    "\n",
    "if realtime_model.ws and realtime_model.ws.connected:\n",
    "    recording_stream: pyaudio.Stream = record_audio(realtime_model)\n",
    "\n",
    "    try:\n",
    "        while not realtime_model.stop_event.is_set():\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main loop: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        print(\"Exiting...\")\n",
    "        realtime_model.stop()\n",
    "        if recording_stream and recording_stream.is_active():\n",
    "            recording_stream.stop_stream()\n",
    "            recording_stream.close()\n",
    "else:\n",
    "    print(\n",
    "        \"WebSocket connection failed. Please check your API key and internet connection.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "QLKCxvuewpXp",
    "P7zY5fho4hOG",
    "JKY6F0d06gRh",
    "detJ21276p31",
    "KU36knXx6ZW5"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
