{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "922c59ab",
   "metadata": {},
   "source": [
    "<!-- docusaurus_head_meta::start\n",
    "---\n",
    "title: Extracting Structured Data from Documents using Instructor and Weave\n",
    "---\n",
    "docusaurus_head_meta::end -->\n",
    "\n",
    "# Extracting Structured Data from Documents using Instructor and Weave\n",
    "\n",
    "LLMs are widely used in downstream applications which necessitates outputs to be structured in a consistent manner. This often requires the LLM-powered applications to parse unstructured documents (such as PDF files) and extract specific information structured according to a specififc schema.\n",
    "\n",
    "In this tutorial, you will learn how to extract specific information from machine learning papers (such as key findings, novel methodologies, research directions, etc.). We will use [Instructor](https://python.useinstructor.com/) to get structured output from an OpenAI [GPT-4o](https://platform.openai.com/docs/models/gpt-4o) model in the form of [Pydantic objects](https://docs.pydantic.dev/latest/concepts/models/). We will also use [Weave](../docs/introduction.md) to track and evaluate our LLM workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d60e63",
   "metadata": {},
   "source": [
    "## Installing the Dependencies\n",
    "\n",
    "We need the following libraries for this tutorial:\n",
    "\n",
    "- [Instructor](https://python.useinstructor.com/) to easily get structured output from LLMs.\n",
    "- [OpenAI](https://openai.com/index/openai-api/) as our LLM vendor.\n",
    "- [Weave](../introduction.md) to track our LLM workflow and evaluate our prompting strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777caadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU pymupdf4llm instructor openai weave wget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d903a1b",
   "metadata": {},
   "source": [
    "Since we'll be using [OpenAI API](https://openai.com/index/openai-api/) as the LLM Vendor, we will also need an OpenAI API key. You can [sign up](https://platform.openai.com/signup) on the OpenAI platform to get your own API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420068b0e4ac55f4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-18T14:51:54.619499Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "api_key = getpass(\"Enter you OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "os.environ[\"WEAVE_PARALLELISM\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919727d4",
   "metadata": {},
   "source": [
    "## Enable Tracking using Weave\n",
    "\n",
    "Weave is currently integrated with OpenAI, and including [`weave.init`](../docs/reference/python-sdk/weave/index.md) at the start of our code lets us automatically trace our OpenAI chat completions which can be explored in the Weave UI. Check out the [Weave integration docs for OpenAI](../docs/guides/integrations/openai.md) to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8fcd3b8c57dedb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-18T14:51:57.427271Z"
    }
   },
   "outputs": [],
   "source": [
    "import weave\n",
    "\n",
    "weave.init(project_name=\"arxiv-data-extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cc9746",
   "metadata": {},
   "source": [
    "## Structured Data Extraction Workflow\n",
    "\n",
    "In order to extract the required structured data from a machine learning paper using GPT-4o and instructor, let's first define our schema as [Pydantic Model](https://docs.pydantic.dev/latest/concepts/models/) outlining the exact information that we need from a paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599c90fa5f660680",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-18T14:52:02.060781Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Finding(BaseModel):\n",
    "    finding_name: str\n",
    "    explanation: str\n",
    "\n",
    "\n",
    "class Method(BaseModel):\n",
    "    method_name: str\n",
    "    explanation: str\n",
    "    citation: Optional[str]\n",
    "\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    metric: str\n",
    "    benchmark: str\n",
    "    value: float\n",
    "    observation: str\n",
    "\n",
    "\n",
    "class PaperInfo(BaseModel):\n",
    "    main_findings: List[Finding]  # The main findings of the paper\n",
    "    novel_methods: List[Method]  # The novel methods proposed in the paper\n",
    "    existing_methods: List[Method]  # The existing methods used in the paper\n",
    "    machine_learning_techniques: List[\n",
    "        Method\n",
    "    ]  # The machine learning techniques used in the paper\n",
    "    metrics: List[Evaluation]  # The evaluation metrics used in the paper\n",
    "    github_repository: (\n",
    "        str  # The link to the GitHub repository of the paper (if there is any)\n",
    "    )\n",
    "    hardware: str  # The hardware or accelerator setup used in the paper\n",
    "    further_research: List[\n",
    "        str\n",
    "    ]  # The further research directions suggested in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6abef63",
   "metadata": {},
   "source": [
    "Next, we write a detailed system prompt that serve as a set of instructions providing context and guidelines to help the model perform the required task.\n",
    "\n",
    "First, we ask the model to play the role of \"helpful assistant to a machine learning researcher who is reading a paper from arXiv\", thus establishing the basic context of the task. Next, we provide the details of all the information it needs to extract from the paper, in accordance with the schema `PaperInfo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fb471315e6c6e9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-18T14:52:02.069925Z"
    }
   },
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant to a machine learning researcher who is reading a paper from arXiv.\n",
    "You are to extract the following information from the paper:\n",
    "\n",
    "- a list of main findings in from the paper and their corresponding detailed explanations\n",
    "- the list of names of the different novel methods proposed in the paper and their corresponding detailed explanations\n",
    "- the list of names of the different existing methods used in the paper, their corresponding detailed explanations, and\n",
    "    their citations\n",
    "- the list of machine learning techniques used in the paper, such as architectures, optimizers, schedulers, etc., their\n",
    "    corresponding detailed explanations, and their citations\n",
    "- the list of evaluation metrics used in the paper, the benchmark datasets used, the values of the metrics, and their\n",
    "    corresponding detailed observation in the paper\n",
    "- the link to the GitHub repository of the paper if there is any\n",
    "- the hardware or accelerators used to perform the experiments in the paper if any\n",
    "- a list of possible further research directions that the paper suggests\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab6c98f",
   "metadata": {},
   "source": [
    ":::note\n",
    "You can checkout OpenAI's [Prompt engineering guide](https://platform.openai.com/docs/guides/prompt-engineering) for more details on writing effective prompts for models like GPT-4o.\n",
    ":::\n",
    "\n",
    "Next, we patch the OpenAI client to return structured outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05ad86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "structured_client = instructor.from_openai(openai_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ce1ffd",
   "metadata": {},
   "source": [
    "Finally, we write our LLM execution workflow as a [Weave Model](../docs/guides/core-types/models.md), thus combining the configurations associated with the workflow along with the code that defines how the model operates into a single object that will now be tracked and versioned using Weave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57701f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "import pymupdf\n",
    "import pymupdf4llm\n",
    "import requests\n",
    "\n",
    "\n",
    "class ArxivModel(weave.Model):\n",
    "    model: str\n",
    "    system_prompt: str\n",
    "    max_retries: int = 5\n",
    "    seed: int = 42\n",
    "\n",
    "    @weave.op()\n",
    "    def get_markdown_from_arxiv(self, url):\n",
    "        response = requests.get(url)\n",
    "        with pymupdf.open(stream=BytesIO(response.content), filetype=\"pdf\") as doc:\n",
    "            return pymupdf4llm.to_markdown(doc)\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, url_pdf: str) -> PaperInfo:\n",
    "        md_text = self.get_markdown_from_arxiv(url_pdf)\n",
    "        return structured_client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            response_model=PaperInfo,\n",
    "            max_retries=self.max_retries,\n",
    "            seed=self.seed,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": md_text},\n",
    "            ],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bda59f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rich\n",
    "\n",
    "arxiv_parser_model = ArxivModel(model=\"gpt-4o\", system_prompt=system_prompt)\n",
    "\n",
    "result = arxiv_parser_model.predict(url_pdf=\"http://arxiv.org/pdf/1711.06288v2.pdf\")\n",
    "rich.print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e671f19",
   "metadata": {},
   "source": [
    ":::warning\n",
    "Executing this LLM workflow will cost approximately $0.05-$0.25 in OpenAI credits, depending on the number of attempts instructor needs makes to get the output in the desired format (which is set to 5).\n",
    ":::\n",
    "\n",
    "| ![](https://i.imgur.com/Etnjoyq.png) |\n",
    "|---|\n",
    "| Here's how you can explore the traces of the `ArxivModel` in the Weave UI |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e35473",
   "metadata": {},
   "source": [
    "## Evaluating the Prompting Workflow\n",
    "\n",
    "Let us now evaluate how accurately our LLM workflow is able to extract the methods from the paper using [Weave Evaluation](../docs/guides/core-types/evaluations.md). For this we will write a simple scoring function that compares the list of novel methods, existing methods, and ML techniques predicted by the prompting workflow against a ground-truth list of methods associated with the paper to compute an accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ab9872a2145ed8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-18T14:52:03.231222Z"
    }
   },
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def arxiv_method_score(\n",
    "    method: List[dict], model_output: Optional[PaperInfo]\n",
    ") -> dict[str, float]:\n",
    "    if model_output is None:\n",
    "        return {\"method_prediction_accuracy\": 0.0}\n",
    "    predicted_methods = (\n",
    "        model_output.novel_methods\n",
    "        + model_output.existing_methods\n",
    "        + model_output.machine_learning_techniques\n",
    "    )\n",
    "    num_correct_methods = 0\n",
    "    for gt_method in method:\n",
    "        for predicted_method in predicted_methods:\n",
    "            predicted_method = (\n",
    "                f\"{predicted_method.method_name}\\n{predicted_method.explanation}\"\n",
    "            )\n",
    "            if (\n",
    "                gt_method[\"name\"].lower() in predicted_method.lower()\n",
    "                or gt_method[\"full_name\"].lower() in predicted_method.lower()\n",
    "            ):\n",
    "                num_correct_methods += 1\n",
    "    return {\"method_prediction_accuracy\": num_correct_methods / len(predicted_methods)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb75609",
   "metadata": {},
   "source": [
    "For this tutorial, we will use a dataset of more than 6000 machine learning research papers and their corresponding metadata created using the [paperswithcode client](https://paperswithcode-client.readthedocs.io/en/latest/) (check [this gist](https://gist.github.com/soumik12345/996c2ea538f6ff5b3747078ba557ece4) for reference). The dataset is stored as a [Weave Dataset](../docs/guides/core-types/datasets.md) which you can explore [here](https://wandb.ai/geekyrakshit/arxiv-data-extraction/weave/objects/cv-papers/versions/7wICKJjt3YyqL3ssICHi08v3swAGSUtD7TF4PVRJ0yc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c6b400",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEAVE_DATASET_REFERENCE = \"weave:///geekyrakshit/arxiv-data-extraction/object/cv-papers:7wICKJjt3YyqL3ssICHi08v3swAGSUtD7TF4PVRJ0yc\"\n",
    "eval_dataset = weave.ref(WEAVE_DATASET_REFERENCE).get()\n",
    "\n",
    "rich.print(f\"{len(eval_dataset.rows)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3f344e",
   "metadata": {},
   "source": [
    "Now, we can evaluate our LLM workflow using [Weave Evalations](../docs/guides/core-types/evaluations.md), that will take each example, pass it through your application and score the output on multiple custom scoring functions. By doing this, you'll have a view of the performance of your application, and a rich UI to drill into individual outputs and scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a6e6a90b94e0b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-18T14:52:03.240596Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluation = weave.Evaluation(\n",
    "    name=\"baseline_workflow_evaluation\",\n",
    "    dataset=eval_dataset.rows[:5],\n",
    "    scorers=[arxiv_method_score],\n",
    ")\n",
    "await evaluation.evaluate(arxiv_parser_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3bb03a",
   "metadata": {},
   "source": [
    ":::warning\n",
    "Running the evaluation on 5 examples from evaluation dataset will cost approximately $0.25-$1.25 in OpenAI credits, depending on the number of attempts instructor needs makes to get the output in the desired format (which is set to 5) in evaluating each example.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53134c6",
   "metadata": {},
   "source": [
    "## Improving the LLM Workflow\n",
    "\n",
    "Let us try to improve the LLM workflow by adding some more instructions to our system prompt. We will provide the model with a set of rules, which act as a set of clues to guide the model to look for specific type of information in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be396767",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt += \"\"\"\n",
    "Here are some rules to follow:\n",
    "1. When looking for the main findings in the paper, you should look for the abstract.\n",
    "2. When looking for the explanations for the main findings, you should look for the introduction and methods section of\n",
    "    the paper.\n",
    "3. When looking for the list of existing methods used in the paper, first look at the citations, and then try explaining\n",
    "    how they were used in the paper.\n",
    "4. When looking for the list of machine learning methods used in the paper, first look at the citations, and then try\n",
    "    explaining how they were used in the paper.\n",
    "5. When looking for the evaluation metrics used in the paper, first look at the results section of the paper, and then\n",
    "    try explaining the observations made from the results. Pay special attention to the tables to find the metrics,\n",
    "    their values, the corresponding benchmark and the observation association with the result.\n",
    "6. If there are no github repositories associated with the paper, simply return \"None\".\n",
    "7. When looking for hardware and accelerators, pay special attentions to the quantity of each type of hardware and\n",
    "    accelerator. If there are no hardware or accelerators used in the paper, simply return \"None\".\n",
    "8. When looking for further research directions, look for the conclusion section of the paper.\n",
    "\"\"\"\n",
    "\n",
    "improved_arxiv_parser_model = ArxivModel(model=\"gpt-4o\", system_prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b0ab4d",
   "metadata": {},
   "source": [
    "We will not evaluate this improved workflow again and try to check if the accuracy has increased or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7845c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = weave.Evaluation(\n",
    "    name=\"improved_workflow_evaluation\",\n",
    "    dataset=eval_dataset.rows[:5],\n",
    "    scorers=[arxiv_method_score],\n",
    ")\n",
    "await evaluation.evaluate(arxiv_parser_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15be477",
   "metadata": {},
   "source": [
    ":::warning\n",
    "Running the evaluation on 5 examples from evaluation dataset will cost approximately $0.25-$1.25 in OpenAI credits, depending on the number of attempts instructor needs makes to get the output in the desired format (which is set to 5) in evaluating each example.\n",
    ":::\n",
    "\n",
    "| ![](https://i.imgur.com/qFbt8T0.png) |\n",
    "|---|\n",
    "| Here's how you can explore and compare the evaluations traces in the Weave UI |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069dfd51",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
