{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68f588c0",
   "metadata": {},
   "source": [
    "# Building and Evaluating LlamaIndex Agents with Query Engine Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d86ba5",
   "metadata": {},
   "source": [
    "You can install all the dependencies for this tutorial using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d3dec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install litellm llama-index-embeddings-google-genai llama-index-llms-google-genai llama-index weave -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881b234c",
   "metadata": {},
   "source": [
    "We’ll use a `.env` file to manage API keys securely. You can also set them manually as environment variables, but for this tutorial, we’ll go ahead with a `.env` setup.  \n",
    "\n",
    "Also include `.env` in your `.gitignore` to avoid accidentally exposing sensitive API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ba8d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ffee26",
   "metadata": {},
   "source": [
    "## Building Agents with Query Engine Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ae62c4",
   "metadata": {},
   "source": [
    "In LlamaIndex, an agent with query engine tools receives a user query and intelligently breaks it down to determine which query engines should handle the original query or specific sub-queries. With parallel tool calling enabled by default, the agent simultaneously sends queries or sub-queries to multiple relevant query engines, then synthesizes the results to provide a comprehensive response. The agent continues this process until the user's query is fully answered or comes to the conclusion that it cannot be answered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543a52c8",
   "metadata": {},
   "source": [
    "### Setting the LLM and Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42120562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.genai import types\n",
    "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "\n",
    "llm = GoogleGenAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    generation_config=types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=0)  # Disables thinking\n",
    "    ),\n",
    ")\n",
    "\n",
    "embed_model = GoogleGenAIEmbedding(model_name=\"text-embedding-004\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfb6d20",
   "metadata": {},
   "source": [
    "### Downloading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae50e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p 'data/10k/'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/lyft_2021.pdf' -O 'data/10k/lyft_2021.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993eda81",
   "metadata": {},
   "source": [
    "### Setting the Vector Database and the Indexing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7234470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "\n",
    "\n",
    "# load data\n",
    "lyft_docs = SimpleDirectoryReader(input_files=[\"./data/10k/lyft_2021.pdf\"]).load_data()\n",
    "uber_docs = SimpleDirectoryReader(input_files=[\"./data/10k/uber_2021.pdf\"]).load_data()\n",
    "\n",
    "# build index\n",
    "lyft_index = VectorStoreIndex.from_documents(lyft_docs, embed_model=embed_model)\n",
    "uber_index = VectorStoreIndex.from_documents(uber_docs, embed_model=embed_model)\n",
    "\n",
    "# persist index\n",
    "lyft_index.storage_context.persist(persist_dir=\"./storage/lyft\")\n",
    "uber_index.storage_context.persist(persist_dir=\"./storage/uber\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a76bb40",
   "metadata": {},
   "source": [
    "### Setting up the Query Engine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10ed987",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyft_engine = lyft_index.as_query_engine(llm=llm, similarity_top_k=3)\n",
    "uber_engine = uber_index.as_query_engine(llm=llm, similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe89bdf",
   "metadata": {},
   "source": [
    "### Making Query Engine Tools for the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a1112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool.from_defaults(\n",
    "        query_engine=lyft_engine,\n",
    "        name=\"lyft_10k\",\n",
    "        description=(\n",
    "            \"Provides information about Lyft financials for year 2021. \"\n",
    "            \"Use a detailed plain text question as input to the tool.\"\n",
    "        ),\n",
    "    ),\n",
    "    QueryEngineTool.from_defaults(\n",
    "        query_engine=uber_engine,\n",
    "        name=\"uber_10k\",\n",
    "        description=(\n",
    "            \"Provides information about Uber financials for year 2021. \"\n",
    "            \"Use a detailed plain text question as input to the tool.\"\n",
    "        ),\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed03ce5",
   "metadata": {},
   "source": [
    "### Setting up the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53e981d",
   "metadata": {},
   "source": [
    "In this step we will provide the Agent with Large Language Model(LLM) which will be responsible for the making the decisions and tools which will provide it capabilities do actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27d258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "\n",
    "agent = FunctionAgent(tools=query_engine_tools, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a460f4f5",
   "metadata": {},
   "source": [
    "### Try It Out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e2b7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import AgentStream, ToolCallResult\n",
    "\n",
    "handler = agent.run(\"What's the revenue for Lyft in 2021 vs Uber?\")\n",
    "\n",
    "async for ev in handler.stream_events():\n",
    "    if isinstance(ev, ToolCallResult):\n",
    "        print(\n",
    "            f\"Call {ev.tool_name} with args {ev.tool_kwargs}\\nReturned: {ev.tool_output}\"\n",
    "        )\n",
    "    elif isinstance(ev, AgentStream):\n",
    "        print(ev.delta, end=\"\", flush=True)\n",
    "\n",
    "response = await handler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21475b2",
   "metadata": {},
   "source": [
    "## Evaluating the Agent with Wandb weave"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877382c0",
   "metadata": {},
   "source": [
    "When using Weave for evaluation, you need three main components:\n",
    "\n",
    "1. **Dataset**: A collection of queries or inputs you want to evaluate your application on.  \n",
    "\n",
    "2.\t**Model**: This is an abstraction that represents the application you want to evaluate. It’s not a literal machine learning model, but a wrapper provided by Weave that defines how your application handles input and produces output.  \n",
    "\n",
    "3. **Scorers**: These are the metrics or scoring functions that assess how well your application performs on the dataset. For example, they might check correctness, retrieval quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1a82c8",
   "metadata": {},
   "source": [
    "### Initializing the Project and Creating the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cf85c5",
   "metadata": {},
   "source": [
    "**Agentic RAG Evaluation: Two Types**\n",
    "\n",
    "There are two main evaluation approaches for Agentic RAG systems:\n",
    "\n",
    "1. **Reference-Based** - Uses queries with golden/ground truth answers to measure correctness\n",
    "2. **Reference-Free** - Uses only queries without ground truth, relying on heuristics or LLM judgment to assess quality\n",
    "\n",
    "\n",
    "We'll demonstrate both approaches by creating two datasets: one with ground truth answers for reference-based evaluation, and one with queries only for reference-free evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d08800",
   "metadata": {},
   "source": [
    "#### Creating Reference-Based Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7561e4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "from weave import Dataset\n",
    "\n",
    "weave.init(project_name=\"llama_index_evaluations\")\n",
    "\n",
    "eval_dataset_with_reference = Dataset(\n",
    "    name=\"agentic-rag-evaluation-dataset-with-reference\",\n",
    "    rows=[\n",
    "        {\n",
    "            \"id\": \"0\",\n",
    "            \"query\": \"What's the revenue for Lyft in 2021 vs Uber?\",\n",
    "            \"reference\": \"In 2021, Lyft's revenue was $3,208,323,000 and Uber's revenue was $17,455 million.\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "weave.publish(eval_dataset_with_reference)\n",
    "dataset_ref = weave.ref(\"agentic-rag-evaluation-dataset-with-reference\").get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50230d47",
   "metadata": {},
   "source": [
    "#### Creating Reference-Free Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687c4959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "from weave import Dataset\n",
    "\n",
    "weave.init(project_name=\"llama_index_evaluations\")\n",
    "\n",
    "eval_dataset_without_reference = Dataset(\n",
    "    name=\"agentic-rag-evaluation-dataset-without-reference\",\n",
    "    rows=[\n",
    "        {\"id\": \"0\", \"query\": \"What's the revenue for Lyft in 2021 vs Uber?\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "weave.publish(eval_dataset_without_reference)\n",
    "dataset_ref = weave.ref(\"agentic-rag-evaluation-dataset-without-reference\").get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a07589",
   "metadata": {},
   "source": [
    "### Setting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76282e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from llama_index.core.agent.workflow import AgentOutput\n",
    "\n",
    "import weave\n",
    "\n",
    "\n",
    "class LlamaIndexAgenticRAG(weave.Model):\n",
    "    @weave.op()\n",
    "    async def predict(self, query: str) -> AgentOutput:\n",
    "        agent = FunctionAgent(tools=query_engine_tools, llm=llm)\n",
    "        handler = agent.run(query)\n",
    "        response = asyncio.run(handler)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa070d45",
   "metadata": {},
   "source": [
    "### Defining the Scorers(without reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5382b42b",
   "metadata": {},
   "source": [
    "When evaluating RAG or Agentic RAG applications, we focus on two core components. The first is retrieval, which measures how effectively the system retrieves relevant information from the knowledge-base based on the input query. The second is generation, which evaluates how well the system generates an answer using the retrieved context.\n",
    "\n",
    "**Defining Scorers**\n",
    "\n",
    "To evaluate both components, we will define scorers to assess both the retrieval and generation process. In this tutorial we'll use two built-in scorers to do the evaluation:\n",
    "\n",
    "- Retrieval Evaluation\n",
    "\n",
    "    [ContextRelevancyScorer](): Measures how relevant the retrieved context is to the input query. Returns a score between 0 and 1, with higher scores indicating better relevance.\n",
    "\n",
    "- Generation Evaluation\n",
    "\n",
    "    [HallucinationFreeScorer](): Checks if the generated response contains hallucinated information by determining whether the answer is faithful to the retrieved context without adding unsupported information.\n",
    "\n",
    "\n",
    "**Evaluation Modes:** We'll perform this evaluation in both reference-free mode (no ground truth answer provided) and reference-based mode (a ground truth answer is available). We will start with the reference-free evaluation and then move on to the reference-based setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89552d92",
   "metadata": {},
   "source": [
    "#### Evaluating Agentic Systems with Multiple RAG Calls\n",
    "\n",
    "In agentic systems, a single user query often breaks down into multiple sub-queries, each triggering its own RAG operation.\n",
    "\n",
    "To evaluate these systems properly, we need to evaluate each individual RAG call and then aggregate those evaluations into a single overall score for the original query.\n",
    "\n",
    "**Wrapper Scorer Approach**\n",
    "\n",
    "We use a wrapper scorer that takes a base scorer (like hallucination or context relevance) and applies it to every RAG call made by the agent. It then combines all results to provide one comprehensive evaluation score for the entire query.\n",
    "\n",
    "This ensures we evaluate every step of the agent's multi-step process, giving us a complete picture of its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec5af22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import weave\n",
    "from weave import Scorer\n",
    "from weave.scorers import ContextRelevancyScorer, HallucinationFreeScorer\n",
    "\n",
    "\n",
    "class AgenticHallucinationFreeScorer(Scorer):\n",
    "    base_scorer: Scorer = HallucinationFreeScorer(model_id=\"gemini/gemini-2.5-flash\")\n",
    "\n",
    "    @weave.op\n",
    "    async def score(self, output: AgentOutput) -> dict:\n",
    "        tool_calls = output.tool_calls\n",
    "        tool_calls = [\n",
    "            {\n",
    "                \"query\": tool_call.tool_kwargs[\"input\"],\n",
    "                \"output\": tool_call.tool_output.content,\n",
    "                \"context\": [\n",
    "                    node.text for node in tool_call.tool_output.raw_output.source_nodes\n",
    "                ],\n",
    "            }\n",
    "            for tool_call in tool_calls\n",
    "        ]\n",
    "\n",
    "        scores = await asyncio.gather(\n",
    "            *[\n",
    "                self.base_scorer.score(\n",
    "                    output=tool_call[\"output\"], context=\"\\n\".join(tool_call[\"context\"])\n",
    "                )\n",
    "                for tool_call in tool_calls\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        final_score = np.mean([score[\"has_hallucination\"] for score in scores])\n",
    "        return {\"hallucination_free_score\": final_score}\n",
    "\n",
    "\n",
    "class AgenticContextRelevancyScorer(Scorer):\n",
    "    base_scorer: Scorer = ContextRelevancyScorer(model_id=\"gemini/gemini-2.5-flash\")\n",
    "\n",
    "    @weave.op\n",
    "    async def score(self, output: AgentOutput) -> dict:\n",
    "        tool_calls = output.tool_calls\n",
    "        tool_calls = [\n",
    "            {\n",
    "                \"query\": tool_call.tool_kwargs[\"input\"],\n",
    "                \"output\": tool_call.tool_output.content,\n",
    "                \"context\": [\n",
    "                    node.text for node in tool_call.tool_output.raw_output.source_nodes\n",
    "                ],\n",
    "            }\n",
    "            for tool_call in tool_calls\n",
    "        ]\n",
    "\n",
    "        scores = await asyncio.gather(\n",
    "            *[\n",
    "                self.base_scorer.score(\n",
    "                    output=tool_call[\"output\"], context=\"\\n\".join(tool_call[\"context\"])\n",
    "                )\n",
    "                for tool_call in tool_calls\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        final_score = np.mean([score[\"relevancy_score\"] for score in scores])\n",
    "        return {\"relevancy_score\": final_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacd929b",
   "metadata": {},
   "source": [
    "### Performing Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79792f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "halucination_free_scorer = AgenticHallucinationFreeScorer()\n",
    "context_relevancy_scorer = AgenticContextRelevancyScorer()\n",
    "\n",
    "evaluation_without_reference = weave.Evaluation(\n",
    "    dataset=eval_dataset_without_reference,\n",
    "    scorers=[context_relevancy_scorer, halucination_free_scorer],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeae9311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "model = LlamaIndexAgenticRAG()\n",
    "result = asyncio.run(evaluation_without_reference.evaluate(model))\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eeb2c9",
   "metadata": {},
   "source": [
    "### Defining the Scorers(with reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a122c41",
   "metadata": {},
   "source": [
    "Now that we’ve covered reference-free evaluation, we’ll move on to reference-based evaluation, where each query includes a ground truth answer.\n",
    "\n",
    "In this setup, we can define custom scorers that compare the generated response against the expected (reference) response. These scorers use both the input query, the retrieved context, and the ground truth answer to evaluate the quality of the generation.\n",
    "\n",
    "Defining Custom Scorers\n",
    "\n",
    "For reference-based evaluation, we’ll define a custom metric that uses the ground truth to assess how closely the generated response matches the expected answer. This allows us to measure response accuracy, completeness, or any other domain-specific quality using direct comparison.\n",
    "\n",
    "In the following section, we will define and apply this custom scorer to perform reference-based evaluations on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6dee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "from typing import Any\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from weave.scorers.scorer_types import LLMScorer\n",
    "\n",
    "\n",
    "class ContextPrecisionResponse(BaseModel):\n",
    "    reason: str = Field(\n",
    "        description=\"Step-by-step reasoning about whether the retrieved context was useful in arriving at the given ground truth answer\"\n",
    "    )\n",
    "    score: int = Field(\n",
    "        description=\"Binary score indicating if the context was useful in producing the answer (1 for useful, 0 for not useful)\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ContextPrecisionWithReferenceScorer(LLMScorer):\n",
    "    name: str = \"context_precision_with_reference\"\n",
    "    prompt_template: str = dedent(\n",
    "        \"\"\"\n",
    "    You are given a question, a retrieved context, and the correct (ground truth) answer.\n",
    "\n",
    "    Your task is to evaluate whether the retrieved context was useful in arriving at the given answer.\n",
    "\n",
    "    - If the context includes information that directly supports or helps generate the answer, return a score of 1.\n",
    "    - If the context is unrelated or not helpful in generating the answer, return a score of 0.\n",
    "\n",
    "    Think step by step and provide a reason for your decision.\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Answer: {answer}\n",
    "\n",
    "    Reasoning:\n",
    "    <your reasoning here>\n",
    "\n",
    "    Final Score (0 or 1):\n",
    "    \"\"\"\n",
    "    )\n",
    "    model_id: str = \"gemini/gemini-2.0-flash\"\n",
    "\n",
    "    @weave.op\n",
    "    async def score(\n",
    "        self, *, output: AgentOutput, query: str, reference: str, **kwargs: Any\n",
    "    ) -> dict:\n",
    "        tool_calls = output.tool_calls\n",
    "        contexts = [\n",
    "            node.text\n",
    "            for tool_call in tool_calls\n",
    "            for node in tool_call.tool_output.raw_output.source_nodes\n",
    "        ]\n",
    "        prompt = self.prompt_template.format(\n",
    "            question=query, context=\"\\n\".join(contexts), answer=reference\n",
    "        )\n",
    "        response = await self._acompletion(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            response_format=ContextPrecisionResponse,\n",
    "            model=self.model_id,\n",
    "        )\n",
    "        response = ContextPrecisionResponse.model_validate_json(\n",
    "            response.choices[0].message.content\n",
    "        )\n",
    "        return response.model_dump()\n",
    "\n",
    "\n",
    "class AnswerCorrectnessResponse(BaseModel):\n",
    "    reason: str = Field(\n",
    "        description=\"Step-by-step reasoning about whether the generated answer covers all the key points in the reference answer\"\n",
    "    )\n",
    "    score: int = Field(\n",
    "        description=\"Binary score: 1 if the generated answer fully captures all factual details from the reference answer, else 0\"\n",
    "    )\n",
    "\n",
    "\n",
    "class AnswerCorrectnessScorer(LLMScorer):\n",
    "    name: str = \"answer_correctness\"\n",
    "    prompt_template: str = dedent(\n",
    "        \"\"\"\n",
    "    You are given a question, a generated answer, and the reference (ground truth) answer.\n",
    "\n",
    "    Your task is to decide whether the generated answer includes **all the key factual points** from the reference answer.\n",
    "\n",
    "    - If it fully matches the reference in meaning and completeness, return 1.\n",
    "    - If anything is missing, inaccurate, or not supported by the reference, return 0.\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Generated Answer: {generated_answer}\n",
    "\n",
    "    Reference Answer: {reference_answer}\n",
    "\n",
    "    Reasoning:\n",
    "    <your step-by-step reasoning here>\n",
    "\n",
    "    Final Score (0 or 1):\n",
    "    \"\"\"\n",
    "    )\n",
    "    model_id: str = \"gemini/gemini-2.0-flash\"\n",
    "\n",
    "    @weave.op\n",
    "    async def score(\n",
    "        self, *, output: AgentOutput, query: str, reference: str, **kwargs: Any\n",
    "    ) -> dict:\n",
    "        generated_answer = output.response.blocks[0].text\n",
    "        prompt = self.prompt_template.format(\n",
    "            question=query,\n",
    "            generated_answer=generated_answer,\n",
    "            reference_answer=reference,\n",
    "        )\n",
    "        response = await self._acompletion(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            response_format=AnswerCorrectnessResponse,\n",
    "            model=self.model_id,\n",
    "        )\n",
    "        response = AnswerCorrectnessResponse.model_validate_json(\n",
    "            response.choices[0].message.content\n",
    "        )\n",
    "        return response.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add598dc",
   "metadata": {},
   "source": [
    "### Performing Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f07a975",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_precision_scorer = ContextPrecisionWithReferenceScorer()\n",
    "answer_correctness_scorer = AnswerCorrectnessScorer()\n",
    "\n",
    "evaluation_with_reference = weave.Evaluation(\n",
    "    dataset=eval_dataset_with_reference,\n",
    "    scorers=[context_precision_scorer, answer_correctness_scorer],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb3ceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "model = LlamaIndexAgenticRAG()\n",
    "result = asyncio.run(evaluation_with_reference.evaluate(model))\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e351de",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eacf19",
   "metadata": {},
   "source": [
    "Now that you have a better understanding of how to evaluate your RAG agents, you can take this further by trying to evaluate web search agents as well. At a high level, while web search is not exactly the same as RAG, the evaluation approach can be quite similar especially in reference-free mode, where you don’t have a ground truth answer.\n",
    "\n",
    "With this understanding of RAG agent evaluation, you can apply similar approaches to web search agents. While web search differs from RAG, the evaluation methods are quite comparable, especially in reference-free scenarios where ground truth answers aren't available.\n",
    "\n",
    "You can use the same evaluation metrics or create custom evaluators tailored to your specific needs. This flexible approach allows you to assess various types of agentic and retrieval-based systems effectively.\n",
    "\n",
    "We hope this tutorial helps you explore and experiment with different evaluation techniques for your own applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
