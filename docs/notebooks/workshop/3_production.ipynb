{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01f67432",
   "metadata": {},
   "source": [
    "# üêù Weave Workshop: Build, Track, and Evaluate LLM Applications\n",
    "\n",
    "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "\n",
    "Welcome to the Weave workshop! In this hands-on session, you'll learn how to use Weave to develop, debug, and evaluate AI-powered applications.\n",
    "\n",
    "**What you'll learn:**\n",
    "- üîç **Trace & Debug**: Track every LLM call, see inputs/outputs, and debug issues\n",
    "- üìä **Evaluate**: Build rigorous evaluations with multiple scoring functions\n",
    "- üèÉ **Compare**: Run A/B tests and compare different approaches\n",
    "- üìà **Monitor**: Track costs, latency, and performance metrics\n",
    "- üéØ **Iterate**: Use data-driven insights to improve your application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434ca60e",
   "metadata": {},
   "source": [
    "## üîë Prerequisites\n",
    "\n",
    "Before we begin, let's set up your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf2c751",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install wandb weave openai pydantic nest_asyncio -qqq\n",
    "\n",
    "import asyncio\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "from getpass import getpass\n",
    "from typing import Any, Optional\n",
    "\n",
    "import weave\n",
    "from weave import Scorer\n",
    "\n",
    "# üîë Setup your API keys\n",
    "print(\"üìù Setting up API keys...\")\n",
    "\n",
    "# Weights & Biases will automatically prompt if needed\n",
    "# It checks: 1) WANDB_API_KEY env var, 2) ~/.netrc, 3) prompts user\n",
    "print(\"‚úÖ W&B authentication will be handled automatically by Weave\")\n",
    "print(\"   (Optional: You can set WANDB_API_KEY env variable if you prefer)\")\n",
    "\n",
    "# OpenAI requires manual setup\n",
    "print(\"\\nü§ñ OpenAI Setup:\")\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    print(\n",
    "        \"You can generate your OpenAI API key here: https://platform.openai.com/api-keys\"\n",
    "    )\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
    "else:\n",
    "    print(\"‚úÖ OpenAI API key found in environment\")\n",
    "\n",
    "print(\"\\n---\")\n",
    "\n",
    "# üè† Initialize your W&B project\n",
    "print(\"üêù Initializing Weave...\")\n",
    "weave_client = weave.init(\"weave-workshop\")  # üêù Your W&B project name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a9db18",
   "metadata": {},
   "source": [
    "## üéØ Part 3: Production Monitoring with Scorers\n",
    "\n",
    "Use Weave's scorer system for real-time guardrails and quality monitoring.\n",
    "This demonstrates the apply_scorer pattern for production use.\n",
    "\n",
    "**Key Concepts**:\n",
    "- **Guardrails**: Block or modify responses (e.g., toxicity filter)\n",
    "- **Monitors**: Track quality metrics without blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75813bb1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Define more realistic production scorers\n",
    "class ContentModerationScorer(Scorer):\n",
    "    \"\"\"Production-ready content moderation scorer.\"\"\"\n",
    "\n",
    "    @weave.op\n",
    "    def score(self, output: dict) -> dict:\n",
    "        \"\"\"Check for inappropriate content using multiple signals.\"\"\"\n",
    "        # Handle both success and error cases\n",
    "        if output.get(\"status\") != \"success\":\n",
    "            return {\"flagged\": False, \"flags\": [], \"severity\": \"none\", \"action\": \"pass\"}\n",
    "\n",
    "        analysis = output.get(\"analysis\", {})\n",
    "        issue_text = analysis.get(\"issue\", \"\").lower()\n",
    "        sentiment = analysis.get(\"sentiment\", \"neutral\")\n",
    "\n",
    "        # Check for various inappropriate content patterns\n",
    "        profanity_patterns = [\n",
    "            \"stupid\",\n",
    "            \"idiotic\",\n",
    "            \"garbage\",\n",
    "            \"trash\",\n",
    "            \"sucks\",\n",
    "            \"terrible\",\n",
    "            \"awful\",\n",
    "            \"worst\",\n",
    "        ]\n",
    "        threat_patterns = [\"sue\", \"lawyer\", \"legal action\", \"court\", \"lawsuit\"]\n",
    "\n",
    "        flags = []\n",
    "        severity = \"none\"\n",
    "\n",
    "        # Check profanity\n",
    "        profanity_found = []\n",
    "        for word in profanity_patterns:\n",
    "            if word in issue_text:\n",
    "                profanity_found.append(word)\n",
    "\n",
    "        if profanity_found:\n",
    "            flags.append(f\"Profanity detected: {', '.join(profanity_found)}\")\n",
    "            severity = \"medium\"\n",
    "\n",
    "        # Check threats\n",
    "        threats_found = []\n",
    "        for pattern in threat_patterns:\n",
    "            if pattern in issue_text:\n",
    "                threats_found.append(pattern)\n",
    "\n",
    "        if threats_found:\n",
    "            flags.append(f\"Legal threat: {', '.join(threats_found)}\")\n",
    "            severity = \"high\"\n",
    "\n",
    "        # Check extreme sentiment with profanity\n",
    "        if sentiment == \"negative\" and profanity_found:\n",
    "            severity = \"high\"\n",
    "            flags.append(\"Negative sentiment with profanity\")\n",
    "\n",
    "        return {\n",
    "            \"flagged\": len(flags) > 0,\n",
    "            \"flags\": flags,\n",
    "            \"severity\": severity,\n",
    "            \"action\": \"block\"\n",
    "            if severity == \"high\"\n",
    "            else (\"review\" if severity == \"medium\" else \"pass\"),\n",
    "        }\n",
    "\n",
    "\n",
    "class ExtractionQualityScorer(Scorer):\n",
    "    \"\"\"Monitor extraction quality and completeness.\"\"\"\n",
    "\n",
    "    @weave.op\n",
    "    def score(self, output: dict, email: str) -> dict:\n",
    "        \"\"\"Comprehensive quality assessment.\"\"\"\n",
    "        if output.get(\"status\") != \"success\":\n",
    "            return {\n",
    "                \"quality_score\": 0.0,\n",
    "                \"passed\": False,\n",
    "                \"issues\": [\"Failed to process email\"],\n",
    "                \"recommendations\": [],\n",
    "                \"extraction_grade\": \"F\",\n",
    "            }\n",
    "\n",
    "        analysis = output.get(\"analysis\", {})\n",
    "        quality_metrics = {\n",
    "            \"completeness\": 0.0,\n",
    "            \"specificity\": 0.0,\n",
    "            \"accuracy\": 0.0,\n",
    "            \"consistency\": 0.0,\n",
    "        }\n",
    "        issues = []\n",
    "        recommendations = []\n",
    "\n",
    "        # 1. Completeness checks (40% weight)\n",
    "        if analysis.get(\"customer_name\") and analysis[\"customer_name\"] not in [\n",
    "            \"Unknown\",\n",
    "            \"\",\n",
    "            None,\n",
    "        ]:\n",
    "            quality_metrics[\"completeness\"] += 0.15\n",
    "        else:\n",
    "            issues.append(\"Missing customer name\")\n",
    "            recommendations.append(\"Check email signatures and greetings for names\")\n",
    "\n",
    "        if analysis.get(\"product\") and analysis[\"product\"] not in [\"Unknown\", \"\", None]:\n",
    "            quality_metrics[\"completeness\"] += 0.15\n",
    "        else:\n",
    "            issues.append(\"Missing product identification\")\n",
    "            recommendations.append(\"Look for product names mentioned in the email\")\n",
    "\n",
    "        if analysis.get(\"issue\") and len(analysis[\"issue\"]) > 10:\n",
    "            quality_metrics[\"completeness\"] += 0.10\n",
    "        else:\n",
    "            issues.append(\"Issue description too brief or missing\")\n",
    "            recommendations.append(\"Extract a more detailed problem description\")\n",
    "\n",
    "        # 2. Specificity checks (30% weight)\n",
    "        product_name = analysis.get(\"product\", \"\")\n",
    "        if product_name and any(char.isdigit() for char in str(product_name)):\n",
    "            # Product includes version/model number\n",
    "            quality_metrics[\"specificity\"] += 0.15\n",
    "        elif product_name:\n",
    "            recommendations.append(\n",
    "                \"Extract product version/model numbers when available\"\n",
    "            )\n",
    "\n",
    "        issue_desc = analysis.get(\"issue\", \"\")\n",
    "        if issue_desc and len(str(issue_desc)) > 30:\n",
    "            quality_metrics[\"specificity\"] += 0.15\n",
    "        elif issue_desc:\n",
    "            recommendations.append(\"Provide more specific issue details\")\n",
    "\n",
    "        # 3. Accuracy checks (20% weight)\n",
    "        # Check if extracted content actually appears in email\n",
    "        email_lower = email.lower()\n",
    "        customer_name = analysis.get(\"customer_name\", \"\")\n",
    "        if customer_name and customer_name != \"Unknown\":\n",
    "            name_parts = customer_name.lower().split()\n",
    "            # Check if at least part of the name appears in email\n",
    "            if any(part in email_lower for part in name_parts if len(part) > 2):\n",
    "                quality_metrics[\"accuracy\"] += 0.10\n",
    "            else:\n",
    "                issues.append(\"Extracted name not found in original email\")\n",
    "\n",
    "        product_mentioned = analysis.get(\"product\", \"\")\n",
    "        if product_mentioned and product_mentioned != \"Unknown\":\n",
    "            # Check for partial matches (product names might be extracted differently)\n",
    "            product_words = product_mentioned.lower().split()\n",
    "            if any(word in email_lower for word in product_words if len(word) > 3):\n",
    "                quality_metrics[\"accuracy\"] += 0.10\n",
    "            else:\n",
    "                issues.append(\"Extracted product not clearly mentioned in email\")\n",
    "\n",
    "        # 4. Consistency checks (10% weight)\n",
    "        sentiment = analysis.get(\"sentiment\", \"neutral\")\n",
    "        urgency = output.get(\"urgency\", \"low\")\n",
    "\n",
    "        # Check sentiment/urgency consistency\n",
    "        consistency_ok = True\n",
    "        if sentiment == \"negative\" and urgency == \"low\":\n",
    "            if not any(\n",
    "                word in issue_desc.lower() for word in [\"minor\", \"small\", \"slight\"]\n",
    "            ):\n",
    "                consistency_ok = False\n",
    "                issues.append(\n",
    "                    \"Negative sentiment but low urgency - might be inconsistent\"\n",
    "                )\n",
    "        elif sentiment == \"positive\" and urgency == \"high\":\n",
    "            consistency_ok = False\n",
    "            issues.append(\"Positive sentiment with high urgency is unusual\")\n",
    "\n",
    "        if consistency_ok:\n",
    "            quality_metrics[\"consistency\"] += 0.10\n",
    "\n",
    "        # Calculate overall score\n",
    "        total_score = sum(quality_metrics.values())\n",
    "\n",
    "        return {\n",
    "            \"quality_score\": total_score,\n",
    "            \"quality_metrics\": quality_metrics,\n",
    "            \"passed\": total_score >= 0.6,  # Lowered threshold for demo\n",
    "            \"issues\": issues,\n",
    "            \"recommendations\": recommendations,\n",
    "            \"extraction_grade\": \"A\"\n",
    "            if total_score >= 0.9\n",
    "            else (\n",
    "                \"B\"\n",
    "                if total_score >= 0.8\n",
    "                else (\n",
    "                    \"C\" if total_score >= 0.6 else (\"D\" if total_score >= 0.4 else \"F\")\n",
    "                )\n",
    "            ),\n",
    "        }\n",
    "\n",
    "\n",
    "@weave.op\n",
    "def production_email_handler(\n",
    "    email: str, request_id: Optional[str] = None\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"Production-ready email handler that returns structured analysis results.\"\"\"\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # Generate request ID if not provided\n",
    "    if not request_id:\n",
    "        request_id = f\"req_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{random.randint(1000, 9999)}\"\n",
    "\n",
    "    try:\n",
    "        # Process the email using our existing analyzer\n",
    "        analysis = analyze_customer_email(email)\n",
    "\n",
    "        # Calculate urgency based on the analysis\n",
    "        urgency = classify_urgency(email, analysis.sentiment)\n",
    "\n",
    "        # Return structured result that scorers expect\n",
    "        return {\n",
    "            \"request_id\": request_id,\n",
    "            \"status\": \"success\",\n",
    "            \"analysis\": {\n",
    "                \"customer_name\": analysis.customer_name,\n",
    "                \"product\": analysis.product,\n",
    "                \"issue\": analysis.issue,\n",
    "                \"sentiment\": analysis.sentiment,\n",
    "            },\n",
    "            \"urgency\": urgency,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log error and return error response\n",
    "        return {\n",
    "            \"request_id\": request_id,\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e),\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "        }\n",
    "\n",
    "\n",
    "# Initialize scorers\n",
    "content_moderation_scorer = ContentModerationScorer()\n",
    "quality_scorer = ExtractionQualityScorer()\n",
    "\n",
    "\n",
    "async def handle_email_with_monitoring(email: str) -> dict[str, Any]:\n",
    "    \"\"\"Handle email with production monitoring and guardrails.\"\"\"\n",
    "    # Process the email and get the Call object\n",
    "    result, call = production_email_handler.call(email)\n",
    "\n",
    "    if result[\"status\"] == \"success\":\n",
    "        # Apply content moderation (guardrail)\n",
    "        moderation_check = await call.apply_scorer(content_moderation_scorer)\n",
    "\n",
    "        # Apply quality monitoring\n",
    "        quality_check = await call.apply_scorer(\n",
    "            quality_scorer, additional_scorer_kwargs={\"email\": email}\n",
    "        )\n",
    "\n",
    "        # Handle moderation results\n",
    "        if moderation_check.result[\"flagged\"]:\n",
    "            action = moderation_check.result[\"action\"]\n",
    "            if action == \"block\":\n",
    "                print(f\"üö´ Content BLOCKED: {moderation_check.result['flags']}\")\n",
    "                result[\"blocked\"] = True\n",
    "                result[\"block_reason\"] = moderation_check.result[\"flags\"]\n",
    "            elif action == \"review\":\n",
    "                print(\n",
    "                    f\"‚ö†Ô∏è Content flagged for review: {moderation_check.result['flags']}\"\n",
    "                )\n",
    "                result[\"needs_review\"] = True\n",
    "                result[\"review_reason\"] = moderation_check.result[\"flags\"]\n",
    "\n",
    "        # Add quality metrics\n",
    "        result[\"quality_metrics\"] = {\n",
    "            \"grade\": quality_check.result[\"extraction_grade\"],\n",
    "            \"score\": quality_check.result[\"quality_score\"],\n",
    "            \"passed\": quality_check.result[\"passed\"],\n",
    "        }\n",
    "\n",
    "        if quality_check.result[\"issues\"]:\n",
    "            print(f\"üìä Quality issues: {quality_check.result['issues']}\")\n",
    "\n",
    "        if quality_check.result[\"recommendations\"]:\n",
    "            print(f\"üí° Recommendations: {quality_check.result['recommendations']}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Test with varied examples showing both success and failure cases\n",
    "print(\"üè≠ Testing production monitoring with realistic scenarios...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "production_test_emails = [\n",
    "    # Good quality extraction - should pass all checks\n",
    "    {\n",
    "        \"email\": \"Hello Support Team,\\n\\nI'm Sarah Mitchell from Acme Corp. Our CloudSync Enterprise v3.2.1 stopped syncing files yesterday at 2pm. The error message says 'Authentication failed'. This is really frustrating and affecting our entire team.\\n\\nBest regards,\\nSarah Mitchell\\nIT Manager, Acme Corp\",\n",
    "        \"expected\": \"‚úÖ High quality extraction with version numbers\",\n",
    "    },\n",
    "    # Profanity with legal threat - should be blocked\n",
    "    {\n",
    "        \"email\": \"This stupid software is absolute garbage! I'm John Davis and your DataSync Pro is the worst trash I've ever used. My lawyer will be contacting you about this terrible product that lost our data!\",\n",
    "        \"expected\": \"üö´ Should be blocked - profanity + legal threat\",\n",
    "    },\n",
    "    # Poor quality but processable - low score but not blocked\n",
    "    {\n",
    "        \"email\": \"Hi support, product broken. Fix please. - Tom\",\n",
    "        \"expected\": \"üìä Low quality - minimal details but processable\",\n",
    "    },\n",
    "    # Good extraction with negative sentiment - quality pass\n",
    "    {\n",
    "        \"email\": \"Dear Support,\\n\\nI'm Mary Johnson, CTO at TechStart Inc. Our DataVault Pro v2.5 backup failed last night with error code 'E501: connection timeout'. This is concerning as we rely on nightly backups for compliance.\\n\\nMary Johnson\\nCTO, TechStart Inc\",\n",
    "        \"expected\": \"‚úÖ Good quality despite negative sentiment\",\n",
    "    },\n",
    "    # Needs review - mild profanity - should flag for review\n",
    "    {\n",
    "        \"email\": \"Mike Wilson here. Your EmailPro system really sucks compared to what was promised, but I guess it's still better than the competition. Can you help me configure the spam filter? It's blocking legitimate emails.\",\n",
    "        \"expected\": \"‚ö†Ô∏è Should flag for review - mild profanity\",\n",
    "    },\n",
    "    # Excellent quality - should get high scores\n",
    "    {\n",
    "        \"email\": \"Hi there,\\n\\nI'm Lisa Chen from GlobalTech Solutions. I wanted to thank you for the excellent support on our CloudBackup Enterprise v4.2 deployment. Everything is working perfectly and the performance improvements are fantastic!\\n\\nBest,\\nLisa Chen\\nVP of Engineering\",\n",
    "        \"expected\": \"‚úÖ Excellent quality with positive sentiment\",\n",
    "    },\n",
    "    # Missing critical info - should fail quality check\n",
    "    {\n",
    "        \"email\": \"Your system crashed and we lost everything! This is unacceptable! Fix this immediately!!!\",\n",
    "        \"expected\": \"‚ùå Should fail quality - missing customer/product info\",\n",
    "    },\n",
    "    # Edge case - urgent but positive\n",
    "    {\n",
    "        \"email\": \"Urgent: I'm Alex Kumar and I love your RapidDeploy tool! Need to purchase 50 more licenses ASAP for our new team starting Monday. Please expedite!\\n\\nAlex Kumar\\nProcurement Manager\",\n",
    "        \"expected\": \"üìä Unusual case - urgent but positive sentiment\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for i, test_case in enumerate(production_test_emails):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìß Test {i+1}/8: {test_case['expected']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Show email preview\n",
    "    email_lines = test_case[\"email\"].split(\"\\n\")\n",
    "    print(\"üìù Email Content:\")\n",
    "    for line in email_lines[:3]:  # Show first 3 lines\n",
    "        if line.strip():\n",
    "            print(f\"   {line[:70]}{'...' if len(line) > 70 else ''}\")\n",
    "    if len(email_lines) > 3:\n",
    "        print(f\"   ... ({len(email_lines)-3} more lines)\")\n",
    "\n",
    "    # Process with monitoring\n",
    "    result = asyncio.run(handle_email_with_monitoring(test_case[\"email\"]))\n",
    "\n",
    "    # Show extraction results\n",
    "    print(\"\\nüîç Extraction Results:\")\n",
    "    if result[\"status\"] == \"success\":\n",
    "        analysis = result[\"analysis\"]\n",
    "        print(f\"   Customer: {analysis.get('customer_name', 'Unknown')}\")\n",
    "        print(f\"   Product: {analysis.get('product', 'Unknown')}\")\n",
    "        print(\n",
    "            f\"   Issue: {analysis.get('issue', 'Unknown')[:50]}{'...' if len(analysis.get('issue', '')) > 50 else ''}\"\n",
    "        )\n",
    "        print(f\"   Sentiment: {analysis.get('sentiment', 'Unknown')}\")\n",
    "        print(f\"   Urgency: {result.get('urgency', 'Unknown')}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Error: {result.get('error', 'Unknown error')}\")\n",
    "\n",
    "    # Show scorer results\n",
    "    print(\"\\nüìä Scorer Results:\")\n",
    "\n",
    "    # 1. Performance\n",
    "    perf = result.get(\"performance\", {})\n",
    "    print(\n",
    "        f\"   ‚è±Ô∏è  Response Time: {perf.get('grade', 'unknown')} ({result.get('processing_time_ms', 0):.0f}ms)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"      SLA Status: {'‚úÖ Met' if perf.get('sla_met', False) else '‚ùå Exceeded'}\"\n",
    "    )\n",
    "\n",
    "    # 2. Content Moderation\n",
    "    if result[\"status\"] == \"success\":\n",
    "        if result.get(\"blocked\"):\n",
    "            print(\"   üö´ Content Moderation: BLOCKED\")\n",
    "            print(f\"      Reason: {result['block_reason']}\")\n",
    "        elif result.get(\"needs_review\"):\n",
    "            print(\"   ‚ö†Ô∏è  Content Moderation: REVIEW NEEDED\")\n",
    "            print(f\"      Flags: {result['review_reason']}\")\n",
    "        else:\n",
    "            print(\"   ‚úÖ Content Moderation: PASSED\")\n",
    "\n",
    "    # 3. Quality Assessment\n",
    "    if result[\"status\"] == \"success\":\n",
    "        quality = result.get(\"quality_metrics\", {})\n",
    "        print(\n",
    "            f\"   üìè Quality Assessment: Grade {quality.get('grade', 'F')} (Score: {quality.get('score', 0):.2f})\"\n",
    "        )\n",
    "\n",
    "        # Show what contributed to the score\n",
    "        if quality.get(\"score\", 0) < 0.6:\n",
    "            print(\n",
    "                f\"      Status: {'‚ö†Ô∏è Below threshold' if quality.get('passed', False) else '‚ùå Failed'}\"\n",
    "            )\n",
    "            # The actual issues are logged by the scorers and visible in Weave UI\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nüéØ Summary of Production Monitoring Demonstration:\")\n",
    "print(\"\\n1. **Successful Cases** (Tests 1, 4, 6):\")\n",
    "print(\"   - High-quality extractions with version numbers\")\n",
    "print(\"   - All required fields present and accurate\")\n",
    "print(\"   - Fast response times meeting SLA\")\n",
    "\n",
    "print(\"\\n2. **Blocked Content** (Test 2):\")\n",
    "print(\"   - Multiple profanity words + legal threats = automatic block\")\n",
    "print(\"   - Protects support agents from abusive content\")\n",
    "\n",
    "print(\"\\n3. **Review Required** (Test 5):\")\n",
    "print(\"   - Mild profanity triggers review flag\")\n",
    "print(\"   - Human can decide if response is appropriate\")\n",
    "\n",
    "print(\"\\n4. **Quality Issues** (Tests 3, 7):\")\n",
    "print(\"   - Missing customer name or product details\")\n",
    "print(\"   - Too brief to be actionable\")\n",
    "print(\"   - Would need human intervention\")\n",
    "\n",
    "print(\"\\n5. **Edge Cases** (Test 8):\")\n",
    "print(\"   - Urgent + positive sentiment (unusual combination)\")\n",
    "print(\"   - System handles it correctly\")\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"   - Different scorers serve different purposes\")\n",
    "print(\"   - Guardrails (block/review) vs Monitors (quality/performance)\")\n",
    "print(\"   - All scorer results are tracked in Weave for analysis\")\n",
    "print(\"\\n‚úÖ Check the Weave UI to see detailed scorer results and traces!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecb1fa0",
   "metadata": {},
   "source": [
    "### üë• Part 3.1: Human Feedback & Dataset Building\n",
    "\n",
    "Learn how to collect human feedback and build datasets from production data.\n",
    "This creates a feedback loop for continuous model improvement.\n",
    "TODO: New Cell: Here, we are going to make an interactive \"app\" that renders in the cell so that the user can directly interact with the model. This will then generate calls in the UI and we can see calls coming into the application. From there we can setup a human feedback column interactively, collect examples, narrow down to the hard cases, and add to a dataset, which can then be used for the next round of evaluations:\n",
    "  1. Create an interactive output that allows for form-fill-style querying of the model\n",
    "  2. Add feedback so that the user can mark the reponse as good or bad (use the API to send this feedback)\n",
    "      * (Show in the UI that you can configure custom columns and form fill directly in the app if you have experts on your side)\n",
    "  3. (UI) Query for the bad results in the UI\n",
    "  4. (UI) Add the bad results to a dataset\n",
    "  5. (Optional) Next cell: create a new evaluation using the new dataset - presumably the models have a harder time... or just say that it is possible"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
