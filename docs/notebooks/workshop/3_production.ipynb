{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c0ee1ea",
   "metadata": {},
   "source": [
    "# Part 3: Production Monitoring with Weave\n",
    "\n",
    "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "\n",
    "Learn how to monitor LLM applications in production using Weave's scorer system for real-time guardrails and quality monitoring.\n",
    "\n",
    "**In this section:**\n",
    "- 🛡️ **Guardrails**: Block or modify responses with content moderation\n",
    "- 📊 **Quality Monitoring**: Track extraction quality and completeness\n",
    "- ⚡ **Performance Tracking**: Monitor response times and SLA compliance\n",
    "- 🔄 **Real-time Scoring**: Apply scorers to live production calls\n",
    "- 👥 **Human Feedback**: Collect feedback and build datasets from production\n",
    "- 📈 **Continuous Improvement**: Use production data to improve models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2789778c",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install dependencies and configure API keys.\n",
    "\n",
    "OpenAI API key can be found at https://platform.openai.com/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fe4ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install wandb weave openai pydantic nest_asyncio ipywidgets set-env-colab-kaggle-dotenv -qqq\n",
    "\n",
    "import asyncio\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "from typing import Any, Optional\n",
    "\n",
    "# For notebooks, use nest_asyncio to handle async properly\n",
    "import nest_asyncio\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from set_env import set_env\n",
    "\n",
    "import weave\n",
    "from weave import Scorer\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Setup API keys\n",
    "os.environ[\"OPENAI_API_KEY\"] = set_env(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize Weave\n",
    "weave_client = weave.init(\"weave-product-tour\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1c6019",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 🎯 Part 3: Production Monitoring\n",
    "\n",
    "Use Weave's scorer system for real-time guardrails and quality monitoring.\n",
    "This demonstrates the apply_scorer pattern for production use.\n",
    "\n",
    "**Key Concepts**:\n",
    "- **Guardrails**: Block or modify responses (e.g., toxicity filter)\n",
    "- **Monitors**: Track quality metrics without blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951ad4f4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Define our data structure\n",
    "class CustomerEmail(BaseModel):\n",
    "    customer_name: str\n",
    "    product: str\n",
    "    issue: str\n",
    "    sentiment: str = Field(description=\"positive, neutral, or negative\")\n",
    "\n",
    "\n",
    "# 🎯 Track functions with @weave.op\n",
    "@weave.op\n",
    "def analyze_customer_email(email: str) -> CustomerEmail:\n",
    "    \"\"\"Analyze a customer support email and extract key information.\"\"\"\n",
    "    client = OpenAI()\n",
    "\n",
    "    # 🔥 OpenAI calls are automatically traced by Weave!\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",  # Using mini model for cost efficiency\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Extract customer name, product, issue, and sentiment.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": email,\n",
    "            },\n",
    "        ],\n",
    "        response_format=CustomerEmail,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.parsed\n",
    "\n",
    "\n",
    "@weave.op\n",
    "def classify_urgency(email: str, sentiment: str) -> str:\n",
    "    \"\"\"Determine urgency level based on content and sentiment.\"\"\"\n",
    "    urgent_keywords = [\n",
    "        \"urgent\",\n",
    "        \"asap\",\n",
    "        \"immediately\",\n",
    "        \"frustrated\",\n",
    "        \"broken\",\n",
    "        \"stopped working\",\n",
    "    ]\n",
    "\n",
    "    # Check for urgent keywords\n",
    "    email_lower = email.lower()\n",
    "    has_urgent_keywords = any(keyword in email_lower for keyword in urgent_keywords)\n",
    "\n",
    "    # Combine sentiment and keywords to determine urgency\n",
    "    if sentiment == \"negative\" and has_urgent_keywords:\n",
    "        return \"high\"\n",
    "    elif sentiment == \"negative\" or has_urgent_keywords:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"low\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6ec6be",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "#### 🛡️ Content Moderation Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fbc5f0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# 🛡️ Define production scorers\n",
    "class ContentModerationScorer(Scorer):\n",
    "    \"\"\"Production-ready content moderation scorer.\"\"\"\n",
    "\n",
    "    @weave.op\n",
    "    def score(self, output: dict) -> dict:\n",
    "        \"\"\"Check for inappropriate content using multiple signals.\"\"\"\n",
    "        # Handle both success and error cases\n",
    "        if output.get(\"status\") != \"success\":\n",
    "            return {\"flagged\": False, \"flags\": [], \"severity\": \"none\", \"action\": \"pass\"}\n",
    "\n",
    "        analysis = output.get(\"analysis\", {})\n",
    "        issue_text = analysis.get(\"issue\", \"\").lower()\n",
    "        sentiment = analysis.get(\"sentiment\", \"neutral\")\n",
    "\n",
    "        # Check for various inappropriate content patterns\n",
    "        profanity_patterns = [\n",
    "            \"stupid\",\n",
    "            \"idiotic\",\n",
    "            \"garbage\",\n",
    "            \"trash\",\n",
    "            \"sucks\",\n",
    "            \"terrible\",\n",
    "            \"awful\",\n",
    "            \"worst\",\n",
    "        ]\n",
    "        threat_patterns = [\"sue\", \"lawyer\", \"legal action\", \"court\", \"lawsuit\"]\n",
    "\n",
    "        flags = []\n",
    "        severity = \"none\"\n",
    "\n",
    "        # Check profanity\n",
    "        profanity_found = []\n",
    "        for word in profanity_patterns:\n",
    "            if word in issue_text:\n",
    "                profanity_found.append(word)\n",
    "\n",
    "        if profanity_found:\n",
    "            flags.append(f\"Profanity detected: {', '.join(profanity_found)}\")\n",
    "            severity = \"medium\"\n",
    "\n",
    "        # Check threats\n",
    "        threats_found = []\n",
    "        for pattern in threat_patterns:\n",
    "            if pattern in issue_text:\n",
    "                threats_found.append(pattern)\n",
    "\n",
    "        if threats_found:\n",
    "            flags.append(f\"Legal threat: {', '.join(threats_found)}\")\n",
    "            severity = \"high\"\n",
    "\n",
    "        # Check extreme sentiment with profanity\n",
    "        if sentiment == \"negative\" and profanity_found:\n",
    "            severity = \"high\"\n",
    "            flags.append(\"Negative sentiment with profanity\")\n",
    "\n",
    "        return {\n",
    "            \"flagged\": len(flags) > 0,\n",
    "            \"flags\": flags,\n",
    "            \"severity\": severity,\n",
    "            \"action\": \"block\"\n",
    "            if severity == \"high\"\n",
    "            else (\"review\" if severity == \"medium\" else \"pass\"),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd079296",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "#### 📊 Quality Assessment Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc23fc7f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class ExtractionQualityScorer(Scorer):\n",
    "    \"\"\"Monitor extraction quality and completeness.\"\"\"\n",
    "\n",
    "    @weave.op\n",
    "    def score(self, output: dict, email: str) -> dict:\n",
    "        \"\"\"Comprehensive quality assessment.\"\"\"\n",
    "        if output.get(\"status\") != \"success\":\n",
    "            return {\n",
    "                \"quality_score\": 0.0,\n",
    "                \"passed\": False,\n",
    "                \"issues\": [\"Failed to process email\"],\n",
    "                \"recommendations\": [],\n",
    "                \"extraction_grade\": \"F\",\n",
    "            }\n",
    "\n",
    "        analysis = output.get(\"analysis\", {})\n",
    "        quality_metrics = {\n",
    "            \"completeness\": 0.0,\n",
    "            \"specificity\": 0.0,\n",
    "            \"accuracy\": 0.0,\n",
    "            \"consistency\": 0.0,\n",
    "        }\n",
    "        issues = []\n",
    "        recommendations = []\n",
    "\n",
    "        # 1. Completeness checks (40% weight)\n",
    "        if analysis.get(\"customer_name\") and analysis[\"customer_name\"] not in [\n",
    "            \"Unknown\",\n",
    "            \"\",\n",
    "            None,\n",
    "        ]:\n",
    "            quality_metrics[\"completeness\"] += 0.15\n",
    "        else:\n",
    "            issues.append(\"Missing customer name\")\n",
    "            recommendations.append(\"Check email signatures and greetings for names\")\n",
    "\n",
    "        if analysis.get(\"product\") and analysis[\"product\"] not in [\"Unknown\", \"\", None]:\n",
    "            quality_metrics[\"completeness\"] += 0.15\n",
    "        else:\n",
    "            issues.append(\"Missing product identification\")\n",
    "            recommendations.append(\"Look for product names mentioned in the email\")\n",
    "\n",
    "        if analysis.get(\"issue\") and len(analysis[\"issue\"]) > 10:\n",
    "            quality_metrics[\"completeness\"] += 0.10\n",
    "        else:\n",
    "            issues.append(\"Issue description too brief or missing\")\n",
    "            recommendations.append(\"Extract a more detailed problem description\")\n",
    "\n",
    "        # 2. Specificity checks (30% weight)\n",
    "        product_name = analysis.get(\"product\", \"\")\n",
    "        if product_name and any(char.isdigit() for char in str(product_name)):\n",
    "            # Product includes version/model number\n",
    "            quality_metrics[\"specificity\"] += 0.15\n",
    "        elif product_name:\n",
    "            recommendations.append(\n",
    "                \"Extract product version/model numbers when available\"\n",
    "            )\n",
    "\n",
    "        issue_desc = analysis.get(\"issue\", \"\")\n",
    "        if issue_desc and len(str(issue_desc)) > 30:\n",
    "            quality_metrics[\"specificity\"] += 0.15\n",
    "        elif issue_desc:\n",
    "            recommendations.append(\"Provide more specific issue details\")\n",
    "\n",
    "        # 3. Accuracy checks (20% weight)\n",
    "        # Check if extracted content actually appears in email\n",
    "        email_lower = email.lower()\n",
    "        customer_name = analysis.get(\"customer_name\", \"\")\n",
    "        if customer_name and customer_name != \"Unknown\":\n",
    "            name_parts = customer_name.lower().split()\n",
    "            # Check if at least part of the name appears in email\n",
    "            if any(part in email_lower for part in name_parts if len(part) > 2):\n",
    "                quality_metrics[\"accuracy\"] += 0.10\n",
    "            else:\n",
    "                issues.append(\"Extracted name not found in original email\")\n",
    "\n",
    "        product_mentioned = analysis.get(\"product\", \"\")\n",
    "        if product_mentioned and product_mentioned != \"Unknown\":\n",
    "            # Check for partial matches (product names might be extracted differently)\n",
    "            product_words = product_mentioned.lower().split()\n",
    "            if any(word in email_lower for word in product_words if len(word) > 3):\n",
    "                quality_metrics[\"accuracy\"] += 0.10\n",
    "            else:\n",
    "                issues.append(\"Extracted product not clearly mentioned in email\")\n",
    "\n",
    "        # 4. Consistency checks (10% weight)\n",
    "        sentiment = analysis.get(\"sentiment\", \"neutral\")\n",
    "        urgency = output.get(\"urgency\", \"low\")\n",
    "\n",
    "        # Check sentiment/urgency consistency\n",
    "        consistency_ok = True\n",
    "        if sentiment == \"negative\" and urgency == \"low\":\n",
    "            if not any(\n",
    "                word in issue_desc.lower() for word in [\"minor\", \"small\", \"slight\"]\n",
    "            ):\n",
    "                consistency_ok = False\n",
    "                issues.append(\n",
    "                    \"Negative sentiment but low urgency - might be inconsistent\"\n",
    "                )\n",
    "        elif sentiment == \"positive\" and urgency == \"high\":\n",
    "            consistency_ok = False\n",
    "            issues.append(\"Positive sentiment with high urgency is unusual\")\n",
    "\n",
    "        if consistency_ok:\n",
    "            quality_metrics[\"consistency\"] += 0.10\n",
    "\n",
    "        # Calculate overall score\n",
    "        total_score = sum(quality_metrics.values())\n",
    "\n",
    "        return {\n",
    "            \"quality_score\": total_score,\n",
    "            \"quality_metrics\": quality_metrics,\n",
    "            \"passed\": total_score >= 0.6,  # Lowered threshold for demo\n",
    "            \"issues\": issues,\n",
    "            \"recommendations\": recommendations,\n",
    "            \"extraction_grade\": \"A\"\n",
    "            if total_score >= 0.9\n",
    "            else (\n",
    "                \"B\"\n",
    "                if total_score >= 0.8\n",
    "                else (\n",
    "                    \"C\" if total_score >= 0.6 else (\"D\" if total_score >= 0.4 else \"F\")\n",
    "                )\n",
    "            ),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b226e9e4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "#### 🏭 Production Email Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539b5054",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "def production_email_handler(\n",
    "    email: str, request_id: Optional[str] = None\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"Production-ready email handler that returns structured analysis results.\"\"\"\n",
    "    # Generate request ID if not provided\n",
    "    if not request_id:\n",
    "        request_id = f\"req_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{random.randint(1000, 9999)}\"\n",
    "\n",
    "    try:\n",
    "        # Process the email using our existing analyzer\n",
    "        analysis = analyze_customer_email(email)\n",
    "\n",
    "        # Calculate urgency based on the analysis\n",
    "        urgency = classify_urgency(email, analysis.sentiment)\n",
    "\n",
    "        # Return structured result that scorers expect\n",
    "        return {\n",
    "            \"request_id\": request_id,\n",
    "            \"status\": \"success\",\n",
    "            \"analysis\": {\n",
    "                \"customer_name\": analysis.customer_name,\n",
    "                \"product\": analysis.product,\n",
    "                \"issue\": analysis.issue,\n",
    "                \"sentiment\": analysis.sentiment,\n",
    "            },\n",
    "            \"urgency\": urgency,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log error and return error response\n",
    "        return {\n",
    "            \"request_id\": request_id,\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e),\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e766e23",
   "metadata": {},
   "source": [
    "#### 🔧 Initialize Scorers and Monitoring Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0ecac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scorers\n",
    "content_moderation_scorer = ContentModerationScorer()\n",
    "quality_scorer = ExtractionQualityScorer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a689420",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def handle_email_with_monitoring(email: str) -> dict[str, Any]:\n",
    "    \"\"\"Handle email with production monitoring and guardrails.\"\"\"\n",
    "    # Process the email and get the Call object\n",
    "    result, call = production_email_handler.call(email)\n",
    "\n",
    "    if result[\"status\"] == \"success\":\n",
    "        # Apply content moderation (guardrail)\n",
    "        moderation_check = await call.apply_scorer(content_moderation_scorer)\n",
    "\n",
    "        # Apply quality monitoring\n",
    "        quality_check = await call.apply_scorer(\n",
    "            quality_scorer, additional_scorer_kwargs={\"email\": email}\n",
    "        )\n",
    "\n",
    "        # Handle moderation results\n",
    "        if moderation_check.result[\"flagged\"]:\n",
    "            action = moderation_check.result[\"action\"]\n",
    "            if action == \"block\":\n",
    "                print(f\"🚫 Content BLOCKED: {moderation_check.result['flags']}\")\n",
    "                result[\"blocked\"] = True\n",
    "                result[\"block_reason\"] = moderation_check.result[\"flags\"]\n",
    "            elif action == \"review\":\n",
    "                print(\n",
    "                    f\"⚠️ Content flagged for review: {moderation_check.result['flags']}\"\n",
    "                )\n",
    "                result[\"needs_review\"] = True\n",
    "                result[\"review_reason\"] = moderation_check.result[\"flags\"]\n",
    "\n",
    "        # Add quality metrics\n",
    "        result[\"quality_metrics\"] = {\n",
    "            \"grade\": quality_check.result[\"extraction_grade\"],\n",
    "            \"score\": quality_check.result[\"quality_score\"],\n",
    "            \"passed\": quality_check.result[\"passed\"],\n",
    "        }\n",
    "\n",
    "        # Show quality issues and recommendations\n",
    "        if quality_check.result[\"issues\"]:\n",
    "            print(f\"📊 Quality issues: {quality_check.result['issues']}\")\n",
    "\n",
    "        if quality_check.result[\"recommendations\"]:\n",
    "            print(f\"💡 Recommendations: {quality_check.result['recommendations']}\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b989abd7",
   "metadata": {},
   "source": [
    "#### 🧪 Test Production Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5385385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧪 Test production monitoring with realistic scenarios\n",
    "print(\"🏭 Testing production monitoring with realistic scenarios...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "production_test_emails = [\n",
    "    # Good quality extraction - should pass all checks\n",
    "    {\n",
    "        \"email\": \"Hello Support Team,\\n\\nI'm Sarah Mitchell from Acme Corp. Our CloudSync Enterprise v3.2.1 stopped syncing files yesterday at 2pm. The error message says 'Authentication failed'. This is really frustrating and affecting our entire team.\\n\\nBest regards,\\nSarah Mitchell\\nIT Manager, Acme Corp\",\n",
    "        \"expected\": \"✅ High quality extraction with version numbers\",\n",
    "    },\n",
    "    # Profanity with legal threat - should be blocked\n",
    "    {\n",
    "        \"email\": \"This stupid software is absolute garbage! I'm John Davis and your DataSync Pro is the worst trash I've ever used. My lawyer will be contacting you about this terrible product that lost our data!\",\n",
    "        \"expected\": \"🚫 Should be blocked - profanity + legal threat\",\n",
    "    },\n",
    "    # Poor quality but processable - low score but not blocked\n",
    "    {\n",
    "        \"email\": \"Hi support, product broken. Fix please. - Tom\",\n",
    "        \"expected\": \"📊 Low quality - minimal details but processable\",\n",
    "    },\n",
    "    # Good extraction with negative sentiment - quality pass\n",
    "    {\n",
    "        \"email\": \"Dear Support,\\n\\nI'm Mary Johnson, CTO at TechStart Inc. Our DataVault Pro v2.5 backup failed last night with error code 'E501: connection timeout'. This is concerning as we rely on nightly backups for compliance.\\n\\nMary Johnson\\nCTO, TechStart Inc\",\n",
    "        \"expected\": \"✅ Good quality despite negative sentiment\",\n",
    "    },\n",
    "    # Needs review - mild profanity - should flag for review\n",
    "    {\n",
    "        \"email\": \"Mike Wilson here. Your EmailPro system really sucks compared to what was promised, but I guess it's still better than the competition. Can you help me configure the spam filter? It's blocking legitimate emails.\",\n",
    "        \"expected\": \"⚠️ Should flag for review - mild profanity\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82dfcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a \"production\" simulation\n",
    "for i, test_case in enumerate(production_test_emails):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"📧 Test {i+1}/5: {test_case['expected']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Show email preview\n",
    "    email_lines = test_case[\"email\"].split(\"\\n\")\n",
    "    print(\"📝 Email Content:\")\n",
    "    for line in email_lines[:3]:  # Show first 3 lines\n",
    "        if line.strip():\n",
    "            print(f\"   {line[:70]}{'...' if len(line) > 70 else ''}\")\n",
    "    if len(email_lines) > 3:\n",
    "        print(f\"   ... ({len(email_lines)-3} more lines)\")\n",
    "\n",
    "    # Process with monitoring\n",
    "    result = asyncio.run(handle_email_with_monitoring(test_case[\"email\"]))\n",
    "\n",
    "    # Show extraction results\n",
    "    print(\"\\n🔍 Extraction Results:\")\n",
    "    if result[\"status\"] == \"success\":\n",
    "        analysis = result[\"analysis\"]\n",
    "        print(f\"   Customer: {analysis.get('customer_name', 'Unknown')}\")\n",
    "        print(f\"   Product: {analysis.get('product', 'Unknown')}\")\n",
    "        print(\n",
    "            f\"   Issue: {analysis.get('issue', 'Unknown')[:50]}{'...' if len(analysis.get('issue', '')) > 50 else ''}\"\n",
    "        )\n",
    "        print(f\"   Sentiment: {analysis.get('sentiment', 'Unknown')}\")\n",
    "        print(f\"   Urgency: {result.get('urgency', 'Unknown')}\")\n",
    "    else:\n",
    "        print(f\"   ❌ Error: {result.get('error', 'Unknown error')}\")\n",
    "\n",
    "    # Show scorer results\n",
    "    print(\"\\n📊 Scorer Results:\")\n",
    "\n",
    "    # Content Moderation\n",
    "    if result[\"status\"] == \"success\":\n",
    "        if result.get(\"blocked\"):\n",
    "            print(\"   🚫 Content Moderation: BLOCKED\")\n",
    "            print(f\"      Reason: {result['block_reason']}\")\n",
    "        elif result.get(\"needs_review\"):\n",
    "            print(\"   ⚠️ Content Moderation: REVIEW NEEDED\")\n",
    "            print(f\"      Flags: {result['review_reason']}\")\n",
    "        else:\n",
    "            print(\"   ✅ Content Moderation: PASSED\")\n",
    "\n",
    "    # Quality Assessment\n",
    "    if result[\"status\"] == \"success\":\n",
    "        quality = result.get(\"quality_metrics\", {})\n",
    "        print(\n",
    "            f\"   📏 Quality Assessment: Grade {quality.get('grade', 'F')} (Score: {quality.get('score', 0):.2f})\"\n",
    "        )\n",
    "\n",
    "        # Show what contributed to the score\n",
    "        if quality.get(\"score\", 0) < 0.6:\n",
    "            print(\n",
    "                f\"      Status: {'⚠️ Below threshold' if quality.get('passed', False) else '❌ Failed'}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328477f0",
   "metadata": {},
   "source": [
    "## 3.1: Human Feedback & Data Collection\n",
    "\n",
    "Learn how to collect human feedback and build datasets from production data.\n",
    "This creates a feedback loop for continuous model improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c050e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b517e520",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "#### 🔄 Interactive Feedback Collection App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a30fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an interactive feedback collection interface\n",
    "class EmailAnalyzerFeedbackApp:\n",
    "    def __init__(self):\n",
    "        self.current_call = None\n",
    "        self.setup_ui()\n",
    "        # Generate initial challenging email\n",
    "        self.generate_challenging_email()\n",
    "\n",
    "    def setup_ui(self):\n",
    "        \"\"\"Create the interactive UI components.\"\"\"\n",
    "        # Input area\n",
    "        self.email_input = widgets.Textarea(\n",
    "            value=\"\",  # Will be populated by generate_challenging_email()\n",
    "            placeholder=\"Enter a customer email to analyze...\",\n",
    "            description=\"Email:\",\n",
    "            layout=widgets.Layout(width=\"100%\", height=\"120px\"),\n",
    "        )\n",
    "\n",
    "        # Action buttons\n",
    "        self.analyze_button = widgets.Button(\n",
    "            description=\"Analyze Email\",\n",
    "            button_style=\"primary\",\n",
    "            layout=widgets.Layout(width=\"150px\"),\n",
    "        )\n",
    "        self.analyze_button.on_click(self.analyze_email)\n",
    "\n",
    "        self.generate_button = widgets.Button(\n",
    "            description=\"Generate New Email\",\n",
    "            button_style=\"info\",\n",
    "            layout=widgets.Layout(width=\"150px\"),\n",
    "        )\n",
    "        self.generate_button.on_click(self.on_generate_email)\n",
    "\n",
    "        # Output area\n",
    "        self.output_area = widgets.Output()\n",
    "\n",
    "        # Feedback buttons (initially hidden)\n",
    "        self.feedback_area = widgets.VBox([])\n",
    "\n",
    "        # Main layout\n",
    "        self.app = widgets.VBox(\n",
    "            [\n",
    "                widgets.HTML(\"<h3>🔄 Interactive Email Analyzer with Feedback</h3>\"),\n",
    "                widgets.HTML(\n",
    "                    \"<p>Analyze challenging emails and provide feedback to improve the model:</p>\"\n",
    "                ),\n",
    "                self.email_input,\n",
    "                widgets.HBox(\n",
    "                    [self.analyze_button, self.generate_button],\n",
    "                    layout=widgets.Layout(margin=\"10px 0\"),\n",
    "                ),\n",
    "                self.output_area,\n",
    "                self.feedback_area,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def analyze_email(self, button):\n",
    "        \"\"\"Analyze the email and show results.\"\"\"\n",
    "        with self.output_area:\n",
    "            clear_output()\n",
    "            print(\"🔄 Analyzing email...\")\n",
    "\n",
    "        try:\n",
    "            # Use the .call() method to get both result and call object\n",
    "            email_text = self.email_input.value.strip()\n",
    "            if not email_text:\n",
    "                with self.output_area:\n",
    "                    clear_output()\n",
    "                    print(\"❌ Please enter an email to analyze.\")\n",
    "                return\n",
    "\n",
    "            # Add session attributes for tracking\n",
    "            with weave.attributes(\n",
    "                {\"session\": str(uuid.uuid4()), \"env\": \"workshop_demo\"}\n",
    "            ):\n",
    "                result, call = production_email_handler.call(email_text)\n",
    "\n",
    "            self.current_call = call\n",
    "\n",
    "            # Display results\n",
    "            with self.output_area:\n",
    "                clear_output()\n",
    "                if result[\"status\"] == \"success\":\n",
    "                    analysis = result[\"analysis\"]\n",
    "                    print(\"✅ Analysis Complete!\")\n",
    "                    print(f\"📧 Customer: {analysis['customer_name']}\")\n",
    "                    print(f\"🏷️ Product: {analysis['product']}\")\n",
    "                    print(f\"📝 Issue: {analysis['issue']}\")\n",
    "                    print(f\"😊 Sentiment: {analysis['sentiment']}\")\n",
    "                    print(f\"⚡ Urgency: {result['urgency']}\")\n",
    "                else:\n",
    "                    print(f\"❌ Error: {result.get('error', 'Unknown error')}\")\n",
    "\n",
    "            # Show feedback buttons\n",
    "            self.show_feedback_buttons()\n",
    "\n",
    "        except Exception as e:\n",
    "            with self.output_area:\n",
    "                clear_output()\n",
    "                print(f\"❌ Error analyzing email: {str(e)}\")\n",
    "\n",
    "    def on_generate_email(self, button):\n",
    "        \"\"\"Generate a new challenging email example.\"\"\"\n",
    "        self.generate_challenging_email()\n",
    "        # Clear any previous analysis and feedback\n",
    "        with self.output_area:\n",
    "            clear_output()\n",
    "            print(\n",
    "                \"🎲 New challenging email generated! Click 'Analyze Email' to test it.\"\n",
    "            )\n",
    "        self.feedback_area.children = []\n",
    "\n",
    "    def show_feedback_buttons(self):\n",
    "        \"\"\"Display feedback buttons after analysis.\"\"\"\n",
    "        if not self.current_call:\n",
    "            return\n",
    "\n",
    "        # Rating slider (0-5)\n",
    "        self.rating_slider = widgets.IntSlider(\n",
    "            value=3,\n",
    "            min=0,\n",
    "            max=5,\n",
    "            step=1,\n",
    "            description=\"Rating:\",\n",
    "            style={\"description_width\": \"initial\"},\n",
    "            layout=widgets.Layout(width=\"300px\"),\n",
    "        )\n",
    "\n",
    "        # Text feedback\n",
    "        self.feedback_text = widgets.Textarea(\n",
    "            placeholder=\"Optional comments about this analysis...\",\n",
    "            description=\"Comments:\",\n",
    "            layout=widgets.Layout(width=\"100%\", height=\"80px\"),\n",
    "        )\n",
    "\n",
    "        # Action buttons\n",
    "        submit_feedback = widgets.Button(\n",
    "            description=\"Submit Feedback\",\n",
    "            button_style=\"primary\",\n",
    "            layout=widgets.Layout(width=\"150px\"),\n",
    "        )\n",
    "\n",
    "        clear_feedback = widgets.Button(\n",
    "            description=\"Clear\",\n",
    "            button_style=\"\",\n",
    "            layout=widgets.Layout(width=\"100px\"),\n",
    "        )\n",
    "\n",
    "        # Feedback status\n",
    "        self.feedback_status = widgets.Output()\n",
    "\n",
    "        # Event handlers\n",
    "        def on_submit_feedback(button):\n",
    "            self.submit_rating_feedback()\n",
    "\n",
    "        def on_clear_feedback(button):\n",
    "            self.clear_feedback_form()\n",
    "\n",
    "        submit_feedback.on_click(on_submit_feedback)\n",
    "        clear_feedback.on_click(on_clear_feedback)\n",
    "\n",
    "        # Layout feedback area\n",
    "        self.feedback_area.children = [\n",
    "            widgets.HTML(\"<hr><h4>📝 Provide Feedback</h4>\"),\n",
    "            self.rating_slider,\n",
    "            self.feedback_text,\n",
    "            widgets.HBox(\n",
    "                [submit_feedback, clear_feedback],\n",
    "                layout=widgets.Layout(margin=\"10px 0\"),\n",
    "            ),\n",
    "            self.feedback_status,\n",
    "        ]\n",
    "\n",
    "    def submit_rating_feedback(self):\n",
    "        \"\"\"Submit rating and comment feedback using the lower-level add method.\"\"\"\n",
    "        if not self.current_call:\n",
    "            with self.feedback_status:\n",
    "                clear_output()\n",
    "                print(\"❌ No call to add feedback to.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            rating = self.rating_slider.value\n",
    "            comment = self.feedback_text.value.strip()\n",
    "\n",
    "            # Use the lower-level add method for custom feedback type\n",
    "            feedback_payload = {\"rating\": rating}\n",
    "            if comment:\n",
    "                feedback_payload[\"comment\"] = comment\n",
    "\n",
    "            self.current_call.feedback.add(\n",
    "                feedback_type=\"user_rating\",\n",
    "                payload=feedback_payload,\n",
    "            )\n",
    "\n",
    "            # Little hack to submit a score that can be operated on - will\n",
    "            # not need this in the future.\n",
    "            @weave.op()\n",
    "            def user_rating(output):\n",
    "                return feedback_payload\n",
    "\n",
    "            asyncio.run(self.current_call.apply_scorer(user_rating))\n",
    "\n",
    "            with self.feedback_status:\n",
    "                clear_output()\n",
    "                feedback_desc = f\"rating ({rating}/5)\"\n",
    "                if comment:\n",
    "                    feedback_desc += \" with comment\"\n",
    "                print(f\"✅ Feedback submitted: {feedback_desc}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            with self.feedback_status:\n",
    "                clear_output()\n",
    "                print(f\"❌ Error submitting feedback: {str(e)}\")\n",
    "\n",
    "    def generate_challenging_email(self):\n",
    "        \"\"\"Generate a challenging customer email using LLM.\"\"\"\n",
    "        try:\n",
    "            client = OpenAI()\n",
    "\n",
    "            # Generate a challenging email scenario\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"\"\"Generate a realistic but challenging customer support email that tests edge cases for extraction:\n",
    "\n",
    "REQUIREMENTS:\n",
    "- Include a clear customer name (but maybe in an unusual place like signature)\n",
    "- Mention a specific product with version/model if possible\n",
    "- Have a clear issue description\n",
    "- Include sentiment (positive/negative/neutral)\n",
    "- Make it challenging by including:\n",
    "  * Multiple people mentioned (but only one is the actual sender)\n",
    "  * Multiple products mentioned (but focus on one with issues)\n",
    "  * Names that could be confused with products or vice versa\n",
    "  * Sarcasm, mixed emotions, or subtle sentiment\n",
    "  * Professional signatures, forwarded emails, or unusual formatting\n",
    "\n",
    "Keep it realistic and professional. Length: 2-4 sentences plus signature.\"\"\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": \"Generate a challenging customer support email:\",\n",
    "                    },\n",
    "                ],\n",
    "                temperature=0.8,  # Higher temperature for variety\n",
    "                max_tokens=200,\n",
    "            )\n",
    "\n",
    "            generated_email = response.choices[0].message.content.strip()\n",
    "            self.email_input.value = generated_email\n",
    "\n",
    "        except Exception as e:\n",
    "            # Fallback to a predefined challenging example if LLM fails\n",
    "            fallback_emails = [\n",
    "                \"Hi Support,\\n\\nSpoke with Jennifer about the CloudSync issue. Still having problems with WorkflowMax Pro v2.1 crashing during exports. Very frustrating!\\n\\nMike O'Brien\\nCEO, TechStart Inc\",\n",
    "                \"RE: Ticket #5678\\n\\nCustomer María García called about DataVault. She says the backup feature in ArchiveMax Enterprise is working great now, but I'm still having sync issues.\\n\\nBest regards,\\nDr. Rajesh Patel\",\n",
    "                \"Johnson recommended your software. Smith loves CloudProcessor. But I'm having terrible issues with it constantly freezing.\\n\\n—James Wilson\\nSenior Developer\",\n",
    "                \"Great product overall! Though the InvoiceGen module crashes sometimes when processing large files. Still recommend it to others.\\n\\nAnna Larsson\\nStockholm Office\",\n",
    "            ]\n",
    "            import random\n",
    "\n",
    "            self.email_input.value = random.choice(fallback_emails)\n",
    "\n",
    "    def clear_feedback_form(self):\n",
    "        \"\"\"Clear the feedback form and reset to defaults.\"\"\"\n",
    "        self.rating_slider.value = 3\n",
    "        self.feedback_text.value = \"\"\n",
    "        # Also clear the email input and generate a new challenging example\n",
    "        self.generate_challenging_email()\n",
    "        with self.feedback_status:\n",
    "            clear_output()\n",
    "            print(\"🔄 Form cleared and new example generated\")\n",
    "\n",
    "    def display(self):\n",
    "        \"\"\"Display the app.\"\"\"\n",
    "        display(self.app)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c98c58",
   "metadata": {},
   "source": [
    "#### 🚀 Launch the Feedback App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f119ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display the feedback app\n",
    "print(\"🚀 Starting Interactive Email Analyzer with Feedback Collection...\")\n",
    "feedback_app = EmailAnalyzerFeedbackApp()\n",
    "feedback_app.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580a17ee",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've learned how to monitor LLM applications in production:\n",
    "\n",
    "- ✅ **Guardrails**: Implemented content moderation to block inappropriate responses\n",
    "- ✅ **Quality Monitoring**: Built comprehensive quality assessment scorers\n",
    "- ✅ **Real-time Scoring**: Applied scorers to production calls with `call.apply_scorer()`\n",
    "- ✅ **Production Patterns**: Handled errors, edge cases, and performance monitoring\n",
    "- ✅ **Human Feedback**: Created interactive feedback collection systems\n",
    "\n",
    "**Next Steps:**\n",
    "- Deploy these patterns in your real applications\n",
    "- Set up automated feedback collection in production\n",
    "- Build custom scorers for domain-specific quality checks\n",
    "- Monitor quality metrics over time in the Weave UI\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Production monitoring requires both guardrails (blocking) and monitors (tracking)\n",
    "- Scorers can be applied in real-time to any Weave-traced function call\n",
    "- Quality assessment should be comprehensive: completeness, accuracy, consistency\n",
    "- Human feedback creates a continuous improvement loop for model development\n",
    "- All scorer results and feedback are automatically tracked and visualized in Weave"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
