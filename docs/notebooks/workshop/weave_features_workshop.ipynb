{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5235d9e",
   "metadata": {},
   "source": [
    "# üêù Weave Workshop: Build, Track, and Evaluate LLM Applications\n",
    "\n",
    "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "\n",
    "Welcome to the Weave workshop! In this hands-on session, you'll learn how to use Weave to develop, debug, and evaluate AI-powered applications.\n",
    "\n",
    "**What you'll learn:**\n",
    "- üîç **Trace & Debug**: Track every LLM call, see inputs/outputs, and debug issues\n",
    "- üìä **Evaluate**: Build rigorous evaluations with multiple scoring functions\n",
    "- üèÉ **Compare**: Run A/B tests and compare different approaches\n",
    "- üìà **Monitor**: Track costs, latency, and performance metrics\n",
    "- üéØ **Iterate**: Use data-driven insights to improve your application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5a4d61",
   "metadata": {},
   "source": [
    "## üîë Prerequisites\n",
    "\n",
    "Before we begin, let's set up your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87d52e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install wandb weave openai pydantic nest_asyncio -qqq\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from datetime import datetime\n",
    "from getpass import getpass\n",
    "from typing import Any, Optional\n",
    "\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import weave\n",
    "from weave import Dataset, Evaluation, EvaluationLogger, Model, Scorer\n",
    "\n",
    "# üîë Setup your API keys\n",
    "print(\"üìù Setting up API keys...\")\n",
    "\n",
    "# Weights & Biases will automatically prompt if needed\n",
    "# It checks: 1) WANDB_API_KEY env var, 2) ~/.netrc, 3) prompts user\n",
    "print(\"‚úÖ W&B authentication will be handled automatically by Weave\")\n",
    "print(\"   (Optional: You can set WANDB_API_KEY env variable if you prefer)\")\n",
    "\n",
    "# OpenAI requires manual setup\n",
    "print(\"\\nü§ñ OpenAI Setup:\")\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    print(\n",
    "        \"You can generate your OpenAI API key here: https://platform.openai.com/api-keys\"\n",
    "    )\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
    "else:\n",
    "    print(\"‚úÖ OpenAI API key found in environment\")\n",
    "\n",
    "print(\"\\n---\")\n",
    "\n",
    "# üè† Initialize your W&B project\n",
    "print(\"üêù Initializing Weave...\")\n",
    "weave_client = weave.init(\"weave-workshop\")  # üêù Your W&B project name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8594b896",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## üîç Part 1: Tracing & Debugging with Weave\n",
    "\n",
    "Let's start by building a simple LLM application and see how Weave automatically tracks everything.\n",
    "\n",
    "Note: We're using `gpt-4o-mini` which supports structured outputs while being cost-effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16118e3b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Define our data structure\n",
    "class CustomerEmail(BaseModel):\n",
    "    customer_name: str\n",
    "    product: str\n",
    "    issue: str\n",
    "    sentiment: str = Field(description=\"positive, neutral, or negative\")\n",
    "\n",
    "\n",
    "# üêù Track functions with @weave.op\n",
    "@weave.op\n",
    "def analyze_customer_email(email: str) -> CustomerEmail:\n",
    "    \"\"\"Analyze a customer support email and extract key information.\"\"\"\n",
    "    client = OpenAI()\n",
    "\n",
    "    # üéØ Note: OpenAI calls are automatically traced by Weave!\n",
    "    # Weave automatically integrates with dozens of popular libraries including:\n",
    "    # OpenAI, Anthropic, LangChain, LlamaIndex, HuggingFace, and more\n",
    "    # See full list: https://weave-docs.wandb.ai/guides/integrations/\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",  # Using mini model for cost efficiency\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Extract customer name, product, issue, and sentiment.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": email,\n",
    "            },\n",
    "        ],\n",
    "        response_format=CustomerEmail,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.parsed\n",
    "\n",
    "\n",
    "# Let's test it!\n",
    "test_email = \"\"\"\n",
    "Hi Support,\n",
    "\n",
    "I'm really frustrated! My new ProWidget 3000 stopped working after just 2 days.\n",
    "The screen went completely black and won't turn on no matter what I try.\n",
    "\n",
    "Please help!\n",
    "Sarah Johnson\n",
    "\"\"\"\n",
    "\n",
    "# üéØ Run the function - Weave will automatically track this call\n",
    "result = analyze_customer_email(test_email)\n",
    "print(\"‚úÖ Analysis complete!\")\n",
    "print(f\"Customer: {result.customer_name}\")\n",
    "print(f\"Sentiment: {result.sentiment}\")\n",
    "print(\"\\nüîç Check the Weave UI to see the trace!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398c1558",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### üêõ Part 1.1: Debugging with Call Traces\n",
    "\n",
    "Weave tracks nested function calls, making debugging easy. Let's build a more complex example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688e5e04",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "@weave.op\n",
    "def preprocess_email(email: str) -> str:\n",
    "    \"\"\"Clean and standardize email text.\"\"\"\n",
    "    # Remove extra whitespace\n",
    "    cleaned = \" \".join(email.split())\n",
    "    # Add some metadata for debugging\n",
    "    print(f\"üìß Original length: {len(email)}, Cleaned length: {len(cleaned)}\")\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "@weave.op\n",
    "def classify_urgency(email: str, sentiment: str) -> str:\n",
    "    \"\"\"Determine urgency level based on content and sentiment.\"\"\"\n",
    "    urgent_keywords = [\n",
    "        \"urgent\",\n",
    "        \"asap\",\n",
    "        \"immediately\",\n",
    "        \"frustrated\",\n",
    "        \"broken\",\n",
    "        \"stopped working\",\n",
    "    ]\n",
    "\n",
    "    # Check for urgent keywords\n",
    "    email_lower = email.lower()\n",
    "    has_urgent_keywords = any(keyword in email_lower for keyword in urgent_keywords)\n",
    "\n",
    "    if sentiment == \"negative\" and has_urgent_keywords:\n",
    "        return \"high\"\n",
    "    elif sentiment == \"negative\" or has_urgent_keywords:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"low\"\n",
    "\n",
    "\n",
    "@weave.op\n",
    "def process_support_ticket(email: str) -> dict[str, Any]:\n",
    "    \"\"\"Complete support ticket processing pipeline.\"\"\"\n",
    "    # Step 1: Clean the email\n",
    "    cleaned_email = preprocess_email(email)\n",
    "\n",
    "    # Step 2: Analyze the email\n",
    "    analysis = analyze_customer_email(cleaned_email)\n",
    "\n",
    "    # Step 3: Determine urgency\n",
    "    urgency = classify_urgency(cleaned_email, analysis.sentiment)\n",
    "\n",
    "    # Return complete ticket info\n",
    "    return {\n",
    "        \"customer_name\": analysis.customer_name,\n",
    "        \"product\": analysis.product,\n",
    "        \"issue\": analysis.issue,\n",
    "        \"sentiment\": analysis.sentiment,\n",
    "        \"urgency\": urgency,\n",
    "        \"needs_immediate_attention\": urgency == \"high\",\n",
    "    }\n",
    "\n",
    "\n",
    "# üéØ Run the pipeline - see the nested traces in Weave!\n",
    "ticket = process_support_ticket(test_email)\n",
    "print(\"\\nüé´ Ticket processed!\")\n",
    "print(f\"Urgency: {ticket['urgency']}\")\n",
    "print(f\"Needs immediate attention: {ticket['needs_immediate_attention']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c9fb58",
   "metadata": {},
   "source": [
    "### üêû Part 1.2: Exception Tracking\n",
    "\n",
    "Weave makes it easy to debug when things go wrong.\n",
    "\n",
    "TODO: Also, this section is pretty verbose just to highlight error tracking - if we can make it simpler so that the student has less to read through and understand, that would be great."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aed4bc4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### üîß Part 1.2b: Simple Exception Examples\n",
    "\n",
    "Here are some simpler examples of how Weave tracks exceptions and failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc877b0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Define a stricter data structure that's more likely to fail\n",
    "class DetailedCustomerEmail(BaseModel):\n",
    "    customer_name: str = Field(description=\"Full name of the customer\")\n",
    "    customer_title: Optional[str] = Field(description=\"Job title if mentioned\")\n",
    "    company: Optional[str] = Field(description=\"Company name if mentioned\")\n",
    "    product: str = Field(description=\"Product name including version\")\n",
    "    product_version: Optional[str] = Field(description=\"Specific version number\")\n",
    "    issue: str = Field(description=\"Detailed issue description\")\n",
    "    severity: str = Field(description=\"critical, high, medium, or low\")\n",
    "    sentiment: str = Field(description=\"positive, neutral, or negative\")\n",
    "\n",
    "\n",
    "@weave.op\n",
    "def analyze_detailed_email(email: str) -> DetailedCustomerEmail:\n",
    "    \"\"\"Analyze email with strict schema - more likely to fail.\"\"\"\n",
    "    client = OpenAI()\n",
    "\n",
    "    # Use a less capable model without structured outputs\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",  # Older model, no structured outputs\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"Extract ALL fields from the email and return as JSON:\n",
    "                {\n",
    "                    \"customer_name\": \"full name\",\n",
    "                    \"customer_title\": \"job title or null\",\n",
    "                    \"company\": \"company name or null\", \n",
    "                    \"product\": \"product name with version\",\n",
    "                    \"product_version\": \"version number or null\",\n",
    "                    \"issue\": \"detailed issue description\",\n",
    "                    \"severity\": \"critical/high/medium/low\",\n",
    "                    \"sentiment\": \"positive/neutral/negative\"\n",
    "                }\n",
    "                Use null for missing optional fields. All fields are required.\"\"\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": email,\n",
    "            },\n",
    "        ],\n",
    "        temperature=0.9,  # High temperature = more unpredictable\n",
    "    )\n",
    "\n",
    "    # Manual JSON parsing - prone to errors!\n",
    "    try:\n",
    "        # Extract JSON from response (model might add extra text)\n",
    "        response_text = response.choices[0].message.content\n",
    "\n",
    "        # Try to find JSON in the response (brittle!)\n",
    "        json_match = re.search(r\"\\{.*\\}\", response_text, re.DOTALL)\n",
    "        if not json_match:\n",
    "            raise ValueError(\"No JSON found in response\")\n",
    "\n",
    "        json_str = json_match.group(0)\n",
    "        data = json.loads(json_str)\n",
    "\n",
    "        # Manual validation and construction (many failure points!)\n",
    "        return DetailedCustomerEmail(\n",
    "            customer_name=data[\"customer_name\"],\n",
    "            customer_title=data.get(\"customer_title\"),\n",
    "            company=data.get(\"company\"),\n",
    "            product=data[\"product\"],\n",
    "            product_version=data.get(\"product_version\"),\n",
    "            issue=data[\"issue\"],\n",
    "            severity=data[\"severity\"],\n",
    "            sentiment=data[\"sentiment\"],\n",
    "        )\n",
    "    except (json.JSONDecodeError, KeyError, ValueError) as e:\n",
    "        # Re-raise with more context\n",
    "        raise ValueError(\n",
    "            f\"Failed to parse model response: {str(e)}. Response was: {response_text[:200]}...\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Test with various emails to see exceptions\n",
    "test_emails_for_errors = [\n",
    "    \"Fix this NOW!\",  # Too short, missing almost everything\n",
    "    \"The thing is broken - Anonymous\",  # Missing key details\n",
    "    \"My product isn't working\",  # No name, no specific product\n",
    "    \"üò°üò°üò°\",  # Just emojis\n",
    "    \"Contact: J. Smith\\nProduct: Version 2.0\\nIssue: Yes\",  # Ambiguous/incomplete\n",
    "    \"HELP! Everything is on fire! Call 911!\",  # Panic mode, no real info\n",
    "    test_email,  # This one might work\n",
    "    \"I am very satisfied with your service. Thank you! -A Customer\",  # Positive but missing product\n",
    "]\n",
    "\n",
    "print(\"üêû Testing exception tracking with challenging emails...\")\n",
    "print(\"=\" * 60)\n",
    "for i, email in enumerate(test_emails_for_errors):\n",
    "    print(f\"\\nüìß Test {i+1}: '{email[:50]}{'...' if len(email) > 50 else ''}'\")\n",
    "    try:\n",
    "        result = analyze_detailed_email(email)\n",
    "        print(f\"‚úÖ Success: {result.customer_name} - {result.product}\")\n",
    "        print(f\"   Severity: {result.severity}, Sentiment: {result.sentiment}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"‚ùå Parsing Error: {str(e)[:100]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {type(e).__name__}: {str(e)[:100]}...\")\n",
    "    print(\"   ‚Üí Check Weave UI for full trace!\")\n",
    "\n",
    "\n",
    "# Demonstrate JSON parsing errors\n",
    "@weave.op\n",
    "def parse_customer_response(response_text: str) -> dict:\n",
    "    \"\"\"Parse a JSON response - demonstrates parsing errors.\"\"\"\n",
    "    # This will fail if response_text is not valid JSON\n",
    "    data = json.loads(response_text)\n",
    "\n",
    "    # This will fail if required fields are missing\n",
    "    return {\n",
    "        \"name\": data[\"customer\"][\"name\"],  # Nested field access\n",
    "        \"issue\": data[\"issue\"][\"description\"],\n",
    "        \"priority\": data[\"priority\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# Test parsing errors\n",
    "test_responses = [\n",
    "    '{\"customer\": {\"name\": \"John\"}, \"issue\": {\"description\": \"Bug\"}, \"priority\": \"high\"}',  # Valid\n",
    "    '{\"name\": \"Jane\", \"issue\": \"Problem\"}',  # Missing nested structure\n",
    "    \"Not JSON at all!\",  # Invalid JSON\n",
    "]\n",
    "\n",
    "print(\"\\n\\nüîç Testing JSON parsing errors...\")\n",
    "for i, response in enumerate(test_responses):\n",
    "    print(f\"\\nüìÑ Test {i+1}:\")\n",
    "    try:\n",
    "        result = parse_customer_response(response)\n",
    "        print(f\"‚úÖ Parsed successfully: {result}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå JSON Error: {e}\")\n",
    "    except KeyError as e:\n",
    "        print(f\"‚ùå Missing field: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error: {type(e).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fdb9c6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### üêû Part 1.2: Exception Tracking\n",
    "\n",
    "Weave makes it easy to debug when things go wrong.\n",
    "\n",
    "TODO: Also, this section is pretty verbose just to highlight error tracking - if we can make it simpler so that the student has less to read through and understand, that would be great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7552ad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "def problematic_analyzer(email: str) -> Optional[CustomerEmail]:\n",
    "    \"\"\"An analyzer that might fail - perfect for debugging!\"\"\"\n",
    "    if \"error\" in email.lower():\n",
    "        raise ValueError(\"Email contains error keyword!\")\n",
    "    if len(email) < 20:\n",
    "        print(\"‚ö†Ô∏è Email too short, returning None\")\n",
    "        return None\n",
    "    if \"urgent\" in email.lower():\n",
    "        # Simulate a timeout\n",
    "        import time\n",
    "\n",
    "        time.sleep(0.1)  # In real scenarios, this might be a network timeout\n",
    "\n",
    "    return analyze_customer_email(email)\n",
    "\n",
    "\n",
    "# Test with problematic inputs\n",
    "debug_test_emails = [\n",
    "    \"Error in system\",  # Will raise exception\n",
    "    \"Too short\",  # Will return None\n",
    "    \"URGENT: Normal email from Alice about ProductX not working properly\",  # Will be slow\n",
    "]\n",
    "\n",
    "print(\"üîç Testing problematic analyzer...\")\n",
    "for email in debug_test_emails:\n",
    "    print(f\"\\nüìß Testing: {email}\")\n",
    "    try:\n",
    "        result = problematic_analyzer(email)\n",
    "        if result:\n",
    "            print(f\"‚úÖ Success: {result.customer_name}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Returned None\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Exception: {type(e).__name__}: {e}\")\n",
    "\n",
    "print(\"\\nüí° Check the Weave UI to see:\")\n",
    "print(\"  - Red highlighted failed calls\")\n",
    "print(\"  - Full stack traces for exceptions\")\n",
    "print(\"  - Timing information for slow calls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35d18ed",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### üé¨ Part 1.3: Media Support & Multimodal Tracing\n",
    "\n",
    "Weave can automatically trace and log various media types including images, videos, audio, and PDFs.\n",
    "This is especially useful for multimodal AI applications.\n",
    "TODO: (NEW CELL still focused on basic tracing) I would like to add another cell on tracing here that showcases Media Classes\n",
    "  * Media Classes\n",
    "      * Image support\n",
    "          * OpenAI's base64 pattern\n",
    "          * Standard PIL images are supported\n",
    "      * Video Support (Using the new annotations pattern)\n",
    "      * Audio support\n",
    "      * PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08d0f9f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### üîí Part 1.4: Custom Serialization & Privacy Controls\n",
    "\n",
    "Control what gets logged and how with Weave's serialization features.\n",
    "Perfect for handling large objects, PII redaction, and privacy controls.\n",
    "TODO: (NEW CELL still focused on basic tracing) I would like to add another cell on tracing here that showcases pre- and post- processing to control what gets serialized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd15f993",
   "metadata": {},
   "source": [
    "### üåä Part 1.5: Streaming & Generators\n",
    "\n",
    "Weave can trace iterators, generators, and streaming data patterns.\n",
    "Essential for real-time applications and memory-efficient processing.\n",
    "TODO: (NEW CELL still focused on basic tracing) We should show that different forms of generators / iterators can be traced as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437cae78",
   "metadata": {},
   "source": [
    "## üìä Part 2: Building Evaluations\n",
    "\n",
    "Now let's evaluate our email analyzer using Weave's evaluation framework.\n",
    "We'll use a more challenging dataset to expose model weaknesses.\n",
    "\n",
    "**Understanding Weave's Evaluation Data Model:**\n",
    "1. An **evaluation** is the pairing of a dataset and a set of scorers (think of it like a test suite for a specific task)\n",
    "2. An **evaluation run** is the result of running an evaluation against a specific model\n",
    "3. Within an evaluation run, there are (num_rows * num_trials) **predict_and_score** blocks which contain the prediction calls and the scoring calls for a single row of the dataset\n",
    "4. Scores are stored within the predict_and_score output, but also directly on the prediction call itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2817479",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create a challenging evaluation dataset with tricky examples\n",
    "eval_examples = [\n",
    "    # Easy examples (even basic model should get these)\n",
    "    {\n",
    "        \"email\": \"Hi Support, I'm John Smith and my DataProcessor-Pro v2.5 isn't working correctly. The data export feature is producing corrupted files. Very frustrated!\",\n",
    "        \"expected_name\": \"John Smith\",\n",
    "        \"expected_product\": \"DataProcessor-Pro v2.5\",\n",
    "        \"expected_sentiment\": \"negative\",\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"Hello, this is Dr. Alice Chen. I wanted to say that your AI-Assistant tool is fantastic! Everything works perfectly. Thank you!\",\n",
    "        \"expected_name\": \"Dr. Alice Chen\",\n",
    "        \"expected_product\": \"AI-Assistant\",\n",
    "        \"expected_sentiment\": \"positive\",\n",
    "    },\n",
    "    # Medium difficulty - ambiguous names/products\n",
    "    {\n",
    "        \"email\": \"Jane from accounting here. The CloudSync Plus works fine but Enterprise Sync Module has delays. Not critical.\",\n",
    "        \"expected_name\": \"Jane\",\n",
    "        \"expected_product\": \"Enterprise Sync Module\",  # NOT CloudSync Plus!\n",
    "        \"expected_sentiment\": \"neutral\",\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"My SmartHub won't connect to anything. Super annoying. - Bob Wilson\\nSenior Manager\\nTech Solutions Inc\",\n",
    "        \"expected_name\": \"Bob Wilson\",\n",
    "        \"expected_product\": \"SmartHub\",  # Model info missing\n",
    "        \"expected_sentiment\": \"negative\",\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"Spoke with Sarah about the issue. Still having problems with WorkflowMax crashing. Mike O'Brien, CEO\",\n",
    "        \"expected_name\": \"Mike O'Brien\",  # NOT Sarah\n",
    "        \"expected_product\": \"WorkflowMax\",\n",
    "        \"expected_sentiment\": \"negative\",\n",
    "    },\n",
    "    # Hard examples - names in unusual places\n",
    "    {\n",
    "        \"email\": \"The new update broke everything! Nothing works anymore on the ProSuite 3000. Call me - signed, frustrated customer Zhang Wei\",\n",
    "        \"expected_name\": \"Zhang Wei\",\n",
    "        \"expected_product\": \"ProSuite 3000\",\n",
    "        \"expected_sentiment\": \"negative\",\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"RE: Ticket #1234\\nCustomer Mar√≠a Garc√≠a called about CloudVault. She says thanks for fixing the sync issue! Works great now.\",\n",
    "        \"expected_name\": \"Mar√≠a Garc√≠a\",\n",
    "        \"expected_product\": \"CloudVault\",\n",
    "        \"expected_sentiment\": \"positive\",\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"My assistant Jennifer will send the logs. The actual problem is with DataMiner Pro, not the viewer. -Raj (Dr. Rajesh Patel)\",\n",
    "        \"expected_name\": \"Dr. Rajesh Patel\",  # NOT Jennifer, and full name from signature\n",
    "        \"expected_product\": \"DataMiner Pro\",  # NOT the viewer\n",
    "        \"expected_sentiment\": \"neutral\",  # Matter-of-fact, not emotional\n",
    "    },\n",
    "    # Very hard - misleading information\n",
    "    {\n",
    "        \"email\": \"Johnson recommended your software. Smith from our team loves CloudSync. But I'm having issues with it. Brown, James Brown.\",\n",
    "        \"expected_name\": \"James Brown\",  # NOT Johnson or Smith\n",
    "        \"expected_product\": \"CloudSync\",\n",
    "        \"expected_sentiment\": \"negative\",  # Having issues despite others liking it\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"Great product! Though the InvoiceGen module crashes sometimes. Still recommend it! Anna from Stockholm\",\n",
    "        \"expected_name\": \"Anna\",\n",
    "        \"expected_product\": \"InvoiceGen module\",\n",
    "        \"expected_sentiment\": \"positive\",  # Overall positive despite crashes\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"Update on case by Thompson: Lee's WorkStation Pro still showing error 0x80004005. Previous tech couldn't resolve.\",\n",
    "        \"expected_name\": \"Lee\",  # NOT Thompson\n",
    "        \"expected_product\": \"WorkStation Pro\",\n",
    "        \"expected_sentiment\": \"negative\",\n",
    "    },\n",
    "    # Extremely hard - complex scenarios\n",
    "    {\n",
    "        \"email\": \"Hi, chatted with your colleague Emma (super helpful!). Anyway, ReportBuilder works ok but takes forever. ‚ÄîSamantha Park, CTO\",\n",
    "        \"expected_name\": \"Samantha Park\",  # NOT Emma\n",
    "        \"expected_product\": \"ReportBuilder\",\n",
    "        \"expected_sentiment\": \"neutral\",  # \"works ok\" but slow - not fully negative\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"FYI - Customer called: Pierre-Alexandre Dubois mentioned the API-Gateway is fantastic, just needs better docs. Direct quote.\",\n",
    "        \"expected_name\": \"Pierre-Alexandre Dubois\",\n",
    "        \"expected_product\": \"API-Gateway\",\n",
    "        \"expected_sentiment\": \"positive\",  # \"fantastic\" outweighs doc complaint\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"Worst experience ever with tech support! Though I admit ProductX works well. O'Sullivan here (Francis).\",\n",
    "        \"expected_name\": \"Francis O'Sullivan\",  # Name split across sentence\n",
    "        \"expected_product\": \"ProductX\",\n",
    "        \"expected_sentiment\": \"negative\",  # Support experience outweighs product working\n",
    "    },\n",
    "    # Trick examples - products that sound like names\n",
    "    {\n",
    "        \"email\": \"Maxwell keeps crashing! This software is terrible. Signed, angry user Li Chen\",\n",
    "        \"expected_name\": \"Li Chen\",\n",
    "        \"expected_product\": \"Maxwell\",  # Maxwell is the product, not a person\n",
    "        \"expected_sentiment\": \"negative\",\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"Please tell Gordon that the Morgan Analytics Suite works perfectly now. Thanks! - Yuki Tanaka\",\n",
    "        \"expected_name\": \"Yuki Tanaka\",  # NOT Gordon\n",
    "        \"expected_product\": \"Morgan Analytics Suite\",  # Morgan is part of product name\n",
    "        \"expected_sentiment\": \"positive\",\n",
    "    },\n",
    "    # Ambiguous sentiment\n",
    "    {\n",
    "        \"email\": \"DataFlow Pro is exactly what I expected from your company. Classic experience. Jo√£o Silva, Product Manager\",\n",
    "        \"expected_name\": \"Jo√£o Silva\",\n",
    "        \"expected_product\": \"DataFlow Pro\",\n",
    "        \"expected_sentiment\": \"negative\",  # Sarcastic - \"expected\" and \"classic\" imply typically bad\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"The ChromaEdit tool works... I guess. Does what it says. Whatever. -Kim\",\n",
    "        \"expected_name\": \"Kim\",\n",
    "        \"expected_product\": \"ChromaEdit tool\",\n",
    "        \"expected_sentiment\": \"neutral\",  # Apathetic, not negative or positive\n",
    "    },\n",
    "    # Multiple products mentioned\n",
    "    {\n",
    "        \"email\": \"Upgraded from TaskMaster to ProjectPro. Having issues with ProjectPro's gantt charts. Anne-Marie Rousseau\",\n",
    "        \"expected_name\": \"Anne-Marie Rousseau\",\n",
    "        \"expected_product\": \"ProjectPro\",  # The one with issues, not TaskMaster\n",
    "        \"expected_sentiment\": \"negative\",\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"Hi! love your VideoEdit, PhotoEdit, and AudioEdit apps! Especially AudioEdit! Muhammad here :)\",\n",
    "        \"expected_name\": \"Muhammad\",\n",
    "        \"expected_product\": \"AudioEdit\",  # The one especially mentioned\n",
    "        \"expected_sentiment\": \"positive\",\n",
    "    },\n",
    "    # Edge cases\n",
    "    {\n",
    "        \"email\": \"Yo! Sup? Ur SystemMonitor thing is broke af. fix it asap!!!! - xXx_Dmitri_xXx\",\n",
    "        \"expected_name\": \"Dmitri\",  # Extract from gamertag\n",
    "        \"expected_product\": \"SystemMonitor\",\n",
    "        \"expected_sentiment\": \"negative\",\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"¬°Hola! Carlos M√©ndez aqu√≠. Su programa FinanceTracker es excelente pero muy caro. Gracias.\",\n",
    "        \"expected_name\": \"Carlos M√©ndez\",\n",
    "        \"expected_product\": \"FinanceTracker\",\n",
    "        \"expected_sentiment\": \"neutral\",  # Good but expensive = neutral\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"Re: Jackson's complaint\\n\\nI disagree with Jackson. The Scheduler App works fine for me.\\n\\nBest,\\nPriya Sharma\\nHead of IT\",\n",
    "        \"expected_name\": \"Priya Sharma\",  # NOT Jackson\n",
    "        \"expected_product\": \"Scheduler App\",\n",
    "        \"expected_sentiment\": \"positive\",  # Disagrees with complaint\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"This is regarding the issue with CloudBackup Pro v3.2.1 that Jennifer Chen reported. I'm her manager, David Kim, following up.\",\n",
    "        \"expected_name\": \"David Kim\",  # The sender, not Jennifer\n",
    "        \"expected_product\": \"CloudBackup Pro v3.2.1\",\n",
    "        \"expected_sentiment\": \"negative\",  # Following up on an issue\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"üò°üò°üò° InventoryMaster deleted everything!!! üò≠üò≠üò≠ - call me back NOW! //Singh\",\n",
    "        \"expected_name\": \"Singh\",\n",
    "        \"expected_product\": \"InventoryMaster\",\n",
    "        \"expected_sentiment\": \"negative\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create a Weave Dataset\n",
    "support_dataset = Dataset(name=\"support_emails\", rows=eval_examples)\n",
    "\n",
    "\n",
    "# üéØ Define scoring functions\n",
    "@weave.op\n",
    "def name_accuracy(expected_name: str, output: CustomerEmail) -> dict[str, Any]:\n",
    "    \"\"\"Check if the extracted name matches.\"\"\"\n",
    "    is_correct = expected_name.lower() == output.customer_name.lower()\n",
    "    return {\"correct\": is_correct, \"score\": 1.0 if is_correct else 0.0}\n",
    "\n",
    "\n",
    "@weave.op\n",
    "def sentiment_accuracy(\n",
    "    expected_sentiment: str, output: CustomerEmail\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"Check if the sentiment analysis is correct.\"\"\"\n",
    "    is_correct = expected_sentiment.lower() == output.sentiment.lower()\n",
    "    return {\"correct\": is_correct, \"score\": 1.0 if is_correct else 0.0}\n",
    "\n",
    "\n",
    "@weave.op\n",
    "def extraction_quality(email: str, output: CustomerEmail) -> dict[str, Any]:\n",
    "    \"\"\"Evaluate overall extraction quality.\"\"\"\n",
    "    score = 0.0\n",
    "    feedback = []\n",
    "\n",
    "    # Check if all fields are extracted\n",
    "    if output.customer_name and output.customer_name != \"Unknown\":\n",
    "        score += 0.33\n",
    "    else:\n",
    "        feedback.append(\"Missing customer name\")\n",
    "\n",
    "    if output.product and output.product != \"Unknown\":\n",
    "        score += 0.33\n",
    "    else:\n",
    "        feedback.append(\"Missing product\")\n",
    "\n",
    "    if output.issue and len(output.issue) > 10:\n",
    "        score += 0.34\n",
    "    else:\n",
    "        feedback.append(\"Issue description too short\")\n",
    "\n",
    "    return {\n",
    "        \"score\": score,\n",
    "        \"feedback\": \"; \".join(feedback)\n",
    "        if feedback\n",
    "        else \"All fields extracted successfully\",\n",
    "    }\n",
    "\n",
    "\n",
    "# üöÄ Run the evaluation (notebook-friendly version)\n",
    "evaluation = Evaluation(\n",
    "    dataset=support_dataset,\n",
    "    scorers=[name_accuracy, sentiment_accuracy, extraction_quality],\n",
    "    trails=3,\n",
    ")\n",
    "\n",
    "print(\"üèÉ Running evaluation...\")\n",
    "# For notebooks, use await instead of asyncio.run\n",
    "# In Jupyter/IPython notebooks, you can use await directly\n",
    "# eval_results = await evaluation.evaluate(analyze_customer_email)\n",
    "# For Python scripts, use:\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "eval_results = asyncio.run(evaluation.evaluate(analyze_customer_email))\n",
    "print(\"‚úÖ Evaluation complete! Check the Weave UI for detailed results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41872b55",
   "metadata": {},
   "source": [
    "### üéØ Part 2.1: Using Pre-built Scorers\n",
    "\n",
    "Weave provides many pre-built scorers for common evaluation tasks!\n",
    "No need to reinvent the wheel for standard metrics.\n",
    "\n",
    "**Note**: To use pre-built scorers, install with: `pip install weave[scorers]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b05e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pre-built scorers\n",
    "try:\n",
    "    from weave.scorers import (\n",
    "        EmbeddingSimilarityScorer,\n",
    "        OpenAIModerationScorer,\n",
    "        PydanticScorer,\n",
    "        ValidJSONScorer,\n",
    "    )\n",
    "\n",
    "    SCORERS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SCORERS_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è Pre-built scorers not available. Install with: pip install weave[scorers]\")\n",
    "\n",
    "# Example 1: ValidJSONScorer - Check if output is valid JSON\n",
    "if SCORERS_AVAILABLE:\n",
    "\n",
    "    @weave.op\n",
    "    def generate_user_data(request: str) -> str:\n",
    "        \"\"\"Generate user data in JSON format.\"\"\"\n",
    "        if \"valid\" in request.lower():\n",
    "            return '{\"name\": \"John Doe\", \"age\": 30, \"email\": \"john@example.com\"}'\n",
    "        elif \"invalid\" in request.lower():\n",
    "            return '{\"name\": \"Jane Doe\", \"age\": 25, \"email\"'  # Invalid JSON\n",
    "        else:\n",
    "            return \"This is not JSON at all\"\n",
    "\n",
    "    # Create a dataset\n",
    "    json_dataset = Dataset(\n",
    "        name=\"json_validation_test\",\n",
    "        rows=[\n",
    "            {\"request\": \"Generate valid user JSON\"},\n",
    "            {\"request\": \"Generate invalid JSON\"},\n",
    "            {\"request\": \"Generate plain text\"},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Use the ValidJSONScorer\n",
    "    json_scorer = ValidJSONScorer()\n",
    "\n",
    "    print(\"üéØ Example 1: ValidJSONScorer\")\n",
    "    # Quick test\n",
    "    test_output = generate_user_data(\"Generate valid user JSON\")\n",
    "    json_result = asyncio.run(json_scorer.score(output=test_output))\n",
    "    print(f\"  Valid JSON? {json_result['json_valid']}\")\n",
    "\n",
    "# Example 2: PydanticScorer - Validate against a schema\n",
    "if SCORERS_AVAILABLE:\n",
    "    from pydantic import EmailStr\n",
    "\n",
    "    class UserData(BaseModel):\n",
    "        name: str\n",
    "        age: int\n",
    "        email: EmailStr\n",
    "\n",
    "    @weave.op\n",
    "    def generate_structured_data(request: str) -> str:\n",
    "        \"\"\"Generate data that should match UserData schema.\"\"\"\n",
    "        if \"correct\" in request.lower():\n",
    "            return '{\"name\": \"Alice Smith\", \"age\": 28, \"email\": \"alice@example.com\"}'\n",
    "        else:\n",
    "            return '{\"name\": \"Bob\", \"age\": \"twenty-five\", \"email\": \"not-an-email\"}'\n",
    "\n",
    "    # Use PydanticScorer with our schema\n",
    "    pydantic_scorer = PydanticScorer(model=UserData)\n",
    "\n",
    "    print(\"\\nüéØ Example 2: PydanticScorer\")\n",
    "    test_output = generate_structured_data(\"Generate correct data\")\n",
    "    pydantic_result = asyncio.run(pydantic_scorer.score(output=test_output))\n",
    "    print(f\"  Valid schema? {pydantic_result['pydantic_valid']}\")\n",
    "\n",
    "# Example 3: EmbeddingSimilarityScorer - Semantic similarity\n",
    "if SCORERS_AVAILABLE:\n",
    "    similarity_dataset = Dataset(\n",
    "        name=\"similarity_test\",\n",
    "        rows=[\n",
    "            {\n",
    "                \"input\": \"What's the weather like?\",\n",
    "                \"target\": \"How is the weather today?\",  # Similar meaning\n",
    "            },\n",
    "            {\n",
    "                \"input\": \"Tell me about dogs\",\n",
    "                \"target\": \"Explain quantum physics\",  # Very different\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    @weave.op\n",
    "    def paraphrase_model(input: str) -> str:\n",
    "        \"\"\"A model that attempts to paraphrase.\"\"\"\n",
    "        # In reality, this would use an LLM\n",
    "        if \"weather\" in input.lower():\n",
    "            return \"What are the weather conditions?\"\n",
    "        else:\n",
    "            return \"Something completely different\"\n",
    "\n",
    "    # Use EmbeddingSimilarityScorer (requires OpenAI API key)\n",
    "    similarity_scorer = EmbeddingSimilarityScorer(\n",
    "        model_id=\"openai/text-embedding-3-small\",\n",
    "        threshold=0.7,  # Cosine similarity threshold\n",
    "    )\n",
    "\n",
    "    print(\"\\nüéØ Example 3: EmbeddingSimilarityScorer\")\n",
    "    print(\"  (Compares semantic similarity between outputs and targets)\")\n",
    "\n",
    "# Example 4: OpenAIModerationScorer - Content safety\n",
    "if SCORERS_AVAILABLE:\n",
    "\n",
    "    @weave.op\n",
    "    def user_content_generator(prompt: str) -> str:\n",
    "        \"\"\"Generate user content based on prompt.\"\"\"\n",
    "        if \"angry\" in prompt.lower():\n",
    "            return \"I'm so frustrated with this terrible service!\"\n",
    "        else:\n",
    "            return \"Thank you for the wonderful support!\"\n",
    "\n",
    "    moderation_dataset = Dataset(\n",
    "        name=\"moderation_test\",\n",
    "        rows=[\n",
    "            {\"prompt\": \"Write an angry review\"},\n",
    "            {\"prompt\": \"Write a positive review\"},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Use OpenAIModerationScorer\n",
    "    moderation_scorer = OpenAIModerationScorer()\n",
    "\n",
    "    print(\"\\nüéØ Example 4: OpenAIModerationScorer\")\n",
    "    print(\"  (Checks for potentially harmful content)\")\n",
    "\n",
    "# Show all available pre-built scorers\n",
    "print(\"\\nüìö Available Pre-built Scorers in Weave:\")\n",
    "print(\"  ‚úÖ ValidJSONScorer - Validate JSON output\")\n",
    "print(\"  ‚úÖ ValidXMLScorer - Validate XML output\")\n",
    "print(\"  ‚úÖ PydanticScorer - Validate against Pydantic models\")\n",
    "print(\"  ‚úÖ EmbeddingSimilarityScorer - Semantic similarity\")\n",
    "print(\"  ‚úÖ OpenAIModerationScorer - Content moderation\")\n",
    "print(\"  ‚úÖ HallucinationFreeScorer - Check for hallucinations\")\n",
    "print(\"  ‚úÖ SummarizationScorer - Evaluate summaries\")\n",
    "print(\"  ‚úÖ ContextEntityRecallScorer - RAGAS entity recall\")\n",
    "print(\"  ‚úÖ ContextRelevancyScorer - RAGAS relevancy\")\n",
    "print(\"\\nüí° Install with: pip install weave[scorers]\")\n",
    "print(\"üìñ Full docs: https://docs.wandb.ai/guides/weave/evaluation/builtin_scorers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c122116",
   "metadata": {},
   "source": [
    "### üìù Part 2.2: Pairwise Scoring\n",
    "TODO: (New Cell) - Let's add a cell specifically to showcase pairwise scoring (Human to link to content here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75274fe",
   "metadata": {},
   "source": [
    "### üìù Part 2.3: Using EvaluationLogger\n",
    "\n",
    "The `EvaluationLogger` provides a flexible way to log evaluation data incrementally.\n",
    "This is perfect when you don't have all your data upfront or want more control.\n",
    "\n",
    "**Important**: Since EvaluationLogger doesn't use Model/Dataset objects, the `model`\n",
    "and `dataset` parameters are crucial for identification.\n",
    "- `model`: Can be a string OR dictionary (for rich metadata)\n",
    "- `dataset`: Must be a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bac4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example using EvaluationLogger for custom evaluation flow\n",
    "# You can use simple strings for identification (commented out on purpose)\n",
    "# eval_logger = EvaluationLogger(\n",
    "#     model=\"email_analyzer_gpt35\",  # Model name/version\n",
    "#     dataset=\"support_emails\",  # Dataset name (must be string)\n",
    "# )\n",
    "\n",
    "# Model can use dictionaries for richer identification (recommended!)\n",
    "# Dataset must be a string\n",
    "eval_logger_rich = EvaluationLogger(\n",
    "    model={\n",
    "        \"name\": \"email_analyzer\",\n",
    "        \"version\": \"v1.2\",\n",
    "        \"llm\": \"gpt-3.5-turbo\",\n",
    "        \"temperature\": 0.7,\n",
    "        \"prompt_version\": \"2024-01\",\n",
    "    },\n",
    "    dataset=\"support_emails_2024Q1\",  # Dataset must be string\n",
    ")\n",
    "\n",
    "# Let's use the rich logger for our demo\n",
    "print(\"üìä Using EvaluationLogger with rich metadata...\")\n",
    "\n",
    "# Process each example with more control\n",
    "for i, example in enumerate(eval_examples[:3]):  # Just first 3 for demo\n",
    "    # Analyze the email\n",
    "    try:\n",
    "        output = analyze_customer_email(example[\"email\"])\n",
    "\n",
    "        # Log the prediction\n",
    "        pred_logger = eval_logger_rich.log_prediction(\n",
    "            inputs={\"email\": example[\"email\"]}, output=output.model_dump()\n",
    "        )\n",
    "\n",
    "        # Log multiple scores for this prediction\n",
    "        # Check name accuracy\n",
    "        name_match = example[\"expected_name\"].lower() == output.customer_name.lower()\n",
    "        pred_logger.log_score(scorer=\"name_accuracy\", score=1.0 if name_match else 0.0)\n",
    "\n",
    "        # Check sentiment\n",
    "        sentiment_match = example[\"expected_sentiment\"] == output.sentiment\n",
    "        pred_logger.log_score(\n",
    "            scorer=\"sentiment_accuracy\", score=1.0 if sentiment_match else 0.0\n",
    "        )\n",
    "\n",
    "        # Custom business logic score\n",
    "        if \"urgent\" in example[\"email\"].lower() and output.sentiment != \"negative\":\n",
    "            pred_logger.log_score(\n",
    "                scorer=\"urgency_detection\",\n",
    "                score=0.0,  # Failed to detect urgency\n",
    "            )\n",
    "        else:\n",
    "            pred_logger.log_score(scorer=\"urgency_detection\", score=1.0)\n",
    "\n",
    "        # Always finish logging for each prediction\n",
    "        pred_logger.finish()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing example {i+1}: {e}\")\n",
    "        # You can still log failed predictions\n",
    "        pred_logger = eval_logger_rich.log_prediction(\n",
    "            inputs={\"email\": example[\"email\"]}, output={\"error\": str(e)}\n",
    "        )\n",
    "        pred_logger.log_score(scorer=\"success\", score=0.0)\n",
    "        pred_logger.finish()\n",
    "\n",
    "# Log summary statistics\n",
    "eval_logger_rich.log_summary(\n",
    "    {\n",
    "        \"total_examples\": 3,\n",
    "        \"evaluation_type\": \"manual\",\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"notes\": \"Workshop demo with rich metadata\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úÖ EvaluationLogger demo complete! Check the Weave UI.\")\n",
    "print(\"üí° Tip: The rich metadata makes it easy to filter and compare evaluations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68528caf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### üèÜ Part 2.4: Model Comparison\n",
    "\n",
    "Let's compare different approaches using Weave's Model class.\n",
    "We'll create models with varying quality to see clear differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5826b06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different model variants\n",
    "class EmailAnalyzerModel(Model):\n",
    "    \"\"\"Base model for email analysis with configurable parameters.\"\"\"\n",
    "\n",
    "    label: str = \"email_analyzer\"\n",
    "    model_name: str = \"gpt-4o-mini\"\n",
    "    temperature: float = 0.1\n",
    "    system_prompt: str = \"You are a customer support analyst.\"\n",
    "\n",
    "    @weave.op\n",
    "    def predict(self, email: str) -> CustomerEmail:\n",
    "        \"\"\"Analyze email with configurable parameters.\"\"\"\n",
    "        client = OpenAI()\n",
    "\n",
    "        response = client.beta.chat.completions.parse(\n",
    "            model=self.model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"Analyze this email:\\n{email}\"},\n",
    "            ],\n",
    "            response_format=CustomerEmail,\n",
    "            temperature=self.temperature,\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.parsed\n",
    "\n",
    "\n",
    "# Create model variants with different quality levels\n",
    "basic_model = EmailAnalyzerModel(\n",
    "    label=\"basic_analyzer\",\n",
    "    system_prompt=\"Extract customer name, product name, issue, and sentiment from email.\",  # Too simple - no guidance\n",
    "    temperature=0.95,  # Very high - more random/mistakes\n",
    ")\n",
    "\n",
    "detailed_model = EmailAnalyzerModel(\n",
    "    label=\"detailed_analyzer\",\n",
    "    system_prompt=\"\"\"You are an expert customer support analyst. Carefully analyze the email:\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. Customer name: Extract the name of the person WRITING the email (not people mentioned)\n",
    "   - Check signatures, sign-offs, and self-introductions\n",
    "   - If multiple names appear, identify who is actually writing\n",
    "   - Include full name if available (e.g., \"Dr. Rajesh Patel\" not just \"Raj\")\n",
    "   \n",
    "2. Product: Identify the SPECIFIC product having issues\n",
    "   - If multiple products mentioned, focus on the problematic one\n",
    "   - Include version numbers if provided\n",
    "   - Don't confuse product names with people names\n",
    "   \n",
    "3. Sentiment: Analyze the OVERALL tone\n",
    "   - positive: satisfied, happy, thankful (even with minor complaints)\n",
    "   - negative: frustrated, angry, disappointed\n",
    "   - neutral: matter-of-fact, indifferent, mixed feelings\n",
    "   - Consider sarcasm and actual meaning beyond words\"\"\",\n",
    "    temperature=0.0,  # Precise\n",
    ")\n",
    "\n",
    "balanced_model = EmailAnalyzerModel(\n",
    "    label=\"balanced_analyzer\",\n",
    "    system_prompt=\"\"\"Extract customer support information from emails.\n",
    "    \n",
    "    Guidelines:\n",
    "    - Customer name: The person sending the email (check signatures)\n",
    "    - Product: The main product being discussed\n",
    "    - Issue: Brief description of the problem\n",
    "    - Sentiment: Overall tone (positive/negative/neutral)\"\"\",\n",
    "    temperature=0.4,  # Moderate temperature\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418369be",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### üîÑ Part 2.5: A/B Testing Models\n",
    "\n",
    "**Important Concept**: When comparing models, we use the SAME evaluation definition\n",
    "(same dataset + scorers) for all models. This ensures fair comparison and allows\n",
    "everyone in the workshop to see aggregated results. Each evaluation run gets a\n",
    "unique ID automatically, but the evaluation definition stays consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ce67b0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "async def compare_models(models: list[Model], dataset: Dataset) -> dict[str, Any]:\n",
    "    \"\"\"Run A/B comparison of multiple models.\"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Create a single evaluation definition that will be used for all models\n",
    "    evaluation = Evaluation(\n",
    "        name=\"email_analyzer_comparison\",  # Same eval for all models\n",
    "        dataset=dataset,\n",
    "        scorers=[name_accuracy, sentiment_accuracy, extraction_quality],\n",
    "    )\n",
    "\n",
    "    for model in models:\n",
    "        print(f\"\\nüìä Evaluating {model.label}...\")\n",
    "\n",
    "        # Run evaluation with optional display name for this specific run\n",
    "        eval_result = await evaluation.evaluate(\n",
    "            model,\n",
    "            __weave={\"display_name\": f\"email_analyzer_comparison - {model.label}\"},\n",
    "        )\n",
    "        results[model.label] = eval_result\n",
    "\n",
    "        print(f\"‚úÖ {model.label} evaluation complete!\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run the comparison\n",
    "print(\"üèÅ Starting model comparison...\")\n",
    "# For notebooks: comparison_results = await compare_models(...)\n",
    "# For scripts:\n",
    "comparison_results = asyncio.run(\n",
    "    compare_models([basic_model, detailed_model, balanced_model], support_dataset)\n",
    ")\n",
    "print(\"\\nüéâ Comparison complete! View the results in the Weave UI.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e11785",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### üéØ Part 2.6: Leaderboard Competition\n",
    "TODO: Interactive challenge time. (leaderboard competition)\n",
    "Now we are going to see who can creat the best model\n",
    "TODO: setup the leaderboard (either interactively in the UI, or add a cell below)\n",
    "Invite students to iterate on the prompt / model to get higher performance (which we will track in the leaderboard!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0285b1e7",
   "metadata": {},
   "source": [
    "## üéØ Part 3: Production Monitoring with Scorers\n",
    "\n",
    "Use Weave's scorer system for real-time guardrails and quality monitoring.\n",
    "This demonstrates the apply_scorer pattern for production use.\n",
    "\n",
    "**Key Concepts**:\n",
    "- **Guardrails**: Block or modify responses (e.g., toxicity filter)\n",
    "- **Monitors**: Track quality metrics without blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d50c3c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Define more realistic production scorers\n",
    "class ContentModerationScorer(Scorer):\n",
    "    \"\"\"Production-ready content moderation scorer.\"\"\"\n",
    "\n",
    "    @weave.op\n",
    "    def score(self, output: dict) -> dict:\n",
    "        \"\"\"Check for inappropriate content using multiple signals.\"\"\"\n",
    "        # Handle both success and error cases\n",
    "        if output.get(\"status\") != \"success\":\n",
    "            return {\"flagged\": False, \"flags\": [], \"severity\": \"none\", \"action\": \"pass\"}\n",
    "\n",
    "        analysis = output.get(\"analysis\", {})\n",
    "        issue_text = analysis.get(\"issue\", \"\").lower()\n",
    "        sentiment = analysis.get(\"sentiment\", \"neutral\")\n",
    "\n",
    "        # Check for various inappropriate content patterns\n",
    "        profanity_patterns = [\n",
    "            \"stupid\",\n",
    "            \"idiotic\",\n",
    "            \"garbage\",\n",
    "            \"trash\",\n",
    "            \"sucks\",\n",
    "            \"terrible\",\n",
    "            \"awful\",\n",
    "            \"worst\",\n",
    "        ]\n",
    "        threat_patterns = [\"sue\", \"lawyer\", \"legal action\", \"court\", \"lawsuit\"]\n",
    "\n",
    "        flags = []\n",
    "        severity = \"none\"\n",
    "\n",
    "        # Check profanity\n",
    "        profanity_found = []\n",
    "        for word in profanity_patterns:\n",
    "            if word in issue_text:\n",
    "                profanity_found.append(word)\n",
    "\n",
    "        if profanity_found:\n",
    "            flags.append(f\"Profanity detected: {', '.join(profanity_found)}\")\n",
    "            severity = \"medium\"\n",
    "\n",
    "        # Check threats\n",
    "        threats_found = []\n",
    "        for pattern in threat_patterns:\n",
    "            if pattern in issue_text:\n",
    "                threats_found.append(pattern)\n",
    "\n",
    "        if threats_found:\n",
    "            flags.append(f\"Legal threat: {', '.join(threats_found)}\")\n",
    "            severity = \"high\"\n",
    "\n",
    "        # Check extreme sentiment with profanity\n",
    "        if sentiment == \"negative\" and profanity_found:\n",
    "            severity = \"high\"\n",
    "            flags.append(\"Negative sentiment with profanity\")\n",
    "\n",
    "        return {\n",
    "            \"flagged\": len(flags) > 0,\n",
    "            \"flags\": flags,\n",
    "            \"severity\": severity,\n",
    "            \"action\": \"block\"\n",
    "            if severity == \"high\"\n",
    "            else (\"review\" if severity == \"medium\" else \"pass\"),\n",
    "        }\n",
    "\n",
    "\n",
    "class ExtractionQualityScorer(Scorer):\n",
    "    \"\"\"Monitor extraction quality and completeness.\"\"\"\n",
    "\n",
    "    @weave.op\n",
    "    def score(self, output: dict, email: str) -> dict:\n",
    "        \"\"\"Comprehensive quality assessment.\"\"\"\n",
    "        if output.get(\"status\") != \"success\":\n",
    "            return {\n",
    "                \"quality_score\": 0.0,\n",
    "                \"passed\": False,\n",
    "                \"issues\": [\"Failed to process email\"],\n",
    "                \"recommendations\": [],\n",
    "                \"extraction_grade\": \"F\",\n",
    "            }\n",
    "\n",
    "        analysis = output.get(\"analysis\", {})\n",
    "        quality_metrics = {\n",
    "            \"completeness\": 0.0,\n",
    "            \"specificity\": 0.0,\n",
    "            \"accuracy\": 0.0,\n",
    "            \"consistency\": 0.0,\n",
    "        }\n",
    "        issues = []\n",
    "        recommendations = []\n",
    "\n",
    "        # 1. Completeness checks (40% weight)\n",
    "        if analysis.get(\"customer_name\") and analysis[\"customer_name\"] not in [\n",
    "            \"Unknown\",\n",
    "            \"\",\n",
    "            None,\n",
    "        ]:\n",
    "            quality_metrics[\"completeness\"] += 0.15\n",
    "        else:\n",
    "            issues.append(\"Missing customer name\")\n",
    "            recommendations.append(\"Check email signatures and greetings for names\")\n",
    "\n",
    "        if analysis.get(\"product\") and analysis[\"product\"] not in [\"Unknown\", \"\", None]:\n",
    "            quality_metrics[\"completeness\"] += 0.15\n",
    "        else:\n",
    "            issues.append(\"Missing product identification\")\n",
    "            recommendations.append(\"Look for product names mentioned in the email\")\n",
    "\n",
    "        if analysis.get(\"issue\") and len(analysis[\"issue\"]) > 10:\n",
    "            quality_metrics[\"completeness\"] += 0.10\n",
    "        else:\n",
    "            issues.append(\"Issue description too brief or missing\")\n",
    "            recommendations.append(\"Extract a more detailed problem description\")\n",
    "\n",
    "        # 2. Specificity checks (30% weight)\n",
    "        product_name = analysis.get(\"product\", \"\")\n",
    "        if product_name and any(char.isdigit() for char in str(product_name)):\n",
    "            # Product includes version/model number\n",
    "            quality_metrics[\"specificity\"] += 0.15\n",
    "        elif product_name:\n",
    "            recommendations.append(\n",
    "                \"Extract product version/model numbers when available\"\n",
    "            )\n",
    "\n",
    "        issue_desc = analysis.get(\"issue\", \"\")\n",
    "        if issue_desc and len(str(issue_desc)) > 30:\n",
    "            quality_metrics[\"specificity\"] += 0.15\n",
    "        elif issue_desc:\n",
    "            recommendations.append(\"Provide more specific issue details\")\n",
    "\n",
    "        # 3. Accuracy checks (20% weight)\n",
    "        # Check if extracted content actually appears in email\n",
    "        email_lower = email.lower()\n",
    "        customer_name = analysis.get(\"customer_name\", \"\")\n",
    "        if customer_name and customer_name != \"Unknown\":\n",
    "            name_parts = customer_name.lower().split()\n",
    "            # Check if at least part of the name appears in email\n",
    "            if any(part in email_lower for part in name_parts if len(part) > 2):\n",
    "                quality_metrics[\"accuracy\"] += 0.10\n",
    "            else:\n",
    "                issues.append(\"Extracted name not found in original email\")\n",
    "\n",
    "        product_mentioned = analysis.get(\"product\", \"\")\n",
    "        if product_mentioned and product_mentioned != \"Unknown\":\n",
    "            # Check for partial matches (product names might be extracted differently)\n",
    "            product_words = product_mentioned.lower().split()\n",
    "            if any(word in email_lower for word in product_words if len(word) > 3):\n",
    "                quality_metrics[\"accuracy\"] += 0.10\n",
    "            else:\n",
    "                issues.append(\"Extracted product not clearly mentioned in email\")\n",
    "\n",
    "        # 4. Consistency checks (10% weight)\n",
    "        sentiment = analysis.get(\"sentiment\", \"neutral\")\n",
    "        urgency = output.get(\"urgency\", \"low\")\n",
    "\n",
    "        # Check sentiment/urgency consistency\n",
    "        consistency_ok = True\n",
    "        if sentiment == \"negative\" and urgency == \"low\":\n",
    "            if not any(\n",
    "                word in issue_desc.lower() for word in [\"minor\", \"small\", \"slight\"]\n",
    "            ):\n",
    "                consistency_ok = False\n",
    "                issues.append(\n",
    "                    \"Negative sentiment but low urgency - might be inconsistent\"\n",
    "                )\n",
    "        elif sentiment == \"positive\" and urgency == \"high\":\n",
    "            consistency_ok = False\n",
    "            issues.append(\"Positive sentiment with high urgency is unusual\")\n",
    "\n",
    "        if consistency_ok:\n",
    "            quality_metrics[\"consistency\"] += 0.10\n",
    "\n",
    "        # Calculate overall score\n",
    "        total_score = sum(quality_metrics.values())\n",
    "\n",
    "        return {\n",
    "            \"quality_score\": total_score,\n",
    "            \"quality_metrics\": quality_metrics,\n",
    "            \"passed\": total_score >= 0.6,  # Lowered threshold for demo\n",
    "            \"issues\": issues,\n",
    "            \"recommendations\": recommendations,\n",
    "            \"extraction_grade\": \"A\"\n",
    "            if total_score >= 0.9\n",
    "            else (\n",
    "                \"B\"\n",
    "                if total_score >= 0.8\n",
    "                else (\n",
    "                    \"C\" if total_score >= 0.6 else (\"D\" if total_score >= 0.4 else \"F\")\n",
    "                )\n",
    "            ),\n",
    "        }\n",
    "\n",
    "\n",
    "@weave.op\n",
    "def production_email_handler(\n",
    "    email: str, request_id: Optional[str] = None\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"Production-ready email handler that returns structured analysis results.\"\"\"\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # Generate request ID if not provided\n",
    "    if not request_id:\n",
    "        request_id = f\"req_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{random.randint(1000, 9999)}\"\n",
    "\n",
    "    try:\n",
    "        # Process the email using our existing analyzer\n",
    "        analysis = analyze_customer_email(email)\n",
    "\n",
    "        # Calculate urgency based on the analysis\n",
    "        urgency = classify_urgency(email, analysis.sentiment)\n",
    "\n",
    "        # Return structured result that scorers expect\n",
    "        return {\n",
    "            \"request_id\": request_id,\n",
    "            \"status\": \"success\",\n",
    "            \"analysis\": {\n",
    "                \"customer_name\": analysis.customer_name,\n",
    "                \"product\": analysis.product,\n",
    "                \"issue\": analysis.issue,\n",
    "                \"sentiment\": analysis.sentiment,\n",
    "            },\n",
    "            \"urgency\": urgency,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log error and return error response\n",
    "        return {\n",
    "            \"request_id\": request_id,\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e),\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "        }\n",
    "\n",
    "\n",
    "# Initialize scorers\n",
    "content_moderation_scorer = ContentModerationScorer()\n",
    "quality_scorer = ExtractionQualityScorer()\n",
    "\n",
    "\n",
    "async def handle_email_with_monitoring(email: str) -> dict[str, Any]:\n",
    "    \"\"\"Handle email with production monitoring and guardrails.\"\"\"\n",
    "    # Process the email and get the Call object\n",
    "    result, call = production_email_handler.call(email)\n",
    "\n",
    "    if result[\"status\"] == \"success\":\n",
    "        # Apply content moderation (guardrail)\n",
    "        moderation_check = await call.apply_scorer(content_moderation_scorer)\n",
    "\n",
    "        # Apply quality monitoring\n",
    "        quality_check = await call.apply_scorer(\n",
    "            quality_scorer, additional_scorer_kwargs={\"email\": email}\n",
    "        )\n",
    "\n",
    "        # Handle moderation results\n",
    "        if moderation_check.result[\"flagged\"]:\n",
    "            action = moderation_check.result[\"action\"]\n",
    "            if action == \"block\":\n",
    "                print(f\"üö´ Content BLOCKED: {moderation_check.result['flags']}\")\n",
    "                result[\"blocked\"] = True\n",
    "                result[\"block_reason\"] = moderation_check.result[\"flags\"]\n",
    "            elif action == \"review\":\n",
    "                print(\n",
    "                    f\"‚ö†Ô∏è Content flagged for review: {moderation_check.result['flags']}\"\n",
    "                )\n",
    "                result[\"needs_review\"] = True\n",
    "                result[\"review_reason\"] = moderation_check.result[\"flags\"]\n",
    "\n",
    "        # Add quality metrics\n",
    "        result[\"quality_metrics\"] = {\n",
    "            \"grade\": quality_check.result[\"extraction_grade\"],\n",
    "            \"score\": quality_check.result[\"quality_score\"],\n",
    "            \"passed\": quality_check.result[\"passed\"],\n",
    "        }\n",
    "\n",
    "        if quality_check.result[\"issues\"]:\n",
    "            print(f\"üìä Quality issues: {quality_check.result['issues']}\")\n",
    "\n",
    "        if quality_check.result[\"recommendations\"]:\n",
    "            print(f\"üí° Recommendations: {quality_check.result['recommendations']}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Test with varied examples showing both success and failure cases\n",
    "print(\"üè≠ Testing production monitoring with realistic scenarios...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "production_test_emails = [\n",
    "    # Good quality extraction - should pass all checks\n",
    "    {\n",
    "        \"email\": \"Hello Support Team,\\n\\nI'm Sarah Mitchell from Acme Corp. Our CloudSync Enterprise v3.2.1 stopped syncing files yesterday at 2pm. The error message says 'Authentication failed'. This is really frustrating and affecting our entire team.\\n\\nBest regards,\\nSarah Mitchell\\nIT Manager, Acme Corp\",\n",
    "        \"expected\": \"‚úÖ High quality extraction with version numbers\",\n",
    "    },\n",
    "    # Profanity with legal threat - should be blocked\n",
    "    {\n",
    "        \"email\": \"This stupid software is absolute garbage! I'm John Davis and your DataSync Pro is the worst trash I've ever used. My lawyer will be contacting you about this terrible product that lost our data!\",\n",
    "        \"expected\": \"üö´ Should be blocked - profanity + legal threat\",\n",
    "    },\n",
    "    # Poor quality but processable - low score but not blocked\n",
    "    {\n",
    "        \"email\": \"Hi support, product broken. Fix please. - Tom\",\n",
    "        \"expected\": \"üìä Low quality - minimal details but processable\",\n",
    "    },\n",
    "    # Good extraction with negative sentiment - quality pass\n",
    "    {\n",
    "        \"email\": \"Dear Support,\\n\\nI'm Mary Johnson, CTO at TechStart Inc. Our DataVault Pro v2.5 backup failed last night with error code 'E501: connection timeout'. This is concerning as we rely on nightly backups for compliance.\\n\\nMary Johnson\\nCTO, TechStart Inc\",\n",
    "        \"expected\": \"‚úÖ Good quality despite negative sentiment\",\n",
    "    },\n",
    "    # Needs review - mild profanity - should flag for review\n",
    "    {\n",
    "        \"email\": \"Mike Wilson here. Your EmailPro system really sucks compared to what was promised, but I guess it's still better than the competition. Can you help me configure the spam filter? It's blocking legitimate emails.\",\n",
    "        \"expected\": \"‚ö†Ô∏è Should flag for review - mild profanity\",\n",
    "    },\n",
    "    # Excellent quality - should get high scores\n",
    "    {\n",
    "        \"email\": \"Hi there,\\n\\nI'm Lisa Chen from GlobalTech Solutions. I wanted to thank you for the excellent support on our CloudBackup Enterprise v4.2 deployment. Everything is working perfectly and the performance improvements are fantastic!\\n\\nBest,\\nLisa Chen\\nVP of Engineering\",\n",
    "        \"expected\": \"‚úÖ Excellent quality with positive sentiment\",\n",
    "    },\n",
    "    # Missing critical info - should fail quality check\n",
    "    {\n",
    "        \"email\": \"Your system crashed and we lost everything! This is unacceptable! Fix this immediately!!!\",\n",
    "        \"expected\": \"‚ùå Should fail quality - missing customer/product info\",\n",
    "    },\n",
    "    # Edge case - urgent but positive\n",
    "    {\n",
    "        \"email\": \"Urgent: I'm Alex Kumar and I love your RapidDeploy tool! Need to purchase 50 more licenses ASAP for our new team starting Monday. Please expedite!\\n\\nAlex Kumar\\nProcurement Manager\",\n",
    "        \"expected\": \"üìä Unusual case - urgent but positive sentiment\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for i, test_case in enumerate(production_test_emails):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìß Test {i+1}/8: {test_case['expected']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Show email preview\n",
    "    email_lines = test_case[\"email\"].split(\"\\n\")\n",
    "    print(\"üìù Email Content:\")\n",
    "    for line in email_lines[:3]:  # Show first 3 lines\n",
    "        if line.strip():\n",
    "            print(f\"   {line[:70]}{'...' if len(line) > 70 else ''}\")\n",
    "    if len(email_lines) > 3:\n",
    "        print(f\"   ... ({len(email_lines)-3} more lines)\")\n",
    "\n",
    "    # Process with monitoring\n",
    "    result = asyncio.run(handle_email_with_monitoring(test_case[\"email\"]))\n",
    "\n",
    "    # Show extraction results\n",
    "    print(\"\\nüîç Extraction Results:\")\n",
    "    if result[\"status\"] == \"success\":\n",
    "        analysis = result[\"analysis\"]\n",
    "        print(f\"   Customer: {analysis.get('customer_name', 'Unknown')}\")\n",
    "        print(f\"   Product: {analysis.get('product', 'Unknown')}\")\n",
    "        print(\n",
    "            f\"   Issue: {analysis.get('issue', 'Unknown')[:50]}{'...' if len(analysis.get('issue', '')) > 50 else ''}\"\n",
    "        )\n",
    "        print(f\"   Sentiment: {analysis.get('sentiment', 'Unknown')}\")\n",
    "        print(f\"   Urgency: {result.get('urgency', 'Unknown')}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Error: {result.get('error', 'Unknown error')}\")\n",
    "\n",
    "    # Show scorer results\n",
    "    print(\"\\nüìä Scorer Results:\")\n",
    "\n",
    "    # 1. Performance\n",
    "    perf = result.get(\"performance\", {})\n",
    "    print(\n",
    "        f\"   ‚è±Ô∏è  Response Time: {perf.get('grade', 'unknown')} ({result.get('processing_time_ms', 0):.0f}ms)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"      SLA Status: {'‚úÖ Met' if perf.get('sla_met', False) else '‚ùå Exceeded'}\"\n",
    "    )\n",
    "\n",
    "    # 2. Content Moderation\n",
    "    if result[\"status\"] == \"success\":\n",
    "        if result.get(\"blocked\"):\n",
    "            print(\"   üö´ Content Moderation: BLOCKED\")\n",
    "            print(f\"      Reason: {result['block_reason']}\")\n",
    "        elif result.get(\"needs_review\"):\n",
    "            print(\"   ‚ö†Ô∏è  Content Moderation: REVIEW NEEDED\")\n",
    "            print(f\"      Flags: {result['review_reason']}\")\n",
    "        else:\n",
    "            print(\"   ‚úÖ Content Moderation: PASSED\")\n",
    "\n",
    "    # 3. Quality Assessment\n",
    "    if result[\"status\"] == \"success\":\n",
    "        quality = result.get(\"quality_metrics\", {})\n",
    "        print(\n",
    "            f\"   üìè Quality Assessment: Grade {quality.get('grade', 'F')} (Score: {quality.get('score', 0):.2f})\"\n",
    "        )\n",
    "\n",
    "        # Show what contributed to the score\n",
    "        if quality.get(\"score\", 0) < 0.6:\n",
    "            print(\n",
    "                f\"      Status: {'‚ö†Ô∏è Below threshold' if quality.get('passed', False) else '‚ùå Failed'}\"\n",
    "            )\n",
    "            # The actual issues are logged by the scorers and visible in Weave UI\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nüéØ Summary of Production Monitoring Demonstration:\")\n",
    "print(\"\\n1. **Successful Cases** (Tests 1, 4, 6):\")\n",
    "print(\"   - High-quality extractions with version numbers\")\n",
    "print(\"   - All required fields present and accurate\")\n",
    "print(\"   - Fast response times meeting SLA\")\n",
    "\n",
    "print(\"\\n2. **Blocked Content** (Test 2):\")\n",
    "print(\"   - Multiple profanity words + legal threats = automatic block\")\n",
    "print(\"   - Protects support agents from abusive content\")\n",
    "\n",
    "print(\"\\n3. **Review Required** (Test 5):\")\n",
    "print(\"   - Mild profanity triggers review flag\")\n",
    "print(\"   - Human can decide if response is appropriate\")\n",
    "\n",
    "print(\"\\n4. **Quality Issues** (Tests 3, 7):\")\n",
    "print(\"   - Missing customer name or product details\")\n",
    "print(\"   - Too brief to be actionable\")\n",
    "print(\"   - Would need human intervention\")\n",
    "\n",
    "print(\"\\n5. **Edge Cases** (Test 8):\")\n",
    "print(\"   - Urgent + positive sentiment (unusual combination)\")\n",
    "print(\"   - System handles it correctly\")\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"   - Different scorers serve different purposes\")\n",
    "print(\"   - Guardrails (block/review) vs Monitors (quality/performance)\")\n",
    "print(\"   - All scorer results are tracked in Weave for analysis\")\n",
    "print(\"\\n‚úÖ Check the Weave UI to see detailed scorer results and traces!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11be3e74",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### üë• Part 3.1: Human Feedback & Dataset Building\n",
    "\n",
    "Learn how to collect human feedback and build datasets from production data.\n",
    "This creates a feedback loop for continuous model improvement.\n",
    "TODO: New Cell: Here, we are going to make an interactive \"app\" that renders in the cell so that the user can directly interact with the model. This will then generate calls in the UI and we can see calls coming into the application. From there we can setup a human feedback column interactively, collect examples, narrow down to the hard cases, and add to a dataset, which can then be used for the next round of evaluations:\n",
    "  1. Create an interactive output that allows for form-fill-style querying of the model\n",
    "  2. Add feedback so that the user can mark the reponse as good or bad (use the API to send this feedback)\n",
    "      * (Show in the UI that you can configure custom columns and form fill directly in the app if you have experts on your side)\n",
    "  3. (UI) Query for the bad results in the UI\n",
    "  4. (UI) Add the bad results to a dataset\n",
    "  5. (Optional) Next cell: create a new evaluation using the new dataset - presumably the models have a harder time... or just say that it is possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdd19ad",
   "metadata": {},
   "source": [
    "### üîó Part 3.2: OpenTelemetry Integration\n",
    "\n",
    "Learn how Weave integrates with OpenTelemetry for enterprise observability.\n",
    "Perfect for connecting Weave traces with your existing monitoring infrastructure.\n",
    "TODO: (New cell) Let's showcase OTEL support and how that works at the end here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d41db9",
   "metadata": {},
   "source": [
    "## üéâ Wrap Up\n",
    "\n",
    "Congratulations! You've learned how to:\n",
    "\n",
    "‚úÖ **Trace** - Track every function call with `@weave.op`\n",
    "‚úÖ **Evaluate** - Build comprehensive evaluation suites\n",
    "‚úÖ **Compare** - Make data-driven model decisions\n",
    "‚úÖ **Monitor** - Use scorers as guardrails and monitors\n",
    "‚úÖ **Debug** - Track exceptions and analyze failures\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "1. **This week**: Add Weave to one of your existing projects\n",
    "2. **Explore**: Try the built-in scorers (HallucinationFreeScorer, SummarizationScorer, etc.)\n",
    "3. **Share**: Join the W&B Community and share your experiences\n",
    "\n",
    "### üìö Resources\n",
    "\n",
    "- [Weave Documentation](https://weave-docs.wandb.ai/)\n",
    "- [Built-in Scorers](https://weave-docs.wandb.ai/guides/evaluation/builtin_scorers)\n",
    "- [W&B Community](https://wandb.ai/community)\n",
    "\n",
    "Happy building with Weave! üêù"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
