{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c59ab18b",
   "metadata": {},
   "source": [
    "# üêù Weave Workshop: Build, Track, and Evaluate LLM Applications\n",
    "\n",
    "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "\n",
    "Welcome to the Weave workshop! In this hands-on session, you'll learn how to use Weave to develop, debug, and evaluate AI-powered applications.\n",
    "\n",
    "**What you'll learn:**\n",
    "- üîç **Trace & Debug**: Track every LLM call, see inputs/outputs, and debug issues\n",
    "- üìä **Evaluate**: Build rigorous evaluations with multiple scoring functions\n",
    "- üèÉ **Compare**: Run A/B tests and compare different approaches\n",
    "- üìà **Monitor**: Track costs, latency, and performance metrics\n",
    "- üéØ **Iterate**: Use data-driven insights to improve your application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ce3bb5",
   "metadata": {},
   "source": [
    "## üîë Prerequisites\n",
    "\n",
    "Before we begin, let's set up your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8b6b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install wandb weave openai pydantic nest_asyncio -qqq\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "from getpass import getpass\n",
    "from typing import Any, Optional\n",
    "\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import weave\n",
    "from weave import Dataset, Evaluation, EvaluationLogger, Model, Scorer\n",
    "\n",
    "# üîë Setup your API keys\n",
    "print(\"---\")\n",
    "print(\n",
    "    \"You can find your Weights and Biases API key here: https://wandb.ai/settings#api\"\n",
    ")\n",
    "if not os.environ.get(\"WANDB_API_KEY\"):\n",
    "    os.environ[\"WANDB_API_KEY\"] = getpass(\"Enter your Weights and Biases API key: \")\n",
    "print(\"---\")\n",
    "print(\"You can generate your OpenAI API key here: https://platform.openai.com/api-keys\")\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
    "print(\"---\")\n",
    "\n",
    "# üè† Initialize your W&B project\n",
    "weave_client = weave.init(\"weave-workshop\")  # üêù Your W&B project name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939f4c3f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## üîç Part 1: Tracing & Debugging with Weave\n",
    "\n",
    "Let's start by building a simple LLM application and see how Weave automatically tracks everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9547fc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our data structure\n",
    "class CustomerEmail(BaseModel):\n",
    "    customer_name: str\n",
    "    product: str\n",
    "    issue: str\n",
    "    sentiment: str = Field(description=\"positive, neutral, or negative\")\n",
    "\n",
    "\n",
    "# üêù Track functions with @weave.op\n",
    "@weave.op\n",
    "def analyze_customer_email(email: str) -> CustomerEmail:\n",
    "    \"\"\"Analyze a customer support email and extract key information.\"\"\"\n",
    "    client = OpenAI()\n",
    "\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-3.5-turbo\",  # Using cheaper model to make errors more likely\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Extract customer name, product, issue, and sentiment.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": email,\n",
    "            },\n",
    "        ],\n",
    "        response_format=CustomerEmail,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.parsed\n",
    "\n",
    "\n",
    "# Let's test it!\n",
    "test_email = \"\"\"\n",
    "Hi Support,\n",
    "\n",
    "I'm really frustrated! My new ProWidget 3000 stopped working after just 2 days.\n",
    "The screen went completely black and won't turn on no matter what I try.\n",
    "\n",
    "Please help!\n",
    "Sarah Johnson\n",
    "\"\"\"\n",
    "\n",
    "# üéØ Run the function - Weave will automatically track this call\n",
    "result = analyze_customer_email(test_email)\n",
    "print(\"‚úÖ Analysis complete!\")\n",
    "print(f\"Customer: {result.customer_name}\")\n",
    "print(f\"Sentiment: {result.sentiment}\")\n",
    "print(\"\\nüîç Check the Weave UI to see the trace!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8a2131",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## üêõ Part 2: Debugging with Call Traces\n",
    "\n",
    "Weave tracks nested function calls, making debugging easy. Let's build a more complex example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b33a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "def preprocess_email(email: str) -> str:\n",
    "    \"\"\"Clean and standardize email text.\"\"\"\n",
    "    # Remove extra whitespace\n",
    "    cleaned = \" \".join(email.split())\n",
    "    # Add some metadata for debugging\n",
    "    print(f\"üìß Original length: {len(email)}, Cleaned length: {len(cleaned)}\")\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "@weave.op\n",
    "def classify_urgency(email: str, sentiment: str) -> str:\n",
    "    \"\"\"Determine urgency level based on content and sentiment.\"\"\"\n",
    "    urgent_keywords = [\n",
    "        \"urgent\",\n",
    "        \"asap\",\n",
    "        \"immediately\",\n",
    "        \"frustrated\",\n",
    "        \"broken\",\n",
    "        \"stopped working\",\n",
    "    ]\n",
    "\n",
    "    # Check for urgent keywords\n",
    "    email_lower = email.lower()\n",
    "    has_urgent_keywords = any(keyword in email_lower for keyword in urgent_keywords)\n",
    "\n",
    "    if sentiment == \"negative\" and has_urgent_keywords:\n",
    "        return \"high\"\n",
    "    elif sentiment == \"negative\" or has_urgent_keywords:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"low\"\n",
    "\n",
    "\n",
    "@weave.op\n",
    "def process_support_ticket(email: str) -> dict[str, Any]:\n",
    "    \"\"\"Complete support ticket processing pipeline.\"\"\"\n",
    "    # Step 1: Clean the email\n",
    "    cleaned_email = preprocess_email(email)\n",
    "\n",
    "    # Step 2: Analyze the email\n",
    "    analysis = analyze_customer_email(cleaned_email)\n",
    "\n",
    "    # Step 3: Determine urgency\n",
    "    urgency = classify_urgency(cleaned_email, analysis.sentiment)\n",
    "\n",
    "    # Return complete ticket info\n",
    "    return {\n",
    "        \"customer_name\": analysis.customer_name,\n",
    "        \"product\": analysis.product,\n",
    "        \"issue\": analysis.issue,\n",
    "        \"sentiment\": analysis.sentiment,\n",
    "        \"urgency\": urgency,\n",
    "        \"needs_immediate_attention\": urgency == \"high\",\n",
    "    }\n",
    "\n",
    "\n",
    "# üéØ Run the pipeline - see the nested traces in Weave!\n",
    "ticket = process_support_ticket(test_email)\n",
    "print(\"\\nüé´ Ticket processed!\")\n",
    "print(f\"Urgency: {ticket['urgency']}\")\n",
    "print(f\"Needs immediate attention: {ticket['needs_immediate_attention']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02af6151",
   "metadata": {},
   "source": [
    "## üìä Part 3: Building Evaluations\n",
    "\n",
    "Now let's evaluate our email analyzer using Weave's evaluation framework.\n",
    "We'll use a more challenging dataset to expose model weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f2b263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a challenging evaluation dataset\n",
    "eval_examples = [\n",
    "    {\n",
    "        \"email\": \"John Smith here. My DataProcessor-Pro v2.5 isn't working correctly. The data export feature that worked yesterday is now producing corrupted files.\",\n",
    "        \"expected_name\": \"John Smith\",\n",
    "        \"expected_product\": \"DataProcessor-Pro v2.5\",\n",
    "        \"expected_sentiment\": \"negative\",\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"Jane from accounting. CloudSync Plus Enterprise works but sometimes the sync is delayed by 10-15 minutes. Not urgent but would like it fixed.\",\n",
    "        \"expected_name\": \"Jane\",\n",
    "        \"expected_product\": \"CloudSync Plus Enterprise\",\n",
    "        \"expected_sentiment\": \"neutral\",\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"My SmartHub (model SH-2000) won't connect. - Bob Wilson, Senior Manager\",\n",
    "        \"expected_name\": \"Bob Wilson\",\n",
    "        \"expected_product\": \"SmartHub SH-2000\",\n",
    "        \"expected_sentiment\": \"negative\",\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"Hi! Dr. Alice Chen writing. The AI-Assistant tool is fantastic! Minor suggestion: could you add dark mode? Overall very happy.\",\n",
    "        \"expected_name\": \"Dr. Alice Chen\",\n",
    "        \"expected_product\": \"AI-Assistant\",\n",
    "        \"expected_sentiment\": \"positive\",\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"URGENT!!! System completely down! Can't access anything on WorkflowMax! This is costing us thousands per hour! Contact: Mike O'Brien, CEO\",\n",
    "        \"expected_name\": \"Mike O'Brien\",\n",
    "        \"expected_product\": \"WorkflowMax\",\n",
    "        \"expected_sentiment\": \"negative\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create a Weave Dataset\n",
    "support_dataset = Dataset(name=\"support_emails\", rows=eval_examples)\n",
    "\n",
    "\n",
    "# üéØ Define scoring functions\n",
    "@weave.op\n",
    "def name_accuracy(expected_name: str, output: CustomerEmail) -> dict[str, Any]:\n",
    "    \"\"\"Check if the extracted name matches.\"\"\"\n",
    "    is_correct = expected_name.lower() == output.customer_name.lower()\n",
    "    return {\"correct\": is_correct, \"score\": 1.0 if is_correct else 0.0}\n",
    "\n",
    "\n",
    "@weave.op\n",
    "def sentiment_accuracy(\n",
    "    expected_sentiment: str, output: CustomerEmail\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"Check if the sentiment analysis is correct.\"\"\"\n",
    "    is_correct = expected_sentiment.lower() == output.sentiment.lower()\n",
    "    return {\"correct\": is_correct, \"score\": 1.0 if is_correct else 0.0}\n",
    "\n",
    "\n",
    "@weave.op\n",
    "def extraction_quality(email: str, output: CustomerEmail) -> dict[str, Any]:\n",
    "    \"\"\"Evaluate overall extraction quality.\"\"\"\n",
    "    score = 0.0\n",
    "    feedback = []\n",
    "\n",
    "    # Check if all fields are extracted\n",
    "    if output.customer_name and output.customer_name != \"Unknown\":\n",
    "        score += 0.33\n",
    "    else:\n",
    "        feedback.append(\"Missing customer name\")\n",
    "\n",
    "    if output.product and output.product != \"Unknown\":\n",
    "        score += 0.33\n",
    "    else:\n",
    "        feedback.append(\"Missing product\")\n",
    "\n",
    "    if output.issue and len(output.issue) > 10:\n",
    "        score += 0.34\n",
    "    else:\n",
    "        feedback.append(\"Issue description too short\")\n",
    "\n",
    "    return {\n",
    "        \"score\": score,\n",
    "        \"feedback\": \"; \".join(feedback)\n",
    "        if feedback\n",
    "        else \"All fields extracted successfully\",\n",
    "    }\n",
    "\n",
    "\n",
    "# üöÄ Run the evaluation (notebook-friendly version)\n",
    "evaluation = Evaluation(\n",
    "    dataset=support_dataset,\n",
    "    scorers=[name_accuracy, sentiment_accuracy, extraction_quality],\n",
    ")\n",
    "\n",
    "print(\"üèÉ Running evaluation...\")\n",
    "# For notebooks, use await instead of asyncio.run\n",
    "# In Jupyter/IPython notebooks, you can use await directly\n",
    "# eval_results = await evaluation.evaluate(analyze_customer_email)\n",
    "# For Python scripts, use:\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "eval_results = asyncio.run(evaluation.evaluate(analyze_customer_email))\n",
    "print(\"‚úÖ Evaluation complete! Check the Weave UI for detailed results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e62352",
   "metadata": {},
   "source": [
    "## üìù Part 3b: Using EvaluationLogger\n",
    "\n",
    "The `EvaluationLogger` provides a flexible way to log evaluation data incrementally.\n",
    "This is perfect when you don't have all your data upfront or want more control.\n",
    "\n",
    "**Important**: Since EvaluationLogger doesn't use Model/Dataset objects, the `model`\n",
    "and `dataset` parameters are crucial for identification. You can use strings or\n",
    "dictionaries for rich metadata!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405ceda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example using EvaluationLogger for custom evaluation flow\n",
    "# You can use simple strings for identification\n",
    "eval_logger = EvaluationLogger(\n",
    "    model=\"email_analyzer_gpt35\",  # Model name/version\n",
    "    dataset=\"support_emails\",  # Dataset name stays consistent\n",
    ")\n",
    "\n",
    "# Or use dictionaries for richer identification (recommended!)\n",
    "eval_logger_rich = EvaluationLogger(\n",
    "    model={\n",
    "        \"name\": \"email_analyzer\",\n",
    "        \"version\": \"v1.2\",\n",
    "        \"llm\": \"gpt-3.5-turbo\",\n",
    "        \"temperature\": 0.7,\n",
    "        \"prompt_version\": \"2024-01\",\n",
    "    },\n",
    "    dataset={\n",
    "        \"name\": \"support_emails\",\n",
    "        \"version\": \"2024Q1\",\n",
    "        \"size\": len(eval_examples),\n",
    "        \"source\": \"production_samples\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Let's use the rich logger for our demo\n",
    "print(\"üìä Using EvaluationLogger with rich metadata...\")\n",
    "\n",
    "# Process each example with more control\n",
    "for i, example in enumerate(eval_examples[:3]):  # Just first 3 for demo\n",
    "    # Analyze the email\n",
    "    try:\n",
    "        output = analyze_customer_email(example[\"email\"])\n",
    "\n",
    "        # Log the prediction\n",
    "        pred_logger = eval_logger_rich.log_prediction(\n",
    "            inputs={\"email\": example[\"email\"]}, output=output.model_dump()\n",
    "        )\n",
    "\n",
    "        # Log multiple scores for this prediction\n",
    "        # Check name accuracy\n",
    "        name_match = example[\"expected_name\"].lower() == output.customer_name.lower()\n",
    "        pred_logger.log_score(scorer=\"name_accuracy\", score=1.0 if name_match else 0.0)\n",
    "\n",
    "        # Check sentiment\n",
    "        sentiment_match = example[\"expected_sentiment\"] == output.sentiment\n",
    "        pred_logger.log_score(\n",
    "            scorer=\"sentiment_accuracy\", score=1.0 if sentiment_match else 0.0\n",
    "        )\n",
    "\n",
    "        # Custom business logic score\n",
    "        if \"urgent\" in example[\"email\"].lower() and output.sentiment != \"negative\":\n",
    "            pred_logger.log_score(\n",
    "                scorer=\"urgency_detection\",\n",
    "                score=0.0,  # Failed to detect urgency\n",
    "            )\n",
    "        else:\n",
    "            pred_logger.log_score(scorer=\"urgency_detection\", score=1.0)\n",
    "\n",
    "        # Always finish logging for each prediction\n",
    "        pred_logger.finish()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing example {i+1}: {e}\")\n",
    "        # You can still log failed predictions\n",
    "        pred_logger = eval_logger_rich.log_prediction(\n",
    "            inputs={\"email\": example[\"email\"]}, output={\"error\": str(e)}\n",
    "        )\n",
    "        pred_logger.log_score(scorer=\"success\", score=0.0)\n",
    "        pred_logger.finish()\n",
    "\n",
    "# Log summary statistics\n",
    "eval_logger_rich.log_summary(\n",
    "    {\n",
    "        \"total_examples\": 3,\n",
    "        \"evaluation_type\": \"manual\",\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"notes\": \"Workshop demo with rich metadata\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úÖ EvaluationLogger demo complete! Check the Weave UI.\")\n",
    "print(\"üí° Tip: The rich metadata makes it easy to filter and compare evaluations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfad5c02",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## üèÜ Part 4: Model Comparison\n",
    "\n",
    "Let's compare different approaches using Weave's Model class.\n",
    "We'll create models with varying quality to see clear differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56141ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different model variants\n",
    "class EmailAnalyzerModel(Model):\n",
    "    \"\"\"Base model for email analysis with configurable parameters.\"\"\"\n",
    "\n",
    "    model_name: str = \"gpt-3.5-turbo\"\n",
    "    temperature: float = 0.1\n",
    "    system_prompt: str = \"You are a customer support analyst.\"\n",
    "\n",
    "    @weave.op\n",
    "    def predict(self, email: str) -> CustomerEmail:\n",
    "        \"\"\"Analyze email with configurable parameters.\"\"\"\n",
    "        client = OpenAI()\n",
    "\n",
    "        response = client.beta.chat.completions.parse(\n",
    "            model=self.model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"Analyze this email:\\n{email}\"},\n",
    "            ],\n",
    "            response_format=CustomerEmail,\n",
    "            temperature=self.temperature,\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.parsed\n",
    "\n",
    "\n",
    "# Create model variants with different quality levels\n",
    "basic_model = EmailAnalyzerModel(\n",
    "    name=\"basic_analyzer\",\n",
    "    system_prompt=\"Extract name, product, issue, sentiment.\",  # Too simple\n",
    "    temperature=0.9,  # Too high - more random\n",
    ")\n",
    "\n",
    "detailed_model = EmailAnalyzerModel(\n",
    "    name=\"detailed_analyzer\",\n",
    "    system_prompt=\"\"\"You are an expert customer support analyst.\n",
    "    Carefully extract:\n",
    "    - Customer name (full name, check signatures and greetings)\n",
    "    - Product name (include version/model numbers)\n",
    "    - Issue description (be specific)\n",
    "    - Sentiment (positive/neutral/negative based on tone and urgency)\n",
    "    \n",
    "    Pay attention to edge cases like names in signatures or product versions.\"\"\",\n",
    "    temperature=0.0,\n",
    "    model_name=\"gpt-4o-mini\",  # Better model\n",
    ")\n",
    "\n",
    "balanced_model = EmailAnalyzerModel(\n",
    "    name=\"balanced_analyzer\",\n",
    "    system_prompt=\"\"\"Extract customer information from support emails.\n",
    "    Look for: customer name (anywhere in email), product (with any version info),\n",
    "    issue description, and sentiment. Be thorough but concise.\"\"\",\n",
    "    temperature=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef386270",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## üîÑ Part 5: A/B Testing Models\n",
    "\n",
    "**Important Concept**: When comparing models, we use the SAME evaluation definition\n",
    "(same dataset + scorers) for all models. This ensures fair comparison and allows\n",
    "everyone in the workshop to see aggregated results. Each evaluation run gets a\n",
    "unique ID automatically, but the evaluation definition stays consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6851d611",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def compare_models(models: list[Model], dataset: Dataset) -> dict[str, Any]:\n",
    "    \"\"\"Run A/B comparison of multiple models.\"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Create a single evaluation definition that will be used for all models\n",
    "    evaluation = Evaluation(\n",
    "        name=\"email_analyzer_comparison\",  # Same eval for all models\n",
    "        dataset=dataset,\n",
    "        scorers=[name_accuracy, sentiment_accuracy, extraction_quality],\n",
    "    )\n",
    "\n",
    "    for model in models:\n",
    "        print(f\"\\nüìä Evaluating {model.name}...\")\n",
    "\n",
    "        # Run evaluation with optional display name for this specific run\n",
    "        eval_result = await evaluation.evaluate(\n",
    "            model, __weave={\"display_name\": f\"Run: {model.name}\"}\n",
    "        )\n",
    "        results[model.name] = eval_result\n",
    "\n",
    "        print(f\"‚úÖ {model.name} evaluation complete!\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run the comparison\n",
    "print(\"üèÅ Starting model comparison...\")\n",
    "# For notebooks: comparison_results = await compare_models(...)\n",
    "# For scripts:\n",
    "comparison_results = asyncio.run(\n",
    "    compare_models([basic_model, detailed_model, balanced_model], support_dataset)\n",
    ")\n",
    "print(\"\\nüéâ Comparison complete! View the results in the Weave UI.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0499ac59",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## üêû Part 6: Exception Tracking\n",
    "\n",
    "Weave automatically tracks exceptions, helping you debug failures in production.\n",
    "This is especially useful when dealing with structured outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d7ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a stricter data structure that's more likely to fail\n",
    "class DetailedCustomerEmail(BaseModel):\n",
    "    customer_name: str = Field(description=\"Full name of the customer\")\n",
    "    customer_title: Optional[str] = Field(description=\"Job title if mentioned\")\n",
    "    company: Optional[str] = Field(description=\"Company name if mentioned\")\n",
    "    product: str = Field(description=\"Product name including version\")\n",
    "    product_version: Optional[str] = Field(description=\"Specific version number\")\n",
    "    issue: str = Field(description=\"Detailed issue description\")\n",
    "    severity: str = Field(description=\"critical, high, medium, or low\")\n",
    "    sentiment: str = Field(description=\"positive, neutral, or negative\")\n",
    "\n",
    "\n",
    "@weave.op\n",
    "def analyze_detailed_email(email: str) -> DetailedCustomerEmail:\n",
    "    \"\"\"Analyze email with strict schema - more likely to fail.\"\"\"\n",
    "    client = OpenAI()\n",
    "\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-3.5-turbo\",  # Using weaker model\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Extract ALL fields. Use 'Unknown' if not found.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": email,\n",
    "            },\n",
    "        ],\n",
    "        response_format=DetailedCustomerEmail,\n",
    "        temperature=0.8,  # Higher temperature = more errors\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.parsed\n",
    "\n",
    "\n",
    "# Test with various emails to see exceptions\n",
    "test_emails_for_errors = [\n",
    "    \"Fix this NOW!\",  # Too short, missing info\n",
    "    \"The thing is broken - Anonymous\",  # Missing key details\n",
    "    test_email,  # Should work\n",
    "]\n",
    "\n",
    "print(\"üêû Testing exception tracking...\")\n",
    "for i, email in enumerate(test_emails_for_errors):\n",
    "    print(f\"\\nüìß Test {i+1}:\")\n",
    "    try:\n",
    "        result = analyze_detailed_email(email)\n",
    "        print(f\"‚úÖ Success: {result.customer_name} - {result.product}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {type(e).__name__}: {str(e)}\")\n",
    "        print(\"   Check Weave UI to see the full error trace!\")\n",
    "\n",
    "\n",
    "# Demonstrate JSON parsing errors\n",
    "@weave.op\n",
    "def parse_customer_response(response_text: str) -> dict:\n",
    "    \"\"\"Parse a JSON response - demonstrates parsing errors.\"\"\"\n",
    "    # This will fail if response_text is not valid JSON\n",
    "    data = json.loads(response_text)\n",
    "\n",
    "    # This will fail if required fields are missing\n",
    "    return {\n",
    "        \"name\": data[\"customer\"][\"name\"],  # Nested field access\n",
    "        \"issue\": data[\"issue\"][\"description\"],\n",
    "        \"priority\": data[\"priority\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# Test parsing errors\n",
    "test_responses = [\n",
    "    '{\"customer\": {\"name\": \"John\"}, \"issue\": {\"description\": \"Bug\"}, \"priority\": \"high\"}',  # Valid\n",
    "    '{\"name\": \"Jane\", \"issue\": \"Problem\"}',  # Missing nested structure\n",
    "    \"Not JSON at all!\",  # Invalid JSON\n",
    "]\n",
    "\n",
    "print(\"\\n\\nüîç Testing JSON parsing errors...\")\n",
    "for i, response in enumerate(test_responses):\n",
    "    print(f\"\\nüìÑ Test {i+1}:\")\n",
    "    try:\n",
    "        result = parse_customer_response(response)\n",
    "        print(f\"‚úÖ Parsed successfully: {result}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå JSON Error: {e}\")\n",
    "    except KeyError as e:\n",
    "        print(f\"‚ùå Missing field: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error: {type(e).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd24b8d3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## üìà Part 7: Cost & Performance Tracking\n",
    "\n",
    "Weave automatically tracks metrics like latency and token usage. Let's explore these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6b16b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "def analyze_with_fallback(\n",
    "    email: str,\n",
    "    primary_model: str = \"gpt-4o-mini\",\n",
    "    fallback_model: str = \"gpt-3.5-turbo\",\n",
    ") -> CustomerEmail:\n",
    "    \"\"\"Analyze email with automatic fallback on error.\"\"\"\n",
    "    client = OpenAI()\n",
    "    try:\n",
    "        response = client.beta.chat.completions.parse(\n",
    "            model=primary_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Extract customer info from email.\"},\n",
    "                {\"role\": \"user\", \"content\": email},\n",
    "            ],\n",
    "            response_format=CustomerEmail,\n",
    "        )\n",
    "        print(f\"‚úÖ Used primary model: {primary_model}\")\n",
    "        return response.choices[0].message.parsed\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Primary model failed: {e}\")\n",
    "        print(f\"üîÑ Falling back to {fallback_model}\")\n",
    "        response = client.beta.chat.completions.parse(\n",
    "            model=fallback_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Extract customer info from email.\"},\n",
    "                {\"role\": \"user\", \"content\": email},\n",
    "            ],\n",
    "            response_format=CustomerEmail,\n",
    "        )\n",
    "        return response.choices[0].message.parsed\n",
    "\n",
    "\n",
    "# Test the fallback mechanism\n",
    "test_emails = [\n",
    "    \"Hi, I'm Alice Brown. My UltraPhone is overheating constantly!\",\n",
    "    \"Bob Green here. The MegaTablet screen is cracked after dropping it.\",\n",
    "    \"Carol White needs help with CloudBackup not syncing properly.\",\n",
    "]\n",
    "\n",
    "print(\"üîÑ Testing fallback mechanism...\")\n",
    "for email in test_emails:\n",
    "    result = analyze_with_fallback(email)\n",
    "    print(f\"  Processed: {result.customer_name} - {result.sentiment}\")\n",
    "\n",
    "print(\"\\nüí∞ Check the Weave UI to see:\")\n",
    "print(\"  - Token usage for each call\")\n",
    "print(\"  - Latency comparisons\")\n",
    "print(\"  - Cost tracking (when available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd1f23e",
   "metadata": {},
   "source": [
    "## üéØ Part 8: Production Monitoring with Scorers\n",
    "\n",
    "Use Weave's scorer system for real-time guardrails and quality monitoring.\n",
    "This demonstrates the apply_scorer pattern for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df5d2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Define custom scorers for production monitoring\n",
    "class ToxicityScorer(Scorer):\n",
    "    @weave.op\n",
    "    def score(self, output: dict) -> dict:\n",
    "        \"\"\"Check for toxic or inappropriate content.\"\"\"\n",
    "        # Simplified toxicity check - in production, use a real model\n",
    "        toxic_words = [\"stupid\", \"hate\", \"terrible\", \"worst\"]\n",
    "\n",
    "        text_to_check = str(output.get(\"issue\", \"\")).lower()\n",
    "\n",
    "        for word in toxic_words:\n",
    "            if word in text_to_check:\n",
    "                return {\n",
    "                    \"flagged\": True,\n",
    "                    \"reason\": f\"Contains potentially toxic word: {word}\",\n",
    "                    \"severity\": \"medium\",\n",
    "                }\n",
    "\n",
    "        return {\"flagged\": False, \"reason\": None, \"severity\": None}\n",
    "\n",
    "\n",
    "class ResponseQualityScorer(Scorer):\n",
    "    @weave.op\n",
    "    def score(self, output: dict, email: str) -> dict:\n",
    "        \"\"\"Evaluate the quality of the extraction.\"\"\"\n",
    "        score = 0.0\n",
    "        issues = []\n",
    "\n",
    "        # Check completeness\n",
    "        if not output.get(\"customer_name\") or output[\"customer_name\"] == \"Unknown\":\n",
    "            issues.append(\"Missing customer name\")\n",
    "        else:\n",
    "            score += 0.25\n",
    "\n",
    "        if not output.get(\"product\") or output[\"product\"] == \"Unknown\":\n",
    "            issues.append(\"Missing product\")\n",
    "        else:\n",
    "            score += 0.25\n",
    "\n",
    "        if not output.get(\"issue\") or len(output[\"issue\"]) < 10:\n",
    "            issues.append(\"Insufficient issue description\")\n",
    "        else:\n",
    "            score += 0.25\n",
    "\n",
    "        # Check relevance\n",
    "        if email and len(output.get(\"issue\", \"\")) > 0:\n",
    "            # Simple relevance check - in production, use embeddings\n",
    "            email_words = set(email.lower().split())\n",
    "            issue_words = set(output.get(\"issue\", \"\").lower().split())\n",
    "            overlap = len(email_words & issue_words) / max(len(email_words), 1)\n",
    "            if overlap > 0.2:\n",
    "                score += 0.25\n",
    "            else:\n",
    "                issues.append(\"Issue description may not match email content\")\n",
    "\n",
    "        return {\"quality_score\": score, \"passed\": score >= 0.75, \"issues\": issues}\n",
    "\n",
    "\n",
    "@weave.op\n",
    "def production_email_handler(\n",
    "    email: str, request_id: Optional[str] = None\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"Production-ready email handler that returns analysis results.\"\"\"\n",
    "    # Generate request ID if not provided\n",
    "    if not request_id:\n",
    "        request_id = f\"req_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{random.randint(1000, 9999)}\"\n",
    "\n",
    "    try:\n",
    "        # Process the email\n",
    "        analysis = analyze_customer_email(email)\n",
    "        urgency = classify_urgency(email, analysis.sentiment)\n",
    "\n",
    "        # Return the result\n",
    "        return {\n",
    "            \"request_id\": request_id,\n",
    "            \"status\": \"success\",\n",
    "            \"analysis\": analysis.model_dump(),\n",
    "            \"urgency\": urgency,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log error and return error response\n",
    "        return {\n",
    "            \"request_id\": request_id,\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e),\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "        }\n",
    "\n",
    "\n",
    "# Initialize scorers once (for performance)\n",
    "toxicity_scorer = ToxicityScorer()\n",
    "quality_scorer = ResponseQualityScorer()\n",
    "\n",
    "\n",
    "async def handle_email_with_monitoring(email: str) -> dict[str, Any]:\n",
    "    \"\"\"Handle email with production monitoring and guardrails.\"\"\"\n",
    "    # Process the email and get the Call object\n",
    "    result, call = production_email_handler.call(email)\n",
    "\n",
    "    if result[\"status\"] == \"success\":\n",
    "        # Apply scorers for monitoring\n",
    "        toxicity_check = await call.apply_scorer(toxicity_scorer)\n",
    "        quality_check = await call.apply_scorer(\n",
    "            quality_scorer, additional_scorer_kwargs={\"email\": email}\n",
    "        )\n",
    "\n",
    "        # Check toxicity (guardrail)\n",
    "        if toxicity_check.result[\"flagged\"]:\n",
    "            print(f\"‚ö†Ô∏è Content flagged: {toxicity_check.result['reason']}\")\n",
    "            # In production, you might modify or block the response\n",
    "            result[\"warning\"] = \"Content flagged for review\"\n",
    "\n",
    "        # Check quality (monitoring)\n",
    "        if not quality_check.result[\"passed\"]:\n",
    "            print(f\"üìä Quality issues: {quality_check.result['issues']}\")\n",
    "            # Log for improvement but don't block\n",
    "            result[\"quality_score\"] = quality_check.result[\"quality_score\"]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Test production monitoring\n",
    "print(\"üè≠ Testing production monitoring with scorers...\")\n",
    "production_test_emails = [\n",
    "    \"Hi, I'm CEO Jane Smith. Our Enterprise Suite is down and this stupid system is costing us money!\",\n",
    "    \"Hi support, my product isn't working. Please help. Thanks, Tom\",\n",
    "    \"My DataVault backup failed. Need help ASAP! The error says 'connection timeout'. - Mary Johnson\",\n",
    "]\n",
    "\n",
    "# For notebooks:\n",
    "# for email in production_test_emails:\n",
    "#     result = await handle_email_with_monitoring(email)\n",
    "# For scripts:\n",
    "for email in production_test_emails:\n",
    "    print(f\"\\nüìß Processing: {email[:50]}...\")\n",
    "    result = asyncio.run(handle_email_with_monitoring(email))\n",
    "    print(f\"  Status: {result['status']}\")\n",
    "    if \"warning\" in result:\n",
    "        print(f\"  ‚ö†Ô∏è Warning: {result['warning']}\")\n",
    "    if \"quality_score\" in result:\n",
    "        print(f\"  üìä Quality Score: {result['quality_score']:.2f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Check the Weave UI to see scorer results attached to each call!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b387bb9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## üêõ Part 9: Debugging Failed Calls\n",
    "\n",
    "Weave makes it easy to debug when things go wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c238b121",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "def problematic_analyzer(email: str) -> Optional[CustomerEmail]:\n",
    "    \"\"\"An analyzer that might fail - perfect for debugging!\"\"\"\n",
    "    if \"error\" in email.lower():\n",
    "        raise ValueError(\"Email contains error keyword!\")\n",
    "    if len(email) < 20:\n",
    "        print(\"‚ö†Ô∏è Email too short, returning None\")\n",
    "        return None\n",
    "    if \"urgent\" in email.lower():\n",
    "        # Simulate a timeout\n",
    "        import time\n",
    "\n",
    "        time.sleep(0.1)  # In real scenarios, this might be a network timeout\n",
    "\n",
    "    return analyze_customer_email(email)\n",
    "\n",
    "\n",
    "# Test with problematic inputs\n",
    "debug_test_emails = [\n",
    "    \"Error in system\",  # Will raise exception\n",
    "    \"Too short\",  # Will return None\n",
    "    \"URGENT: Normal email from Alice about ProductX not working properly\",  # Will be slow\n",
    "]\n",
    "\n",
    "print(\"üîç Testing problematic analyzer...\")\n",
    "for email in debug_test_emails:\n",
    "    print(f\"\\nüìß Testing: {email}\")\n",
    "    try:\n",
    "        result = problematic_analyzer(email)\n",
    "        if result:\n",
    "            print(f\"‚úÖ Success: {result.customer_name}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Returned None\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Exception: {type(e).__name__}: {e}\")\n",
    "\n",
    "print(\"\\nüí° Check the Weave UI to see:\")\n",
    "print(\"  - Red highlighted failed calls\")\n",
    "print(\"  - Full stack traces for exceptions\")\n",
    "print(\"  - Timing information for slow calls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5557161f",
   "metadata": {},
   "source": [
    "## üéâ Wrap Up\n",
    "\n",
    "Congratulations! You've learned how to:\n",
    "\n",
    "‚úÖ **Trace** - Track every function call with `@weave.op`\n",
    "‚úÖ **Evaluate** - Build comprehensive evaluation suites\n",
    "‚úÖ **Compare** - Make data-driven model decisions\n",
    "‚úÖ **Monitor** - Use scorers as guardrails and monitors\n",
    "‚úÖ **Debug** - Track exceptions and analyze failures\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "1. **This week**: Add Weave to one of your existing projects\n",
    "2. **Explore**: Try the built-in scorers (HallucinationFreeScorer, SummarizationScorer, etc.)\n",
    "3. **Share**: Join the W&B Community and share your experiences\n",
    "\n",
    "### üìö Resources\n",
    "\n",
    "- [Weave Documentation](https://weave-docs.wandb.ai/)\n",
    "- [Built-in Scorers](https://weave-docs.wandb.ai/guides/evaluation/builtin_scorers)\n",
    "- [W&B Community](https://wandb.ai/community)\n",
    "\n",
    "Happy building with Weave! üêù"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
