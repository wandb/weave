{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "849f5fb7",
   "metadata": {},
   "source": [
    "# Advanced Evaluation Examples for the Workshop\n",
    "\n",
    "This notebook provides additional examples of using Weave's evaluation features\n",
    "for the prompt engineering workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb39b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from typing import Any\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "import weave\n",
    "from weave import Dataset, Evaluation, EvaluationLogger, Model\n",
    "\n",
    "# Initialize Weave\n",
    "weave.init(\"prompt_engineering_workshop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4342502",
   "metadata": {},
   "source": [
    "## Example 1: Custom Scorer Class\n",
    "\n",
    "Sometimes you want to create reusable scorer classes with their own configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee85444",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from weave import Scorer\n",
    "\n",
    "\n",
    "class LLMJudgeScorer(Scorer):\n",
    "    \"\"\"A scorer that uses an LLM to judge the quality of extractions\"\"\"\n",
    "\n",
    "    model_name: str = \"gpt-4o-mini\"\n",
    "    judge_prompt: str = \"\"\"You are an expert evaluator of information extraction systems.\n",
    "\n",
    "    Given an email and the extracted information, rate the extraction quality on a scale of 1-10.\n",
    "    Consider:\n",
    "    - Accuracy of extracted information\n",
    "    - Completeness (did it miss anything important?)\n",
    "    - Conciseness of the issue description\n",
    "\n",
    "    Email: {email}\n",
    "\n",
    "    Extracted Information:\n",
    "    - Customer Name: {customer_name}\n",
    "    - Product Model: {product_model}\n",
    "    - Issue Description: {issue_description}\n",
    "\n",
    "    Provide a score from 1-10 and a brief explanation.\n",
    "    Return as JSON with fields: score (number), explanation (string)\n",
    "    \"\"\"\n",
    "\n",
    "    @weave.op\n",
    "    def score(self, email: str, output: dict[str, Any]) -> dict[str, Any]:\n",
    "        \"\"\"Score using LLM as judge\"\"\"\n",
    "        client = OpenAI()\n",
    "\n",
    "        prompt = self.judge_prompt.format(\n",
    "            email=email,\n",
    "            customer_name=output.get(\"customer_name\", \"N/A\"),\n",
    "            product_model=output.get(\"product_model\", \"N/A\"),\n",
    "            issue_description=output.get(\"issue_description\", \"N/A\"),\n",
    "        )\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "\n",
    "        result = json.loads(response.choices[0].message.content)\n",
    "        return {\n",
    "            \"llm_judge_score\": result[\"score\"] / 10.0,  # Normalize to 0-1\n",
    "            \"llm_judge_explanation\": result[\"explanation\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960fc980",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Example 2: Batch Evaluation with Progress Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663b44fe",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "@weave.op\n",
    "def batch_evaluate_with_progress(models: list[Model], dataset: Dataset, scorers: list):\n",
    "    \"\"\"Evaluate multiple models and show progress\"\"\"\n",
    "    results = {}\n",
    "    total_evaluations = len(models)\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        model_name = getattr(model, \"name\", model.__class__.__name__)\n",
    "        print(f\"\\nðŸ“Š Evaluating {model_name} ({i+1}/{total_evaluations})...\")\n",
    "\n",
    "        evaluation = Evaluation(\n",
    "            dataset=dataset, scorers=scorers, name=f\"Batch Evaluation - {model_name}\"\n",
    "        )\n",
    "\n",
    "        # Run evaluation\n",
    "        result = asyncio.run(evaluation.evaluate(model))\n",
    "        results[model_name] = result\n",
    "\n",
    "        print(f\"âœ… Completed {model_name}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae70481d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Example 3: Multi-Stage Evaluation Pipeline\n",
    "\n",
    "For complex evaluations, you might want to run different stages with different criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc10e40",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class MultiStageEvaluator:\n",
    "    \"\"\"Run multi-stage evaluation with different criteria\"\"\"\n",
    "\n",
    "    def __init__(self, model, dataset):\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.logger = EvaluationLogger(\n",
    "            model=model.__class__.__name__, dataset=dataset.name\n",
    "        )\n",
    "\n",
    "    @weave.op\n",
    "    def evaluate_accuracy_stage(self, example):\n",
    "        \"\"\"Stage 1: Evaluate extraction accuracy\"\"\"\n",
    "        output = self.model.predict(example[\"email\"])\n",
    "\n",
    "        pred_logger = self.logger.log_prediction(\n",
    "            inputs={\"email\": example[\"email\"]}, output=output.model_dump()\n",
    "        )\n",
    "\n",
    "        # Check exact matches\n",
    "        for field in [\"customer_name\", \"product_model\"]:\n",
    "            expected = example.get(f\"expected_{field}\", \"\")\n",
    "            actual = getattr(output, field, \"\")\n",
    "            match = expected.lower() == actual.lower()\n",
    "            pred_logger.log_score(scorer=f\"{field}_exact\", score=match)\n",
    "\n",
    "        return pred_logger\n",
    "\n",
    "    @weave.op\n",
    "    def evaluate_quality_stage(self, example, pred_logger):\n",
    "        \"\"\"Stage 2: Evaluate output quality\"\"\"\n",
    "        output = self.model.predict(example[\"email\"])\n",
    "\n",
    "        # Check issue description quality\n",
    "        issue_desc = output.issue_description\n",
    "\n",
    "        # Length check (should be concise)\n",
    "        is_concise = 10 <= len(issue_desc) <= 100\n",
    "        pred_logger.log_score(scorer=\"description_concise\", score=is_concise)\n",
    "\n",
    "        # Contains key information\n",
    "        email_lower = example[\"email\"].lower()\n",
    "        desc_lower = issue_desc.lower()\n",
    "\n",
    "        # Simple keyword overlap check\n",
    "        keywords = [word for word in desc_lower.split() if len(word) > 3]\n",
    "        keyword_coverage = sum(1 for kw in keywords if kw in email_lower) / max(\n",
    "            len(keywords), 1\n",
    "        )\n",
    "        pred_logger.log_score(scorer=\"keyword_coverage\", score=keyword_coverage)\n",
    "\n",
    "        pred_logger.finish()\n",
    "\n",
    "    @weave.op\n",
    "    def run_full_evaluation(self):\n",
    "        \"\"\"Run complete multi-stage evaluation\"\"\"\n",
    "        print(f\"ðŸš€ Starting multi-stage evaluation for {self.model.__class__.__name__}\")\n",
    "\n",
    "        for example in self.dataset.rows:\n",
    "            # Stage 1: Accuracy\n",
    "            pred_logger = self.evaluate_accuracy_stage(example)\n",
    "\n",
    "            # Stage 2: Quality\n",
    "            self.evaluate_quality_stage(example, pred_logger)\n",
    "\n",
    "        # Summary metrics\n",
    "        self.logger.log_summary(\n",
    "            {\"evaluation_type\": \"multi_stage\", \"stages\": [\"accuracy\", \"quality\"]}\n",
    "        )\n",
    "\n",
    "        print(\"âœ… Multi-stage evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64239023",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Example 4: A/B Testing Framework\n",
    "\n",
    "Compare two models head-to-head on the same examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf074ca4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "@weave.op\n",
    "def ab_test_models(model_a: Model, model_b: Model, dataset: Dataset) -> dict[str, Any]:\n",
    "    \"\"\"Run A/B test between two models\"\"\"\n",
    "    logger_a = EvaluationLogger(\n",
    "        model=f\"{model_a.__class__.__name__}_A\", dataset=dataset.name\n",
    "    )\n",
    "    logger_b = EvaluationLogger(\n",
    "        model=f\"{model_b.__class__.__name__}_B\", dataset=dataset.name\n",
    "    )\n",
    "\n",
    "    wins_a = 0\n",
    "    wins_b = 0\n",
    "    ties = 0\n",
    "\n",
    "    for example in dataset.rows:\n",
    "        # Get predictions from both models\n",
    "        output_a = model_a.predict(example[\"email\"])\n",
    "        output_b = model_b.predict(example[\"email\"])\n",
    "\n",
    "        # Log predictions\n",
    "        pred_a = logger_a.log_prediction(\n",
    "            inputs={\"email\": example[\"email\"]}, output=output_a.model_dump()\n",
    "        )\n",
    "        pred_b = logger_b.log_prediction(\n",
    "            inputs={\"email\": example[\"email\"]}, output=output_b.model_dump()\n",
    "        )\n",
    "\n",
    "        # Calculate scores for both\n",
    "        score_a = 0\n",
    "        score_b = 0\n",
    "\n",
    "        for field in [\"customer_name\", \"product_model\", \"issue_description\"]:\n",
    "            expected = example.get(f\"expected_{field}\", \"\")\n",
    "\n",
    "            # Model A\n",
    "            actual_a = getattr(output_a, field, \"\")\n",
    "            if expected.lower() == actual_a.lower():\n",
    "                score_a += 1\n",
    "                pred_a.log_score(scorer=f\"{field}_correct\", score=True)\n",
    "            else:\n",
    "                pred_a.log_score(scorer=f\"{field}_correct\", score=False)\n",
    "\n",
    "            # Model B\n",
    "            actual_b = getattr(output_b, field, \"\")\n",
    "            if expected.lower() == actual_b.lower():\n",
    "                score_b += 1\n",
    "                pred_b.log_score(scorer=f\"{field}_correct\", score=True)\n",
    "            else:\n",
    "                pred_b.log_score(scorer=f\"{field}_correct\", score=False)\n",
    "\n",
    "        # Determine winner for this example\n",
    "        if score_a > score_b:\n",
    "            wins_a += 1\n",
    "            pred_a.log_score(scorer=\"head_to_head\", score=1)\n",
    "            pred_b.log_score(scorer=\"head_to_head\", score=0)\n",
    "        elif score_b > score_a:\n",
    "            wins_b += 1\n",
    "            pred_a.log_score(scorer=\"head_to_head\", score=0)\n",
    "            pred_b.log_score(scorer=\"head_to_head\", score=1)\n",
    "        else:\n",
    "            ties += 1\n",
    "            pred_a.log_score(scorer=\"head_to_head\", score=0.5)\n",
    "            pred_b.log_score(scorer=\"head_to_head\", score=0.5)\n",
    "\n",
    "        pred_a.finish()\n",
    "        pred_b.finish()\n",
    "\n",
    "    # Log summaries\n",
    "    total = len(dataset.rows)\n",
    "    logger_a.log_summary({\"ab_test_wins\": wins_a, \"ab_test_win_rate\": wins_a / total})\n",
    "    logger_b.log_summary({\"ab_test_wins\": wins_b, \"ab_test_win_rate\": wins_b / total})\n",
    "\n",
    "    # Return results\n",
    "    return {\n",
    "        \"model_a_wins\": wins_a,\n",
    "        \"model_b_wins\": wins_b,\n",
    "        \"ties\": ties,\n",
    "        \"model_a_win_rate\": wins_a / total,\n",
    "        \"model_b_win_rate\": wins_b / total,\n",
    "        \"winner\": \"Model A\"\n",
    "        if wins_a > wins_b\n",
    "        else (\"Model B\" if wins_b > wins_a else \"Tie\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17a3964",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Example 5: Cross-Validation Style Evaluation\n",
    "\n",
    "Split your dataset and evaluate on different subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d902c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "def cross_validate_model(\n",
    "    model: Model, dataset: Dataset, n_folds: int = 3\n",
    ") -> list[dict]:\n",
    "    \"\"\"Perform k-fold cross-validation style evaluation\"\"\"\n",
    "    rows = dataset.rows\n",
    "    fold_size = len(rows) // n_folds\n",
    "    results = []\n",
    "\n",
    "    for fold in range(n_folds):\n",
    "        # Create train/test split\n",
    "        start_idx = fold * fold_size\n",
    "        end_idx = start_idx + fold_size if fold < n_folds - 1 else len(rows)\n",
    "\n",
    "        test_rows = rows[start_idx:end_idx]\n",
    "        train_rows = rows[:start_idx] + rows[end_idx:]\n",
    "\n",
    "        # Create fold dataset\n",
    "        fold_dataset = Dataset(name=f\"{dataset.name}_fold_{fold+1}\", rows=test_rows)\n",
    "\n",
    "        # Create fold-specific logger\n",
    "        logger = EvaluationLogger(\n",
    "            model=f\"{model.__class__.__name__}_fold_{fold+1}\", dataset=fold_dataset.name\n",
    "        )\n",
    "\n",
    "        # Evaluate on this fold\n",
    "        fold_scores = []\n",
    "        for example in test_rows:\n",
    "            output = model.predict(example[\"email\"])\n",
    "\n",
    "            pred = logger.log_prediction(\n",
    "                inputs={\"email\": example[\"email\"]}, output=output.model_dump()\n",
    "            )\n",
    "\n",
    "            # Calculate accuracy\n",
    "            correct_fields = 0\n",
    "            total_fields = 3\n",
    "\n",
    "            for field in [\"customer_name\", \"product_model\", \"issue_description\"]:\n",
    "                expected = example.get(f\"expected_{field}\", \"\")\n",
    "                actual = getattr(output, field, \"\")\n",
    "                if expected.lower() == actual.lower():\n",
    "                    correct_fields += 1\n",
    "                    pred.log_score(scorer=f\"{field}_match\", score=True)\n",
    "                else:\n",
    "                    pred.log_score(scorer=f\"{field}_match\", score=False)\n",
    "\n",
    "            accuracy = correct_fields / total_fields\n",
    "            pred.log_score(scorer=\"accuracy\", score=accuracy)\n",
    "            fold_scores.append(accuracy)\n",
    "\n",
    "            pred.finish()\n",
    "\n",
    "        # Log fold summary\n",
    "        avg_accuracy = sum(fold_scores) / len(fold_scores) if fold_scores else 0\n",
    "        logger.log_summary(\n",
    "            {\n",
    "                \"fold_number\": fold + 1,\n",
    "                \"fold_size\": len(test_rows),\n",
    "                \"average_accuracy\": avg_accuracy,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"fold\": fold + 1,\n",
    "                \"test_size\": len(test_rows),\n",
    "                \"train_size\": len(train_rows),\n",
    "                \"average_accuracy\": avg_accuracy,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    overall_accuracy = sum(r[\"average_accuracy\"] for r in results) / len(results)\n",
    "    accuracy_std = (\n",
    "        sum((r[\"average_accuracy\"] - overall_accuracy) ** 2 for r in results)\n",
    "        / len(results)\n",
    "    ) ** 0.5\n",
    "\n",
    "    print(\"\\nðŸ“Š Cross-Validation Results:\")\n",
    "    print(f\"Average Accuracy: {overall_accuracy:.3f} Â± {accuracy_std:.3f}\")\n",
    "    for r in results:\n",
    "        print(f\"  Fold {r['fold']}: {r['average_accuracy']:.3f}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240c6edc",
   "metadata": {},
   "source": [
    "## Usage Examples\n",
    "\n",
    "Here's how you might use these advanced evaluation patterns in your workshop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8d4a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (commented out to avoid execution)\n",
    "\"\"\"\n",
    "# 1. Using custom LLM Judge scorer\n",
    "llm_judge = LLMJudgeScorer()\n",
    "evaluation_with_judge = Evaluation(\n",
    "    dataset=your_dataset,\n",
    "    scorers=[exact_match_scorer, llm_judge],\n",
    "    name=\"Evaluation with LLM Judge\"\n",
    ")\n",
    "\n",
    "# 2. Running multi-stage evaluation\n",
    "evaluator = MultiStageEvaluator(model=your_model, dataset=your_dataset)\n",
    "evaluator.run_full_evaluation()\n",
    "\n",
    "# 3. A/B Testing two models\n",
    "results = ab_test_models(model_v1, model_v2, your_dataset)\n",
    "print(f\"A/B Test Winner: {results['winner']}\")\n",
    "\n",
    "# 4. Cross-validation\n",
    "cv_results = cross_validate_model(your_model, your_dataset, n_folds=5)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0b80f9",
   "metadata": {},
   "source": [
    "## Key Takeaways for Advanced Evaluation\n",
    "\n",
    "1. **Custom Scorers**: Create reusable scorer classes for complex evaluation logic\n",
    "2. **Multi-Stage**: Break down evaluation into logical stages for better insights\n",
    "3. **A/B Testing**: Compare models head-to-head on the same data\n",
    "4. **Cross-Validation**: Get more robust evaluation metrics with multiple folds\n",
    "5. **Progress Tracking**: Use EvaluationLogger for real-time progress updates\n",
    "\n",
    "These patterns help you build more sophisticated evaluation pipelines that go beyond simple accuracy metrics!\n",
    "\n",
    "\n",
    "@weave.op\n",
    "def evaluate_model(\n",
    "    model: Model,\n",
    "    dataset: Dataset,\n",
    "    scorers: list[Scorer],\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"Run evaluation on a model.\"\"\"\n",
    "    results = {}\n",
    "    for scorer in scorers:\n",
    "        score = scorer(model, dataset)\n",
    "        results[scorer.__name__] = score\n",
    "    return results\n",
    "\n",
    "\n",
    "@weave.op\n",
    "def compare_models(\n",
    "    models: list[Model],\n",
    "    dataset: Dataset,\n",
    "    scorers: list[Scorer],\n",
    ") -> dict[str, dict[str, Any]]:\n",
    "    \"\"\"Compare multiple models using the same evaluation.\"\"\"\n",
    "    results = {}\n",
    "    for model in models:\n",
    "        results[model.name] = evaluate_model(model, dataset, scorers)\n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
