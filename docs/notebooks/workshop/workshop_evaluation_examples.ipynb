{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74604b77",
   "metadata": {},
   "source": [
    "# 🚀 Advanced Weave Evaluation Patterns\n",
    "\n",
    "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "\n",
    "Welcome to the advanced evaluation patterns workshop! This notebook extends the main Weave workshop\n",
    "with sophisticated evaluation techniques for production use cases.\n",
    "\n",
    "**What you'll learn:**\n",
    "- 🎯 **Custom Scorers**: Build domain-specific evaluation metrics\n",
    "- 📊 **Multi-Stage Evaluation**: Break complex evaluations into stages\n",
    "- 🏃 **A/B Testing**: Statistical comparison of models\n",
    "- 🔄 **Cross-Validation**: Robust evaluation with data splits\n",
    "- 🏭 **Production Patterns**: Real-world evaluation workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedcdc34",
   "metadata": {},
   "source": [
    "## 🔑 Prerequisites & Setup\n",
    "\n",
    "Run this notebook after completing the main Weave workshop, or as a standalone advanced tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd124c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (for Colab compatibility)\n",
    "%pip install wandb weave openai pydantic nest_asyncio litellm -qqq\n",
    "\n",
    "import os\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import nest_asyncio\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import weave\n",
    "from weave import Dataset, Evaluation, EvaluationLogger, Model, Scorer\n",
    "\n",
    "# Enable nested asyncio for notebooks\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# 🔑 Setup API keys\n",
    "print(\"📝 Setting up environment...\")\n",
    "\n",
    "# OpenAI API key setup\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    print(\"⚠️ OpenAI API key not found!\")\n",
    "    print(\"Set it with: os.environ['OPENAI_API_KEY'] = 'your-key-here'\")\n",
    "    # For Colab, you might use:\n",
    "    # from google.colab import userdata\n",
    "    # os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "else:\n",
    "    print(\"✅ OpenAI API key found\")\n",
    "\n",
    "# Initialize Weave\n",
    "print(\"🐝 Initializing Weave...\")\n",
    "weave_client = weave.init(\"advanced-evaluation-workshop\")\n",
    "print(\"✅ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718e996f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 📋 Shared Data Models\n",
    "\n",
    "We'll use the same models from the main workshop for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141a0cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our data structures (from main workshop)\n",
    "class CustomerEmail(BaseModel):\n",
    "    \"\"\"Basic customer email analysis.\"\"\"\n",
    "\n",
    "    customer_name: str\n",
    "    product: str\n",
    "    issue: str\n",
    "    sentiment: str = Field(description=\"positive, neutral, or negative\")\n",
    "\n",
    "\n",
    "class DetailedCustomerEmail(BaseModel):\n",
    "    \"\"\"Extended analysis with more fields.\"\"\"\n",
    "\n",
    "    customer_name: str\n",
    "    customer_title: Optional[str] = Field(description=\"Job title if mentioned\")\n",
    "    company: Optional[str] = Field(description=\"Company name if mentioned\")\n",
    "    product: str\n",
    "    product_version: Optional[str] = Field(description=\"Specific version number\")\n",
    "    issue: str\n",
    "    issue_category: str = Field(\n",
    "        description=\"technical, billing, feature_request, or other\"\n",
    "    )\n",
    "    severity: str = Field(description=\"critical, high, medium, or low\")\n",
    "    sentiment: str = Field(description=\"positive, neutral, or negative\")\n",
    "\n",
    "\n",
    "# Create a sample dataset for our examples\n",
    "print(\"📊 Creating sample dataset...\")\n",
    "advanced_dataset = Dataset(\n",
    "    name=\"advanced_support_emails\",\n",
    "    rows=[\n",
    "        {\n",
    "            \"email\": \"Hi Support, I'm Sarah Chen, CTO at TechCorp. Our Enterprise CloudSync v3.2 cluster is experiencing critical latency issues affecting 5000+ users. Need immediate assistance!\",\n",
    "            \"expected\": {\n",
    "                \"customer_name\": \"Sarah Chen\",\n",
    "                \"severity\": \"critical\",\n",
    "                \"product\": \"Enterprise CloudSync v3.2\",\n",
    "                \"issue_category\": \"technical\",\n",
    "                \"company\": \"TechCorp\",\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"email\": \"Hello, this is Mike Johnson. I'd like to request a feature for DataProcessor Pro - ability to export to Parquet format. Not urgent but would be very helpful.\",\n",
    "            \"expected\": {\n",
    "                \"customer_name\": \"Mike Johnson\",\n",
    "                \"severity\": \"low\",\n",
    "                \"product\": \"DataProcessor Pro\",\n",
    "                \"issue_category\": \"feature_request\",\n",
    "                \"sentiment\": \"positive\",\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"email\": \"Billing issue! We were charged twice for CloudVault licenses last month. Please refund ASAP. - Janet Williams, Accounting Manager at FinanceInc\",\n",
    "            \"expected\": {\n",
    "                \"customer_name\": \"Janet Williams\",\n",
    "                \"severity\": \"high\",\n",
    "                \"product\": \"CloudVault\",\n",
    "                \"issue_category\": \"billing\",\n",
    "                \"company\": \"FinanceInc\",\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(f\"✅ Created dataset with {len(advanced_dataset.rows)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61311924",
   "metadata": {},
   "source": [
    "## 🎯 Example 1: Domain-Specific Custom Scorers\n",
    "\n",
    "Build sophisticated scorers that understand your business logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0644cd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"🎯 EXAMPLE 1: Domain-Specific Custom Scorers\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "class BusinessImpactScorer(Scorer):\n",
    "    \"\"\"Score based on potential business impact of the issue.\"\"\"\n",
    "\n",
    "    @weave.op\n",
    "    def score(self, expected: dict, output: DetailedCustomerEmail) -> dict[str, Any]:\n",
    "        \"\"\"Calculate business impact score based on multiple factors.\"\"\"\n",
    "        impact_score = 0.0\n",
    "        factors = []\n",
    "\n",
    "        # 1. Severity weight (40%)\n",
    "        severity_weights = {\"critical\": 1.0, \"high\": 0.7, \"medium\": 0.4, \"low\": 0.1}\n",
    "        severity_score = severity_weights.get(output.severity, 0.5) * 0.4\n",
    "        impact_score += severity_score\n",
    "        factors.append(f\"Severity ({output.severity}): {severity_score:.2f}\")\n",
    "\n",
    "        # 2. Customer type weight (30%)\n",
    "        customer_score = 0.0\n",
    "        if output.company:\n",
    "            customer_score += 0.2  # B2B customer\n",
    "            if \"enterprise\" in output.product.lower():\n",
    "                customer_score += 0.1  # Enterprise product\n",
    "        else:\n",
    "            customer_score += 0.1  # B2C customer\n",
    "        impact_score += customer_score\n",
    "        factors.append(f\"Customer type: {customer_score:.2f}\")\n",
    "\n",
    "        # 3. Issue category weight (30%)\n",
    "        category_weights = {\n",
    "            \"technical\": 0.25,  # Can affect operations\n",
    "            \"billing\": 0.20,  # Revenue impact\n",
    "            \"feature_request\": 0.05,  # Future value\n",
    "            \"other\": 0.10,\n",
    "        }\n",
    "        category_score = category_weights.get(output.issue_category, 0.1) * 1.0\n",
    "        impact_score += category_score\n",
    "        factors.append(f\"Category ({output.issue_category}): {category_score:.2f}\")\n",
    "\n",
    "        # Business rules\n",
    "        requires_escalation = (\n",
    "            output.severity in [\"critical\", \"high\"]\n",
    "            or output.issue_category == \"billing\"\n",
    "            or (output.company and \"enterprise\" in output.product.lower())\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"business_impact_score\": impact_score,\n",
    "            \"impact_factors\": factors,\n",
    "            \"requires_escalation\": requires_escalation,\n",
    "            \"priority_level\": \"P1\"\n",
    "            if impact_score > 0.7\n",
    "            else (\"P2\" if impact_score > 0.4 else \"P3\"),\n",
    "        }\n",
    "\n",
    "\n",
    "class SLAComplianceScorer(Scorer):\n",
    "    \"\"\"Check if response meets SLA requirements.\"\"\"\n",
    "\n",
    "    def __init__(self, sla_rules: Optional[Dict] = None):\n",
    "        \"\"\"Initialize with SLA rules.\"\"\"\n",
    "        self.sla_rules = sla_rules or {\n",
    "            \"critical\": {\"response_time_hours\": 1, \"resolution_time_hours\": 4},\n",
    "            \"high\": {\"response_time_hours\": 4, \"resolution_time_hours\": 24},\n",
    "            \"medium\": {\"response_time_hours\": 24, \"resolution_time_hours\": 72},\n",
    "            \"low\": {\"response_time_hours\": 72, \"resolution_time_hours\": 168},\n",
    "        }\n",
    "\n",
    "    @weave.op\n",
    "    def score(\n",
    "        self, output: DetailedCustomerEmail, response_time_hours: float = 0\n",
    "    ) -> dict[str, Any]:\n",
    "        \"\"\"Check SLA compliance.\"\"\"\n",
    "        sla = self.sla_rules.get(output.severity, self.sla_rules[\"medium\"])\n",
    "\n",
    "        response_sla_met = response_time_hours <= sla[\"response_time_hours\"]\n",
    "        sla_margin = sla[\"response_time_hours\"] - response_time_hours\n",
    "\n",
    "        return {\n",
    "            \"sla_compliance\": response_sla_met,\n",
    "            \"response_time_hours\": response_time_hours,\n",
    "            \"sla_limit_hours\": sla[\"response_time_hours\"],\n",
    "            \"sla_margin_hours\": sla_margin,\n",
    "            \"severity\": output.severity,\n",
    "            \"requires_immediate_action\": sla_margin < 0.5 and not response_sla_met,\n",
    "        }\n",
    "\n",
    "\n",
    "# Test the custom scorers\n",
    "print(\"\\n🧪 Testing Business Impact Scorer...\")\n",
    "\n",
    "\n",
    "# Create a simple model for testing\n",
    "class SimpleAnalyzer(Model):\n",
    "    \"\"\"Simple analyzer for testing scorers.\"\"\"\n",
    "\n",
    "    @weave.op\n",
    "    def predict(self, email: str) -> DetailedCustomerEmail:\n",
    "        \"\"\"Simplified analysis for demonstration.\"\"\"\n",
    "        # In real scenarios, this would use an LLM\n",
    "        if \"critical\" in email.lower() or \"immediate\" in email.lower():\n",
    "            severity = \"critical\"\n",
    "        elif \"asap\" in email.lower() or \"urgent\" in email.lower():\n",
    "            severity = \"high\"\n",
    "        else:\n",
    "            severity = \"medium\"\n",
    "\n",
    "        return DetailedCustomerEmail(\n",
    "            customer_name=\"Test User\",\n",
    "            product=\"Test Product\",\n",
    "            issue=\"Test issue from email\",\n",
    "            issue_category=\"technical\" if \"technical\" in email.lower() else \"other\",\n",
    "            severity=severity,\n",
    "            sentiment=\"negative\" if severity in [\"critical\", \"high\"] else \"neutral\",\n",
    "            company=\"TestCorp\" if \"corp\" in email.lower() else None,\n",
    "        )\n",
    "\n",
    "\n",
    "# Run evaluation with custom scorers\n",
    "model = SimpleAnalyzer()\n",
    "business_scorer = BusinessImpactScorer()\n",
    "sla_scorer = SLAComplianceScorer()\n",
    "\n",
    "print(\"\\n📊 Running evaluation with custom scorers...\")\n",
    "custom_eval = Evaluation(\n",
    "    name=\"business_impact_evaluation\",\n",
    "    dataset=advanced_dataset,\n",
    "    scorers=[business_scorer],\n",
    ")\n",
    "\n",
    "# Note: For full execution, you would run:\n",
    "# results = asyncio.run(custom_eval.evaluate(model))\n",
    "print(\"✅ Custom scorers configured and ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b08b7e8",
   "metadata": {},
   "source": [
    "## 🔄 Example 2: Multi-Stage Evaluation Pipeline\n",
    "\n",
    "Break complex evaluations into logical stages for better insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea94ff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"🔄 EXAMPLE 2: Multi-Stage Evaluation Pipeline\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "class MultiStageEvaluator:\n",
    "    \"\"\"Sophisticated multi-stage evaluation with detailed tracking.\"\"\"\n",
    "\n",
    "    def __init__(self, model: Model, dataset: Dataset):\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        # Use rich metadata for the model\n",
    "        model_metadata = {\n",
    "            \"name\": model.__class__.__name__,\n",
    "            \"version\": \"1.0.0\",\n",
    "            \"evaluation_type\": \"multi_stage\",\n",
    "            \"stages\": [\"extraction\", \"accuracy\", \"quality\", \"business_logic\"],\n",
    "        }\n",
    "        self.logger = EvaluationLogger(\n",
    "            model=model_metadata, dataset=f\"{dataset.name}_multistage\"\n",
    "        )\n",
    "        self.stage_results = {}\n",
    "\n",
    "    @weave.op\n",
    "    def stage1_extraction_completeness(self, example: dict) -> dict:\n",
    "        \"\"\"Stage 1: Check if all required fields are extracted.\"\"\"\n",
    "        print(\"\\n  🔍 Stage 1: Extraction Completeness\")\n",
    "\n",
    "        output = self.model.predict(example[\"email\"])\n",
    "        pred_logger = self.logger.log_prediction(\n",
    "            inputs={\"email\": example[\"email\"]}, output=output.model_dump()\n",
    "        )\n",
    "\n",
    "        # Check required fields\n",
    "        required_fields = [\"customer_name\", \"product\", \"issue\", \"severity\", \"sentiment\"]\n",
    "        optional_fields = [\"company\", \"customer_title\", \"product_version\"]\n",
    "\n",
    "        required_complete = 0\n",
    "        optional_complete = 0\n",
    "        missing_fields = []\n",
    "\n",
    "        for field in required_fields:\n",
    "            value = getattr(output, field, None)\n",
    "            if value and value != \"Unknown\":\n",
    "                required_complete += 1\n",
    "            else:\n",
    "                missing_fields.append(field)\n",
    "\n",
    "        for field in optional_fields:\n",
    "            value = getattr(output, field, None)\n",
    "            if value:\n",
    "                optional_complete += 1\n",
    "\n",
    "        completeness_score = (required_complete / len(required_fields)) * 0.8 + (\n",
    "            optional_complete / len(optional_fields)\n",
    "        ) * 0.2\n",
    "\n",
    "        pred_logger.log_score(scorer=\"completeness\", score=completeness_score)\n",
    "        pred_logger.log_score(scorer=\"missing_fields_count\", score=len(missing_fields))\n",
    "\n",
    "        self.stage_results[\"extraction\"] = {\n",
    "            \"completeness\": completeness_score,\n",
    "            \"missing_fields\": missing_fields,\n",
    "        }\n",
    "\n",
    "        print(f\"    Completeness: {completeness_score:.2%}\")\n",
    "        print(f\"    Missing fields: {missing_fields if missing_fields else 'None'}\")\n",
    "\n",
    "        return pred_logger\n",
    "\n",
    "    @weave.op\n",
    "    def stage2_accuracy_check(self, example: dict, pred_logger) -> None:\n",
    "        \"\"\"Stage 2: Check accuracy against expected values.\"\"\"\n",
    "        print(\"  🎯 Stage 2: Accuracy Check\")\n",
    "\n",
    "        output = self.model.predict(example[\"email\"])\n",
    "        expected = example.get(\"expected\", {})\n",
    "\n",
    "        accuracy_scores = {}\n",
    "\n",
    "        # Check each expected field\n",
    "        for field, expected_value in expected.items():\n",
    "            actual_value = getattr(output, field, None)\n",
    "            if actual_value:\n",
    "                # Exact match or close enough\n",
    "                is_correct = str(actual_value).lower() == str(expected_value).lower()\n",
    "                accuracy_scores[field] = 1.0 if is_correct else 0.0\n",
    "                pred_logger.log_score(\n",
    "                    scorer=f\"{field}_accuracy\", score=accuracy_scores[field]\n",
    "                )\n",
    "\n",
    "        overall_accuracy = (\n",
    "            sum(accuracy_scores.values()) / len(accuracy_scores)\n",
    "            if accuracy_scores\n",
    "            else 0\n",
    "        )\n",
    "        pred_logger.log_score(scorer=\"overall_accuracy\", score=overall_accuracy)\n",
    "\n",
    "        self.stage_results[\"accuracy\"] = {\n",
    "            \"overall\": overall_accuracy,\n",
    "            \"per_field\": accuracy_scores,\n",
    "        }\n",
    "\n",
    "        print(f\"    Overall accuracy: {overall_accuracy:.2%}\")\n",
    "        print(f\"    Per-field: {accuracy_scores}\")\n",
    "\n",
    "    @weave.op\n",
    "    def stage3_quality_assessment(self, example: dict, pred_logger) -> None:\n",
    "        \"\"\"Stage 3: Assess quality of extraction.\"\"\"\n",
    "        print(\"  📏 Stage 3: Quality Assessment\")\n",
    "\n",
    "        output = self.model.predict(example[\"email\"])\n",
    "\n",
    "        quality_metrics = {\"specificity\": 0.0, \"consistency\": 0.0, \"actionability\": 0.0}\n",
    "\n",
    "        # Specificity: Are extracted values specific enough?\n",
    "        if output.product and any(char.isdigit() for char in output.product):\n",
    "            quality_metrics[\"specificity\"] += 0.5  # Has version info\n",
    "        if output.issue and len(output.issue) > 50:\n",
    "            quality_metrics[\"specificity\"] += 0.5  # Detailed description\n",
    "\n",
    "        # Consistency: Do severity and sentiment align?\n",
    "        severity_sentiment_map = {\n",
    "            \"critical\": \"negative\",\n",
    "            \"high\": \"negative\",\n",
    "            \"medium\": \"neutral\",\n",
    "            \"low\": \"positive\",\n",
    "        }\n",
    "        expected_sentiment = severity_sentiment_map.get(output.severity, \"neutral\")\n",
    "        if output.sentiment == expected_sentiment:\n",
    "            quality_metrics[\"consistency\"] = 1.0\n",
    "\n",
    "        # Actionability: Can we route this effectively?\n",
    "        if output.severity and output.issue_category:\n",
    "            quality_metrics[\"actionability\"] = 1.0\n",
    "\n",
    "        for metric, score in quality_metrics.items():\n",
    "            pred_logger.log_score(scorer=f\"quality_{metric}\", score=score)\n",
    "\n",
    "        overall_quality = sum(quality_metrics.values()) / len(quality_metrics)\n",
    "        pred_logger.log_score(scorer=\"overall_quality\", score=overall_quality)\n",
    "\n",
    "        self.stage_results[\"quality\"] = quality_metrics\n",
    "\n",
    "        print(f\"    Overall quality: {overall_quality:.2%}\")\n",
    "        print(f\"    Metrics: {quality_metrics}\")\n",
    "\n",
    "    @weave.op\n",
    "    def run_full_evaluation(self) -> dict:\n",
    "        \"\"\"Run all evaluation stages.\"\"\"\n",
    "        print(\"\\n🚀 Starting Multi-Stage Evaluation\")\n",
    "        print(f\"   Model: {self.model.__class__.__name__}\")\n",
    "        print(f\"   Dataset: {self.dataset.name} ({len(self.dataset.rows)} examples)\")\n",
    "\n",
    "        all_results = []\n",
    "\n",
    "        for i, example in enumerate(self.dataset.rows):\n",
    "            print(f\"\\n📧 Example {i+1}/{len(self.dataset.rows)}\")\n",
    "            print(f\"   Email preview: {example['email'][:80]}...\")\n",
    "\n",
    "            # Run all stages\n",
    "            pred_logger = self.stage1_extraction_completeness(example)\n",
    "            self.stage2_accuracy_check(example, pred_logger)\n",
    "            self.stage3_quality_assessment(example, pred_logger)\n",
    "\n",
    "            pred_logger.finish()\n",
    "            all_results.append(self.stage_results.copy())\n",
    "\n",
    "        # Calculate aggregate metrics\n",
    "        aggregate_metrics = {\n",
    "            \"avg_completeness\": statistics.mean(\n",
    "                [r[\"extraction\"][\"completeness\"] for r in all_results]\n",
    "            ),\n",
    "            \"avg_accuracy\": statistics.mean(\n",
    "                [r[\"accuracy\"][\"overall\"] for r in all_results]\n",
    "            ),\n",
    "            \"avg_quality\": statistics.mean(\n",
    "                [r[\"quality\"][m] for r in all_results for m in r[\"quality\"]]\n",
    "            )\n",
    "            / 3,\n",
    "        }\n",
    "\n",
    "        # Log summary\n",
    "        self.logger.log_summary(\n",
    "            {\n",
    "                \"total_examples\": len(self.dataset.rows),\n",
    "                \"aggregate_metrics\": aggregate_metrics,\n",
    "                \"evaluation_completed\": datetime.now().isoformat(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(\"\\n📊 Evaluation Complete!\")\n",
    "        print(f\"   Average Completeness: {aggregate_metrics['avg_completeness']:.2%}\")\n",
    "        print(f\"   Average Accuracy: {aggregate_metrics['avg_accuracy']:.2%}\")\n",
    "        print(f\"   Average Quality: {aggregate_metrics['avg_quality']:.2%}\")\n",
    "\n",
    "        return aggregate_metrics\n",
    "\n",
    "\n",
    "# Demonstrate multi-stage evaluation\n",
    "evaluator = MultiStageEvaluator(model, advanced_dataset)\n",
    "# To run: evaluator.run_full_evaluation()\n",
    "print(\"\\n✅ Multi-stage evaluator ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c1f327",
   "metadata": {},
   "source": [
    "## 🏃 Example 3: Statistical A/B Testing\n",
    "\n",
    "Compare models with statistical rigor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d75610",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"🏃 EXAMPLE 3: Statistical A/B Testing\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "@weave.op\n",
    "def statistical_ab_test(\n",
    "    model_a: Model, model_b: Model, dataset: Dataset, confidence_level: float = 0.95\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"Perform A/B test with statistical significance testing.\"\"\"\n",
    "    print(\"\\n🔬 Running Statistical A/B Test\")\n",
    "    print(f\"   Model A: {model_a.__class__.__name__}\")\n",
    "    print(f\"   Model B: {model_b.__class__.__name__}\")\n",
    "    print(f\"   Confidence Level: {confidence_level:.0%}\")\n",
    "\n",
    "    # Create loggers with rich metadata\n",
    "    logger_a = EvaluationLogger(\n",
    "        model={\n",
    "            \"name\": f\"{model_a.__class__.__name__}\",\n",
    "            \"variant\": \"A\",\n",
    "            \"test_id\": f\"ab_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "        },\n",
    "        dataset=f\"{dataset.name}_ab_test\",\n",
    "    )\n",
    "    logger_b = EvaluationLogger(\n",
    "        model={\n",
    "            \"name\": f\"{model_b.__class__.__name__}\",\n",
    "            \"variant\": \"B\",\n",
    "            \"test_id\": f\"ab_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "        },\n",
    "        dataset=f\"{dataset.name}_ab_test\",\n",
    "    )\n",
    "\n",
    "    # Collect scores for statistical analysis\n",
    "    scores_a = []\n",
    "    scores_b = []\n",
    "    head_to_head = {\"a_wins\": 0, \"b_wins\": 0, \"ties\": 0}\n",
    "\n",
    "    print(f\"\\n📊 Processing {len(dataset.rows)} examples...\")\n",
    "\n",
    "    for i, example in enumerate(dataset.rows):\n",
    "        # Get predictions\n",
    "        output_a = model_a.predict(example[\"email\"])\n",
    "        output_b = model_b.predict(example[\"email\"])\n",
    "\n",
    "        # Log predictions\n",
    "        pred_a = logger_a.log_prediction(\n",
    "            inputs={\"email\": example[\"email\"]}, output=output_a.model_dump()\n",
    "        )\n",
    "        pred_b = logger_b.log_prediction(\n",
    "            inputs={\"email\": example[\"email\"]}, output=output_b.model_dump()\n",
    "        )\n",
    "\n",
    "        # Score each model (simplified scoring for demo)\n",
    "        score_a = 0\n",
    "        score_b = 0\n",
    "\n",
    "        # Basic scoring: check key fields\n",
    "        if output_a.customer_name and output_a.customer_name != \"Unknown\":\n",
    "            score_a += 0.25\n",
    "        if output_b.customer_name and output_b.customer_name != \"Unknown\":\n",
    "            score_b += 0.25\n",
    "\n",
    "        if output_a.severity in [\"critical\", \"high\", \"medium\", \"low\"]:\n",
    "            score_a += 0.25\n",
    "        if output_b.severity in [\"critical\", \"high\", \"medium\", \"low\"]:\n",
    "            score_b += 0.25\n",
    "\n",
    "        if len(output_a.issue) > 20:\n",
    "            score_a += 0.25\n",
    "        if len(output_b.issue) > 20:\n",
    "            score_b += 0.25\n",
    "\n",
    "        if output_a.issue_category in [\"technical\", \"billing\", \"feature_request\"]:\n",
    "            score_a += 0.25\n",
    "        if output_b.issue_category in [\"technical\", \"billing\", \"feature_request\"]:\n",
    "            score_b += 0.25\n",
    "\n",
    "        scores_a.append(score_a)\n",
    "        scores_b.append(score_b)\n",
    "\n",
    "        # Head-to-head comparison\n",
    "        if score_a > score_b:\n",
    "            head_to_head[\"a_wins\"] += 1\n",
    "        elif score_b > score_a:\n",
    "            head_to_head[\"b_wins\"] += 1\n",
    "        else:\n",
    "            head_to_head[\"ties\"] += 1\n",
    "\n",
    "        pred_a.log_score(scorer=\"quality_score\", score=score_a)\n",
    "        pred_b.log_score(scorer=\"quality_score\", score=score_b)\n",
    "\n",
    "        pred_a.finish()\n",
    "        pred_b.finish()\n",
    "\n",
    "    # Statistical analysis\n",
    "    import statistics as stats\n",
    "\n",
    "    mean_a = stats.mean(scores_a)\n",
    "    mean_b = stats.mean(scores_b)\n",
    "    std_a = stats.stdev(scores_a) if len(scores_a) > 1 else 0\n",
    "    std_b = stats.stdev(scores_b) if len(scores_b) > 1 else 0\n",
    "\n",
    "    # Simple t-test approximation (in production, use scipy.stats)\n",
    "    n = len(scores_a)\n",
    "    score_diff = mean_a - mean_b\n",
    "    pooled_std = ((std_a**2 + std_b**2) / 2) ** 0.5\n",
    "    t_statistic = score_diff / (pooled_std / (n**0.5)) if pooled_std > 0 else 0\n",
    "\n",
    "    # Approximate p-value (simplified)\n",
    "    is_significant = abs(t_statistic) > 1.96  # ~95% confidence\n",
    "\n",
    "    results = {\n",
    "        \"model_a\": {\n",
    "            \"mean_score\": mean_a,\n",
    "            \"std_dev\": std_a,\n",
    "            \"wins\": head_to_head[\"a_wins\"],\n",
    "        },\n",
    "        \"model_b\": {\n",
    "            \"mean_score\": mean_b,\n",
    "            \"std_dev\": std_b,\n",
    "            \"wins\": head_to_head[\"b_wins\"],\n",
    "        },\n",
    "        \"comparison\": {\n",
    "            \"score_difference\": score_diff,\n",
    "            \"t_statistic\": t_statistic,\n",
    "            \"is_significant\": is_significant,\n",
    "            \"ties\": head_to_head[\"ties\"],\n",
    "            \"winner\": \"Model A\" if score_diff > 0 else \"Model B\",\n",
    "            \"confidence\": \"High\" if is_significant else \"Low\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Log summaries\n",
    "    logger_a.log_summary(\n",
    "        {\n",
    "            \"mean_score\": mean_a,\n",
    "            \"wins\": head_to_head[\"a_wins\"],\n",
    "            \"win_rate\": head_to_head[\"a_wins\"] / n,\n",
    "        }\n",
    "    )\n",
    "    logger_b.log_summary(\n",
    "        {\n",
    "            \"mean_score\": mean_b,\n",
    "            \"wins\": head_to_head[\"b_wins\"],\n",
    "            \"win_rate\": head_to_head[\"b_wins\"] / n,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"\\n📈 Results:\")\n",
    "    print(f\"   Model A: {mean_a:.3f} ± {std_a:.3f}\")\n",
    "    print(f\"   Model B: {mean_b:.3f} ± {std_b:.3f}\")\n",
    "    print(f\"   Difference: {score_diff:.3f}\")\n",
    "    print(f\"   Statistical Significance: {'YES' if is_significant else 'NO'}\")\n",
    "    print(\n",
    "        f\"   Winner: {results['comparison']['winner']} ({results['comparison']['confidence']} confidence)\"\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Create a second model for comparison\n",
    "class ImprovedAnalyzer(SimpleAnalyzer):\n",
    "    \"\"\"Slightly better analyzer for A/B testing.\"\"\"\n",
    "\n",
    "    @weave.op\n",
    "    def predict(self, email: str) -> DetailedCustomerEmail:\n",
    "        \"\"\"Improved analysis.\"\"\"\n",
    "        base_result = super().predict(email)\n",
    "        # Add some improvements\n",
    "        if \"enterprise\" in email.lower():\n",
    "            base_result.product = \"Enterprise \" + base_result.product\n",
    "        if \"billing\" in email.lower():\n",
    "            base_result.issue_category = \"billing\"\n",
    "        return base_result\n",
    "\n",
    "\n",
    "model_b = ImprovedAnalyzer()\n",
    "print(\"\\n✅ A/B testing framework ready!\")\n",
    "# To run: statistical_ab_test(model, model_b, advanced_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52a15fe",
   "metadata": {},
   "source": [
    "## 🔄 Example 4: Cross-Validation Evaluation\n",
    "\n",
    "Ensure robust evaluation with k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c6f84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"🔄 EXAMPLE 4: Cross-Validation Evaluation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "@weave.op\n",
    "def cross_validate_model(\n",
    "    model: Model, dataset: Dataset, n_folds: int = 3, stratify_by: Optional[str] = None\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"Perform k-fold cross-validation with optional stratification.\"\"\"\n",
    "    print(f\"\\n🔄 Running {n_folds}-Fold Cross-Validation\")\n",
    "    print(f\"   Model: {model.__class__.__name__}\")\n",
    "    print(f\"   Dataset: {dataset.name} ({len(dataset.rows)} examples)\")\n",
    "\n",
    "    rows = dataset.rows\n",
    "    fold_size = len(rows) // n_folds\n",
    "    fold_results = []\n",
    "\n",
    "    # Shuffle for randomness (in production, use a fixed seed for reproducibility)\n",
    "    import random\n",
    "\n",
    "    shuffled_rows = rows.copy()\n",
    "    random.shuffle(shuffled_rows)\n",
    "\n",
    "    for fold in range(n_folds):\n",
    "        print(f\"\\n📁 Fold {fold + 1}/{n_folds}\")\n",
    "\n",
    "        # Create train/test split\n",
    "        start_idx = fold * fold_size\n",
    "        end_idx = start_idx + fold_size if fold < n_folds - 1 else len(shuffled_rows)\n",
    "\n",
    "        test_rows = shuffled_rows[start_idx:end_idx]\n",
    "        train_rows = shuffled_rows[:start_idx] + shuffled_rows[end_idx:]\n",
    "\n",
    "        print(f\"   Train: {len(train_rows)} examples\")\n",
    "        print(f\"   Test: {len(test_rows)} examples\")\n",
    "\n",
    "        # Create fold dataset\n",
    "        fold_dataset = Dataset(name=f\"{dataset.name}_fold_{fold+1}\", rows=test_rows)\n",
    "\n",
    "        # Create fold-specific logger with metadata\n",
    "        logger = EvaluationLogger(\n",
    "            model={\n",
    "                \"name\": model.__class__.__name__,\n",
    "                \"cross_validation\": {\n",
    "                    \"fold\": fold + 1,\n",
    "                    \"total_folds\": n_folds,\n",
    "                    \"train_size\": len(train_rows),\n",
    "                    \"test_size\": len(test_rows),\n",
    "                },\n",
    "            },\n",
    "            dataset=f\"{dataset.name}_cv_fold_{fold+1}\",\n",
    "        )\n",
    "\n",
    "        # Evaluate on this fold\n",
    "        fold_scores = []\n",
    "        fold_metrics = {\"completeness\": [], \"accuracy\": [], \"consistency\": []}\n",
    "\n",
    "        for example in test_rows:\n",
    "            output = model.predict(example[\"email\"])\n",
    "\n",
    "            pred = logger.log_prediction(\n",
    "                inputs={\"email\": example[\"email\"]}, output=output.model_dump()\n",
    "            )\n",
    "\n",
    "            # Calculate various metrics\n",
    "            score = 0.0\n",
    "\n",
    "            # Completeness\n",
    "            if output.customer_name and output.customer_name != \"Unknown\":\n",
    "                score += 0.25\n",
    "                fold_metrics[\"completeness\"].append(1.0)\n",
    "            else:\n",
    "                fold_metrics[\"completeness\"].append(0.0)\n",
    "\n",
    "            # Basic accuracy (simplified)\n",
    "            if output.severity in [\"critical\", \"high\", \"medium\", \"low\"]:\n",
    "                score += 0.25\n",
    "                fold_metrics[\"accuracy\"].append(1.0)\n",
    "            else:\n",
    "                fold_metrics[\"accuracy\"].append(0.0)\n",
    "\n",
    "            # Consistency check\n",
    "            if (\n",
    "                output.severity in [\"critical\", \"high\"]\n",
    "                and output.sentiment == \"negative\"\n",
    "            ) or (\n",
    "                output.severity == \"low\" and output.sentiment in [\"positive\", \"neutral\"]\n",
    "            ):\n",
    "                score += 0.25\n",
    "                fold_metrics[\"consistency\"].append(1.0)\n",
    "            else:\n",
    "                fold_metrics[\"consistency\"].append(0.0)\n",
    "\n",
    "            fold_scores.append(score)\n",
    "            pred.log_score(scorer=\"overall_score\", score=score)\n",
    "            pred.finish()\n",
    "\n",
    "        # Calculate fold statistics\n",
    "        fold_mean = statistics.mean(fold_scores)\n",
    "        fold_std = statistics.stdev(fold_scores) if len(fold_scores) > 1 else 0\n",
    "\n",
    "        fold_result = {\n",
    "            \"fold\": fold + 1,\n",
    "            \"mean_score\": fold_mean,\n",
    "            \"std_dev\": fold_std,\n",
    "            \"metrics\": {\n",
    "                metric: statistics.mean(scores) if scores else 0\n",
    "                for metric, scores in fold_metrics.items()\n",
    "            },\n",
    "        }\n",
    "        fold_results.append(fold_result)\n",
    "\n",
    "        # Log fold summary\n",
    "        logger.log_summary(\n",
    "            {\n",
    "                \"fold_number\": fold + 1,\n",
    "                \"mean_score\": fold_mean,\n",
    "                \"std_dev\": fold_std,\n",
    "                \"metrics\": fold_result[\"metrics\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(f\"   Mean Score: {fold_mean:.3f} ± {fold_std:.3f}\")\n",
    "\n",
    "    # Calculate overall cross-validation metrics\n",
    "    all_means = [f[\"mean_score\"] for f in fold_results]\n",
    "    cv_mean = statistics.mean(all_means)\n",
    "    cv_std = statistics.stdev(all_means) if len(all_means) > 1 else 0\n",
    "\n",
    "    # Calculate metric stability across folds\n",
    "    metric_stability = {}\n",
    "    for metric in [\"completeness\", \"accuracy\", \"consistency\"]:\n",
    "        metric_values = [f[\"metrics\"][metric] for f in fold_results]\n",
    "        metric_stability[metric] = {\n",
    "            \"mean\": statistics.mean(metric_values),\n",
    "            \"std\": statistics.stdev(metric_values) if len(metric_values) > 1 else 0,\n",
    "        }\n",
    "\n",
    "    results = {\n",
    "        \"overall_mean\": cv_mean,\n",
    "        \"overall_std\": cv_std,\n",
    "        \"confidence_interval\": (cv_mean - 1.96 * cv_std, cv_mean + 1.96 * cv_std),\n",
    "        \"fold_results\": fold_results,\n",
    "        \"metric_stability\": metric_stability,\n",
    "    }\n",
    "\n",
    "    print(\"\\n📊 Cross-Validation Summary:\")\n",
    "    print(f\"   Overall Score: {cv_mean:.3f} ± {cv_std:.3f}\")\n",
    "    print(\n",
    "        f\"   95% CI: [{results['confidence_interval'][0]:.3f}, {results['confidence_interval'][1]:.3f}]\"\n",
    "    )\n",
    "    print(\"\\n   Metric Stability:\")\n",
    "    for metric, stats in metric_stability.items():\n",
    "        print(f\"     {metric}: {stats['mean']:.3f} ± {stats['std']:.3f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"\\n✅ Cross-validation framework ready!\")\n",
    "# To run: cross_validate_model(model, advanced_dataset, n_folds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f5ce60",
   "metadata": {},
   "source": [
    "## 🏭 Example 5: Production Evaluation Pipeline\n",
    "\n",
    "A complete evaluation pipeline suitable for production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ac5fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"🏭 EXAMPLE 5: Production Evaluation Pipeline\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "class ProductionEvaluationPipeline:\n",
    "    \"\"\"Complete evaluation pipeline for production use.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        models: List[Model],\n",
    "        test_dataset: Dataset,\n",
    "        baseline_model: Optional[Model] = None,\n",
    "    ):\n",
    "        self.models = models\n",
    "        self.test_dataset = test_dataset\n",
    "        self.baseline_model = baseline_model\n",
    "        self.results = {}\n",
    "\n",
    "    @weave.op\n",
    "    def run_comprehensive_evaluation(self) -> dict:\n",
    "        \"\"\"Run full evaluation pipeline.\"\"\"\n",
    "        print(\"\\n🏭 Starting Production Evaluation Pipeline\")\n",
    "        print(f\"   Models: {[m.__class__.__name__ for m in self.models]}\")\n",
    "        print(f\"   Dataset: {self.test_dataset.name}\")\n",
    "        print(\n",
    "            f\"   Baseline: {self.baseline_model.__class__.__name__ if self.baseline_model else 'None'}\"\n",
    "        )\n",
    "\n",
    "        # Phase 1: Individual model evaluation\n",
    "        print(\"\\n📊 Phase 1: Individual Model Evaluation\")\n",
    "        for model in self.models:\n",
    "            print(f\"\\n   Evaluating {model.__class__.__name__}...\")\n",
    "            self.results[model.__class__.__name__] = self._evaluate_single_model(model)\n",
    "\n",
    "        # Phase 2: Comparative analysis\n",
    "        if len(self.models) > 1:\n",
    "            print(\"\\n📊 Phase 2: Comparative Analysis\")\n",
    "            self.results[\"comparison\"] = self._compare_models()\n",
    "\n",
    "        # Phase 3: Baseline comparison\n",
    "        if self.baseline_model:\n",
    "            print(\"\\n📊 Phase 3: Baseline Comparison\")\n",
    "            self.results[\"baseline_comparison\"] = self._compare_to_baseline()\n",
    "\n",
    "        # Phase 4: Production readiness check\n",
    "        print(\"\\n📊 Phase 4: Production Readiness Check\")\n",
    "        self.results[\"production_readiness\"] = self._check_production_readiness()\n",
    "\n",
    "        # Generate final report\n",
    "        report = self._generate_report()\n",
    "\n",
    "        return report\n",
    "\n",
    "    def _evaluate_single_model(self, model: Model) -> dict:\n",
    "        \"\"\"Evaluate a single model comprehensively.\"\"\"\n",
    "        # Create evaluation logger\n",
    "        logger = EvaluationLogger(\n",
    "            model={\n",
    "                \"name\": model.__class__.__name__,\n",
    "                \"evaluation_timestamp\": datetime.now().isoformat(),\n",
    "                \"evaluation_type\": \"production_comprehensive\",\n",
    "            },\n",
    "            dataset=f\"{self.test_dataset.name}_production\",\n",
    "        )\n",
    "\n",
    "        metrics = {\n",
    "            \"accuracy\": [],\n",
    "            \"latency_ms\": [],\n",
    "            \"completeness\": [],\n",
    "            \"business_value\": [],\n",
    "        }\n",
    "\n",
    "        for example in self.test_dataset.rows:\n",
    "            start_time = datetime.now()\n",
    "\n",
    "            try:\n",
    "                output = model.predict(example[\"email\"])\n",
    "                latency = (datetime.now() - start_time).total_seconds() * 1000\n",
    "\n",
    "                pred_logger = logger.log_prediction(\n",
    "                    inputs={\"email\": example[\"email\"]}, output=output.model_dump()\n",
    "                )\n",
    "\n",
    "                # Calculate metrics\n",
    "                accuracy = self._calculate_accuracy(output, example.get(\"expected\", {}))\n",
    "                completeness = self._calculate_completeness(output)\n",
    "                business_value = self._calculate_business_value(output)\n",
    "\n",
    "                metrics[\"accuracy\"].append(accuracy)\n",
    "                metrics[\"latency_ms\"].append(latency)\n",
    "                metrics[\"completeness\"].append(completeness)\n",
    "                metrics[\"business_value\"].append(business_value)\n",
    "\n",
    "                pred_logger.log_score(scorer=\"accuracy\", score=accuracy)\n",
    "                pred_logger.log_score(scorer=\"latency_ms\", score=latency)\n",
    "                pred_logger.log_score(scorer=\"completeness\", score=completeness)\n",
    "                pred_logger.log_score(scorer=\"business_value\", score=business_value)\n",
    "\n",
    "                pred_logger.finish()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"     ❌ Error: {str(e)}\")\n",
    "                metrics[\"accuracy\"].append(0)\n",
    "                metrics[\"latency_ms\"].append(0)\n",
    "                metrics[\"completeness\"].append(0)\n",
    "                metrics[\"business_value\"].append(0)\n",
    "\n",
    "        # Calculate summary statistics\n",
    "        summary = {\n",
    "            metric: {\n",
    "                \"mean\": statistics.mean(values) if values else 0,\n",
    "                \"std\": statistics.stdev(values) if len(values) > 1 else 0,\n",
    "                \"min\": min(values) if values else 0,\n",
    "                \"max\": max(values) if values else 0,\n",
    "            }\n",
    "            for metric, values in metrics.items()\n",
    "        }\n",
    "\n",
    "        logger.log_summary(summary)\n",
    "\n",
    "        return summary\n",
    "\n",
    "    def _calculate_accuracy(self, output, expected):\n",
    "        \"\"\"Calculate accuracy score.\"\"\"\n",
    "        if not expected:\n",
    "            return 0.5  # No ground truth\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for field, expected_value in expected.items():\n",
    "            if hasattr(output, field):\n",
    "                actual = getattr(output, field)\n",
    "                if str(actual).lower() == str(expected_value).lower():\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "        return correct / total if total > 0 else 0\n",
    "\n",
    "    def _calculate_completeness(self, output):\n",
    "        \"\"\"Calculate completeness score.\"\"\"\n",
    "        required_fields = [\"customer_name\", \"product\", \"issue\", \"severity\"]\n",
    "        score = 0\n",
    "\n",
    "        for field in required_fields:\n",
    "            value = getattr(output, field, None)\n",
    "            if value and value != \"Unknown\":\n",
    "                score += 0.25\n",
    "\n",
    "        return score\n",
    "\n",
    "    def _calculate_business_value(self, output):\n",
    "        \"\"\"Calculate business value score.\"\"\"\n",
    "        value = 0\n",
    "\n",
    "        # Proper severity identification\n",
    "        if output.severity in [\"critical\", \"high\", \"medium\", \"low\"]:\n",
    "            value += 0.3\n",
    "\n",
    "        # Actionable categorization\n",
    "        if output.issue_category in [\"technical\", \"billing\", \"feature_request\"]:\n",
    "            value += 0.3\n",
    "\n",
    "        # Customer identification\n",
    "        if output.customer_name and output.customer_name != \"Unknown\":\n",
    "            value += 0.2\n",
    "\n",
    "        # Product identification\n",
    "        if output.product and any(char.isdigit() for char in output.product):\n",
    "            value += 0.2\n",
    "\n",
    "        return value\n",
    "\n",
    "    def _compare_models(self):\n",
    "        \"\"\"Compare all models.\"\"\"\n",
    "        comparison = {}\n",
    "\n",
    "        # Rank models by each metric\n",
    "        for metric in [\"accuracy\", \"completeness\", \"business_value\"]:\n",
    "            scores = [\n",
    "                (name, results[metric][\"mean\"])\n",
    "                for name, results in self.results.items()\n",
    "                if isinstance(results, dict) and metric in results\n",
    "            ]\n",
    "            scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            comparison[f\"{metric}_ranking\"] = scores\n",
    "\n",
    "        return comparison\n",
    "\n",
    "    def _compare_to_baseline(self):\n",
    "        \"\"\"Compare models to baseline.\"\"\"\n",
    "        if not self.baseline_model:\n",
    "            return {}\n",
    "\n",
    "        baseline_results = self._evaluate_single_model(self.baseline_model)\n",
    "\n",
    "        comparison = {}\n",
    "        for model_name, results in self.results.items():\n",
    "            if isinstance(results, dict) and \"accuracy\" in results:\n",
    "                comparison[model_name] = {\n",
    "                    \"accuracy_improvement\": results[\"accuracy\"][\"mean\"]\n",
    "                    - baseline_results[\"accuracy\"][\"mean\"],\n",
    "                    \"latency_change\": results[\"latency_ms\"][\"mean\"]\n",
    "                    - baseline_results[\"latency_ms\"][\"mean\"],\n",
    "                    \"business_value_improvement\": results[\"business_value\"][\"mean\"]\n",
    "                    - baseline_results[\"business_value\"][\"mean\"],\n",
    "                }\n",
    "\n",
    "        return comparison\n",
    "\n",
    "    def _check_production_readiness(self):\n",
    "        \"\"\"Check if any model meets production criteria.\"\"\"\n",
    "        criteria = {\n",
    "            \"min_accuracy\": 0.8,\n",
    "            \"max_latency_ms\": 1000,\n",
    "            \"min_completeness\": 0.9,\n",
    "            \"min_business_value\": 0.7,\n",
    "        }\n",
    "\n",
    "        readiness = {}\n",
    "\n",
    "        for model_name, results in self.results.items():\n",
    "            if isinstance(results, dict) and \"accuracy\" in results:\n",
    "                readiness[model_name] = {\n",
    "                    \"accuracy_ok\": results[\"accuracy\"][\"mean\"]\n",
    "                    >= criteria[\"min_accuracy\"],\n",
    "                    \"latency_ok\": results[\"latency_ms\"][\"mean\"]\n",
    "                    <= criteria[\"max_latency_ms\"],\n",
    "                    \"completeness_ok\": results[\"completeness\"][\"mean\"]\n",
    "                    >= criteria[\"min_completeness\"],\n",
    "                    \"business_value_ok\": results[\"business_value\"][\"mean\"]\n",
    "                    >= criteria[\"min_business_value\"],\n",
    "                }\n",
    "                readiness[model_name][\"production_ready\"] = all(\n",
    "                    readiness[model_name].values()\n",
    "                )\n",
    "\n",
    "        return readiness\n",
    "\n",
    "    def _generate_report(self):\n",
    "        \"\"\"Generate comprehensive evaluation report.\"\"\"\n",
    "        report = {\n",
    "            \"evaluation_date\": datetime.now().isoformat(),\n",
    "            \"dataset\": self.test_dataset.name,\n",
    "            \"models_evaluated\": [m.__class__.__name__ for m in self.models],\n",
    "            \"results\": self.results,\n",
    "            \"recommendations\": [],\n",
    "        }\n",
    "\n",
    "        # Add recommendations\n",
    "        prod_ready = self.results.get(\"production_readiness\", {})\n",
    "        for model_name, readiness in prod_ready.items():\n",
    "            if readiness.get(\"production_ready\"):\n",
    "                report[\"recommendations\"].append(f\"✅ {model_name} is production ready\")\n",
    "            else:\n",
    "                issues = [\n",
    "                    k.replace(\"_ok\", \"\")\n",
    "                    for k, v in readiness.items()\n",
    "                    if not v and k != \"production_ready\"\n",
    "                ]\n",
    "                report[\"recommendations\"].append(\n",
    "                    f\"❌ {model_name} needs improvement in: {', '.join(issues)}\"\n",
    "                )\n",
    "\n",
    "        return report\n",
    "\n",
    "\n",
    "# Create pipeline instance\n",
    "pipeline = ProductionEvaluationPipeline(\n",
    "    models=[model, model_b], test_dataset=advanced_dataset, baseline_model=model\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Production evaluation pipeline ready!\")\n",
    "print(\"\\n💡 To run the complete pipeline:\")\n",
    "print(\"   report = pipeline.run_comprehensive_evaluation()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5177a2",
   "metadata": {},
   "source": [
    "## 🎉 Summary & Best Practices\n",
    "\n",
    "You've learned advanced evaluation patterns for production use:\n",
    "\n",
    "### 🔑 Key Takeaways\n",
    "\n",
    "1. **Custom Scorers** - Build domain-specific metrics that matter to your business\n",
    "2. **Multi-Stage Evaluation** - Break complex evaluations into logical stages\n",
    "3. **Statistical Testing** - Use A/B testing with statistical significance\n",
    "4. **Cross-Validation** - Ensure robust evaluation with k-fold validation\n",
    "5. **Production Pipelines** - Comprehensive evaluation before deployment\n",
    "\n",
    "### 📋 Best Practices\n",
    "\n",
    "- **Use Rich Metadata**: Leverage EvaluationLogger's dictionary support for models\n",
    "- **Track Everything**: Log intermediate results for debugging\n",
    "- **Statistical Rigor**: Don't just compare means - check significance\n",
    "- **Business Metrics**: Align evaluation metrics with business outcomes\n",
    "- **Automate**: Build pipelines that can run in CI/CD\n",
    "\n",
    "### 🚀 Next Steps\n",
    "\n",
    "1. Apply these patterns to your own models\n",
    "2. Create custom scorers for your domain\n",
    "3. Build automated evaluation pipelines\n",
    "4. Share your results with your team using Weave's UI\n",
    "\n",
    "Happy evaluating! 🐝"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
