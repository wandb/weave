{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2322c79a",
   "metadata": {},
   "source": [
    "# Part 2: Evaluations with Weave\n",
    "\n",
    "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "\n",
    "Learn how to systematically evaluate LLM applications using Weave's evaluation framework.\n",
    "\n",
    "**In this section:**\n",
    "- üìä **Dataset Creation**: Build evaluation datasets with challenging examples\n",
    "- üéØ **Custom Scorers**: Write scoring functions to measure performance\n",
    "- üèÉ **Running Evaluations**: Execute evaluations and analyze results\n",
    "- üìà **Pre-built Scorers**: Use Weave's built-in evaluation metrics\n",
    "- üîÑ **Model Comparison**: Compare different models and configurations\n",
    "- üìù **EvaluationLogger**: Flexible evaluation logging for custom workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5942540",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install dependencies and configure API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73db0716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install wandb weave openai pydantic nest_asyncio 'weave[scorers]' 'pydantic[email]' -qqq\n",
    "\n",
    "import asyncio\n",
    "import os\n",
    "from datetime import datetime\n",
    "from getpass import getpass\n",
    "from typing import Any\n",
    "\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import weave\n",
    "from weave import Dataset, Evaluation, EvaluationLogger, Model\n",
    "\n",
    "# Setup API keys\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    print(\"Get your OpenAI API key: https://platform.openai.com/api-keys\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "# Initialize Weave\n",
    "weave_client = weave.init(\"weave-workshop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bbd0fe",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## üìä Part 2: Building Evaluations\n",
    "\n",
    "Let's evaluate our email analyzer using Weave's evaluation framework with a challenging dataset.\n",
    "\n",
    "**Understanding Weave's Evaluation Data Model:**\n",
    "1. An **evaluation** is the pairing of a dataset and a set of scorers\n",
    "2. An **evaluation run** is the result of running an evaluation against a specific model\n",
    "3. Within an evaluation run, there are **predict_and_score** blocks for each dataset row\n",
    "4. Scores are stored in the predict_and_score output and on the prediction call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d308a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our data structure\n",
    "class CustomerEmail(BaseModel):\n",
    "    customer_name: str\n",
    "    product: str\n",
    "    issue: str\n",
    "    sentiment: str = Field(description=\"positive, neutral, or negative\")\n",
    "\n",
    "\n",
    "# üéØ Track functions with @weave.op\n",
    "@weave.op\n",
    "def analyze_customer_email(email: str) -> CustomerEmail:\n",
    "    \"\"\"Analyze a customer support email and extract key information.\"\"\"\n",
    "    client = OpenAI()\n",
    "\n",
    "    # üî• OpenAI calls are automatically traced by Weave!\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",  # Using mini model for cost efficiency\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Extract customer name, product, issue, and sentiment.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": email,\n",
    "            },\n",
    "        ],\n",
    "        response_format=CustomerEmail,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.parsed\n",
    "\n",
    "\n",
    "# Create a challenging evaluation dataset with tricky examples\n",
    "eval_examples = [\n",
    "    # Easy examples (even basic models should get these)\n",
    "    {\n",
    "        \"email\": \"Hi Support, I'm John Smith and my DataProcessor-Pro v2.5 isn't working correctly. The data export feature is producing corrupted files. Very frustrated!\",\n",
    "        \"expected_name\": \"John Smith\",\n",
    "        \"expected_product\": \"DataProcessor-Pro v2.5\",\n",
    "        \"expected_sentiment\": \"negative\",\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"Hello, this is Dr. Alice Chen. I wanted to say that your AI-Assistant tool is fantastic! Everything works perfectly. Thank you!\",\n",
    "        \"expected_name\": \"Dr. Alice Chen\",\n",
    "        \"expected_product\": \"AI-Assistant\",\n",
    "        \"expected_sentiment\": \"positive\",\n",
    "    },\n",
    "    # Medium difficulty - ambiguous names/products\n",
    "    {\n",
    "        \"email\": \"Jane from accounting here. The CloudSync Plus works fine but Enterprise Sync Module has delays. Not critical.\",\n",
    "        \"expected_name\": \"Jane\",\n",
    "        \"expected_product\": \"Enterprise Sync Module\",  # NOT CloudSync Plus!\n",
    "        \"expected_sentiment\": \"neutral\",\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"My SmartHub won't connect to anything. Super annoying. - Bob Wilson\\nSenior Manager\\nTech Solutions Inc\",\n",
    "        \"expected_name\": \"Bob Wilson\",\n",
    "        \"expected_product\": \"SmartHub\",\n",
    "        \"expected_sentiment\": \"negative\",\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"Spoke with Sarah about the issue. Still having problems with WorkflowMax crashing. Mike O'Brien, CEO\",\n",
    "        \"expected_name\": \"Mike O'Brien\",  # NOT Sarah\n",
    "        \"expected_product\": \"WorkflowMax\",\n",
    "        \"expected_sentiment\": \"negative\",\n",
    "    },\n",
    "    # Hard examples - names in unusual places\n",
    "    {\n",
    "        \"email\": \"The new update broke everything! Nothing works anymore on the ProSuite 3000. Call me - signed, frustrated customer Zhang Wei\",\n",
    "        \"expected_name\": \"Zhang Wei\",\n",
    "        \"expected_product\": \"ProSuite 3000\",\n",
    "        \"expected_sentiment\": \"negative\",\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"RE: Ticket #1234\\nCustomer Mar√≠a Garc√≠a called about CloudVault. She says thanks for fixing the sync issue! Works great now.\",\n",
    "        \"expected_name\": \"Mar√≠a Garc√≠a\",\n",
    "        \"expected_product\": \"CloudVault\",\n",
    "        \"expected_sentiment\": \"positive\",\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"My assistant Jennifer will send the logs. The actual problem is with DataMiner Pro, not the viewer. -Raj (Dr. Rajesh Patel)\",\n",
    "        \"expected_name\": \"Dr. Rajesh Patel\",  # NOT Jennifer, full name from signature\n",
    "        \"expected_product\": \"DataMiner Pro\",  # NOT the viewer\n",
    "        \"expected_sentiment\": \"neutral\",  # Matter-of-fact, not emotional\n",
    "    },\n",
    "    # Very hard - misleading information\n",
    "    {\n",
    "        \"email\": \"Johnson recommended your software. Smith from our team loves CloudSync. But I'm having issues with it. Brown, James Brown.\",\n",
    "        \"expected_name\": \"James Brown\",  # NOT Johnson or Smith\n",
    "        \"expected_product\": \"CloudSync\",\n",
    "        \"expected_sentiment\": \"negative\",  # Having issues despite others liking it\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"Great product! Though the InvoiceGen module crashes sometimes. Still recommend it! Anna from Stockholm\",\n",
    "        \"expected_name\": \"Anna\",\n",
    "        \"expected_product\": \"InvoiceGen module\",\n",
    "        \"expected_sentiment\": \"positive\",  # Overall positive despite crashes\n",
    "    },\n",
    "    # Additional challenging examples\n",
    "    {\n",
    "        \"email\": \"Update on case by Thompson: Lee's WorkStation Pro still showing error 0x80004005. Previous tech couldn't resolve.\",\n",
    "        \"expected_name\": \"Lee\",  # NOT Thompson\n",
    "        \"expected_product\": \"WorkStation Pro\",\n",
    "        \"expected_sentiment\": \"negative\",\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"Hi, chatted with your colleague Emma (super helpful!). Anyway, ReportBuilder works ok but takes forever. ‚ÄîSamantha Park, CTO\",\n",
    "        \"expected_name\": \"Samantha Park\",  # NOT Emma\n",
    "        \"expected_product\": \"ReportBuilder\",\n",
    "        \"expected_sentiment\": \"neutral\",  # \"works ok\" but slow\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"FYI - Customer called: Pierre-Alexandre Dubois mentioned the API-Gateway is fantastic, just needs better docs. Direct quote.\",\n",
    "        \"expected_name\": \"Pierre-Alexandre Dubois\",\n",
    "        \"expected_product\": \"API-Gateway\",\n",
    "        \"expected_sentiment\": \"positive\",  # \"fantastic\" outweighs doc complaint\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"Worst experience ever with tech support! Though I admit ProductX works well. O'Sullivan here (Francis).\",\n",
    "        \"expected_name\": \"Francis O'Sullivan\",  # Name split across sentence\n",
    "        \"expected_product\": \"ProductX\",\n",
    "        \"expected_sentiment\": \"negative\",  # Support experience outweighs product working\n",
    "    },\n",
    "    # Trick examples - products that sound like names\n",
    "    {\n",
    "        \"email\": \"Maxwell keeps crashing! This software is terrible. Signed, angry user Li Chen\",\n",
    "        \"expected_name\": \"Li Chen\",\n",
    "        \"expected_product\": \"Maxwell\",  # Maxwell is the product, not a person\n",
    "        \"expected_sentiment\": \"negative\",\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"Please tell Gordon that the Morgan Analytics Suite works perfectly now. Thanks! - Yuki Tanaka\",\n",
    "        \"expected_name\": \"Yuki Tanaka\",  # NOT Gordon\n",
    "        \"expected_product\": \"Morgan Analytics Suite\",  # Morgan is part of product name\n",
    "        \"expected_sentiment\": \"positive\",\n",
    "    },\n",
    "    # Edge cases and complex scenarios\n",
    "    {\n",
    "        \"email\": \"DataFlow Pro is exactly what I expected from your company. Classic experience. Jo√£o Silva, Product Manager\",\n",
    "        \"expected_name\": \"Jo√£o Silva\",\n",
    "        \"expected_product\": \"DataFlow Pro\",\n",
    "        \"expected_sentiment\": \"negative\",  # Sarcastic - \"expected\" and \"classic\" imply typically bad\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"The ChromaEdit tool works... I guess. Does what it says. Whatever. -Kim\",\n",
    "        \"expected_name\": \"Kim\",\n",
    "        \"expected_product\": \"ChromaEdit tool\",\n",
    "        \"expected_sentiment\": \"neutral\",  # Apathetic, not negative or positive\n",
    "    },\n",
    "    # Multiple products mentioned\n",
    "    {\n",
    "        \"email\": \"Upgraded from TaskMaster to ProjectPro. Having issues with ProjectPro's gantt charts. Anne-Marie Rousseau\",\n",
    "        \"expected_name\": \"Anne-Marie Rousseau\",\n",
    "        \"expected_product\": \"ProjectPro\",  # The one with issues, not TaskMaster\n",
    "        \"expected_sentiment\": \"negative\",\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"Hi! love your VideoEdit, PhotoEdit, and AudioEdit apps! Especially AudioEdit! Muhammad here :)\",\n",
    "        \"expected_name\": \"Muhammad\",\n",
    "        \"expected_product\": \"AudioEdit\",  # The one especially mentioned\n",
    "        \"expected_sentiment\": \"positive\",\n",
    "    },\n",
    "    # Edge cases\n",
    "    {\n",
    "        \"email\": \"Yo! Sup? Ur SystemMonitor thing is broke af. fix it asap!!!! - xXx_Dmitri_xXx\",\n",
    "        \"expected_name\": \"Dmitri\",  # Extract from gamertag\n",
    "        \"expected_product\": \"SystemMonitor\",\n",
    "        \"expected_sentiment\": \"negative\",\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"¬°Hola! Carlos M√©ndez aqu√≠. Su programa FinanceTracker es excelente pero muy caro. Gracias.\",\n",
    "        \"expected_name\": \"Carlos M√©ndez\",\n",
    "        \"expected_product\": \"FinanceTracker\",\n",
    "        \"expected_sentiment\": \"neutral\",  # Good but expensive = neutral\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"Re: Jackson's complaint\\n\\nI disagree with Jackson. The Scheduler App works fine for me.\\n\\nBest,\\nPriya Sharma\\nHead of IT\",\n",
    "        \"expected_name\": \"Priya Sharma\",  # NOT Jackson\n",
    "        \"expected_product\": \"Scheduler App\",\n",
    "        \"expected_sentiment\": \"positive\",  # Disagrees with complaint\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"This is regarding the issue with CloudBackup Pro v3.2.1 that Jennifer Chen reported. I'm her manager, David Kim, following up.\",\n",
    "        \"expected_name\": \"David Kim\",  # The sender, not Jennifer\n",
    "        \"expected_product\": \"CloudBackup Pro v3.2.1\",\n",
    "        \"expected_sentiment\": \"negative\",  # Following up on an issue\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"üò°üò°üò° InventoryMaster deleted everything!!! üò≠üò≠üò≠ - call me back NOW! //Singh\",\n",
    "        \"expected_name\": \"Singh\",\n",
    "        \"expected_product\": \"InventoryMaster\",\n",
    "        \"expected_sentiment\": \"negative\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create a Weave Dataset\n",
    "support_dataset = Dataset(name=\"support_emails\", rows=eval_examples)\n",
    "\n",
    "\n",
    "# üéØ Define scoring functions\n",
    "@weave.op\n",
    "def name_accuracy(expected_name: str, output: CustomerEmail) -> dict[str, Any]:\n",
    "    \"\"\"Check if the extracted name matches.\"\"\"\n",
    "    is_correct = expected_name.lower() == output.customer_name.lower()\n",
    "    return {\"correct\": is_correct, \"score\": 1.0 if is_correct else 0.0}\n",
    "\n",
    "\n",
    "@weave.op\n",
    "def sentiment_accuracy(\n",
    "    expected_sentiment: str, output: CustomerEmail\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"Check if the sentiment analysis is correct.\"\"\"\n",
    "    is_correct = expected_sentiment.lower() == output.sentiment.lower()\n",
    "    return {\"correct\": is_correct, \"score\": 1.0 if is_correct else 0.0}\n",
    "\n",
    "\n",
    "@weave.op\n",
    "def extraction_quality(email: str, output: CustomerEmail) -> dict[str, Any]:\n",
    "    \"\"\"Evaluate overall extraction quality.\"\"\"\n",
    "    score = 0.0\n",
    "    feedback = []\n",
    "\n",
    "    # Check if all fields are extracted\n",
    "    if output.customer_name and output.customer_name != \"Unknown\":\n",
    "        score += 0.33\n",
    "    else:\n",
    "        feedback.append(\"Missing customer name\")\n",
    "\n",
    "    if output.product and output.product != \"Unknown\":\n",
    "        score += 0.33\n",
    "    else:\n",
    "        feedback.append(\"Missing product\")\n",
    "\n",
    "    if output.issue and len(output.issue) > 10:\n",
    "        score += 0.34\n",
    "    else:\n",
    "        feedback.append(\"Issue description too short\")\n",
    "\n",
    "    return {\n",
    "        \"score\": score,\n",
    "        \"feedback\": \"; \".join(feedback)\n",
    "        if feedback\n",
    "        else \"All fields extracted successfully\",\n",
    "    }\n",
    "\n",
    "\n",
    "# üöÄ Run the evaluation\n",
    "evaluation = Evaluation(\n",
    "    dataset=support_dataset,\n",
    "    scorers=[name_accuracy, sentiment_accuracy, extraction_quality],\n",
    "    trials=3,  # Run each example 3 times to check consistency\n",
    ")\n",
    "\n",
    "# For notebooks, use nest_asyncio to handle async properly\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "eval_results = asyncio.run(evaluation.evaluate(analyze_customer_email))\n",
    "print(\"‚úÖ Evaluation complete! Check the Weave UI for detailed results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f5cd82",
   "metadata": {},
   "source": [
    "### üéØ Part 2.1: Pre-built Scorers\n",
    "\n",
    "Weave provides many pre-built scorers for common evaluation tasks.\n",
    "No need to reinvent the wheel for standard metrics!\n",
    "\n",
    "**Note**: To use pre-built scorers, install with: `pip install weave[scorers]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7daf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pre-built scorers\n",
    "from weave.scorers import (\n",
    "    EmbeddingSimilarityScorer,\n",
    "    PydanticScorer,\n",
    "    ValidJSONScorer,\n",
    ")\n",
    "\n",
    "# Example 1: ValidJSONScorer - Check if output is valid JSON\n",
    "json_scorer = ValidJSONScorer()\n",
    "\n",
    "print(\"üéØ Example 1: ValidJSONScorer\")\n",
    "# Test with valid JSON\n",
    "valid_json = '{\"name\": \"John Doe\", \"age\": 30, \"email\": \"john@example.com\"}'\n",
    "json_result = json_scorer.score(output=valid_json)\n",
    "print(f\"  Valid JSON: {json_result['json_valid']}\")\n",
    "\n",
    "# Test with invalid JSON\n",
    "invalid_json = (\n",
    "    '{\"name\": \"Jane Doe\", \"age\": 25, \"email\"'  # Missing closing quote and brace\n",
    ")\n",
    "invalid_result = json_scorer.score(output=invalid_json)\n",
    "print(f\"  Invalid JSON: {invalid_result['json_valid']}\")\n",
    "\n",
    "# Example 2: PydanticScorer - Validate against a schema\n",
    "from pydantic import EmailStr\n",
    "\n",
    "\n",
    "class UserData(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    email: EmailStr\n",
    "\n",
    "\n",
    "# Use PydanticScorer with our schema\n",
    "pydantic_scorer = PydanticScorer(model=UserData)\n",
    "\n",
    "print(\"\\nüéØ Example 2: PydanticScorer\")\n",
    "# Test with valid data\n",
    "valid_data = '{\"name\": \"Alice Smith\", \"age\": 28, \"email\": \"alice@example.com\"}'\n",
    "pydantic_result = pydantic_scorer.score(output=valid_data)\n",
    "print(f\"  Valid schema: {pydantic_result['valid_pydantic']}\")\n",
    "\n",
    "# Test with invalid data\n",
    "invalid_data = '{\"name\": \"Bob\", \"age\": \"twenty-five\", \"email\": \"not-an-email\"}'\n",
    "invalid_pydantic_result = pydantic_scorer.score(output=invalid_data)\n",
    "print(f\"  Invalid schema: {invalid_pydantic_result['valid_pydantic']}\")\n",
    "\n",
    "# Example 3: EmbeddingSimilarityScorer - Semantic similarity\n",
    "# Use EmbeddingSimilarityScorer (requires OpenAI API key)\n",
    "similarity_scorer = EmbeddingSimilarityScorer(\n",
    "    model_id=\"openai/text-embedding-3-small\",\n",
    "    threshold=0.7,  # Cosine similarity threshold\n",
    ")\n",
    "\n",
    "print(\"\\nüéØ Example 3: EmbeddingSimilarityScorer\")\n",
    "# Test semantic similarity between two similar phrases\n",
    "output = \"What are the weather conditions today?\"\n",
    "target = \"How is the weather right now?\"\n",
    "\n",
    "similarity_result = asyncio.run(similarity_scorer.score(output=output, target=target))\n",
    "print(f\"  Similarity score: {similarity_result['similarity_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d9c476",
   "metadata": {},
   "source": [
    "### üìù Part 2.2: Pairwise Evaluation\n",
    "\n",
    "Compare outputs from two models by ranking them relative to each other.\n",
    "This is particularly useful for subjective tasks where absolute scoring is difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe9a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from weave.flow.model import ApplyModelError, apply_model_async\n",
    "\n",
    "\n",
    "# Create two different email analysis models for comparison\n",
    "class BasicEmailModel(Model):\n",
    "    \"\"\"A basic email analyzer with simple prompts.\"\"\"\n",
    "\n",
    "    @weave.op\n",
    "    def predict(self, email: str) -> CustomerEmail:\n",
    "        client = OpenAI()\n",
    "        response = client.beta.chat.completions.parse(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Extract customer info from email.\"},\n",
    "                {\"role\": \"user\", \"content\": email},\n",
    "            ],\n",
    "            response_format=CustomerEmail,\n",
    "            temperature=0.7,  # Higher temperature for more variation\n",
    "        )\n",
    "        return response.choices[0].message.parsed\n",
    "\n",
    "\n",
    "class AdvancedEmailModel(Model):\n",
    "    \"\"\"An advanced email analyzer with detailed prompts.\"\"\"\n",
    "\n",
    "    @weave.op\n",
    "    def predict(self, email: str) -> CustomerEmail:\n",
    "        client = OpenAI()\n",
    "        response = client.beta.chat.completions.parse(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"\"\"You are an expert customer support analyst. Extract information carefully:\n",
    "                    - Customer name: The person WRITING the email (check signatures)\n",
    "                    - Product: The specific product with issues\n",
    "                    - Issue: Brief description of the problem\n",
    "                    - Sentiment: Overall emotional tone\"\"\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": email},\n",
    "            ],\n",
    "            response_format=CustomerEmail,\n",
    "            temperature=0.1,  # Lower temperature for consistency\n",
    "        )\n",
    "        return response.choices[0].message.parsed\n",
    "\n",
    "\n",
    "class EmailPreferenceScorer(weave.Scorer):\n",
    "    \"\"\"Compare two email analysis models and determine which performs better.\"\"\"\n",
    "\n",
    "    other_model: Model\n",
    "\n",
    "    @weave.op\n",
    "    async def _get_other_model_output(self, example: dict) -> Any:\n",
    "        \"\"\"Get output from the comparison model.\"\"\"\n",
    "        try:\n",
    "            other_model_result = await apply_model_async(\n",
    "                self.other_model,\n",
    "                example,\n",
    "                None,\n",
    "            )\n",
    "\n",
    "            if isinstance(other_model_result, ApplyModelError):\n",
    "                return None\n",
    "\n",
    "            return other_model_result.model_output\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    @weave.op\n",
    "    async def score(\n",
    "        self,\n",
    "        output: CustomerEmail,\n",
    "        email: str,\n",
    "        expected_name: str,\n",
    "        expected_sentiment: str,\n",
    "    ) -> dict:\n",
    "        \"\"\"Compare primary model output with other model output.\"\"\"\n",
    "        other_output = await self._get_other_model_output({\"email\": email})\n",
    "\n",
    "        if other_output is None:\n",
    "            return {\n",
    "                \"primary_is_better\": False,\n",
    "                \"reason\": \"Comparison model failed\",\n",
    "                \"primary_score\": 0,\n",
    "                \"other_score\": 0,\n",
    "            }\n",
    "\n",
    "        # Score both models on accuracy\n",
    "        primary_score = 0\n",
    "        other_score = 0\n",
    "\n",
    "        # Check name accuracy\n",
    "        if output.customer_name.lower() == expected_name.lower():\n",
    "            primary_score += 1\n",
    "        if other_output.customer_name.lower() == expected_name.lower():\n",
    "            other_score += 1\n",
    "\n",
    "        # Check sentiment accuracy\n",
    "        if output.sentiment.lower() == expected_sentiment.lower():\n",
    "            primary_score += 1\n",
    "        if other_output.sentiment.lower() == expected_sentiment.lower():\n",
    "            other_score += 1\n",
    "\n",
    "        primary_is_better = primary_score > other_score\n",
    "\n",
    "        if primary_score == other_score:\n",
    "            reason = f\"Tie: Both models scored {primary_score}/2\"\n",
    "        else:\n",
    "            winner = \"Primary\" if primary_is_better else \"Other\"\n",
    "            reason = f\"{winner} model more accurate ({primary_score} vs {other_score})\"\n",
    "\n",
    "        return {\n",
    "            \"primary_is_better\": primary_is_better,\n",
    "            \"reason\": reason,\n",
    "            \"primary_score\": primary_score,\n",
    "            \"other_score\": other_score,\n",
    "        }\n",
    "\n",
    "\n",
    "# Create test dataset for pairwise comparison\n",
    "pairwise_examples = [\n",
    "    {\n",
    "        \"email\": \"Hi, I'm Sarah Johnson and my ProWidget 3000 is broken. Very frustrated!\",\n",
    "        \"expected_name\": \"Sarah Johnson\",\n",
    "        \"expected_sentiment\": \"negative\",\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"Thanks for the help! The DataSync tool works perfectly now. - Mike Chen\",\n",
    "        \"expected_name\": \"Mike Chen\",\n",
    "        \"expected_sentiment\": \"positive\",\n",
    "    },\n",
    "    {\n",
    "        \"email\": \"My assistant will call about the CloudVault issue. Regards, Dr. Patel\",\n",
    "        \"expected_name\": \"Dr. Patel\",\n",
    "        \"expected_sentiment\": \"neutral\",\n",
    "    },\n",
    "]\n",
    "\n",
    "pairwise_dataset = Dataset(name=\"pairwise_comparison\", rows=pairwise_examples)\n",
    "\n",
    "# Set up models and scorer\n",
    "basic_model = BasicEmailModel()\n",
    "advanced_model = AdvancedEmailModel()\n",
    "\n",
    "# Create preference scorer that compares basic model (primary) vs advanced model (other)\n",
    "preference_scorer = EmailPreferenceScorer(other_model=advanced_model)\n",
    "\n",
    "# Run pairwise evaluation\n",
    "pairwise_evaluation = Evaluation(\n",
    "    name=\"email_model_pairwise\", dataset=pairwise_dataset, scorers=[preference_scorer]\n",
    ")\n",
    "\n",
    "print(\"ü•ä Running pairwise evaluation: Basic vs Advanced model...\")\n",
    "pairwise_results = asyncio.run(pairwise_evaluation.evaluate(basic_model))\n",
    "print(\"‚úÖ Pairwise evaluation complete! Check Weave UI for detailed comparisons.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a231ff",
   "metadata": {},
   "source": [
    "### üìù Part 2.3: EvaluationLogger\n",
    "\n",
    "The `EvaluationLogger` provides flexible evaluation logging for custom workflows.\n",
    "This is perfect when you don't have all your data upfront or want more control.\n",
    "\n",
    "**Important**: Since EvaluationLogger doesn't use Model/Dataset objects, the `model`\n",
    "and `dataset` parameters are crucial for identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3df7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation logger with rich metadata\n",
    "# Model can use dictionaries for richer identification (recommended!)\n",
    "eval_logger = EvaluationLogger(\n",
    "    model={\n",
    "        \"name\": \"email_analyzer\",\n",
    "        \"version\": \"v1.2\",\n",
    "        \"llm\": \"gpt-3.5-turbo\",\n",
    "        \"temperature\": 0.7,\n",
    "        \"prompt_version\": \"2024-01\",\n",
    "    },\n",
    "    dataset=\"support_emails_2024Q1\",  # Dataset must be string\n",
    ")\n",
    "\n",
    "print(\"üìä Using EvaluationLogger with rich metadata...\")\n",
    "\n",
    "# Process examples with custom logging - more control than standard Evaluation\n",
    "for i, example in enumerate(eval_examples[:3]):  # First 3 for demo\n",
    "    try:\n",
    "        output = analyze_customer_email(example[\"email\"])\n",
    "\n",
    "        # Log the prediction\n",
    "        pred_logger = eval_logger.log_prediction(\n",
    "            inputs={\"email\": example[\"email\"]}, output=output.model_dump()\n",
    "        )\n",
    "\n",
    "        # Log multiple scores for this prediction\n",
    "        # Check name accuracy\n",
    "        name_match = example[\"expected_name\"].lower() == output.customer_name.lower()\n",
    "        pred_logger.log_score(scorer=\"name_accuracy\", score=1.0 if name_match else 0.0)\n",
    "\n",
    "        # Check sentiment\n",
    "        sentiment_match = example[\"expected_sentiment\"] == output.sentiment\n",
    "        pred_logger.log_score(\n",
    "            scorer=\"sentiment_accuracy\", score=1.0 if sentiment_match else 0.0\n",
    "        )\n",
    "\n",
    "        # Custom business logic score\n",
    "        if \"urgent\" in example[\"email\"].lower() and output.sentiment != \"negative\":\n",
    "            pred_logger.log_score(\n",
    "                scorer=\"urgency_detection\",\n",
    "                score=0.0,  # Failed to detect urgency\n",
    "            )\n",
    "        else:\n",
    "            pred_logger.log_score(scorer=\"urgency_detection\", score=1.0)\n",
    "\n",
    "        # Always finish logging for each prediction\n",
    "        pred_logger.finish()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing example {i+1}: {e}\")\n",
    "        # You can still log failed predictions\n",
    "        pred_logger = eval_logger.log_prediction(\n",
    "            inputs={\"email\": example[\"email\"]}, output={\"error\": str(e)}\n",
    "        )\n",
    "        pred_logger.log_score(scorer=\"success\", score=0.0)\n",
    "        pred_logger.finish()\n",
    "\n",
    "# Log summary statistics\n",
    "eval_logger.log_summary(\n",
    "    {\n",
    "        \"total_examples\": 3,\n",
    "        \"evaluation_type\": \"manual\",\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"notes\": \"Workshop demo with rich metadata\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úÖ EvaluationLogger demo complete! Check the Weave UI.\")\n",
    "print(\"üí° Tip: The rich metadata makes it easy to filter and compare evaluations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937dfa40",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### üèÜ Part 2.4: Model Comparison\n",
    "\n",
    "Compare different approaches using Weave's Model class with varying quality levels.\n",
    "We'll create models with different quality to see clear differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12886f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different model variants\n",
    "class EmailAnalyzerModel(Model):\n",
    "    \"\"\"Base model for email analysis with configurable parameters.\"\"\"\n",
    "\n",
    "    label: str = \"email_analyzer\"\n",
    "    model_name: str = \"gpt-4o-mini\"\n",
    "    temperature: float = 0.1\n",
    "    system_prompt: str = \"You are a customer support analyst.\"\n",
    "\n",
    "    @weave.op\n",
    "    def predict(self, email: str) -> CustomerEmail:\n",
    "        \"\"\"Analyze email with configurable parameters.\"\"\"\n",
    "        client = OpenAI()\n",
    "\n",
    "        response = client.beta.chat.completions.parse(\n",
    "            model=self.model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"Analyze this email:\\n{email}\"},\n",
    "            ],\n",
    "            response_format=CustomerEmail,\n",
    "            temperature=self.temperature,\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.parsed\n",
    "\n",
    "\n",
    "# Create model variants with different quality levels\n",
    "basic_model = EmailAnalyzerModel(\n",
    "    label=\"basic_analyzer\",\n",
    "    system_prompt=\"Extract customer name, product name, issue, and sentiment from email.\",  # Too simple - no guidance\n",
    "    temperature=0.95,  # Very high - more random/mistakes\n",
    ")\n",
    "\n",
    "detailed_model = EmailAnalyzerModel(\n",
    "    label=\"detailed_analyzer\",\n",
    "    system_prompt=\"\"\"You are an expert customer support analyst. Carefully analyze the email:\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. Customer name: Extract the name of the person WRITING the email (not people mentioned)\n",
    "   - Check signatures, sign-offs, and self-introductions\n",
    "   - If multiple names appear, identify who is actually writing\n",
    "   - Include full name if available (e.g., \"Dr. Rajesh Patel\" not just \"Raj\")\n",
    "   \n",
    "2. Product: Identify the SPECIFIC product having issues\n",
    "   - If multiple products mentioned, focus on the problematic one\n",
    "   - Include version numbers if provided\n",
    "   - Don't confuse product names with people names\n",
    "   \n",
    "3. Sentiment: Analyze the OVERALL tone\n",
    "   - positive: satisfied, happy, thankful (even with minor complaints)\n",
    "   - negative: frustrated, angry, disappointed\n",
    "   - neutral: matter-of-fact, indifferent, mixed feelings\n",
    "   - Consider sarcasm and actual meaning beyond words\"\"\",\n",
    "    temperature=0.0,  # Precise\n",
    ")\n",
    "\n",
    "balanced_model = EmailAnalyzerModel(\n",
    "    label=\"balanced_analyzer\",\n",
    "    system_prompt=\"\"\"Extract customer support information from emails.\n",
    "    \n",
    "    Guidelines:\n",
    "    - Customer name: The person sending the email (check signatures)\n",
    "    - Product: The main product being discussed\n",
    "    - Issue: Brief description of the problem\n",
    "    - Sentiment: Overall tone (positive/negative/neutral)\"\"\",\n",
    "    temperature=0.4,  # Moderate temperature\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9637d3f",
   "metadata": {},
   "source": [
    "### üîÑ Part 2.5: A/B Testing Models\n",
    "\n",
    "**Important Concept**: When comparing models, we use the SAME evaluation definition\n",
    "(same dataset + scorers) for all models. This ensures fair comparison and allows\n",
    "everyone in the workshop to see aggregated results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9d07aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single evaluation definition that will be used for all models\n",
    "evaluation = Evaluation(\n",
    "    name=\"email_analyzer_comparison\",  # Same eval for all models\n",
    "    dataset=support_dataset,\n",
    "    scorers=[name_accuracy, sentiment_accuracy, extraction_quality],\n",
    ")\n",
    "\n",
    "\n",
    "async def compare_models(models: list[Model]) -> dict[str, Any]:\n",
    "    \"\"\"Run A/B comparison of multiple models.\"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for model in models:\n",
    "        print(f\"\\nüìä Evaluating {model.label}...\")\n",
    "\n",
    "        # Run evaluation with optional display name for this specific run\n",
    "        eval_result = await evaluation.evaluate(\n",
    "            model,\n",
    "            __weave={\"display_name\": f\"email_analyzer_comparison - {model.label}\"},\n",
    "        )\n",
    "        results[model.label] = eval_result\n",
    "\n",
    "        print(f\"‚úÖ {model.label} evaluation complete!\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run the comparison\n",
    "print(\"üèÅ Starting model comparison...\")\n",
    "# For notebooks: comparison_results = await compare_models(...)\n",
    "# For scripts:\n",
    "comparison_results = asyncio.run(\n",
    "    compare_models([basic_model, detailed_model, balanced_model])\n",
    ")\n",
    "print(\"\\nüéâ Comparison complete! View the results in the Weave UI.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629d132f",
   "metadata": {},
   "source": [
    "### üéØ Part 2.6: Leaderboard Competition\n",
    "\n",
    "Now it's time for a friendly competition! We'll use Weave's leaderboard feature to track\n",
    "who can create the best email analysis model. Everyone will use the same evaluation\n",
    "(`email_analyzer_comparison`) so results are directly comparable.\n",
    "\n",
    "**Your challenge**: Improve the prompt/model to get the highest scores on:\n",
    "- Name accuracy\n",
    "- Sentiment accuracy\n",
    "- Overall extraction quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87a2249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from weave.flow import leaderboard\n",
    "from weave.trace.ref_util import get_ref\n",
    "\n",
    "# Create a leaderboard for the workshop competition\n",
    "leaderboard_spec = leaderboard.Leaderboard(\n",
    "    name=\"Email Analysis Workshop Competition\",\n",
    "    description=\"\"\"\n",
    "This leaderboard tracks the best email analysis models from workshop participants.\n",
    "\n",
    "### Scoring Metrics\n",
    "\n",
    "1. **Name Accuracy**: Fraction of emails where the customer name was correctly extracted\n",
    "2. **Sentiment Accuracy**: Fraction of emails where the sentiment was correctly identified  \n",
    "3. **Extraction Quality**: Overall quality score for extracting all required fields\n",
    "\n",
    "### Tips for Success\n",
    "- Focus on clear, specific prompts\n",
    "- Handle edge cases (names in signatures, multiple products mentioned)\n",
    "- Consider the context and nuances in sentiment analysis\n",
    "\"\"\",\n",
    "    columns=[\n",
    "        leaderboard.LeaderboardColumn(\n",
    "            evaluation_object_ref=get_ref(evaluation).uri(),\n",
    "            scorer_name=\"name_accuracy\",\n",
    "            summary_metric_path=\"score.mean\",\n",
    "        ),\n",
    "        leaderboard.LeaderboardColumn(\n",
    "            evaluation_object_ref=get_ref(evaluation).uri(),\n",
    "            scorer_name=\"sentiment_accuracy\",\n",
    "            summary_metric_path=\"score.mean\",\n",
    "        ),\n",
    "        leaderboard.LeaderboardColumn(\n",
    "            evaluation_object_ref=get_ref(evaluation).uri(),\n",
    "            scorer_name=\"extraction_quality\",\n",
    "            summary_metric_path=\"score.mean\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Publish the leaderboard\n",
    "leaderboard_ref = weave.publish(leaderboard_spec)\n",
    "print(\"üèÜ Leaderboard created! View it in the Weave UI\")\n",
    "print(f\"üìä All participants will use the same evaluation: {evaluation.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4037db",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### üöÄ Your Turn: Create Your Best Model\n",
    "\n",
    "**Instructions:**\n",
    "1. Modify the system prompt below to improve performance\n",
    "2. Run the evaluation to see your scores\n",
    "3. Iterate and improve!\n",
    "4. Your results will automatically appear on the leaderboard\n",
    "\n",
    "**Pro Tips:**\n",
    "- Study the challenging examples in the dataset\n",
    "- Be specific about edge cases (signatures, multiple names, etc.)\n",
    "- Consider temperature settings (lower = more consistent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee33a242",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEmailModel(Model):\n",
    "    \"\"\"Your custom email analysis model - modify the prompt to improve performance!\"\"\"\n",
    "\n",
    "    # TODO: Modify this prompt to get better results!\n",
    "    system_prompt: str = \"\"\"You are an expert customer support analyst. Extract information from emails:\n",
    "\n",
    "1. Customer name: The person WRITING the email (check signatures and sign-offs)\n",
    "2. Product: The specific product mentioned that has issues\n",
    "3. Issue: Brief description of the problem\n",
    "4. Sentiment: positive, negative, or neutral based on overall tone\n",
    "\n",
    "Be careful with edge cases and ambiguous information.\"\"\"\n",
    "\n",
    "    temperature: float = 0.1  # You can adjust this too!\n",
    "\n",
    "    @weave.op\n",
    "    def predict(self, email: str) -> CustomerEmail:\n",
    "        client = OpenAI()\n",
    "\n",
    "        response = client.beta.chat.completions.parse(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"Analyze this email:\\n\\n{email}\"},\n",
    "            ],\n",
    "            response_format=CustomerEmail,\n",
    "            temperature=self.temperature,\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.parsed\n",
    "\n",
    "\n",
    "# Create your model instance\n",
    "my_model = MyEmailModel()\n",
    "\n",
    "# Test on a single example first\n",
    "test_result = my_model.predict(eval_examples[0][\"email\"])\n",
    "print(\"üß™ Test result:\")\n",
    "print(f\"  Name: {test_result.customer_name}\")\n",
    "print(f\"  Product: {test_result.product}\")\n",
    "print(f\"  Sentiment: {test_result.sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b17119",
   "metadata": {},
   "source": [
    "### üèÉ Submit to Leaderboard\n",
    "\n",
    "Run this cell to evaluate your model and submit to the leaderboard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc3f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your model on the full evaluation\n",
    "print(\"üèÉ Running your model on the competition dataset...\")\n",
    "print(\"‚è±Ô∏è  This may take a minute...\")\n",
    "\n",
    "my_results = asyncio.run(\n",
    "    evaluation.evaluate(\n",
    "        my_model,\n",
    "        __weave={\n",
    "            \"display_name\": \"email_analyzer_comparison - MyModel\"\n",
    "        },  # You can customize this name\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Evaluation complete!\")\n",
    "print(\"üèÜ Check the leaderboard in the Weave UI to see how you rank!\")\n",
    "print(\n",
    "    \"üí° Tip: Iterate on your prompt above and run this cell again to improve your score\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7106eb",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've learned how to use Weave's evaluation framework:\n",
    "\n",
    "- ‚úÖ **Dataset Creation**: Built challenging evaluation datasets\n",
    "- ‚úÖ **Custom Scorers**: Created scoring functions for specific metrics\n",
    "- ‚úÖ **Pre-built Scorers**: Used Weave's built-in evaluation tools\n",
    "- ‚úÖ **Pairwise Evaluation**: Compared models head-to-head\n",
    "- ‚úÖ **Model Comparison**: Ran systematic A/B tests\n",
    "- ‚úÖ **Leaderboards**: Tracked performance across participants\n",
    "\n",
    "**Next Steps:**\n",
    "- Continue to Part 3: Production Monitoring\n",
    "- Experiment with different prompts and models\n",
    "- Try the evaluation framework on your own use cases\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Systematic evaluation reveals model strengths and weaknesses\n",
    "- Challenging datasets expose edge cases and failure modes\n",
    "- Leaderboards encourage continuous improvement and collaboration\n",
    "- Weave makes it easy to compare models and track progress over time"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
