{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tjh24iCFw8TH"
   },
   "source": [
    "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "\n",
    "Weave is a toolkit for developing AI-powered applications.\n",
    "\n",
    "You can use Weave to:\n",
    "- Log and debug language model inputs, outputs, and traces.\n",
    "- Build rigorous, apples-to-apples evaluations for language model use cases.\n",
    "- Organize all the information generated across the LLM workflow, from experimentation to evaluations to production.\n",
    "\n",
    "This notebook demonstrates how to evaluate a model or function using Weaveâ€™s Evaluation API. Evaluation is a core concept in Weave that helps you measure and iterate on your application by running it against a dataset of examples and scoring the outputs using custom-defined functions. You'll define a simple model, create a labeled dataset, track scoring functions with `@weave.op`, and run an evaluation that automatically tracks results in the Weave UI. This forms the foundation for more advanced workflows like LLM fine-tuning, regression testing, or model comparison.\n",
    "\n",
    "To get started, complete the prerequisites. Then, define a Weave `Model` with a `predict` method, create a labeled dataset and scoring function, and run an evaluation using `weave.Evaluation.evaluate()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McE7cuqSxMiP"
   },
   "source": [
    "## ðŸ”‘ Prerequisites\n",
    "\n",
    "Before you can run a Weave evaluation, complete the following prerequisites.\n",
    "\n",
    "1. Install the W&B Weave SDK and log in with your [API key](https://wandb.ai/settings#api).\n",
    "2. Install the OpenAI SDK and log in with your [API key](https://platform.openai.com/api-keys).\n",
    "3. Initialize your W&B project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56XteuP7s7sm"
   },
   "outputs": [],
   "source": [
    "# Install dependancies and imports\n",
    "!pip install wandb weave openai -q\n",
    "\n",
    "import os\n",
    "import openai\n",
    "import json\n",
    "import weave\n",
    "\n",
    "from getpass import getpass\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# ðŸ”‘ Setup your API keys\n",
    "# Running this cell will prompt you for your API key with `getpass` and will not echo to the terminal.\n",
    "#####\n",
    "print(\"---\")\n",
    "print(\"You can find your Weights and Biases API key here: https://wandb.ai/settings#api\")\n",
    "os.environ[\"WANDB_API_KEY\"] = getpass('Enter your Weights and Biases API key: ')\n",
    "print(\"---\")\n",
    "print(\"You can generate your OpenAI API key here: https://platform.openai.com/api-keys\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass('Enter your OpenAI API key: ')\n",
    "print(\"---\")\n",
    "#####\n",
    "\n",
    "# ðŸ  Enter your W&B project name\n",
    "weave_client = weave.init('MY_PROJECT_NAME') # ðŸ Your W&B project name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mmzfm_cxr6Z"
   },
   "source": [
    "## ðŸ Run your first evaluation\n",
    "\n",
    "The following code sample shows how to evaluate an LLM using Weaveâ€™s `Model` and `Evaluation` APIs. First, define a Weave model by subclassing `weave.Model`, specifying the model name and prompt format, and tracking a `predict` method with `@weave.op`. The `predict` method sends a prompt to OpenAI and parses the response into a structured output using a Pydantic schema (`FruitExtract`). Then, create a small evaluation dataset consisting of input sentences and expected targets. Next, define a custom scoring function (also tracked using `@weave.op`) that compares the modelâ€™s output to the target label. Finally,  wrap everything in a `weave.Evaluation`, specifying your dataset and scorers, and call `evaluate()` to run the evaluation pipeline asynchronously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1w-C5MHtjRg"
   },
   "outputs": [],
   "source": [
    "# 1. Construct a Weave model\n",
    "class FruitExtract(BaseModel):\n",
    "    fruit: str\n",
    "    color: str\n",
    "    flavor: str\n",
    "\n",
    "class ExtractFruitsModel(weave.Model):\n",
    "    model_name: str\n",
    "    prompt_template: str\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, sentence: str) -> dict:\n",
    "        client = OpenAI()\n",
    "\n",
    "        response = client.beta.chat.completions.parse(\n",
    "            model=self.model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": self.prompt_template.format(sentence=sentence)}\n",
    "            ],\n",
    "            response_format=FruitExtract\n",
    "        )\n",
    "        result = response.choices[0].message.parsed\n",
    "        return result\n",
    "\n",
    "model = ExtractFruitsModel(\n",
    "    name='gpt4o',\n",
    "    model_name='gpt-4o',\n",
    "    prompt_template='Extract fields (\"fruit\": <str>, \"color\": <str>, \"flavor\": <str>) as json, from the following text : {sentence}'\n",
    ")\n",
    "\n",
    "# 2. Collect some samples\n",
    "sentences = [\n",
    "    \"There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy.\",\n",
    "    \"Pounits are a bright green color and are more savory than sweet.\",\n",
    "    \"Finally, there are fruits called glowls, which have a very sour and bitter taste which is acidic and caustic, and a pale orange tinge to them.\"\n",
    "]\n",
    "labels = [\n",
    "    {'fruit': 'neoskizzles', 'color': 'purple', 'flavor': 'candy'},\n",
    "    {'fruit': 'pounits', 'color': 'green', 'flavor': 'savory'},\n",
    "    {'fruit': 'glowls', 'color': 'orange', 'flavor': 'sour, bitter'}\n",
    "]\n",
    "examples = [\n",
    "    {'id': '0', 'sentence': sentences[0], 'target': labels[0]},\n",
    "    {'id': '1', 'sentence': sentences[1], 'target': labels[1]},\n",
    "    {'id': '2', 'sentence': sentences[2], 'target': labels[2]}\n",
    "]\n",
    "\n",
    "# 3. Define a scoring function for your evaluation\n",
    "@weave.op()\n",
    "def fruit_name_score(target: dict, output: FruitExtract) -> dict:\n",
    "    target_flavors = [f.strip().lower() for f in target['flavor'].split(',')]\n",
    "    output_flavors = [f.strip().lower() for f in output.flavor.split(',')]\n",
    "    # Check if any target flavor is present in the output flavors\n",
    "    matches = any(tf in of for tf in target_flavors for of in output_flavors)\n",
    "    return {'correct': matches}\n",
    "\n",
    "# 4. Run your evaluation\n",
    "evaluation = weave.Evaluation(\n",
    "    name='fruit_eval',\n",
    "    dataset=examples, scorers=[fruit_name_score],\n",
    ")\n",
    "await evaluation.evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGqeyYMmw7Hl"
   },
   "source": [
    "## ðŸš€ Looking for more examples?\n",
    "\n",
    "- Learn how to build an [evlauation pipeline end-to-end](https://weave-docs.wandb.ai/tutorial-eval). \n",
    "- Learn how to evaluate a [RAG application by building](https://weave-docs.wandb.ai/tutorial-rag)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
