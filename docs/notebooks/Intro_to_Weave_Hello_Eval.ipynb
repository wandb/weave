{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tjh24iCFw8TH"
   },
   "source": [
    "# Introduction to Evaluations\n",
    "\n",
    "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "\n",
    "Weave is a toolkit for developing AI-powered applications.\n",
    "\n",
    "You can use Weave to:\n",
    "- Log and debug language model inputs, outputs, and traces.\n",
    "- Build rigorous, apples-to-apples evaluations for language model use cases.\n",
    "- Organize all the information generated across the LLM workflow, from experimentation to evaluations to production.\n",
    "\n",
    "This notebook demonstrates how to evaluate a model or function using Weave’s Evaluation API. Evaluation is a core concept in Weave that helps you measure and iterate on your application by running it against a dataset of examples and scoring the outputs using custom-defined functions. You'll define a simple model, create a labeled dataset, track scoring functions with `@weave.op`, and run an evaluation that automatically tracks results in the Weave UI. This forms the foundation for more advanced workflows like LLM fine-tuning, regression testing, or model comparison.\n",
    "\n",
    "To get started, complete the prerequisites. Then, define a Weave `Model` with a `predict` method, create a labeled dataset and scoring function, and run an evaluation using `weave.Evaluation.evaluate()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McE7cuqSxMiP"
   },
   "source": [
    "## 🔑 Prerequisites\n",
    "\n",
    "Before you can run a Weave evaluation, complete the following prerequisites.\n",
    "\n",
    "1. Install the W&B Weave SDK and log in with your [API key](https://wandb.ai/settings#api).\n",
    "2. Install the OpenAI SDK and log in with your [API key](https://platform.openai.com/api-keys).\n",
    "3. Initialize your W&B project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56XteuP7s7sm"
   },
   "outputs": [],
   "source": [
    "# Install dependancies and imports\n",
    "!pip install wandb weave openai -q\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import weave\n",
    "\n",
    "# 🔑 Setup your API keys\n",
    "# Running this cell will prompt you for your API key with `getpass` and will not echo to the terminal.\n",
    "#####\n",
    "print(\"---\")\n",
    "print(\n",
    "    \"You can find your Weights and Biases API key here: https://wandb.ai/settings#api\"\n",
    ")\n",
    "os.environ[\"WANDB_API_KEY\"] = getpass(\"Enter your Weights and Biases API key: \")\n",
    "print(\"---\")\n",
    "print(\"You can generate your OpenAI API key here: https://platform.openai.com/api-keys\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
    "print(\"---\")\n",
    "#####\n",
    "\n",
    "# 🏠 Enter your W&B project name\n",
    "weave_client = weave.init(\"MY_PROJECT_NAME\")  # 🐝 Your W&B project name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mmzfm_cxr6Z"
   },
   "source": [
    "## 🐝 Run your first evaluation\n",
    "\n",
    "The following code sample shows how to evaluate an LLM using Weave’s `Model` and `Evaluation` APIs. First, define a Weave model by subclassing `weave.Model`, specifying the model name and prompt format, and tracking a `predict` method with `@weave.op`. The `predict` method sends a prompt to OpenAI and parses the response into a structured output using a Pydantic schema (`FruitExtract`). Then, create a small evaluation dataset consisting of input sentences and expected targets. Next, define a custom scoring function (also tracked using `@weave.op`) that compares the model’s output to the target label. Finally,  wrap everything in a `weave.Evaluation`, specifying your dataset and scorers, and call `evaluate()` to run the evaluation pipeline asynchronously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1w-C5MHtjRg"
   },
   "outputs": [],
   "source": [
    "# 1. Construct a Weave model\n",
    "class FruitExtract(BaseModel):\n",
    "    fruit: str\n",
    "    color: str\n",
    "    flavor: str\n",
    "\n",
    "\n",
    "class ExtractFruitsModel(weave.Model):\n",
    "    model_name: str\n",
    "    prompt_template: str\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, sentence: str) -> dict:\n",
    "        client = OpenAI()\n",
    "\n",
    "        response = client.beta.chat.completions.parse(\n",
    "            model=self.model_name,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": self.prompt_template.format(sentence=sentence),\n",
    "                }\n",
    "            ],\n",
    "            response_format=FruitExtract,\n",
    "        )\n",
    "        result = response.choices[0].message.parsed\n",
    "        return result\n",
    "\n",
    "\n",
    "model = ExtractFruitsModel(\n",
    "    name=\"gpt4o\",\n",
    "    model_name=\"gpt-4o\",\n",
    "    prompt_template='Extract fields (\"fruit\": <str>, \"color\": <str>, \"flavor\": <str>) as json, from the following text : {sentence}',\n",
    ")\n",
    "\n",
    "# 2. Collect some samples\n",
    "sentences = [\n",
    "    \"There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy.\",\n",
    "    \"Pounits are a bright green color and are more savory than sweet.\",\n",
    "    \"Finally, there are fruits called glowls, which have a very sour and bitter taste which is acidic and caustic, and a pale orange tinge to them.\",\n",
    "]\n",
    "labels = [\n",
    "    {\"fruit\": \"neoskizzles\", \"color\": \"purple\", \"flavor\": \"candy\"},\n",
    "    {\"fruit\": \"pounits\", \"color\": \"green\", \"flavor\": \"savory\"},\n",
    "    {\"fruit\": \"glowls\", \"color\": \"orange\", \"flavor\": \"sour, bitter\"},\n",
    "]\n",
    "examples = [\n",
    "    {\"id\": \"0\", \"sentence\": sentences[0], \"target\": labels[0]},\n",
    "    {\"id\": \"1\", \"sentence\": sentences[1], \"target\": labels[1]},\n",
    "    {\"id\": \"2\", \"sentence\": sentences[2], \"target\": labels[2]},\n",
    "]\n",
    "\n",
    "\n",
    "# 3. Define a scoring function for your evaluation\n",
    "@weave.op()\n",
    "def fruit_name_score(target: dict, output: FruitExtract) -> dict:\n",
    "    target_flavors = [f.strip().lower() for f in target[\"flavor\"].split(\",\")]\n",
    "    output_flavors = [f.strip().lower() for f in output.flavor.split(\",\")]\n",
    "    # Check if any target flavor is present in the output flavors\n",
    "    matches = any(tf in of for tf in target_flavors for of in output_flavors)\n",
    "    return {\"correct\": matches}\n",
    "\n",
    "\n",
    "# 4. Run your evaluation\n",
    "evaluation = weave.Evaluation(\n",
    "    name=\"fruit_eval\",\n",
    "    dataset=examples,\n",
    "    scorers=[fruit_name_score],\n",
    ")\n",
    "await evaluation.evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGqeyYMmw7Hl"
   },
   "source": [
    "## 🚀 Looking for more examples?\n",
    "\n",
    "- Learn how to build an [evlauation pipeline end-to-end](https://weave-docs.wandb.ai/tutorial-eval). \n",
    "- Learn how to evaluate a [RAG application by building](https://weave-docs.wandb.ai/tutorial-rag)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
