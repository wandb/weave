{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- docusaurus_head_meta::start\n",
    "---\n",
    "title: Using HuggingFace Datasets in evaluations with `preprocess_model_input`\n",
    "---\n",
    "docusaurus_head_meta::end -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oF8wOj2QOJb3"
   },
   "source": [
    "# Using HuggingFace Datasets in evaluations with `preprocess_model_input`\n",
    "\n",
    "## Note: This is a temporary workaround\n",
    "> This guide demonstrates a workaround for using HuggingFace Datasets with Weave evaluations.<br /><br/>\n",
    "We are actively working on developing more seamless integrations that will simplify this process.\\\n",
    "> While this approach works, expect improvements and updates in the near future that will make working with external datasets more straightforward.\n",
    "\n",
    "## Setup and imports\n",
    "First, we initialize Weave and connect to Weights & Biases for tracking experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-J9Ildh24eQ"
   },
   "outputs": [],
   "source": [
    "!pip install datasets wandb weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5YDPevzE5bl2"
   },
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "HUGGINGFACE_DATASET = \"wandb/ragbench-test-sample\"\n",
    "WANDB_KEY = \"\"\n",
    "WEAVE_TEAM = \"\"\n",
    "WEAVE_PROJECT = \"\"\n",
    "\n",
    "# Init weave and required libraries\n",
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "\n",
    "import weave\n",
    "from weave import Evaluation\n",
    "\n",
    "# Login to wandb and initialize weave\n",
    "wandb.login(key=WANDB_KEY)\n",
    "client = weave.init(f\"{WEAVE_TEAM}/{WEAVE_PROJECT}\")\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops (needed for some notebook environments)\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUm6FDrLN67a"
   },
   "source": [
    "## Load and prepare HuggingFace dataset\n",
    "\n",
    "- We load a HuggingFace dataset.\n",
    "- Create an index mapping to reference the dataset rows.\n",
    "- This index approach allows us to maintain references to the original dataset.\n",
    "\n",
    "> **Note:**<br/>\n",
    "In the index, we encode the `hf_hub_name` along with the `hf_id` to ensure each row has a unique identifier.\\\n",
    "This unique digest value is used for tracking and referencing specific dataset entries during evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ir2JI8aaNL0c"
   },
   "outputs": [],
   "source": [
    "# Load the HuggingFace dataset\n",
    "ds = load_dataset(HUGGINGFACE_DATASET)\n",
    "row_count = ds[\"train\"].num_rows\n",
    "\n",
    "# Create an index mapping for the dataset\n",
    "# This creates a list of dictionaries with HF dataset indices\n",
    "# Example: [{\"hf_id\": 0}, {\"hf_id\": 1}, {\"hf_id\": 2}, ...]\n",
    "hf_index = [{\"hf_id\": i, \"hf_hub_name\": HUGGINGFACE_DATASET} for i in range(row_count)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIx8GrU0NzBX"
   },
   "source": [
    "## Define processing and evaluation functions\n",
    "\n",
    "### Processing pipeline\n",
    "- `preprocess_example`: Transforms the index reference into the actual data needed for evaluation\n",
    "- `hf_eval`: Defines how to score the model outputs\n",
    "- `function_to_evaluate`: The actual function/model being evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPQTlVKxNxJN"
   },
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def preprocess_example(example):\n",
    "    \"\"\"\n",
    "    Preprocesses each example before evaluation.\n",
    "    Args:\n",
    "        example: Dict containing hf_id\n",
    "    Returns:\n",
    "        Dict containing the prompt from the HF dataset\n",
    "    \"\"\"\n",
    "    hf_row = ds[\"train\"][example[\"hf_id\"]]\n",
    "    return {\"prompt\": hf_row[\"question\"], \"answer\": hf_row[\"response\"]}\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def hf_eval(hf_id: int, output: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Scoring function for evaluating model outputs.\n",
    "    Args:\n",
    "        hf_id: Index in the HF dataset\n",
    "        output: The output from the model to evaluate\n",
    "    Returns:\n",
    "        Dict containing evaluation scores\n",
    "    \"\"\"\n",
    "    hf_row = ds[\"train\"][hf_id]\n",
    "    return {\"scorer_value\": True}\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def function_to_evaluate(prompt: str):\n",
    "    \"\"\"\n",
    "    The function that will be evaluated (e.g., your model or pipeline).\n",
    "    Args:\n",
    "        prompt: Input prompt from the dataset\n",
    "    Returns:\n",
    "        Dict containing model output\n",
    "    \"\"\"\n",
    "    return {\"generated_text\": \"testing \"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swIOS3EzOTGe"
   },
   "source": [
    "### Create and run evaluation\n",
    "\n",
    "- For each index in hf_index:\n",
    "  1. `preprocess_example` gets the corresponding data from the HF dataset.\n",
    "  2. The preprocessed data is passed to `function_to_evaluate`.\n",
    "  3. The output is scored using `hf_eval`.\n",
    "  4. Results are tracked in Weave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3FEKJchpOYQk"
   },
   "outputs": [],
   "source": [
    "# Create evaluation object\n",
    "evaluation = Evaluation(\n",
    "    dataset=hf_index,  # Use our index mapping\n",
    "    scorers=[hf_eval],  # List of scoring functions\n",
    "    preprocess_model_input=preprocess_example,  # Function to prepare inputs\n",
    ")\n",
    "\n",
    "\n",
    "# Run evaluation asynchronously\n",
    "async def main():\n",
    "    await evaluation.evaluate(function_to_evaluate)\n",
    "\n",
    "\n",
    "asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
