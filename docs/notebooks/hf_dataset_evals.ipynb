{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- docusaurus_head_meta::start\n",
        "---\n",
        "title: Using HuggingFace Datasets in evaluations with `preprocess_model_input`\n",
        "---\n",
        "docusaurus_head_meta::end -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF8wOj2QOJb3"
      },
      "source": [
        "# Using HuggingFace Datasets in evaluations with `preprocess_model_input`\n",
        "\n",
        "## Setup and imports\n",
        "First, we initialize Weave and connect to Weights & Biases for tracking experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-J9Ildh24eQ"
      },
      "outputs": [],
      "source": [
        "!pip install datasets wandb weave"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YDPevzE5bl2"
      },
      "outputs": [],
      "source": [
        "# Init weave and required libraries\n",
        "import wandb\n",
        "import weave\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from datasets import load_dataset\n",
        "from weave import Evaluation\n",
        "\n",
        "# Login to wandb and initialize weave\n",
        "wandb.login(key=\"\")\n",
        "client = weave.init(\"\")\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops (needed for some notebook environments)\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUm6FDrLN67a"
      },
      "source": [
        "## Load and prepare HuggingFace dataset\n",
        "\n",
        "- We load a HuggingFace dataset.\n",
        "- Create an index mapping to reference the dataset rows.\n",
        "- This index approach allows us to maintain references to the original dataset.\n",
        "\n",
        "> **Note:**\n",
        "We're using the index here to make it more generalizable, but it would be better to use a unique `id` column in the HF dataset.\n",
        "This could also be saved as a pointer back to the HF dataset row, in a Weave dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ir2JI8aaNL0c"
      },
      "outputs": [],
      "source": [
        "# Load the HuggingFace dataset\n",
        "ds = load_dataset(\"wandb/ragbench-sentence-relevance-balanced\")\n",
        "row_count = ds['train'].num_rows\n",
        "\n",
        "# Create an index mapping for the dataset\n",
        "# This creates a list of dictionaries with HF dataset indices\n",
        "# Example: [{\"hf_id\": 0}, {\"hf_id\": 1}, {\"hf_id\": 2}, ...]\n",
        "hf_index = [{\"hf_id\": i} for i in range(row_count)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIx8GrU0NzBX"
      },
      "source": [
        "## Define processing and evaluation functions\n",
        "\n",
        "### Processing pipeline\n",
        "- `preprocess_example`: Transforms the index reference into the actual data needed for evaluation\n",
        "- `hf_eval`: Defines how to score the model outputs\n",
        "- `function_to_evaluate`: The actual function/model being evaluated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPQTlVKxNxJN"
      },
      "outputs": [],
      "source": [
        "@weave.op()\n",
        "def preprocess_example(example):\n",
        "    \"\"\"\n",
        "    Preprocesses each example before evaluation.\n",
        "    Args:\n",
        "        example: Dict containing hf_id\n",
        "    Returns:\n",
        "        Dict containing the prompt from the HF dataset\n",
        "    \"\"\"\n",
        "    hf_row = ds['train'][example['hf_id']]\n",
        "    return {\"prompt\": hf_row['question']}\n",
        "\n",
        "@weave.op()\n",
        "def hf_eval(hf_id: int, output: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Scoring function for evaluating model outputs.\n",
        "    Args:\n",
        "        hf_id: Index in the HF dataset\n",
        "        output: The output from the model to evaluate\n",
        "    Returns:\n",
        "        Dict containing evaluation scores\n",
        "    \"\"\"\n",
        "    hf_row = ds['train'][hf_id]\n",
        "    return {'scorer_value': True}\n",
        "\n",
        "@weave.op()\n",
        "def function_to_evaluate(prompt: str):\n",
        "    \"\"\"\n",
        "    The function that will be evaluated (e.g., your model or pipeline).\n",
        "    Args:\n",
        "        prompt: Input prompt from the dataset\n",
        "    Returns:\n",
        "        Dict containing model output\n",
        "    \"\"\"\n",
        "    return {'generated_text': 'testing '}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swIOS3EzOTGe"
      },
      "source": [
        "### Create and run evaluation\n",
        "\n",
        "- For each index in hf_index:\n",
        "  1. `preprocess_example` gets the corresponding data from the HF dataset.\n",
        "  2. The preprocessed data is passed to `function_to_evaluate`.\n",
        "  3. The output is scored using `hf_eval`.\n",
        "  4. Results are tracked in Weave."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FEKJchpOYQk"
      },
      "outputs": [],
      "source": [
        "# Create evaluation object\n",
        "evaluation = Evaluation(\n",
        "    dataset=hf_index,  # Use our index mapping\n",
        "    scorers=[hf_eval],  # List of scoring functions\n",
        "    preprocess_model_input=preprocess_example  # Function to prepare inputs\n",
        ")\n",
        "\n",
        "# Run evaluation asynchronously\n",
        "async def main():\n",
        "    await evaluation.evaluate(function_to_evaluate)\n",
        "\n",
        "asyncio.run(main())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
