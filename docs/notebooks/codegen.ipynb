{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- docusaurus_head_meta::start\n",
    "---\n",
    "title: Code Generation\n",
    "---\n",
    "docusaurus_head_meta::end -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Generation using Weave and OpenAI\n",
    "\n",
    "Generating high-quality code with proper structure, documentation, and tests is a challenging task. This guide demonstrates how to implement a code generation pipeline. You'll learn to create a code generation pipeline that produces high-quality Python functions against the humaneval test suite.\n",
    "\n",
    "We'll use Weave for evaluation comparison and tracking, and OpenAI's GPT models for code generation using structured outputs.\n",
    "\n",
    "![Evaluation](../../media/codegen/eval_dash.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Demonstration\n",
    "\n",
    "For a visual demonstration of the code generation pipeline using Weave, Groq, and E2B check out this video:\n",
    "\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/B70jJYPVAzE?si=75Z4Fg_DBxAiu9_6&amp\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n",
    "\n",
    "This video provides a step-by-step walkthrough of the process, showcasing how Weave integrates with Groq to create a powerful code generation tool and then running the code in E2B, to validate the code. We use OpenAI in the following example, but you can use any LLM provider with Weave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why use Weave?\n",
    "\n",
    "In this tutorial, we'll use Weave to implement and evaluate a code generation pipeline. You'll learn how to:\n",
    "\n",
    "1. **Track your LLM pipeline**: Log inputs, outputs, and intermediate steps of your code generation process.\n",
    "2. **Evaluate LLM outputs**: Create and compare evaluations of your generated code with rich debugging tools and visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "\n",
    "First, let's set up our environment and import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU autopep8 autoflake weave isort openai set-env-colab-kaggle-dotenv datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Temporary workaround to fix bug in openai:\n",
    "# TypeError: Client.__init__() got an unexpected keyword argument 'proxies'\n",
    "# See https://community.openai.com/t/error-with-openai-1-56-0-client-init-got-an-unexpected-keyword-argument-proxies/1040332/15\n",
    "!pip install \"httpx<0.28\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import tempfile\n",
    "import traceback\n",
    "\n",
    "import autopep8\n",
    "import isort\n",
    "from autoflake import fix_code\n",
    "from datasets import load_dataset\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "from set_env import set_env\n",
    "\n",
    "import weave\n",
    "from weave import Dataset, Evaluation\n",
    "\n",
    "set_env(\"WANDB_API_KEY\")\n",
    "set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEAVE_PROJECT = \"codegen-cookbook-example\"\n",
    "weave.init(WEAVE_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_eval = load_dataset(\"openai_humaneval\")\n",
    "selected_examples = human_eval[\"test\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::note\n",
    "Weave automatically tracks OpenAI API calls, including inputs, outputs, and metadata. This means you don't need to add any additional logging code for your OpenAI interactions â€“ Weave handles it seamlessly in the background.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leveraging Structured Outputs and Pydantic Models\n",
    "\n",
    "In this code generation pipeline, we utilize OpenAI's [structured outputs mode](https://platform.openai.com/docs/guides/structured-outputs) and Pydantic models to ensure consistent and well-formatted responses from the language model. This approach offers several advantages:\n",
    "\n",
    "\n",
    "1. **Type Safety**: By defining Pydantic models for our expected outputs, we enforce a strict structure for the generated code, program runners, and unit tests.\n",
    "2. **Easier Parsing**: The structured output mode allows us to directly parse the model's response into our predefined Pydantic models, reducing the need for complex post-processing.\n",
    "3. **Improved Reliability**: By specifying the exact format we expect, we reduce the likelihood of unexpected or malformed outputs from the language model.\n",
    "\n",
    "Here's an example of how we define our Pydantic models and use them with OpenAI's structured outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratedCode(BaseModel):\n",
    "    function_signature: str\n",
    "    function_args_with_docstring_within_triple_quotes: str\n",
    "    code_logic: str\n",
    "\n",
    "\n",
    "class FormattedGeneratedCode(BaseModel):\n",
    "    full_code: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Code Formatter\n",
    "\n",
    "To ensure consistent and clean code output, we implement a `CodeFormatter` class using Weave operations. This formatter applies various linting and styling rules to the generated code, program runner, and unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeFormatter(BaseModel):\n",
    "    @weave.op()\n",
    "    def lint_code(self, code: str) -> str:\n",
    "        # Replace escaped newlines with actual newlines\n",
    "        code = code.replace(\"\\\\n\", \"\\n\")\n",
    "\n",
    "        # Remove unused imports and variables\n",
    "        code = fix_code(\n",
    "            code, remove_all_unused_imports=True, remove_unused_variables=True\n",
    "        )\n",
    "\n",
    "        # Sort imports\n",
    "        code = isort.code(code)\n",
    "\n",
    "        # Apply PEP 8 formatting\n",
    "        code = autopep8.fix_code(code, options={\"aggressive\": 2})\n",
    "\n",
    "        return code\n",
    "\n",
    "    @weave.op()\n",
    "    def add_imports(self, code: str) -> str:\n",
    "        tree = ast.parse(code)\n",
    "        from_imports = {}\n",
    "        global_names = set()\n",
    "\n",
    "        for node in ast.walk(tree):\n",
    "            if isinstance(node, ast.Name):\n",
    "                if node.id not in dir(__builtins__):\n",
    "                    global_names.add(node.id)\n",
    "\n",
    "        # Only add typing imports that are actually used\n",
    "        typing_imports = global_names.intersection(\n",
    "            {\"List\", \"Dict\", \"Tuple\", \"Set\", \"Optional\", \"Union\"}\n",
    "        )\n",
    "        if typing_imports:\n",
    "            from_imports[\"typing\"] = typing_imports\n",
    "\n",
    "        # Remove names that are defined within the function\n",
    "        function_def = next(\n",
    "            node for node in tree.body if isinstance(node, ast.FunctionDef)\n",
    "        )\n",
    "        local_names = {arg.arg for arg in function_def.args.args}\n",
    "        local_names.update(\n",
    "            node.id\n",
    "            for node in ast.walk(function_def)\n",
    "            if isinstance(node, ast.Name) and isinstance(node.ctx, ast.Store)\n",
    "        )\n",
    "\n",
    "        global_names -= local_names\n",
    "        global_names -= {\"sorted\"}  # Remove built-in functions\n",
    "\n",
    "        # Construct the import statements\n",
    "        import_statements = []\n",
    "        for module, names in from_imports.items():\n",
    "            names_str = \", \".join(sorted(names))\n",
    "            import_statements.append(f\"from {module} import {names_str}\")\n",
    "\n",
    "        return (\n",
    "            \"\\n\".join(import_statements) + (\"\\n\\n\" if import_statements else \"\") + code\n",
    "        )\n",
    "\n",
    "    @weave.op()\n",
    "    def format_generated_code(\n",
    "        self, generated_code: GeneratedCode\n",
    "    ) -> FormattedGeneratedCode:\n",
    "        # Combine the code parts\n",
    "        full_code = f\"{generated_code.function_signature}\\n{generated_code.function_args_with_docstring_within_triple_quotes}\\n{generated_code.code_logic}\"\n",
    "\n",
    "        # Ensure proper indentation\n",
    "        lines = full_code.split(\"\\n\")\n",
    "        indented_lines = []\n",
    "        for i, line in enumerate(lines):\n",
    "            if i == 0:  # Function signature\n",
    "                indented_lines.append(line)\n",
    "            elif i == 1:  # Function arguments (docstring)\n",
    "                indented_lines.append(\"    \" + line)\n",
    "            else:  # Function body\n",
    "                indented_lines.append(\"    \" + line)\n",
    "        full_code = \"\\n\".join(indented_lines)\n",
    "\n",
    "        # Lint the code\n",
    "        full_code = self.lint_code(full_code)\n",
    "\n",
    "        # Add imports\n",
    "        cleaned_code = self.add_imports(full_code)\n",
    "\n",
    "        return FormattedGeneratedCode(full_code=cleaned_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `CodeFormatter` class provides several Weave operations to clean and format the generated code:\n",
    "   - Replacing escaped newlines with actual newlines\n",
    "   - Removing unused imports and variables\n",
    "   - Sorting imports\n",
    "   - Applying PEP 8 formatting\n",
    "   - Adding missing imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the CodeGenerationPipeline\n",
    "\n",
    "![Code Generation Pipeline](../../media/codegen/codegen_trace.png)\n",
    "\n",
    "Now, let's implement the core code generation logic:\n",
    "\n",
    "We're using a `weave.Model` so that it's automatically versioned when it changes. We're also keeping the `model_name` as an attribute so that we can experiment with it and easily diff & compare it in Weave. We're tracking our function calls with `@weave.op` so the inputs & outputs are logged to help with error tracking and debugging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeGenerationPipeline(weave.Model):\n",
    "    model_name: str\n",
    "    formatter: CodeFormatter\n",
    "\n",
    "    def __init__(\n",
    "        self, model_name: str = \"gpt-4o\", formatter: CodeFormatter = CodeFormatter()\n",
    "    ):\n",
    "        super().__init__(model_name=model_name, formatter=formatter)\n",
    "        self.model_name = model_name\n",
    "        self.formatter = formatter\n",
    "\n",
    "    @weave.op()\n",
    "    async def predict(self, prompt: str):\n",
    "        generated_code = self.generate_code(prompt)\n",
    "        formatted_generated_code = self.formatter.format_generated_code(generated_code)\n",
    "\n",
    "        return formatted_generated_code.full_code\n",
    "\n",
    "    @weave.op()\n",
    "    def generate_code(self, prompt: str) -> GeneratedCode:\n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            model=self.model_name,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an expert Python code generator.\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            response_format=GeneratedCode,\n",
    "        )\n",
    "        message = completion.choices[0].message\n",
    "        if message.parsed:\n",
    "            return message.parsed\n",
    "        else:\n",
    "            raise ValueError(message.refusal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `CodeGenerationPipeline` class encapsulates our code generation logic as a Weave Model, providing several key benefits:\n",
    "\n",
    "1. Automatic experiment tracking: Weave captures inputs, outputs, and parameters for each run of the model.\n",
    "2. Versioning: Changes to the model's attributes or code are automatically versioned, creating a clear history of how your code generation pipeline evolves over time.\n",
    "3. Reproducibility: The versioning and tracking make it easy to reproduce any previous result or configuration of your code generation pipeline.\n",
    "4. Hyperparameter management: Model attributes (like `model_name`) are clearly defined and tracked across different runs, facilitating experimentation.\n",
    "5. Integration with Weave ecosystem: Using `weave.Model` allows seamless integration with other Weave tools, such as evaluations and serving capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement evaluation metrics\n",
    "\n",
    "To assess the quality of our generated code, we'll implement simple evaluation metrics using a `weave.Scorer` subclass. This will run `score` on every `model_output` from our dataset. `model_output` comes from the output of the `predict` function in our `weave.Model`. `prompt` is taken from our dataset `human-eval`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_TEMPLATE = \"\"\"\n",
    "{model_output}\n",
    "\n",
    "{test}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check({entry_point})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "async def score_humaneval_test(test: str, entry_point: str, output: str):\n",
    "    generated_code = output\n",
    "\n",
    "    # Extract test cases from the test string\n",
    "    test_cases = re.findall(r\"assert.*\", test)\n",
    "    test_cases_str = \"\\n            \".join(test_cases)\n",
    "\n",
    "    # Generate the full source code\n",
    "    full_code = CODE_TEMPLATE.format(\n",
    "        model_output=generated_code,\n",
    "        test=test,\n",
    "        test_cases=test_cases_str,\n",
    "        entry_point=entry_point,\n",
    "    )\n",
    "\n",
    "    # Create a temporary file to store the code\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".py\") as tmp_file:\n",
    "        # Write the generated code to the temporary file\n",
    "        tmp_file.write(full_code.encode())\n",
    "        tmp_file_path = tmp_file.name\n",
    "\n",
    "    try:\n",
    "        # Run the temporary Python file as a subprocess with a timeout\n",
    "        result = subprocess.run(\n",
    "            [\"python\", tmp_file_path],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=10,  # Timeout of 10 seconds\n",
    "        )\n",
    "\n",
    "        print(result)\n",
    "\n",
    "        if result.returncode == 0:\n",
    "            return {\"correct\": True}\n",
    "        else:\n",
    "            return {\"correct\": False, \"error\": result.stderr, \"output\": result.stdout}\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return {\"correct\": False, \"error\": \"TimeoutExpired\"}\n",
    "    except Exception as e:\n",
    "        return {\"correct\": False, \"error\": traceback.format_exc()}\n",
    "    finally:\n",
    "        # Ensure the temporary file is removed after execution\n",
    "        os.remove(tmp_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These evaluation functions run the generated code and return a boolean value indicating whether the code passed the test provided from the dataset.\n",
    "\n",
    "![Evaluation](../../media/codegen/eval_trace.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Weave Dataset and run evaluation\n",
    "\n",
    "To evaluate our pipeline, we'll create a Weave Dataset and run an evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_selected_examples = [\n",
    "    {\n",
    "        \"task_id\": task_id,\n",
    "        \"prompt\": prompt,\n",
    "        \"canonical_solution\": solution,\n",
    "        \"test\": test,\n",
    "        \"entry_point\": entry_point,\n",
    "    }\n",
    "    for task_id, prompt, solution, test, entry_point in zip(\n",
    "        selected_examples[\"task_id\"],\n",
    "        selected_examples[\"prompt\"],\n",
    "        selected_examples[\"canonical_solution\"],\n",
    "        selected_examples[\"test\"],\n",
    "        selected_examples[\"entry_point\"],\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_dataset = Dataset(\n",
    "    name=\"humaneval_code_gen_example\",\n",
    "    rows=[\n",
    "        {\n",
    "            \"prompt\": example[\"prompt\"],\n",
    "            \"test\": example[\"test\"],\n",
    "            \"entry_point\": example[\"entry_point\"],\n",
    "        }\n",
    "        for example in formatted_selected_examples\n",
    "    ],\n",
    ")\n",
    "weave.publish(prompt_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_RUN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in [\"gpt-4o-2024-08-06\"]:\n",
    "    pipeline = CodeGenerationPipeline(model_name=model_name)\n",
    "    if not EVAL_RUN:\n",
    "        dataset = prompt_dataset.rows[2]\n",
    "        result = await pipeline.predict(dataset[\"prompt\"])\n",
    "        score_result = await score_humaneval_test(\n",
    "            dataset[\"test\"], dataset[\"entry_point\"], result[\"generated_code\"].full_code\n",
    "        )\n",
    "    else:\n",
    "        evaluation = Evaluation(\n",
    "            name=\"minimal_code_gen_evaluation\",\n",
    "            dataset=prompt_dataset,\n",
    "            scorers=[score_humaneval_test],\n",
    "        )\n",
    "        results = await evaluation.evaluate(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates a dataset with our sample prompts, defines our humaneval test scorer, and runs an evaluation of our code generation pipeline.\n",
    "\n",
    "![Final Evaluation](../../media/codegen/eval_dash.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this example, we've demonstrated how to implement a code generation pipeline using Weave and OpenAI's language models. We've shown how to:\n",
    "\n",
    "1. Create Weave operations for each step of the code generation process\n",
    "2. Wrap the pipeline in a Weave Model for easy tracking and evaluation\n",
    "3. Implement custom evaluation metrics using Weave operations\n",
    "4. Create a dataset and run an evaluation of the pipeline\n",
    "\n",
    "Weave's seamless integration allows us to track inputs, outputs, and intermediate steps throughout the code generation process, making it easier to debug, optimize, and evaluate our LLM application.\n",
    "\n",
    "For more information on Weave and its capabilities, check out the [Weave documentation](https://docs.wandb.ai/weave). You can extend this example to handle larger datasets, implement more sophisticated evaluation metrics, or integrate with other LLM workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
