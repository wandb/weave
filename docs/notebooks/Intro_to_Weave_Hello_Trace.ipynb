{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1VP8lmFD3Dk"
      },
      "source": [
        "# Introduction to Traces\n",
        "\n",
        "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
        "\n",
        "Get started using Weave to:\n",
        "- Log and debug language model inputs, outputs, and traces\n",
        "- Build rigorous, apples-to-apples evaluations for language model use cases\n",
        "- Organize all the information generated across the LLM workflow, from experimentation to evaluations to production\n",
        "\n",
        "See the full Weave documentation [here](https://wandb.me/weave)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McE7cuqSxMiP"
      },
      "source": [
        "## üîë Prerequisites\n",
        "\n",
        "Before you can begin tracing in Weave, complete the following prerequisites.\n",
        "\n",
        "1. Install the W&B Weave SDK and log in with your [API key](https://wandb.ai/settings#api), and initialize your project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56XteuP7s7sm"
      },
      "outputs": [],
      "source": [
        "# Install dependancies and imports\n",
        "!pip install wandb weave openai -q\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import weave\n",
        "from getpass import getpass\n",
        "from openai import OpenAI\n",
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "# üîë Setup your Weights and Biases API key\n",
        "# Running this cell will prompt you for your API key with `getpass` and will not echo to the terminal.\n",
        "print(\"---\")\n",
        "print(\"You can find your Weights and Biases API key here: https://wandb.ai/settings#api\")\n",
        "os.environ[\"WANDB_API_KEY\"] = getpass(\"Enter your Weights and Biases API key and hit [ENTER]: \")\n",
        "print(\"---\")\n",
        "\n",
        "# üè† Enter your W&B project name\n",
        "weave_client = weave.init(\"MY_PROJECT_NAME\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mmzfm_cxr6Z"
      },
      "source": [
        "## üêù Run your first trace\n",
        "\n",
        "The following code sample shows how to capture and visualize a trace in Weave using the `@weave.op` decorator. It defines a function called `extract_fruit` that sends a prompt to OpenAI's GPT-4o to extract structured data (fruit, color, and flavor) from a sentence. By decorating the function with `@weave.op`, Weave automatically tracks the function execution, including inputs, outputs, and intermediate steps. When the function is called with a sample sentence, the full trace is saved and viewable in the Weave UI."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create your first traces with a `Hello world!` example and a mock LLM request"
      ],
      "metadata": {
        "id": "3dvDwG1TFjPs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1w-C5MHtjRg"
      },
      "outputs": [],
      "source": [
        "# üêù Any function is trace-able within Weave\n",
        "@weave.op()\n",
        "def hello_world():\n",
        "  return \"Hello world!\"\n",
        "\n",
        "# ‚ñ∂Ô∏è Run the example\n",
        "hello_world()\n",
        "\n",
        "# üîå Mock function: Emulates an OpenAI request\n",
        "@weave.op(name=\"openai.chat.completions.create\")\n",
        "def mock_openai_call(\n",
        "    messages: list,\n",
        "    model: str,\n",
        "    response_format: dict,\n",
        "    temperature: float\n",
        ") -> dict:\n",
        "    model_response = {\n",
        "        \"fruit\": \"neoskizzles\",\n",
        "        \"color\": \"purple\",\n",
        "        \"flavor\": \"candy\"\n",
        "    }\n",
        "    mock_response = {\n",
        "        \"choices\": [\n",
        "            {\n",
        "                \"message\": {\n",
        "                    \"content\": json.dumps(model_response, indent=2),\n",
        "                    \"role\": \"assistant\"\n",
        "                }\n",
        "            }\n",
        "        ],\n",
        "        \"model\": model,\n",
        "        \"usage\": {\n",
        "            \"completion_tokens\": 29,\n",
        "            \"prompt_tokens\": 60,\n",
        "            \"total_tokens\": 89\n",
        "        }\n",
        "    }\n",
        "    return mock_response\n",
        "\n",
        "# üêù Main function: call our mock LLM call, simulate parsing data, and create a trace\n",
        "@weave.op()\n",
        "def extract_fruit(sentence: str) -> dict:\n",
        "    model = \"gpt-4o\"\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Parse sentences into a JSON dict with keys: fruit, color and flavor.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": sentence\n",
        "        }\n",
        "    ]\n",
        "    response_format = {\n",
        "        \"type\": \"json_object\"\n",
        "    }\n",
        "    temperature = 0.7\n",
        "    llm_response = mock_openai_call(\n",
        "        messages=messages,\n",
        "        model=model,\n",
        "        response_format=response_format,\n",
        "        temperature=temperature\n",
        "    )\n",
        "    response_content = llm_response['choices'][0]['message']['content']\n",
        "    parsed_response = json.loads(response_content)\n",
        "    time.sleep(1)\n",
        "    return parsed_response\n",
        "\n",
        "# ‚ñ∂Ô∏è Run the example\n",
        "sentence = \"There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy.\"\n",
        "extract_fruit(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try tracking a real-life LLM request using OpenAI\n",
        "\n",
        "Your can find your OpenAI API key in your [OpenAI platform dashboard](https://platform.openai.com/api-keys)."
      ],
      "metadata": {
        "id": "gmGLqX8PFtMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üîë Setup your OpenAI API key\n",
        "# Running this cell will prompt you for your API key with `getpass` and will not echo to the terminal.\n",
        "print(\"---\")\n",
        "print(\"You can generate your OpenAI API key here: https://platform.openai.com/api-keys\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key and hit [ENTER]: \")\n",
        "print(\"---\")\n",
        "\n",
        "# üêù Decorator to trace your LLM call\n",
        "@weave.op()\n",
        "def extract_fruit(sentence: str) -> dict:\n",
        "    client = OpenAI()\n",
        "    system_prompt = (\n",
        "        \"Parse sentences into a JSON dict with keys: fruit, color and flavor.\"\n",
        "    )\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": sentence},\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "    )\n",
        "    extracted = response.choices[0].message.content\n",
        "    return json.loads(extracted)\n",
        "\n",
        "# ‚ñ∂Ô∏è Run the example\n",
        "sentence = \"There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy.\"\n",
        "extract_fruit(sentence)"
      ],
      "metadata": {
        "id": "wC2n2RNmD-31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try an agentic request using OpenAI and an agent-loop\n",
        "In this example we'll create a simple deterministic agent to take an input from a user and write a science fiction story based on it.\\\n",
        "The agent will write an outline, review the outline for quality (and retry as needed), and then write a story based on that outline."
      ],
      "metadata": {
        "id": "wpE1DuGyX7qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OutlineCheckerOutput(BaseModel):\n",
        "    good_quality: bool\n",
        "    is_scifi: bool\n",
        "\n",
        "# üêù Step 1: Generate a story outline\n",
        "@weave.op()\n",
        "def generate_story_outline(input_prompt: str) -> str:\n",
        "    client = OpenAI()\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Generate a very short story outline based on the user's input.\"},\n",
        "            {\"role\": \"user\", \"content\": input_prompt}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# üêù Step 2: Review and check story outline\n",
        "@weave.op()\n",
        "def check_outline(outline: str) -> OutlineCheckerOutput:\n",
        "    client = OpenAI()\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Read the given story outline, and judge the quality. Also, determine if it is a scifi story. Respond with a JSON object with two boolean fields: good_quality and is_scifi.\"},\n",
        "            {\"role\": \"user\", \"content\": outline}\n",
        "        ],\n",
        "        temperature=0.2,\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "    )\n",
        "    result = json.loads(response.choices[0].message.content)\n",
        "    return OutlineCheckerOutput(good_quality=result[\"good_quality\"], is_scifi=result[\"is_scifi\"])\n",
        "\n",
        "# üêù Step 3: Write a story based on the story outline\n",
        "@weave.op()\n",
        "def write_story(outline: str) -> str:\n",
        "    client = OpenAI()\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Write a short story based on the given outline.\"},\n",
        "            {\"role\": \"user\", \"content\": outline}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# üêù Main function: Orchestrating the loop, will retry a maximum of 5 times before throwing an error.\n",
        "@weave.op()\n",
        "def story_writer_agent(input_prompt: str, max_retries: int = 5):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        outline = generate_story_outline(input_prompt)\n",
        "        check_result = check_outline(outline)\n",
        "\n",
        "        if check_result.good_quality and check_result.is_scifi:\n",
        "            story = write_story(outline)\n",
        "            return story\n",
        "\n",
        "        retries += 1\n",
        "\n",
        "    raise RuntimeError(\"Failed to generate a good sci-fi outline after several tries.\")\n",
        "\n",
        "# ‚ñ∂Ô∏è Run the example\n",
        "input_prompt = \"A story about a futuristic city where robots help humans.\"\n",
        "story_writer_agent(input_prompt)"
      ],
      "metadata": {
        "id": "VuEipbu8Q544"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGqeyYMmw7Hl"
      },
      "source": [
        "## üöÄ Looking for more examples?\n",
        "- Check out the [Quickstart guide](https://weave-docs.wandb.ai/quickstart).\n",
        "- Learn more about [advanced tracing topics](https://weave-docs.wandb.ai/tutorial-tracing_2).\n",
        "- Learn more about [tracing in Weave](https://weave-docs.wandb.ai/guides/tracking/tracing)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}