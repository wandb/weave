{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- docusaurus_head_meta::start\n",
    "---\n",
    "title: Evaluating with Production Data and Expert Feedback\n",
    "---\n",
    "docusaurus_head_meta::end -->\n",
    "\n",
    "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "<!--- @wandbcode{cod-notebook} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating with Production Data and Expert Feedback\n",
    "\n",
    "Once we gathered test data from actual users (either in internal test environments or from test groups in production) typically the next step is to create an evaluation dataset in which we can run systematic benchmarks. \n",
    "\n",
    "In this tutorial, we'll continue on our RAG Chatbot example and extract questions and answers from our production data that were rated as negative by the user. We'll then expose this data to experts to correct our models answer and feed this data back into a new evaluation dataset. To do collect the production and to evaluate the performance we'll use W&B Weave and for the annotation we'll use Streamlit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup\n",
    "To follow along this tutorial you'll need to install the following packages. We will refer to the following Weave workspace where we created a `RagModel` from scratch based on a `faiss` vectorstore and tracked some interaction with user feedback (see previous cookbooks). \n",
    "\n",
    "Check out this [RAG Weave workspace](https://wandb.ai/wandb-smle/weave-cookboook-demo/weave/evaluations), [this codebase](https://github.com/NiWaRe/knowledge-worker-weave) on Github, and this [video explanation on Youtube](https://www.youtube.com/watch?v=EVJ1K3fyb0c) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install weave openai streamlit asyncio requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementation\n",
    "In order to create a golden dataset based on which we can systematically benchmark our models we have to: \n",
    "\n",
    "1. Collect relevant data from production\n",
    "2. Annotate data with expert feedback\n",
    "3. Run benchmarks on the annotated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Collect relevant data from production\n",
    "When we deploy our RAG Chatbot either to a control group in production or to a user group chances are high that we collect a lot of conversations between the chatbot and different users. Not all of them are actually relevant and to keep our evaluation as high-signal and efficient as possible we want to only gather the data that is relevant for our evaluation - in this example, only data that was rated as bad by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "\n",
    "# 1. Initialize the Weave client\n",
    "client = weave.init('wandb-smle/weave-cookboook-demo')\n",
    "\n",
    "# 2. Query the Weave database for all calls that were rated as bad by the user\n",
    "calls = client.feedback(reaction=\"ðŸ‘Ž\").refs().calls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Annotate production data with expert feedback\n",
    "Now that we have the questions, bad answers, and user comments we can expose this data to our experts and ask them to correct the answer. To do this we'll use Streamlit to create a simple annotation interface. Another popular open-source possibility is to use Argilla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/NiWaRe/knowledge-worker-weave/blob/master/screenshots/expert_annotation_ui.png?raw=true\" width=\"1000\" alt=\"Streamlmait annotation UI\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "\n",
    "@st.cache_resource\n",
    "def start_weave(entity: str, project_name: str):\n",
    "    return weave.init(entity + \"/\" + project_name)\n",
    "\n",
    "@st.cache_resource\n",
    "def assemble_feedback_data(entity:str, project_name: str) -> pd.DataFrame:\n",
    "    data = []\n",
    "    client = start_weave(entity, project_name)\n",
    "\n",
    "    # get feedback\n",
    "    thumbs_down = client.feedback(reaction=\"ðŸ‘Ž\")\n",
    "    calls = thumbs_down.refs().calls()\n",
    "\n",
    "\n",
    "    for call in calls:\n",
    "        last_reaction, last_comment = None, None\n",
    "        for f in call.feedback[::-1]:\n",
    "            if f.feedback_type == \"wandb.reaction.1\":\n",
    "                last_reaction = f.payload[\"emoji\"]\n",
    "            elif f.feedback_type == \"wandb.note.1\":\n",
    "                last_comment = f.payload[\"note\"]\n",
    "            if last_reaction and last_comment:\n",
    "                break\n",
    "\n",
    "        # NOTE: this can be easily customized based on the needed feedback structure\n",
    "        data.append({\n",
    "            # prediction - used as question and target answer and url in dataset\n",
    "            \"query\": call.inputs['example']['query'],\n",
    "            \"prediction\": call.output[\"model_output\"][\"result\"][\"content\"],\n",
    "            \"used_main_source\": call.output[\"model_output\"][\"source_documents\"][0][\"url\"],\n",
    "            # feedback - used to guide the annotation\n",
    "            \"feedback_reaction\": last_reaction,\n",
    "            \"feedback_comment\": last_comment,\n",
    "        })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# store the calls in a pandas DF\n",
    "weave_dataset_df = assemble_feedback_data(\"prod_team\", \"rag_project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're using Streamlit's power to create a simple annotation UI with a single line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edited_df = st.data_editor(weave_dataset_df, num_rows=\"dynamic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally save the new changes as a new version of the existing evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = st.text_input(\"Enter Existing Dataset NAME:VERSION\", \"gen_eval_dataset:latest\")\n",
    "dataset_name = dataset_name.split(\":\")[0]\n",
    "\n",
    "# get the exisiting dataset as a list of dictionaries\n",
    "rows = [dict(elem) for elem in weave.ref(dataset_name).get().rows]\n",
    "\n",
    "# add the newly annotated production ðŸ‘Ž calls to the existing dataset\n",
    "for elem in edited_df.to_dict(orient=\"records\"):\n",
    "    rows.append(\n",
    "        {\n",
    "            \"query\": elem[\"query\"], \n",
    "            \"answer\": elem[\"prediction\"], \n",
    "            \"main_source\": elem[\"used_main_source\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "# update the dataset with the new rows\n",
    "if st.button(\"Update Dataset\"):\n",
    "    dataset = weave.Dataset(\n",
    "        name=dataset_name, \n",
    "        rows=rows,\n",
    "    )\n",
    "    weave.publish(dataset)\n",
    "    st.success(\"Successfully updated data to Weave!\", icon=\"âœ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Run evaluation with new data\n",
    "After gathering new ðŸ‘Ž feedback and annotating it, we can now run our evaluation code again. This time we will use the new version of the evaluation dataset and use the comparison feature to understand how the impact of ðŸ‘Ž calls impacted the model performance. Of course it also makes sense to include positive annotated calls into the dataset to balance evaluation dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we added three new rows compared to our previous version - see the actual dataset [here](https://wandb.ai/wandb-smle/weave-cookboook-demo/weave/object-versions?filter=%7B%22objectName%22%3A%22gen_eval_dataset%22%7D&peekPath=%2Fwandb-smle%2Fweave-cookboook-demo%2Fobjects%2Fgen_eval_dataset%2Fversions%2F7PrGXU1xmpgMyd15zcMuWRXvO0lqV6tqNhMYm9mnZRw%3F%26).\n",
    "\n",
    "<img src=\"https://github.com/NiWaRe/knowledge-worker-weave/blob/master/screenshots/new_annotated_dataset_weave.png?raw=true\" width=\"1000\" alt=\"New annotated prod dataset in Weave\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this new version of our evaluation dataset we can easily run a new evaluation. This will make the evaluation results more representative for your use-case and user-group in production. \n",
    "\n",
    "To make sure that you are aware of what version of the dataset you used to calculate the metrics Weave will also let you know whether you are comparing two models with different metrics or evaluation datasets - see the \"Dataset inconsistency detected\". For more information on the evaluation workflow see the [Evaluation](./tutorial-eval.md) tutorial for more details.\n",
    "\n",
    "<img src=\"https://github.com/NiWaRe/knowledge-worker-weave/blob/master/screenshots/weave_dataset_inconsistency.png?raw=true\" width=\"1000\" alt=\"New annotated prod dataset in Weave\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this cookbook we learned how to effectively collected feedback from users and experts to improve on the evaluation dataset to make it more representative for your use-case and user-group in production. We explain how to use Weave to collect specific data from production, let experts annotate it, and then create a new evaluation dataset to evaluate on.\n",
    "\n",
    "It is as important to continously iterate on your evaluation pipeline as it is to iterate on your actual LLM model - with Weave is much easier to track calls in production, to add variuos types of feedback, and to systematically improve the evaluation pipeline.\n",
    "\n",
    "Give it a try today and run the attached code to build your own RAG Chatbot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
