{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- docusaurus_head_meta::start\n",
    "---\n",
    "title: Leaderboard Quickstart\n",
    "---\n",
    "docusaurus_head_meta::end -->\n",
    "\n",
    "<!--- @wandbcode{leaderboard-demo} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaderboard Quickstart\n",
    "\n",
    "In this notebook we will learn to use Weave's Leaderboard to compare model performance across different datasets and scoring functions. Specifically, we will:\n",
    "\n",
    "1. Generate a dataset of fake zip code data\n",
    "2. Author some scoring functions and evaluate a baseline model.\n",
    "3. Use these techniques to evaluate a matrix of models vs evaluations.\n",
    "4. Review the leaderboard in the Weave UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate a dataset of fake zip code data\n",
    "\n",
    "First we will create a function `generate_dataset_rows` that generates a list of fake zip code data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Row(BaseModel):\n",
    "    zip_code: str\n",
    "    city: str\n",
    "    state: str\n",
    "    avg_temp_f: float\n",
    "    population: int\n",
    "    median_income: int\n",
    "    known_for: str\n",
    "\n",
    "\n",
    "class Rows(BaseModel):\n",
    "    rows: list[Row]\n",
    "\n",
    "\n",
    "def generate_dataset_rows(\n",
    "    location: str = \"United States\", count: int = 5, year: int = 2022\n",
    "):\n",
    "    client = OpenAI()\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Please generate {count} rows of data for random zip codes in {location} for the year {year}.\",\n",
    "            },\n",
    "        ],\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"response_format\",\n",
    "                \"schema\": Rows.model_json_schema(),\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return json.loads(completion.choices[0].message.content)[\"rows\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "\n",
    "weave.init(\"leaderboard-demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Author scoring functions\n",
    "\n",
    "Next we will author 3 scoring functions:\n",
    "\n",
    "1. `check_concrete_fields`: Checks if the model output matches the expected city and state.\n",
    "2. `check_value_fields`: Checks if the model output is within 10% of the expected population and median income.\n",
    "3. `check_subjective_fields`: Uses a LLM to check if the model output matches the expected \"known for\" field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "def check_concrete_fields(city: str, state: str, output: dict):\n",
    "    return {\n",
    "        \"city_match\": city == output[\"city\"],\n",
    "        \"state_match\": state == output[\"state\"],\n",
    "    }\n",
    "\n",
    "\n",
    "@weave.op\n",
    "def check_value_fields(\n",
    "    avg_temp_f: float, population: int, median_income: int, output: dict\n",
    "):\n",
    "    return {\n",
    "        \"avg_temp_f_err\": abs(avg_temp_f - output[\"avg_temp_f\"]) / avg_temp_f,\n",
    "        \"population_err\": abs(population - output[\"population\"]) / population,\n",
    "        \"median_income_err\": abs(median_income - output[\"median_income\"])\n",
    "        / median_income,\n",
    "    }\n",
    "\n",
    "\n",
    "@weave.op\n",
    "def check_subjective_fields(zip_code: str, known_for: str, output: dict):\n",
    "    client = OpenAI()\n",
    "\n",
    "    class Response(BaseModel):\n",
    "        correct_known_for: bool\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"My student was asked what the zip code {zip_code} is best known best for. The right answer is '{known_for}', and they said '{output['known_for']}'. Is their answer correct?\",\n",
    "            },\n",
    "        ],\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"response_format\",\n",
    "                \"schema\": Response.model_json_schema(),\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return json.loads(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create a simple Evaluation\n",
    "\n",
    "Next we define a simple evaliation using our fake data and scoring functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = generate_dataset_rows()\n",
    "evaluation = weave.Evaluation(\n",
    "    name=\"United States - 2022\",\n",
    "    dataset=rows,\n",
    "    scorers=[\n",
    "        check_concrete_fields,\n",
    "        check_value_fields,\n",
    "        check_subjective_fields,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate a baseline model\n",
    "\n",
    "Now we will evaluate a baseline model which returns a static response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "def baseline_model(zip_code: str):\n",
    "    return {\n",
    "        \"city\": \"New York\",\n",
    "        \"state\": \"NY\",\n",
    "        \"avg_temp_f\": 50.0,\n",
    "        \"population\": 1000000,\n",
    "        \"median_income\": 100000,\n",
    "        \"known_for\": \"The Big Apple\",\n",
    "    }\n",
    "\n",
    "\n",
    "await evaluation.evaluate(baseline_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create more Models\n",
    "\n",
    "Now we will create 2 more models to compare against the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "def gpt_4o_mini_no_context(zip_code: str):\n",
    "    client = OpenAI()\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"\"\"Zip code {zip_code}\"\"\"}],\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"response_format\",\n",
    "                \"schema\": Row.model_json_schema(),\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return json.loads(completion.choices[0].message.content)\n",
    "\n",
    "\n",
    "await evaluation.evaluate(gpt_4o_mini_no_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "def gpt_4o_mini_with_context(zip_code: str):\n",
    "    client = OpenAI()\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"Please answer the following questions about the zip code {zip_code}:\n",
    "                   1. What is the city?\n",
    "                   2. What is the state?\n",
    "                   3. What is the average temperature in Fahrenheit?\n",
    "                   4. What is the population?\n",
    "                   5. What is the median income?\n",
    "                   6. What is the most well known thing about this zip code?\n",
    "                   \"\"\",\n",
    "            }\n",
    "        ],\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"response_format\",\n",
    "                \"schema\": Row.model_json_schema(),\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return json.loads(completion.choices[0].message.content)\n",
    "\n",
    "\n",
    "await evaluation.evaluate(gpt_4o_mini_with_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create more Evaluations\n",
    "\n",
    "Now we will evaluate a matrix of models vs evaluations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorers = [\n",
    "    check_concrete_fields,\n",
    "    check_value_fields,\n",
    "    check_subjective_fields,\n",
    "]\n",
    "evaluations = [\n",
    "    weave.Evaluation(\n",
    "        name=\"United States - 2022\",\n",
    "        dataset=weave.Dataset(\n",
    "            name=\"United States - 2022\",\n",
    "            rows=generate_dataset_rows(\"United States\", 5, 2022),\n",
    "        ),\n",
    "        scorers=scorers,\n",
    "    ),\n",
    "    weave.Evaluation(\n",
    "        name=\"California - 2022\",\n",
    "        dataset=weave.Dataset(\n",
    "            name=\"California - 2022\", rows=generate_dataset_rows(\"California\", 5, 2022)\n",
    "        ),\n",
    "        scorers=scorers,\n",
    "    ),\n",
    "    weave.Evaluation(\n",
    "        name=\"United States - 2000\",\n",
    "        dataset=weave.Dataset(\n",
    "            name=\"United States - 2000\",\n",
    "            rows=generate_dataset_rows(\"United States\", 5, 2000),\n",
    "        ),\n",
    "        scorers=scorers,\n",
    "    ),\n",
    "]\n",
    "models = [\n",
    "    baseline_model,\n",
    "    gpt_4o_mini_no_context,\n",
    "    gpt_4o_mini_with_context,\n",
    "]\n",
    "\n",
    "for evaluation in evaluations:\n",
    "    for model in models:\n",
    "        await evaluation.evaluate(\n",
    "            model, __weave={\"display_name\": evaluation.name + \":\" + model.__name__}\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Review the Leaderboard\n",
    "\n",
    "You can create a new leaderboard by navigating to the leaderboard tab in the UI and clicking \"Create Leaderboard\".\n",
    "\n",
    "We can also generate a leaderboard directly from Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Published to https://wandb.ai/timssweeney/leaderboard-demo/weave/leaderboards/Zip-Code-World-Knowledge\n"
     ]
    }
   ],
   "source": [
    "from weave.flow import leaderboard\n",
    "from weave.trace.weave_client import get_ref\n",
    "\n",
    "spec = leaderboard.Leaderboard(\n",
    "    name=\"Zip Code World Knowledge\",\n",
    "    description=\"\"\"\n",
    "This leaderboard compares the performance of models in terms of world knowledge about zip codes.\n",
    "\n",
    "### Columns\n",
    "\n",
    "1. **State Match against `United States - 2022`**: The fraction of zip codes that the model correctly identified the state for.\n",
    "2. **Avg Temp F Error against `California - 2022`**: The mean absolute error of the model's average temperature prediction.\n",
    "3. **Correct Known For against `United States - 2000`**: The fraction of zip codes that the model correctly identified the most well known thing about the zip code.\n",
    "\"\"\",\n",
    "    columns=[\n",
    "        leaderboard.LeaderboardColumn(\n",
    "            evaluation_object_ref=get_ref(evaluations[0]).uri(),\n",
    "            scorer_name=\"check_concrete_fields\",\n",
    "            summary_metric_path=\"state_match.true_fraction\",\n",
    "        ),\n",
    "        leaderboard.LeaderboardColumn(\n",
    "            evaluation_object_ref=get_ref(evaluations[1]).uri(),\n",
    "            scorer_name=\"check_value_fields\",\n",
    "            should_minimize=True,\n",
    "            summary_metric_path=\"avg_temp_f_err.mean\",\n",
    "        ),\n",
    "        leaderboard.LeaderboardColumn(\n",
    "            evaluation_object_ref=get_ref(evaluations[2]).uri(),\n",
    "            scorer_name=\"check_subjective_fields\",\n",
    "            summary_metric_path=\"correct_known_for.true_fraction\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "ref = weave.publish(spec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wandb-weave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
