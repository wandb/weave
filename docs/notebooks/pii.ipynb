{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- docusaurus_head_meta::start\n",
    "---\n",
    "title: Handling and Redacting PII\n",
    "---\n",
    "docusaurus_head_meta::end -->\n",
    "\n",
    "<!--- @wandbcode{cod-notebook} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m752k2fWKDql"
   },
   "source": [
    "# How to use Weave with PII data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C70egOGRLCgm"
   },
   "source": [
    "In this guide, you'll learn how to use W&B Weave while ensuring your Personally Identifiable Information (PII) data remains private. The guide demonstrates the following methods to identify, redact and anonymize PII data:\n",
    "\n",
    "1. __Regular expressions__ to identify PII data and redact it.\n",
    "2. __Microsoft's [Presidio](https://microsoft.github.io/presidio/)__, a python-based data protection SDK. This tool provides redaction and replacement functionalities.\n",
    "3. __[Faker](https://faker.readthedocs.io/en/master/)__, a Python library to generate fake data, combined with Presidio to anonymize PII data.\n",
    "\n",
    "Additionally, you'll learn how to use _`weave.op` input/output logging customization_ and _`autopatch_settings`_ to integrate PII redaction and anonymization into the workflow. For more information, see [Customize logged inputs and outputs](https://weave-docs.wandb.ai/guides/tracking/ops/#customize-logged-inputs-and-outputs).\n",
    "\n",
    "To get started, do the following:\n",
    "\n",
    "1. Review the [Overview](#overview) section.\n",
    "2. Complete the [prerequisites](#prerequisites).\n",
    "3. Review the [available methods](#redaction-methods-overview) for identifying, redacting and anonymizing PII data.\n",
    "4. [Apply the methods to Weave calls](#apply-the-methods-to-weave-calls)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "\n",
    "The following section provides an overview of input and output logging using `weave.op`, as well as best practices for working with PII data in Weave.\n",
    "\n",
    "### Customize input and output logging using `weave.op`\n",
    "\n",
    "Weave Ops allow you to define input and output postprocessing functions. Using these functions, you can modify the data that is passed to your LLM call or logged to Weave.\n",
    "\n",
    "In the following example, two postprocessing functions are defined and passed as arguments to `weave.op()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "\n",
    "import weave\n",
    "\n",
    "# Inputs Wrapper Class\n",
    "@dataclass\n",
    "class CustomObject:\n",
    "    x: int\n",
    "    secret_password: str\n",
    "\n",
    "# First we define functions for input and output postprocessing:\n",
    "def postprocess_inputs(inputs: dict[str, Any]) -> dict[str, Any]:\n",
    "    return {k:v for k,v in inputs.items() if k != \"hide_me\"}\n",
    "\n",
    "def postprocess_output(output: CustomObject) -> CustomObject:\n",
    "    return CustomObject(x=output.x, secret_password=\"REDACTED\")\n",
    "\n",
    "# Then, when we use the `@weave.op` decorator, we pass these processing functions as arguments to the decorator:\n",
    "@weave.op(\n",
    "    postprocess_inputs=postprocess_inputs,\n",
    "    postprocess_output=postprocess_output,\n",
    ")\n",
    "def some_llm_call(a: int, hide_me: str) -> CustomObject:\n",
    "    return CustomObject(x=a, secret_password=hide_me)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best practices for using Weave with PII data \n",
    "\n",
    "Before using Weave with PII data, review the best practices for using Weave with PII data.\n",
    "\n",
    "#### During testing\n",
    "- Log anonymized data to check PII detection\n",
    "- Track PII handling processes with Weave Traces\n",
    "- Measure anonymization performance without exposing real PII\n",
    "\n",
    "#### In production\n",
    "- Never log raw PII\n",
    "- Encrypt sensitive fields before logging\n",
    "\n",
    "#### Encryption tips\n",
    "- Use reversible encryption for data you need to decrypt later\n",
    "- Apply one-way hashing for unique IDs you don't need to reverse\n",
    "- Consider specialized encryption for data you need to analyze while encrypted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. First, install the required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# @title required python packages:\n",
    "!pip install cryptography\n",
    "!pip install presidio_analyzer\n",
    "!pip install presidio_anonymizer\n",
    "!python -m spacy download en_core_web_lg    # Presidio uses spacy NLP engine\n",
    "!pip install Faker                          # we'll use Faker to replace PII data with fake data\n",
    "!pip install weave                          # To leverage Traces\n",
    "!pip install set-env-colab-kaggle-dotenv -q # for env var\n",
    "!pip install anthropic                      # to use sonnet\n",
    "!pip install cryptography                   # to encrypt our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Set up your API keys. You can find your API keys at the following links.\n",
    "\n",
    "   - [W&B](https://wandb.ai/authorize)\n",
    "   - [Anthropic](https://console.anthropic.com/settings/keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# @title Make sure to set up set up your API keys correctly\n",
    "# See: https://pypi.org/project/set-env-colab-kaggle-dotenv/ for usage instructions.\n",
    "\n",
    "from set_env import set_env\n",
    "\n",
    "_ = set_env(\"ANTHROPIC_API_KEY\")\n",
    "_ = set_env(\"WANDB_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Initialize your Weave project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<weave.trace.weave_client.WeaveClient at 0x14a0a63c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import weave\n",
    "\n",
    "# Start a new Weave project\n",
    "WEAVE_PROJECT = \"pii_cookbook\"\n",
    "weave.init(WEAVE_PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Load the demo PII dataset, which contains 10 text blocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PII data first sample: \"I remember the day like it was yesterday. I was a waitress at a busy restaurant in New York City, Mohammed Gross is the name. The place was packed, and I was running around like a crazy person, trying to keep up with the demand. Suddenly, I noticed a customer sitting at one of my tables. He looked very upset. I went over to see what was wrong. \"My food is cold,\" he said. \"I've been waiting for it for over an hour.\" I apologized and asked him if he would like me to get him a new meal. He said yes, so I went back to the kitchen and told the chef. The chef was very apologetic and made the customer a new meal right away. I brought it out to the customer and he seemed satisfied. \"Thank you,\" he said. \"That's much better.\" I was glad that I was able to resolve the situation and make the customer happy. It's always rewarding to be able to help people, and it's one of the things I love about my job. I believe that customer service is very important, and I always try to go the extra mile for my customers. If you're ever in New York City, be sure to stop by the restaurant and ask for Mohammed Gross. I'll be happy to serve you! Feel free to email me at mohammedgross@msn.edu if you have any questions or concerns. You can also reach me by mail at 8202 Dudley Way, [City, State, ZIP].\"\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/wandb/weave/master/docs/notebooks/10_pii_data.json\"\n",
    "response = requests.get(url)\n",
    "pii_data = response.json()\n",
    "\n",
    "print('PII data first sample: \"' + pii_data[0][\"text\"] + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redaction methods overview\n",
    "\n",
    "Once you've completed the [setup](#setup), you can \n",
    "\n",
    "To detect and protect our PII data, we'll identify and redact PII data and optionally anonymize it using the following methods:\n",
    "\n",
    "1. __Regular expressions__ to identify PII data and redact it.\n",
    "2. __Microsoft [Presidio](https://microsoft.github.io/presidio/)__, a Python-based data protection SDK that provides redaction and replacement functionality.\n",
    "3. __[Faker](https://faker.readthedocs.io/en/master/)__, a Python library for generating fake data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Filter using regular expressions\n",
    "\n",
    "[Regular expressions (regex)](https://docs.python.org/3/library/re.html) are the simplest method to identify and redact PII data. Regex allows you to define patterns that can match various formats of sensitive information like phone numbers, email addresses, and social security numbers. Using regex, you can scan through large volumes of text and replace or redact information without the need for more complex NLP techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "# Define a function to clean PII data using regex\n",
    "def redact_with_regex(text):\n",
    "    # Phone number pattern\n",
    "    # \\b         : Word boundary\n",
    "    # \\d{3}      : Exactly 3 digits\n",
    "    # [-.]?      : Optional hyphen or dot\n",
    "    # \\d{3}      : Another 3 digits\n",
    "    # [-.]?      : Optional hyphen or dot\n",
    "    # \\d{4}      : Exactly 4 digits\n",
    "    # \\b         : Word boundary\n",
    "    text = re.sub(r\"\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b\", \"<PHONE>\", text)\n",
    "\n",
    "    # Email pattern\n",
    "    # \\b         : Word boundary\n",
    "    # [A-Za-z0-9._%+-]+ : One or more characters that can be in an email username\n",
    "    # @          : Literal @ symbol\n",
    "    # [A-Za-z0-9.-]+ : One or more characters that can be in a domain name\n",
    "    # \\.         : Literal dot\n",
    "    # [A-Z|a-z]{2,} : Two or more uppercase or lowercase letters (TLD)\n",
    "    # \\b         : Word boundary\n",
    "    text = re.sub(\n",
    "        r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\", \"<EMAIL>\", text\n",
    "    )\n",
    "\n",
    "    # SSN pattern\n",
    "    # \\b         : Word boundary\n",
    "    # \\d{3}      : Exactly 3 digits\n",
    "    # -          : Literal hyphen\n",
    "    # \\d{2}      : Exactly 2 digits\n",
    "    # -          : Literal hyphen\n",
    "    # \\d{4}      : Exactly 4 digits\n",
    "    # \\b         : Word boundary\n",
    "    text = re.sub(r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\", \"<SSN>\", text)\n",
    "\n",
    "    # Simple name pattern (this is not comprehensive)\n",
    "    # \\b         : Word boundary\n",
    "    # [A-Z]      : One uppercase letter\n",
    "    # [a-z]+     : One or more lowercase letters\n",
    "    # \\s         : One whitespace character\n",
    "    # [A-Z]      : One uppercase letter\n",
    "    # [a-z]+     : One or more lowercase letters\n",
    "    # \\b         : Word boundary\n",
    "    text = re.sub(r\"\\b[A-Z][a-z]+ [A-Z][a-z]+\\b\", \"<NAME>\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the function with a sample text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text:\n",
      "\tMy name is John Doe, my email is john.doe@example.com, my phone is 123-456-7890, and my SSN is 123-45-6789.\n",
      "Redacted text:\n",
      "\tMy name is <NAME>, my email is <EMAIL>, my phone is <PHONE>, and my SSN is <SSN>.\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "test_text = \"My name is John Doe, my email is john.doe@example.com, my phone is 123-456-7890, and my SSN is 123-45-6789.\"\n",
    "cleaned_text = redact_with_regex(test_text)\n",
    "print(f\"Raw text:\\n\\t{test_text}\")\n",
    "print(f\"Redacted text:\\n\\t{cleaned_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Redact using Microsoft Presidio \n",
    "The next method involves complete removal of PII data using [Microsoft Presidio](https://microsoft.github.io/presidio/). Presidio redacts PII and replaces it with a placeholder representing the PII type. For example, Presidio replaces `Alex` in `\"My name is Alex\"` with `<PERSON>`.\n",
    "\n",
    "Presidio comes with a built-in support for [common entities](https://microsoft.github.io/presidio/supported_entities/). In the below example, we redact all entities that are a `PHONE_NUMBER`, `PERSON`, `LOCATION`, `EMAIL_ADDRESS` or `US_SSN`. The Presidio process is encapsulated in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Bh6MgL3g6sc2"
   },
   "outputs": [],
   "source": [
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "\n",
    "# Set up the Analyzer, which loads an NLP module (spaCy model by default) and other PII recognizers.\n",
    "analyzer = AnalyzerEngine()\n",
    "\n",
    "# Set up the Anonymizer, which will use the analyzer results to anonymize the text.\n",
    "anonymizer = AnonymizerEngine()\n",
    "\n",
    "\n",
    "# Encapsulate the Presidio redaction process into a function\n",
    "def redact_with_presidio(text):\n",
    "    # Analyze the text to identify PII data\n",
    "    results = analyzer.analyze(\n",
    "        text=text,\n",
    "        entities=[\"PHONE_NUMBER\", \"PERSON\", \"LOCATION\", \"EMAIL_ADDRESS\", \"US_SSN\"],\n",
    "        language=\"en\",\n",
    "    )\n",
    "    # Anonymize the identified PII data\n",
    "    anonymized_text = anonymizer.anonymize(text=text, analyzer_results=results)\n",
    "    return anonymized_text.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the function with a sample text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text:\n",
      "\tMy phone number is 212-555-5555 and my name is alex\n",
      "Redacted text:\n",
      "\tMy phone number is <PHONE_NUMBER> and my name is <PERSON>\n"
     ]
    }
   ],
   "source": [
    "text = \"My phone number is 212-555-5555 and my name is alex\"\n",
    "\n",
    "# Test the function\n",
    "anonymized_text = redact_with_presidio(text)\n",
    "\n",
    "print(f\"Raw text:\\n\\t{text}\")\n",
    "print(f\"Redacted text:\\n\\t{anonymized_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: Anonymize with replacement using Faker and Presidio\n",
    "\n",
    "Instead of redacting text, you can anonymize it by using MS Presidio to swap PII like names and phone numbers with fake data generated using the [Faker](https://faker.readthedocs.io/en/master/) Python library. For example, suppose you have the following data:\n",
    "\n",
    "`\"My name is Raphael and I like to fish. My phone number is 212-555-5555\"` \n",
    "\n",
    "Once the data has been processed using Presidio and Faker, it might look like:\n",
    "\n",
    "`\"My name is Katherine Dixon and I like to fish. My phone number is 667.431.7379\"`\n",
    "\n",
    "To effectively use Presidio and Faker together, we must supply references to our custom operators. These operators will direct Presidio to the Faker functions responsible for swapping PII with fake data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "3XJa7u5T_WYd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text:\n",
      "\tMy name is Raphael and I like to fish. My phone number is 212-555-5555\n",
      "Anonymized text:\n",
      "\tMy name is Jennifer Waters and I like to fish. My phone number is 8007771735\n"
     ]
    }
   ],
   "source": [
    "from faker import Faker\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_anonymizer.entities import OperatorConfig\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "\n",
    "# Create faker functions (note that it has to receive a value)\n",
    "def fake_name(x):\n",
    "    return fake.name()\n",
    "\n",
    "\n",
    "def fake_number(x):\n",
    "    return fake.phone_number()\n",
    "\n",
    "\n",
    "# Create custom operator for the PERSON and PHONE_NUMBER\" entities\n",
    "operators = {\n",
    "    \"PERSON\": OperatorConfig(\"custom\", {\"lambda\": fake_name}),\n",
    "    \"PHONE_NUMBER\": OperatorConfig(\"custom\", {\"lambda\": fake_number}),\n",
    "}\n",
    "\n",
    "text_to_anonymize = (\n",
    "    \"My name is Raphael and I like to fish. My phone number is 212-555-5555\"\n",
    ")\n",
    "\n",
    "# Analyzer output\n",
    "analyzer_results = analyzer.analyze(\n",
    "    text=text_to_anonymize, entities=[\"PHONE_NUMBER\", \"PERSON\"], language=\"en\"\n",
    ")\n",
    "\n",
    "anonymizer = AnonymizerEngine()\n",
    "\n",
    "# do not forget to pass the operators from above to the anonymizer\n",
    "anonymized_results = anonymizer.anonymize(\n",
    "    text=text_to_anonymize, analyzer_results=analyzer_results, operators=operators\n",
    ")\n",
    "\n",
    "print(f\"Raw text:\\n\\t{text_to_anonymize}\")\n",
    "print(f\"Anonymized text:\\n\\t{anonymized_results.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4HJcskpSYdL"
   },
   "source": [
    "Let's consolidate our code into a single class and expand the list of entities to include the additional ones identified earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "yIaharNAWGh5"
   },
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_anonymizer.entities import OperatorConfig\n",
    "\n",
    "\n",
    "# A custom class for generating fake data that extends Faker\n",
    "class my_faker(Faker):\n",
    "    # Create faker functions (note that it has to receive a value)\n",
    "    def fake_address(x):\n",
    "        return fake.address()\n",
    "\n",
    "    def fake_ssn(x):\n",
    "        return fake.ssn()\n",
    "\n",
    "    def fake_name(x):\n",
    "        return fake.name()\n",
    "\n",
    "    def fake_number(x):\n",
    "        return fake.phone_number()\n",
    "\n",
    "    def fake_email(x):\n",
    "        return fake.email()\n",
    "\n",
    "    # Create custom operators for the entities\n",
    "    operators = {\n",
    "        \"PERSON\": OperatorConfig(\"custom\", {\"lambda\": fake_name}),\n",
    "        \"PHONE_NUMBER\": OperatorConfig(\"custom\", {\"lambda\": fake_number}),\n",
    "        \"EMAIL_ADDRESS\": OperatorConfig(\"custom\", {\"lambda\": fake_email}),\n",
    "        \"LOCATION\": OperatorConfig(\"custom\", {\"lambda\": fake_address}),\n",
    "        \"US_SSN\": OperatorConfig(\"custom\", {\"lambda\": fake_ssn}),\n",
    "    }\n",
    "\n",
    "    def redact_and_anonymize_with_faker(self, text):\n",
    "        anonymizer = AnonymizerEngine()\n",
    "        analyzer_results = analyzer.analyze(\n",
    "            text=text,\n",
    "            entities=[\"PHONE_NUMBER\", \"PERSON\", \"LOCATION\", \"EMAIL_ADDRESS\", \"US_SSN\"],\n",
    "            language=\"en\",\n",
    "        )\n",
    "        anonymized_results = anonymizer.anonymize(\n",
    "            text=text, analyzer_results=analyzer_results, operators=self.operators\n",
    "        )\n",
    "        return anonymized_results.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the function with a sample text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text:\n",
      "\tMy name is Raphael and I like to fish. My phone number is 212-555-5555\n",
      "Anonymized text:\n",
      "\tMy name is Amy Adkins and I like to fish. My phone number is +1-481-464-7524x69850\n"
     ]
    }
   ],
   "source": [
    "faker = my_faker()\n",
    "text_to_anonymize = (\n",
    "    \"My name is Raphael and I like to fish. My phone number is 212-555-5555\"\n",
    ")\n",
    "anonymized_text = faker.redact_and_anonymize_with_faker(text_to_anonymize)\n",
    "\n",
    "print(f\"Raw text:\\n\\t{text_to_anonymize}\")\n",
    "print(f\"Anonymized text:\\n\\t{anonymized_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 4: Use `autopatch_settings` \n",
    "\n",
    "You can use `autopatch_settings` to configure PII handling directly during initialization for one or more of the supported LLM integrations. The advantages of this method are:\n",
    "\n",
    "1. PII handling logic is centralized and scoped at initialization, reducing the need for scattered custom logic.\n",
    "2. PII processing workflows can be customized or disabled entirely for specific intergations.\n",
    "\n",
    "To use `autopatch_settings` to configure PII handling, define `postprocess_inputs` and/or `postprocess_output` in `op_settings` for any one of the supported LLM integrations. \n",
    "\n",
    "```python \n",
    "\n",
    "def postprocess(inputs: dict) -> dict:\n",
    "    if \"SENSITIVE_KEY\" in inputs:\n",
    "        inputs[\"SENSITIVE_KEY\"] = \"REDACTED\"\n",
    "    return inputs\n",
    "\n",
    "client = weave.init(\n",
    "    ...,\n",
    "    autopatch_settings={\n",
    "        \"openai\": {\n",
    "            \"op_settings\": {\n",
    "                \"postprocess_inputs\": postprocess,\n",
    "                \"postprocess_output\": ...,\n",
    "            }\n",
    "        },\n",
    "        \"anthropic\": {\n",
    "            \"op_settings\": {\n",
    "                \"postprocess_inputs\": ...,\n",
    "                \"postprocess_output\": ...,\n",
    "            }\n",
    "        }\n",
    "    },\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the methods to Weave calls\n",
    "\n",
    "In the following examples, we will integrate our PII redaction and anonymization methods into Weave Models and preview the results in Weave Traces.\n",
    "\n",
    "First, we'll create a [Weave Model](https://wandb.github.io/weave/guides/core-types/models). A Weave Model is a combination of information like configuration settings, model weights, and code that defines how the model operates. \n",
    "\n",
    "In our model, we will include our predict function where the Anthropic API will be called. Anthropic's Claude Sonnet is used to perform sentiment analysis while tracing LLM calls using [Traces](https://wandb.github.io/weave/quickstart). Claude Sonnet will receive a block of text and output one of the following sentiment classifications: _positive_, _negative_, or _neutral_. Additionally, we will include our postprocessing functions to ensure that our PII data is redacted or anonymized before it is sent to the LLM.\n",
    "\n",
    "Once you run this code, you will receive a links to the Weave project page, as well as the specific trace (LLM calls) you ran."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex method \n",
    "\n",
    "In the simplest case, we can use regex to identify and redact PII data from the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any\n",
    "\n",
    "import anthropic\n",
    "\n",
    "import weave\n",
    "\n",
    "\n",
    "# Define an input postprocessing function that applies our regex redaction for the model prediction Weave Op\n",
    "def postprocess_inputs_regex(inputs: dict[str, Any]) -> dict:\n",
    "    inputs[\"text_block\"] = redact_with_regex(inputs[\"text_block\"])\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# Weave model / predict function\n",
    "class sentiment_analysis_regex_pii_model(weave.Model):\n",
    "    model_name: str\n",
    "    system_prompt: str\n",
    "    temperature: int\n",
    "\n",
    "    @weave.op(\n",
    "        postprocess_inputs=postprocess_inputs_regex,\n",
    "    )\n",
    "    async def predict(self, text_block: str) -> dict:\n",
    "        client = anthropic.AsyncAnthropic()\n",
    "        response = await client.messages.create(\n",
    "            max_tokens=1024,\n",
    "            model=self.model_name,\n",
    "            system=self.system_prompt,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": text_block}]}\n",
    "            ],\n",
    "        )\n",
    "        result = response.content[0].text\n",
    "        if result is None:\n",
    "            raise ValueError(\"No response from model\")\n",
    "        parsed = json.loads(result)\n",
    "        return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  sentiment_analysis_regex_pii_model(name='claude-3-sonnet', description=None, model_name='claude-3-5-sonnet-20240620', system_prompt='You are a Sentiment Analysis classifier. You will be classifying text based on their sentiment. Your input will be a block of text. You will answer with one the following rating option[\"positive\", \"negative\", \"neutral\"]. Your answer should be one word in json format: {classification}. Ensure that it is valid JSON.', temperature=0)\n",
      "游꼴 https://wandb.ai/wandb/pii_cookbook/r/call/01924a61-b36e-7f60-a856-6569856128c9\n",
      "游꼴 https://wandb.ai/wandb/pii_cookbook/r/call/01924a61-b9e3-7751-95e9-7949b34cd9c3\n",
      "游꼴 https://wandb.ai/wandb/pii_cookbook/r/call/01924a61-bd4c-72b3-bd33-3940cb2c3812\n",
      "游꼴 https://wandb.ai/wandb/pii_cookbook/r/call/01924a61-c0a8-7390-a5f4-3ed0767d2b87\n",
      "游꼴 https://wandb.ai/wandb/pii_cookbook/r/call/01924a61-c442-7fc2-9523-3e20f1e25dc0\n",
      "游꼴 https://wandb.ai/wandb/pii_cookbook/r/call/01924a61-c7d3-7a30-9975-29cab0ec0fd7\n",
      "游꼴 https://wandb.ai/wandb/pii_cookbook/r/call/01924a61-cb5a-76d2-a7ed-e8cd55a4f280\n",
      "游꼴 https://wandb.ai/wandb/pii_cookbook/r/call/01924a61-cef1-7220-852b-2d9a1e30e8ab\n",
      "游꼴 https://wandb.ai/wandb/pii_cookbook/r/call/01924a61-d284-7bc3-9181-d2284f291906\n",
      "游꼴 https://wandb.ai/wandb/pii_cookbook/r/call/01924a61-d637-7713-8fea-392f83ae8af1\n"
     ]
    }
   ],
   "source": [
    "# create our LLM model with a system prompt\n",
    "model = sentiment_analysis_regex_pii_model(\n",
    "    name=\"claude-3-sonnet\",\n",
    "    model_name=\"claude-3-5-sonnet-20240620\",\n",
    "    system_prompt='You are a Sentiment Analysis classifier. You will be classifying text based on their sentiment. Your input will be a block of text. You will answer with one the following rating option[\"positive\", \"negative\", \"neutral\"]. Your answer should be one word in json format: {classification}. Ensure that it is valid JSON.',\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(\"Model: \", model)\n",
    "# for every block of text, anonymized first and then predict\n",
    "for entry in pii_data:\n",
    "    await model.predict(entry[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presidio redaction method\n",
    "\n",
    "Next, we will use Presidio to identify and redact PII data from the original text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../../media/pii/redact.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any\n",
    "\n",
    "import anthropic\n",
    "\n",
    "import weave\n",
    "\n",
    "\n",
    "# Define an input postprocessing function that applies our Presidio redaction for the model prediction Weave Op\n",
    "def postprocess_inputs_presidio(inputs: dict[str, Any]) -> dict:\n",
    "    inputs[\"text_block\"] = redact_with_presidio(inputs[\"text_block\"])\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# Weave model / predict function\n",
    "class sentiment_analysis_presidio_pii_model(weave.Model):\n",
    "    model_name: str\n",
    "    system_prompt: str\n",
    "    temperature: int\n",
    "\n",
    "    @weave.op(\n",
    "        postprocess_inputs=postprocess_inputs_presidio,\n",
    "    )\n",
    "    async def predict(self, text_block: str) -> dict:\n",
    "        client = anthropic.AsyncAnthropic()\n",
    "        response = await client.messages.create(\n",
    "            max_tokens=1024,\n",
    "            model=self.model_name,\n",
    "            system=self.system_prompt,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": text_block}]}\n",
    "            ],\n",
    "        )\n",
    "        result = response.content[0].text\n",
    "        if result is None:\n",
    "            raise ValueError(\"No response from model\")\n",
    "        parsed = json.loads(result)\n",
    "        return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  sentiment_analysis_presidio_pii_model(name='claude-3-sonnet', description=None, model_name='claude-3-5-sonnet-20240620', system_prompt='You are a Sentiment Analysis classifier. You will be classifying text based on their sentiment. Your input will be a block of text. You will answer with one the following rating option[\"positive\", \"negative\", \"neutral\"]. Your answer should be one word in json format: {classification}. Ensure that it is valid JSON.', temperature=0)\n",
      "游꼴 https://wandb.ai/wandb/pii_cookbook/r/call/01924a61-dd6f-7851-8c99-18529ddc63f4\n",
      "游꼴 https://wandb.ai/wandb/pii_cookbook/r/call/01924a61-e17e-7cc3-a28f-7be155d87eb1\n",
      "游꼴 https://wandb.ai/wandb/pii_cookbook/r/call/01924a61-e538-76d3-a5a3-0e89bf4ff6b9\n",
      "游꼴 https://wandb.ai/wandb/pii_cookbook/r/call/01924a61-e8cf-7fb1-b942-11e5442dcf22\n",
      "游꼴 https://wandb.ai/wandb/pii_cookbook/r/call/01924a61-ef00-7793-8799-10ff5a32e129\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# for every block of text, anonymized first and then predict\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m pii_data:\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m model\u001b[38;5;241m.\u001b[39mpredict(entry[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/weave/trace/op.py:464\u001b[0m, in \u001b[0;36mop.<locals>.op_deco.<locals>.create_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m     log_once(\n\u001b[1;32m    460\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror,\n\u001b[1;32m    461\u001b[0m         ASYNC_CALL_CREATE_MSG\u001b[38;5;241m.\u001b[39mformat(traceback\u001b[38;5;241m.\u001b[39mformat_exc()),\n\u001b[1;32m    462\u001b[0m     )\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 464\u001b[0m res, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m _execute_call(wrapper, call, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/weave/trace/op.py:274\u001b[0m, in \u001b[0;36m_execute_call.<locals>._call_async\u001b[0;34m()\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_async\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Coroutine[Any, Any, Any]:\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_exception(e)\n",
      "Cell \u001b[0;32mIn[19], line 23\u001b[0m, in \u001b[0;36msentiment_analysis_presidio_pii_model.predict\u001b[0;34m(self, text_block)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;129m@weave\u001b[39m\u001b[38;5;241m.\u001b[39mop(\n\u001b[1;32m     19\u001b[0m     postprocess_inputs\u001b[38;5;241m=\u001b[39mpostprocess_inputs_presidio,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_block: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m     22\u001b[0m     client \u001b[38;5;241m=\u001b[39m anthropic\u001b[38;5;241m.\u001b[39mAsyncAnthropic()\n\u001b[0;32m---> 23\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m client\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     24\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m     25\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name,\n\u001b[1;32m     26\u001b[0m         system\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msystem_prompt,\n\u001b[1;32m     27\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     28\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: text_block}]}\n\u001b[1;32m     29\u001b[0m         ],\n\u001b[1;32m     30\u001b[0m     )\n\u001b[1;32m     31\u001b[0m     result \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mcontent[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/weave/trace/op.py:464\u001b[0m, in \u001b[0;36mop.<locals>.op_deco.<locals>.create_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m     log_once(\n\u001b[1;32m    460\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror,\n\u001b[1;32m    461\u001b[0m         ASYNC_CALL_CREATE_MSG\u001b[38;5;241m.\u001b[39mformat(traceback\u001b[38;5;241m.\u001b[39mformat_exc()),\n\u001b[1;32m    462\u001b[0m     )\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 464\u001b[0m res, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m _execute_call(wrapper, call, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/weave/trace/op.py:274\u001b[0m, in \u001b[0;36m_execute_call.<locals>._call_async\u001b[0;34m()\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_async\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Coroutine[Any, Any, Any]:\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_exception(e)\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/weave/integrations/anthropic/anthropic_sdk.py:100\u001b[0m, in \u001b[0;36mcreate_wrapper_async.<locals>.wrapper.<locals>._fn_wrapper.<locals>._async_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_async_wrapper\u001b[39m(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;241m*\u001b[39margs: typing\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: typing\u001b[38;5;241m.\u001b[39mAny\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/anthropic/resources/messages.py:1799\u001b[0m, in \u001b[0;36mAsyncMessages.create\u001b[0;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m DEPRECATED_MODELS:\n\u001b[1;32m   1793\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1794\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will reach end-of-life on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEPRECATED_MODELS[model]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1795\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m   1796\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m   1797\u001b[0m     )\n\u001b[0;32m-> 1799\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/v1/messages\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m   1802\u001b[0m         {\n\u001b[1;32m   1803\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   1804\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   1805\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   1806\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m   1807\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop_sequences,\n\u001b[1;32m   1808\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   1809\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m: system,\n\u001b[1;32m   1810\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   1811\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   1812\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   1813\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_k,\n\u001b[1;32m   1814\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   1815\u001b[0m         },\n\u001b[1;32m   1816\u001b[0m         message_create_params\u001b[38;5;241m.\u001b[39mMessageCreateParams,\n\u001b[1;32m   1817\u001b[0m     ),\n\u001b[1;32m   1818\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   1819\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1820\u001b[0m     ),\n\u001b[1;32m   1821\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mMessage,\n\u001b[1;32m   1822\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1823\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[RawMessageStreamEvent],\n\u001b[1;32m   1824\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/anthropic/_base_client.py:1816\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1803\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1804\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1811\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1812\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1813\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1814\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1815\u001b[0m     )\n\u001b[0;32m-> 1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/anthropic/_base_client.py:1510\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m   1502\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1503\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     remaining_retries: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1509\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1511\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1512\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1513\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1514\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1515\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m   1516\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/anthropic/_base_client.py:1549\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1546\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauth\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_auth\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1549\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\n\u001b[1;32m   1550\u001b[0m         request,\n\u001b[1;32m   1551\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream_response_body(request\u001b[38;5;241m=\u001b[39mrequest),\n\u001b[1;32m   1552\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1553\u001b[0m     )\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1555\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/httpx/_client.py:1674\u001b[0m, in \u001b[0;36mAsyncClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m   1672\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m-> 1674\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[1;32m   1675\u001b[0m     request,\n\u001b[1;32m   1676\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[1;32m   1677\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1678\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m   1679\u001b[0m )\n\u001b[1;32m   1680\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1681\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/httpx/_client.py:1702\u001b[0m, in \u001b[0;36mAsyncClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1699\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m auth_flow\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__anext__\u001b[39m()\n\u001b[1;32m   1701\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1702\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[1;32m   1703\u001b[0m         request,\n\u001b[1;32m   1704\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1705\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m   1706\u001b[0m     )\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/httpx/_client.py:1739\u001b[0m, in \u001b[0;36mAsyncClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1736\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[0;32m-> 1739\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[1;32m   1740\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1741\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/httpx/_client.py:1776\u001b[0m, in \u001b[0;36mAsyncClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1772\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an sync request with an AsyncClient instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1773\u001b[0m     )\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1776\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m transport\u001b[38;5;241m.\u001b[39mhandle_async_request(request)\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, AsyncByteStream)\n\u001b[1;32m   1779\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/httpx/_transports/default.py:377\u001b[0m, in \u001b[0;36mAsyncHTTPTransport.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    364\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    365\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    366\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    375\u001b[0m )\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 377\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_async_request(req)\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mAsyncIterable)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    382\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    383\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    384\u001b[0m     stream\u001b[38;5;241m=\u001b[39mAsyncResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    385\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    386\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:216\u001b[0m, in \u001b[0;36mAsyncConnectionPool.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, AsyncIterable)\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:196\u001b[0m, in \u001b[0;36mAsyncConnectionPool.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m connection\u001b[38;5;241m.\u001b[39mhandle_async_request(\n\u001b[1;32m    197\u001b[0m         pool_request\u001b[38;5;241m.\u001b[39mrequest\n\u001b[1;32m    198\u001b[0m     )\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/httpcore/_async/connection.py:101\u001b[0m, in \u001b[0;36mAsyncHTTPConnection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_async_request(request)\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/httpcore/_async/http11.py:143\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/httpcore/_async/http11.py:113\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/httpcore/_async/http11.py:186\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_event(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/httpcore/_async/http11.py:224\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\u001b[38;5;241m.\u001b[39mread(\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREAD_NUM_BYTES, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    226\u001b[0m     )\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/httpcore/_backends/anyio.py:35\u001b[0m, in \u001b[0;36mAnyIOStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39mfail_after(timeout):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream\u001b[38;5;241m.\u001b[39mreceive(max_bytes\u001b[38;5;241m=\u001b[39mmax_bytes)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39mEndOfStream:  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/anyio/streams/tls.py:205\u001b[0m, in \u001b[0;36mTLSStream.receive\u001b[0;34m(self, max_bytes)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreceive\u001b[39m(\u001b[38;5;28mself\u001b[39m, max_bytes: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m65536\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbytes\u001b[39m:\n\u001b[0;32m--> 205\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_sslobject_method(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ssl_object\u001b[38;5;241m.\u001b[39mread, max_bytes)\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m EndOfStream\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/anyio/streams/tls.py:147\u001b[0m, in \u001b[0;36mTLSStream._call_sslobject_method\u001b[0;34m(self, func, *args)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_bio\u001b[38;5;241m.\u001b[39mpending:\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransport_stream\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_bio\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m--> 147\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransport_stream\u001b[38;5;241m.\u001b[39mreceive()\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EndOfStream:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_bio\u001b[38;5;241m.\u001b[39mwrite_eof()\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1198\u001b[0m, in \u001b[0;36mSocketStream.receive\u001b[0;34m(self, max_bytes)\u001b[0m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1193\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol\u001b[38;5;241m.\u001b[39mread_event\u001b[38;5;241m.\u001b[39mis_set()\n\u001b[1;32m   1194\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mis_closing()\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol\u001b[38;5;241m.\u001b[39mis_at_eof\n\u001b[1;32m   1196\u001b[0m ):\n\u001b[1;32m   1197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mresume_reading()\n\u001b[0;32m-> 1198\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol\u001b[38;5;241m.\u001b[39mread_event\u001b[38;5;241m.\u001b[39mwait()\n\u001b[1;32m   1199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mpause_reading()\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/asyncio/locks.py:212\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiters\u001b[38;5;241m.\u001b[39mappend(fut)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create our LLM model with a system prompt\n",
    "model = sentiment_analysis_presidio_pii_model(\n",
    "    name=\"claude-3-sonnet\",\n",
    "    model_name=\"claude-3-5-sonnet-20240620\",\n",
    "    system_prompt='You are a Sentiment Analysis classifier. You will be classifying text based on their sentiment. Your input will be a block of text. You will answer with one the following rating option[\"positive\", \"negative\", \"neutral\"]. Your answer should be one word in json format: {classification}. Ensure that it is valid JSON.',\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(\"Model: \", model)\n",
    "# for every block of text, anonymized first and then predict\n",
    "for entry in pii_data:\n",
    "    await model.predict(entry[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faker and Presidio replacement method\n",
    "\n",
    "In this example, we use Faker to generate anonymized replacement PII data and use Presidio to identify and replace the PII data in the original text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../../media/pii/replace.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any\n",
    "\n",
    "import anthropic\n",
    "\n",
    "import weave\n",
    "\n",
    "# Define an input postprocessing function that applies our Faker anonymization and Presidio redaction for the model prediction Weave Op\n",
    "faker = my_faker()\n",
    "\n",
    "\n",
    "def postprocess_inputs_faker(inputs: dict[str, Any]) -> dict:\n",
    "    inputs[\"text_block\"] = faker.redact_and_anonymize_with_faker(inputs[\"text_block\"])\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# Weave model / predict function\n",
    "class sentiment_analysis_faker_pii_model(weave.Model):\n",
    "    model_name: str\n",
    "    system_prompt: str\n",
    "    temperature: int\n",
    "\n",
    "    @weave.op(\n",
    "        postprocess_inputs=postprocess_inputs_faker,\n",
    "    )\n",
    "    async def predict(self, text_block: str) -> dict:\n",
    "        client = anthropic.AsyncAnthropic()\n",
    "        response = await client.messages.create(\n",
    "            max_tokens=1024,\n",
    "            model=self.model_name,\n",
    "            system=self.system_prompt,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": text_block}]}\n",
    "            ],\n",
    "        )\n",
    "        result = response.content[0].text\n",
    "        if result is None:\n",
    "            raise ValueError(\"No response from model\")\n",
    "        parsed = json.loads(result)\n",
    "        return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  sentiment_analysis_faker_pii_model(name='claude-3-sonnet', description=None, model_name='claude-3-5-sonnet-20240620', system_prompt='You are a Sentiment Analysis classifier. You will be classifying text based on their sentiment. Your input will be a block of text. You will answer with one the following rating option[\"positive\", \"negative\", \"neutral\"]. Your answer should be one word in json format: {classification}. Ensure that it is valid JSON.', temperature=0)\n",
      "游꼴 https://wandb.ai/wandb/pii_cookbook/r/call/01924a62-34d0-70a0-a442-d08e59d745de\n",
      "游꼴 https://wandb.ai/wandb/pii_cookbook/r/call/01924a62-386a-7632-8785-8102ac495ae8\n",
      "游꼴 https://wandb.ai/wandb/pii_cookbook/r/call/01924a62-3c15-70b1-8bd9-b143e0aa8b20\n",
      "游꼴 https://wandb.ai/wandb/pii_cookbook/r/call/01924a62-4105-7af1-8446-572485bab118\n",
      "游꼴 https://wandb.ai/wandb/pii_cookbook/r/call/01924a62-44c5-7760-8d93-65743f1f2152\n",
      "游꼴 https://wandb.ai/wandb/pii_cookbook/r/call/01924a62-48c1-7b71-ab41-0e02dddd795c\n",
      "游꼴 https://wandb.ai/wandb/pii_cookbook/r/call/01924a62-4d50-7320-9f1b-03e9a3efeb4b\n",
      "游꼴 https://wandb.ai/wandb/pii_cookbook/r/call/01924a62-5155-78e3-a7a5-605fc663a8c3\n",
      "游꼴 https://wandb.ai/wandb/pii_cookbook/r/call/01924a62-54fc-71d1-9b6c-775570e84b24\n",
      "游꼴 https://wandb.ai/wandb/pii_cookbook/r/call/01924a62-589a-7d22-a8d5-eaeed5e8df40\n"
     ]
    }
   ],
   "source": [
    "# create our LLM model with a system prompt\n",
    "model = sentiment_analysis_faker_pii_model(\n",
    "    name=\"claude-3-sonnet\",\n",
    "    model_name=\"claude-3-5-sonnet-20240620\",\n",
    "    system_prompt='You are a Sentiment Analysis classifier. You will be classifying text based on their sentiment. Your input will be a block of text. You will answer with one the following rating option[\"positive\", \"negative\", \"neutral\"]. Your answer should be one word in json format: {classification}. Ensure that it is valid JSON.',\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(\"Model: \", model)\n",
    "# for every block of text, anonymized first and then predict\n",
    "for entry in pii_data:\n",
    "    await model.predict(entry[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `autopatch_settings` method\n",
    "\n",
    "In the following example, we set `postprocess_inputs` for `anthropic` to the `postprocess_inputs_regex()` function () at initialization. The `postprocess_inputs_regex` function applies the`redact_with_regex` method defined in [Method 1: Regular Expression Filtering](#method-1-regular-expression-filtering). Now, `redact_with_regex` will be applied to all inputs to any `anthropic` models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any\n",
    "\n",
    "import anthropic\n",
    "\n",
    "import weave\n",
    "\n",
    "client = weave.init(\n",
    "    ...,\n",
    "    autopatch_settings={\n",
    "        \"anthropic\": {\n",
    "            \"op_settings\": {\n",
    "                \"postprocess_inputs\": postprocess_inputs_regex,\n",
    "            }\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "# Define an input postprocessing function that applies our regex redaction for the model prediction Weave Op\n",
    "def postprocess_inputs_regex(inputs: dict[str, Any]) -> dict:\n",
    "    inputs[\"text_block\"] = redact_with_regex(inputs[\"text_block\"])\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# Weave model / predict function\n",
    "class sentiment_analysis_regex_pii_model(weave.Model):\n",
    "    model_name: str\n",
    "    system_prompt: str\n",
    "    temperature: int\n",
    "\n",
    "    async def predict(self, text_block: str) -> dict:\n",
    "        client = anthropic.AsyncAnthropic()\n",
    "        response = await client.messages.create(\n",
    "            max_tokens=1024,\n",
    "            model=self.model_name,\n",
    "            system=self.system_prompt,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": text_block}]}\n",
    "            ],\n",
    "        )\n",
    "        result = response.content[0].text\n",
    "        if result is None:\n",
    "            raise ValueError(\"No response from model\")\n",
    "        parsed = json.loads(result)\n",
    "        return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our LLM model with a system prompt\n",
    "model = sentiment_analysis_regex_pii_model(\n",
    "    name=\"claude-3-sonnet\",\n",
    "    model_name=\"claude-3-5-sonnet-20240620\",\n",
    "    system_prompt='You are a Sentiment Analysis classifier. You will be classifying text based on their sentiment. Your input will be a block of text. You will answer with one the following rating option[\"positive\", \"negative\", \"neutral\"]. Your answer should be one word in json format: {classification}. Ensure that it is valid JSON.',\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(\"Model: \", model)\n",
    "# for every block of text, anonymized first and then predict\n",
    "for entry in pii_data:\n",
    "    await model.predict(entry[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Encrypt your data \n",
    "\n",
    "![](../../media/pii/encrypt.png)\n",
    "\n",
    "In addition to anonymizing PII, you can add an extra layer of security by encrypting your data using the cryptography library's [Fernet](https://cryptography.io/en/latest/fernet/) symmetric encryption. This approach ensures that even if the anonymized data is intercepted, it remains unreadable without the encryption key.\n",
    "\n",
    "```python\n",
    "import os\n",
    "from cryptography.fernet import Fernet\n",
    "from pydantic import BaseModel, ValidationInfo, model_validator\n",
    "\n",
    "def get_fernet_key():\n",
    "    # Check if the key exists in environment variables\n",
    "    key = os.environ.get('FERNET_KEY')\n",
    "    \n",
    "    if key is None:\n",
    "        # If the key doesn't exist, generate a new one\n",
    "        key = Fernet.generate_key()\n",
    "        # Save the key to an environment variable\n",
    "        os.environ['FERNET_KEY'] = key.decode()\n",
    "    else:\n",
    "        # If the key exists, ensure it's in bytes\n",
    "        key = key.encode()\n",
    "    \n",
    "    return key\n",
    "\n",
    "cipher_suite = Fernet(get_fernet_key())\n",
    "\n",
    "class EncryptedSentimentAnalysisInput(BaseModel):\n",
    "    encrypted_text: str = None\n",
    "\n",
    "    @model_validator(mode=\"before\")\n",
    "    def encrypt_fields(cls, values):\n",
    "        if \"text\" in values and values[\"text\"] is not None:\n",
    "            values[\"encrypted_text\"] = cipher_suite.encrypt(values[\"text\"].encode()).decode()\n",
    "            del values[\"text\"]\n",
    "        return values\n",
    "\n",
    "    @property\n",
    "    def text(self):\n",
    "        if self.encrypted_text:\n",
    "            return cipher_suite.decrypt(self.encrypted_text.encode()).decode()\n",
    "        return None\n",
    "\n",
    "    @text.setter\n",
    "    def text(self, value):\n",
    "        self.encrypted_text = cipher_suite.encrypt(str(value).encode()).decode()\n",
    "\n",
    "    @classmethod\n",
    "    def encrypt(cls, text: str):\n",
    "        return cls(text=text)\n",
    "\n",
    "    def decrypt(self):\n",
    "        return self.text\n",
    "\n",
    "# Modified sentiment_analysis_model to use the new EncryptedSentimentAnalysisInput\n",
    "class sentiment_analysis_model(weave.Model):\n",
    "    model_name: str\n",
    "    system_prompt: str\n",
    "    temperature: int\n",
    "\n",
    "    @weave.op()\n",
    "    async def predict(self, encrypted_input: EncryptedSentimentAnalysisInput) -> dict:\n",
    "        client = AsyncAnthropic()\n",
    "\n",
    "        decrypted_text = encrypted_input.decrypt() # We use the custom class to decrypt the text\n",
    "\n",
    "        response = await client.messages.create(\n",
    "            max_tokens=1024,\n",
    "            model=self.model_name,\n",
    "            system=self.system_prompt,\n",
    "            messages=[\n",
    "                {   \"role\": \"user\",\n",
    "                    \"content\":[\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": decrypted_text\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        result = response.content[0].text\n",
    "        if result is None:\n",
    "            raise ValueError(\"No response from model\")\n",
    "        parsed = json.loads(result)\n",
    "        return parsed\n",
    "\n",
    "model = sentiment_analysis_model(\n",
    "    name=\"claude-3-sonnet\",\n",
    "    model_name=\"claude-3-5-sonnet-20240620\",\n",
    "    system_prompt=\"You are a Sentiment Analysis classifier. You will be classifying text based on their sentiment. Your input will be a block of text. You will answer with one the following rating option[\\\"positive\\\", \\\"negative\\\", \\\"neutral\\\"]. Your answer should one word in json format dict where the key is classification.\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "for entry in pii_data:\n",
    "    encrypted_input = EncryptedSentimentAnalysisInput.encrypt(entry[\"text\"])\n",
    "    await model.predict(encrypted_input)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "XiRkYiiIj2KX"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
