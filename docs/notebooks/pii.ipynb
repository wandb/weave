{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- docusaurus_head_meta::start\n",
    "---\n",
    "title: Handling and Redacting PII\n",
    "---\n",
    "docusaurus_head_meta::end -->\n",
    "\n",
    "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "<!--- @wandbcode{cod-notebook} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m752k2fWKDql"
   },
   "source": [
    "# How to use Weave with PII data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C70egOGRLCgm"
   },
   "source": [
    "In this tutorial, we'll demonstrate how to utilize Weave while preventing your Personally Identifiable Information (PII) data from being incorporated into Weave or the LLMs you employ.\n",
    "\n",
    "To protect our PII data, we'll employ a couple techniques. First, we'll use regular expressions to identify PII data and redact it. Second, we'll use Microsoft's [Presidio](https://microsoft.github.io/presidio/), a python-based data protection SDK. This tool provides redaction and replacement functionalities, both of which we will implement in this tutorial.\n",
    "\n",
    "For this use-case. We will leverage Anthropic's Claude Sonnet to perform sentiment analysis. While we use Weave's [Traces](https://wandb.github.io/weave/quickstart) to track and analize the LLM's API calls. Sonnet will receive a block of text and output one of the following sentiment classifications:\n",
    "1. positive\n",
    "2. negative\n",
    "3. neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "qi-VNJT35v2j",
    "outputId": "8870f124-e141-4fee-8789-024d81944c56"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# @title required python packages:\n",
    "!pip install presidio_analyzer\n",
    "!pip install presidio_anonymizer\n",
    "!python -m spacy download en_core_web_lg # Presidio uses spacy NLP engine\n",
    "!pip install Faker                       # we'll use Faker to replace PII data with fake data\n",
    "!pip install weave                        # To leverage Traces\n",
    "!pip install set-env-colab-kaggle-dotenv -q # for env var\n",
    "!pip install anthropic                      # to use sonnet\n",
    "!pip install cryptography                   # to encrypt our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Make sure to set up set up your API keys correctly\n",
    "from set_env import set_env\n",
    "\n",
    "_ = set_env(\"ANTHROPIC_API_KEY\")\n",
    "_ = set_env(\"WANDB_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "\n",
    "WEAVE_PROJECT = \"pii_cookbook\"\n",
    "weave.init(WEAVE_PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our initial PII data. For demonstration purposes, we'll use a dataset containing 10 text blocks. A larger dataset with 1000 entries is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/wandb/weave/master/docs/notebooks/10_pii_data.json\"\n",
    "response = requests.get(url)\n",
    "pii_data = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Weave Safely with PII Data\n",
    "\n",
    "## During Testing\n",
    "- Log anonymized data to check PII detection\n",
    "- Track PII handling processes with Weave traces\n",
    "- Measure anonymization performance without exposing real PII\n",
    "\n",
    "## In Production\n",
    "- Never log raw PII\n",
    "- Encrypt sensitive fields before logging\n",
    "\n",
    "## Encryption Tips\n",
    "- Use reversible encryption for data you need to decrypt later\n",
    "- Apply one-way hashing for unique IDs you don't need to reverse\n",
    "- Consider specialized encryption for data you need to analyze while encrypted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1: \n",
    "\n",
    "Our initial method is to use [regular expressions (regex)](https://docs.python.org/3/library/re.html) to identify PII data and redact it. It allows us to define patterns that can match various formats of sensitive information like phone numbers, email addresses, and social security numbers. By using regex, we can scan through large volumes of text and replace or redact information without the need for more complex NLP techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def clean_pii_with_regex(text):\n",
    "    # Phone number pattern\n",
    "    # \\b         : Word boundary\n",
    "    # \\d{3}      : Exactly 3 digits\n",
    "    # [-.]?      : Optional hyphen or dot\n",
    "    # \\d{3}      : Another 3 digits\n",
    "    # [-.]?      : Optional hyphen or dot\n",
    "    # \\d{4}      : Exactly 4 digits\n",
    "    # \\b         : Word boundary\n",
    "    text = re.sub(r\"\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b\", \"<PHONE>\", text)\n",
    "\n",
    "    # Email pattern\n",
    "    # \\b         : Word boundary\n",
    "    # [A-Za-z0-9._%+-]+ : One or more characters that can be in an email username\n",
    "    # @          : Literal @ symbol\n",
    "    # [A-Za-z0-9.-]+ : One or more characters that can be in a domain name\n",
    "    # \\.         : Literal dot\n",
    "    # [A-Z|a-z]{2,} : Two or more uppercase or lowercase letters (TLD)\n",
    "    # \\b         : Word boundary\n",
    "    text = re.sub(\n",
    "        r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\", \"<EMAIL>\", text\n",
    "    )\n",
    "\n",
    "    # SSN pattern\n",
    "    # \\b         : Word boundary\n",
    "    # \\d{3}      : Exactly 3 digits\n",
    "    # -          : Literal hyphen\n",
    "    # \\d{2}      : Exactly 2 digits\n",
    "    # -          : Literal hyphen\n",
    "    # \\d{4}      : Exactly 4 digits\n",
    "    # \\b         : Word boundary\n",
    "    text = re.sub(r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\", \"<SSN>\", text)\n",
    "\n",
    "    # Simple name pattern (this is not comprehensive)\n",
    "    # \\b         : Word boundary\n",
    "    # [A-Z]      : One uppercase letter\n",
    "    # [a-z]+     : One or more lowercase letters\n",
    "    # \\s         : One whitespace character\n",
    "    # [A-Z]      : One uppercase letter\n",
    "    # [a-z]+     : One or more lowercase letters\n",
    "    # \\b         : Word boundary\n",
    "    text = re.sub(r\"\\b[A-Z][a-z]+ [A-Z][a-z]+\\b\", \"<NAME>\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# Test the function\n",
    "test_text = \"My name is John Doe, my email is john.doe@example.com, my phone is 123-456-7890, and my SSN is 123-45-6789.\"\n",
    "cleaned_text = clean_pii_with_regex(test_text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2: Microsoft Presidio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll create a [Weave Model](https://wandb.github.io/weave/guides/core-types/models) which is a combination of data (which can include configuration, trained model weights, or other information) and code that defines how the model operates.\n",
    "In this model, we will include our predict function where the Anthropic API will be called.\n",
    "\n",
    "Once you run this code you will receive a link to the Weave project page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from anthropic import AsyncAnthropic\n",
    "\n",
    "import weave\n",
    "\n",
    "\n",
    "# Weave model / predict function\n",
    "class sentiment_analysis_model(weave.Model):\n",
    "    model_name: str\n",
    "    system_prompt: str\n",
    "    temperature: int\n",
    "\n",
    "    @weave.op()\n",
    "    async def predict(self, text_block: str) -> dict:\n",
    "        client = AsyncAnthropic()\n",
    "\n",
    "        response = await client.messages.create(\n",
    "            max_tokens=1024,\n",
    "            model=self.model_name,\n",
    "            system=self.system_prompt,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": text_block}]}\n",
    "            ],\n",
    "        )\n",
    "        result = response.content[0].text\n",
    "        if result is None:\n",
    "            raise ValueError(\"No response from model\")\n",
    "        parsed = json.loads(result)\n",
    "        return parsed\n",
    "\n",
    "    # create our LLM model with a system prompt\n",
    "\n",
    "\n",
    "model = sentiment_analysis_model(\n",
    "    name=\"claude-3-sonnet\",\n",
    "    model_name=\"claude-3-5-sonnet-20240620\",\n",
    "    system_prompt='You are a Sentiment Analysis classifier. You will be classifying text based on their sentiment. Your input will be a block of text. You will answer with one the following rating option[\"positive\", \"negative\", \"neutral\"]. Your answer should be one word in json format: {classification}. Ensure that it is valid JSON.',\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYEX_yPqMDOk"
   },
   "source": [
    "# Method 2A:\n",
    "Our next method involves complete removal of PII data using Presidio. This approach redacts PII and replaces it with a placeholder representing the PII type. For example:\n",
    "```\n",
    " \"My name is Alex\"\n",
    "```\n",
    "\n",
    "Will be:\n",
    "\n",
    "```\n",
    " \"My name is <PERSON>\"\n",
    "```\n",
    "Presidio comes with a built-in [list of recognizable entities](https://microsoft.github.io/presidio/supported_entities/). We can select the ones that are important for our use case. In the below example, we are only looking at redicating names and phone numbers from our text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../../media/pii/redact.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bh6MgL3g6sc2"
   },
   "outputs": [],
   "source": [
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "\n",
    "text = \"My phone number is 212-555-5555 and my name is alex\"\n",
    "\n",
    "# Set up the engine, loads the NLP module (spaCy model by default)\n",
    "# and other PII recognizers\n",
    "analyzer = AnalyzerEngine()\n",
    "\n",
    "# Call analyzer to get results\n",
    "results = analyzer.analyze(\n",
    "    text=text, entities=[\"PHONE_NUMBER\", \"PERSON\"], language=\"en\"\n",
    ")\n",
    "\n",
    "# Analyzer results are passed to the AnonymizerEngine for anonymization\n",
    "\n",
    "anonymizer = AnonymizerEngine()\n",
    "\n",
    "anonymized_text = anonymizer.anonymize(text=text, analyzer_results=results)\n",
    "\n",
    "print(anonymized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zq1abYAUl4rV"
   },
   "source": [
    "Let's encapsulate the previous step into a function and expand the entity recognition capabilities. We will expand our redaction scope to include addresses, email addresses, and US Social Security numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "rQyUmN5nmQrj",
    "outputId": "4a538fda-c079-4cea-a65f-ccbeee58f17e"
   },
   "outputs": [],
   "source": [
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "\n",
    "analyzer = AnalyzerEngine()\n",
    "anonymizer = AnonymizerEngine()\n",
    "\"\"\"\n",
    "The below function will take a block of text, process it using presidio\n",
    "and return a block of text with the PII data redicated.\n",
    "PII data to be redicated:\n",
    "- Phone Numbers\n",
    "- Names\n",
    "- Addresses\n",
    "- Email addresses\n",
    "- US Social Security Numbers\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def anonymize_my_text(text):\n",
    "    results = analyzer.analyze(\n",
    "        text=text,\n",
    "        entities=[\"PHONE_NUMBER\", \"PERSON\", \"LOCATION\", \"EMAIL_ADDRESS\", \"US_SSN\"],\n",
    "        language=\"en\",\n",
    "    )\n",
    "    anonymized_text = anonymizer.anonymize(text=text, analyzer_results=results)\n",
    "    return anonymized_text.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sTGbemoMtcO8"
   },
   "outputs": [],
   "source": [
    "# for every block of text, anonymized first and then predict\n",
    "for entry in pii_data:\n",
    "    anonymized_entry = anonymize_my_text(entry[\"text\"])\n",
    "    (await model.predict(anonymized_entry))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7omtvsdpMuel"
   },
   "source": [
    "# Method 2B: Replace PII data with fake data\n",
    "\n",
    "Instead of redacting text, we can anonymize it by swapping PII (like names and phone numbers) with fake data generated using the [Faker](https://faker.readthedocs.io/en/master/) Python library. For example:\n",
    "\n",
    "\n",
    "```\n",
    "\"My name is Raphael and I like to fish. My phone number is 212-555-5555\"\n",
    "```\n",
    "Will be:\n",
    "\n",
    "\n",
    "```\n",
    "\"My name is Katherine Dixon and I like to fish. My phone number is 667.431.7379\"\n",
    "\n",
    "```\n",
    "\n",
    "To effectively utilize Presidio, we must supply references to our custom operators. These operators will direct Presidio to the functions responsible for swapping PII with fake data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../../media/pii/replace.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3XJa7u5T_WYd"
   },
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_anonymizer.entities import OperatorConfig\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "\n",
    "# Create faker functions (note that it has to receive a value)\n",
    "def fake_name(x):\n",
    "    return fake.name()\n",
    "\n",
    "\n",
    "def fake_number(x):\n",
    "    return fake.phone_number()\n",
    "\n",
    "\n",
    "# Create custom operator for the PERSON and PHONE_NUMBER\" entities\n",
    "operators = {\n",
    "    \"PERSON\": OperatorConfig(\"custom\", {\"lambda\": fake_name}),\n",
    "    \"PHONE_NUMBER\": OperatorConfig(\"custom\", {\"lambda\": fake_number}),\n",
    "}\n",
    "\n",
    "\n",
    "text_to_anonymize = (\n",
    "    \"My name is Raphael and I like to fish. My phone number is 212-555-5555\"\n",
    ")\n",
    "\n",
    "# Analyzer output\n",
    "analyzer_results = analyzer.analyze(\n",
    "    text=text_to_anonymize, entities=[\"PHONE_NUMBER\", \"PERSON\"], language=\"en\"\n",
    ")\n",
    "\n",
    "\n",
    "anonymizer = AnonymizerEngine()\n",
    "\n",
    "# do not forget to pass the operators from above to the anonymizer\n",
    "anonymized_results = anonymizer.anonymize(\n",
    "    text=text_to_anonymize, analyzer_results=analyzer_results, operators=operators\n",
    ")\n",
    "\n",
    "print(anonymized_results.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4HJcskpSYdL"
   },
   "source": [
    "Let's consolidate our code into a single class and expand the list of entities to include the additional ones we identified earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "yIaharNAWGh5"
   },
   "outputs": [],
   "source": [
    "from anthropic import AsyncAnthropic\n",
    "from faker import Faker\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_anonymizer.entities import OperatorConfig\n",
    "\n",
    "import weave\n",
    "\n",
    "\n",
    "# Let's build a custom class for generating fake data that will extend Faker\n",
    "class my_faker(Faker):\n",
    "    # Create faker functions (note that it has to receive a value)\n",
    "    def fake_address(x):\n",
    "        return fake.address()\n",
    "\n",
    "    def fake_ssn(x):\n",
    "        return fake.ssn()\n",
    "\n",
    "    def fake_name(x):\n",
    "        return fake.name()\n",
    "\n",
    "    def fake_number(x):\n",
    "        return fake.phone_number()\n",
    "\n",
    "    def fake_email(x):\n",
    "        return fake.email()\n",
    "\n",
    "    # Create custom operators for the entities\n",
    "    operators = {\n",
    "        \"PERSON\": OperatorConfig(\"custom\", {\"lambda\": fake_name}),\n",
    "        \"PHONE_NUMBER\": OperatorConfig(\"custom\", {\"lambda\": fake_number}),\n",
    "        \"EMAIL_ADDRESS\": OperatorConfig(\"custom\", {\"lambda\": fake_email}),\n",
    "        \"LOCATION\": OperatorConfig(\"custom\", {\"lambda\": fake_address}),\n",
    "        \"US_SSN\": OperatorConfig(\"custom\", {\"lambda\": fake_ssn}),\n",
    "    }\n",
    "\n",
    "    def anonymize_my_text(self, text):\n",
    "        anonymizer = AnonymizerEngine()\n",
    "        analyzer_results = analyzer.analyze(\n",
    "            text=text,\n",
    "            entities=[\"PHONE_NUMBER\", \"PERSON\", \"LOCATION\", \"EMAIL_ADDRESS\", \"US_SSN\"],\n",
    "            language=\"en\",\n",
    "        )\n",
    "        anonymized_results = anonymizer.anonymize(\n",
    "            text=text, analyzer_results=analyzer_results, operators=self.operators\n",
    "        )\n",
    "        return anonymized_results.text\n",
    "\n",
    "\n",
    "faker = my_faker()\n",
    "for entry in pii_data:\n",
    "    anonymized_entry = faker.anonymize_my_text(entry[\"text\"])\n",
    "    (await model.predict(anonymized_entry))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> (Optional) Encrypting our data </summary>\n",
    "\n",
    "![](../../media/pii/encrypt.png)\n",
    "\n",
    "In addition to anonymizing PII, we can add an extra layer of security by encrypting our data using the cryptography library's [Fernet](https://cryptography.io/en/latest/fernet/) symmetric encryption. This approach ensures that even if the anonymized data is intercepted, it remains unreadable without the encryption key.\n",
    "\n",
    "```python\n",
    "import os\n",
    "from cryptography.fernet import Fernet\n",
    "from pydantic import BaseModel, ValidationInfo, model_validator\n",
    "\n",
    "def get_fernet_key():\n",
    "    # Check if the key exists in environment variables\n",
    "    key = os.environ.get('FERNET_KEY')\n",
    "    \n",
    "    if key is None:\n",
    "        # If the key doesn't exist, generate a new one\n",
    "        key = Fernet.generate_key()\n",
    "        # Save the key to an environment variable\n",
    "        os.environ['FERNET_KEY'] = key.decode()\n",
    "    else:\n",
    "        # If the key exists, ensure it's in bytes\n",
    "        key = key.encode()\n",
    "    \n",
    "    return key\n",
    "\n",
    "cipher_suite = Fernet(get_fernet_key())\n",
    "\n",
    "class EncryptedSentimentAnalysisInput(BaseModel):\n",
    "    encrypted_text: str = None\n",
    "\n",
    "    @model_validator(mode=\"before\")\n",
    "    def encrypt_fields(cls, values):\n",
    "        if \"text\" in values and values[\"text\"] is not None:\n",
    "            values[\"encrypted_text\"] = cipher_suite.encrypt(values[\"text\"].encode()).decode()\n",
    "            del values[\"text\"]\n",
    "        return values\n",
    "\n",
    "    @property\n",
    "    def text(self):\n",
    "        if self.encrypted_text:\n",
    "            return cipher_suite.decrypt(self.encrypted_text.encode()).decode()\n",
    "        return None\n",
    "\n",
    "    @text.setter\n",
    "    def text(self, value):\n",
    "        self.encrypted_text = cipher_suite.encrypt(str(value).encode()).decode()\n",
    "\n",
    "    @classmethod\n",
    "    def encrypt(cls, text: str):\n",
    "        return cls(text=text)\n",
    "\n",
    "    def decrypt(self):\n",
    "        return self.text\n",
    "\n",
    "# Modified sentiment_analysis_model to use the new EncryptedSentimentAnalysisInput\n",
    "class sentiment_analysis_model(weave.Model):\n",
    "    model_name: str\n",
    "    system_prompt: str\n",
    "    temperature: int\n",
    "\n",
    "    @weave.op()\n",
    "    async def predict(self, encrypted_input: EncryptedSentimentAnalysisInput) -> dict:\n",
    "        client = AsyncAnthropic()\n",
    "\n",
    "        decrypted_text = encrypted_input.decrypt() # We use the custom class to decrypt the text\n",
    "\n",
    "        response = await client.messages.create(\n",
    "            max_tokens=1024,\n",
    "            model=self.model_name,\n",
    "            system=self.system_prompt,\n",
    "            messages=[\n",
    "                {   \"role\": \"user\",\n",
    "                    \"content\":[\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": decrypted_text\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        result = response.content[0].text\n",
    "        if result is None:\n",
    "            raise ValueError(\"No response from model\")\n",
    "        parsed = json.loads(result)\n",
    "        return parsed\n",
    "\n",
    "model = sentiment_analysis_model(\n",
    "    name=\"claude-3-sonnet\",\n",
    "    model_name=\"claude-3-5-sonnet-20240620\",\n",
    "    system_prompt=\"You are a Sentiment Analysis classifier. You will be classifying text based on their sentiment. Your input will be a block of text. You will answer with one the following rating option[\\\"positive\\\", \\\"negative\\\", \\\"neutral\\\"]. Your answer should one word in json format dict where the key is classification.\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "for entry in pii_data:\n",
    "    encrypted_input = EncryptedSentimentAnalysisInput.encrypt(entry[\"text\"])\n",
    "    await model.predict(encrypted_input)\n",
    "```\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "XiRkYiiIj2KX"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
