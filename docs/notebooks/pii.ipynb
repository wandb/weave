{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- docusaurus_head_meta::start\n",
    "---\n",
    "title: Handling and Redacting PII\n",
    "---\n",
    "docusaurus_head_meta::end -->\n",
    "\n",
    "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "<!--- @wandbcode{cod-notebook} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m752k2fWKDql"
   },
   "source": [
    "# How to use Weave with PII data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C70egOGRLCgm"
   },
   "source": [
    "In this tutorial, we'll demonstrate how to utilize Weave while ensuring your Personally Identifiable Information (PII) data remains private. Weave supports removing PII from LLM calls and preventing PII from being displayed in the Weave UI. \n",
    "\n",
    "To detect and protect our PII data, we'll identify and redact PII data and optionally anonymize it with the following methods:\n",
    "1. __Regular expressions__ to identify PII data and redact it.\n",
    "2. __Microsoft's [Presidio](https://microsoft.github.io/presidio/)__, a python-based data protection SDK. This tool provides redaction and replacement functionalities.\n",
    "3. __[Faker](https://faker.readthedocs.io/en/master/)__, a Python library to generate fake data, combined with Presidio to anonymize PII data.\n",
    "\n",
    "Additionally, we'll make use of _Weave Ops input/output logging customization_ to seamlessly integrate PII redaction and anonymization into the workflow. See [here](https://weave-docs.wandb.ai/guides/tracking/ops/#customize-logged-inputs-and-outputs) for more information.\n",
    "\n",
    "For this use-case, we will leverage Anthropic's Claude Sonnet to perform sentiment analysis while tracing the LLM calls using Weave's [Traces](https://wandb.github.io/weave/quickstart). Sonnet will receive a block of text and output one of the following sentiment classifications: _positive_, _negative_, or _neutral_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Weave Ops Input/Output Logging Customization\n",
    "\n",
    "Weave Ops support defining input and output postprocessing functions. These functions allow you to modify the data that is passed to your LLM call or logged to Weave, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "\n",
    "import weave\n",
    "\n",
    "# Inputs Wrapper Class\n",
    "@dataclass\n",
    "class CustomObject:\n",
    "    x: int\n",
    "    secret_password: str\n",
    "\n",
    "# First we define functions for input and output postprocessing:\n",
    "def postprocess_inputs(inputs: dict[str, Any]) -> dict[str, Any]:\n",
    "    return {k:v for k,v in inputs.items() if k != \"hide_me\"}\n",
    "\n",
    "def postprocess_output(output: CustomObject) -> CustomObject:\n",
    "    return CustomObject(x=output.x, secret_password=\"REDACTED\")\n",
    "\n",
    "# Then, when we use the `@weave.op` decorator, we pass these processing functions as arguments to the decorator:\n",
    "@weave.op(\n",
    "    postprocess_inputs=postprocess_inputs,\n",
    "    postprocess_output=postprocess_output,\n",
    ")\n",
    "def some_llm_call(a: int, hide_me: str) -> CustomObject:\n",
    "    return CustomObject(x=a, secret_password=hide_me)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Let's install the required packages and set up our API keys. Your Weights & Biases API key can be found [here](https://wandb.ai/authorize), and your Anthropic API keys are [here](https://console.anthropic.com/settings/keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# @title required python packages:\n",
    "!pip install presidio_analyzer\n",
    "!pip install presidio_anonymizer\n",
    "!python -m spacy download en_core_web_lg    # Presidio uses spacy NLP engine\n",
    "!pip install Faker                          # we'll use Faker to replace PII data with fake data\n",
    "!pip install weave                          # To leverage Traces\n",
    "!pip install set-env-colab-kaggle-dotenv -q # for env var\n",
    "!pip install anthropic                      # to use sonnet\n",
    "!pip install cryptography                   # to encrypt our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# @title Make sure to set up set up your API keys correctly\n",
    "# See: https://pypi.org/project/set-env-colab-kaggle-dotenv/ for usage instructions.\n",
    "\n",
    "from set_env import set_env\n",
    "\n",
    "_ = set_env(\"ANTHROPIC_API_KEY\")\n",
    "_ = set_env(\"WANDB_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<weave.trace.weave_client.WeaveClient at 0x14a0a63c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import weave\n",
    "\n",
    "# Start a new Weave project\n",
    "WEAVE_PROJECT = \"pii_cookbook\"\n",
    "weave.init(WEAVE_PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our initial PII data. For demonstration purposes, we'll use a dataset containing 10 text blocks. A larger dataset with 1000 entries is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PII data first sample: \"I remember the day like it was yesterday. I was a waitress at a busy restaurant in New York City, Mohammed Gross is the name. The place was packed, and I was running around like a crazy person, trying to keep up with the demand. Suddenly, I noticed a customer sitting at one of my tables. He looked very upset. I went over to see what was wrong. \"My food is cold,\" he said. \"I've been waiting for it for over an hour.\" I apologized and asked him if he would like me to get him a new meal. He said yes, so I went back to the kitchen and told the chef. The chef was very apologetic and made the customer a new meal right away. I brought it out to the customer and he seemed satisfied. \"Thank you,\" he said. \"That's much better.\" I was glad that I was able to resolve the situation and make the customer happy. It's always rewarding to be able to help people, and it's one of the things I love about my job. I believe that customer service is very important, and I always try to go the extra mile for my customers. If you're ever in New York City, be sure to stop by the restaurant and ask for Mohammed Gross. I'll be happy to serve you! Feel free to email me at mohammedgross@msn.edu if you have any questions or concerns. You can also reach me by mail at 8202 Dudley Way, [City, State, ZIP].\"\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/wandb/weave/master/docs/notebooks/10_pii_data.json\"\n",
    "response = requests.get(url)\n",
    "pii_data = response.json()\n",
    "\n",
    "print('PII data first sample: \"' + pii_data[0][\"text\"] + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redaction Methods Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Regular Expression Filtering\n",
    "\n",
    "Our initial method is to use [regular expressions (regex)](https://docs.python.org/3/library/re.html) to identify PII data and redact it. It allows us to define patterns that can match various formats of sensitive information like phone numbers, email addresses, and social security numbers. By using regex, we can scan through large volumes of text and replace or redact information without the need for more complex NLP techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "# Define a function to clean PII data using regex\n",
    "def redact_with_regex(text):\n",
    "    # Phone number pattern\n",
    "    # \\b         : Word boundary\n",
    "    # \\d{3}      : Exactly 3 digits\n",
    "    # [-.]?      : Optional hyphen or dot\n",
    "    # \\d{3}      : Another 3 digits\n",
    "    # [-.]?      : Optional hyphen or dot\n",
    "    # \\d{4}      : Exactly 4 digits\n",
    "    # \\b         : Word boundary\n",
    "    text = re.sub(r\"\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b\", \"<PHONE>\", text)\n",
    "\n",
    "    # Email pattern\n",
    "    # \\b         : Word boundary\n",
    "    # [A-Za-z0-9._%+-]+ : One or more characters that can be in an email username\n",
    "    # @          : Literal @ symbol\n",
    "    # [A-Za-z0-9.-]+ : One or more characters that can be in a domain name\n",
    "    # \\.         : Literal dot\n",
    "    # [A-Z|a-z]{2,} : Two or more uppercase or lowercase letters (TLD)\n",
    "    # \\b         : Word boundary\n",
    "    text = re.sub(\n",
    "        r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\", \"<EMAIL>\", text\n",
    "    )\n",
    "\n",
    "    # SSN pattern\n",
    "    # \\b         : Word boundary\n",
    "    # \\d{3}      : Exactly 3 digits\n",
    "    # -          : Literal hyphen\n",
    "    # \\d{2}      : Exactly 2 digits\n",
    "    # -          : Literal hyphen\n",
    "    # \\d{4}      : Exactly 4 digits\n",
    "    # \\b         : Word boundary\n",
    "    text = re.sub(r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\", \"<SSN>\", text)\n",
    "\n",
    "    # Simple name pattern (this is not comprehensive)\n",
    "    # \\b         : Word boundary\n",
    "    # [A-Z]      : One uppercase letter\n",
    "    # [a-z]+     : One or more lowercase letters\n",
    "    # \\s         : One whitespace character\n",
    "    # [A-Z]      : One uppercase letter\n",
    "    # [a-z]+     : One or more lowercase letters\n",
    "    # \\b         : Word boundary\n",
    "    text = re.sub(r\"\\b[A-Z][a-z]+ [A-Z][a-z]+\\b\", \"<NAME>\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the function with a sample text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text:\n",
      "\tMy name is John Doe, my email is john.doe@example.com, my phone is 123-456-7890, and my SSN is 123-45-6789.\n",
      "Redacted text:\n",
      "\tMy name is <NAME>, my email is <EMAIL>, my phone is <PHONE>, and my SSN is <SSN>.\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "test_text = \"My name is John Doe, my email is john.doe@example.com, my phone is 123-456-7890, and my SSN is 123-45-6789.\"\n",
    "cleaned_text = redact_with_regex(test_text)\n",
    "print(f\"Raw text:\\n\\t{test_text}\")\n",
    "print(f\"Redacted text:\\n\\t{cleaned_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Microsoft Presidio Redaction\n",
    "Our next method involves complete removal of PII data using Presidio. This approach redacts PII and replaces it with a placeholder representing the PII type. \n",
    "\n",
    "For example:\n",
    "`\"My name is Alex\"` becomes `\"My name is <PERSON>\"`.\n",
    "\n",
    "Presidio comes with a built-in [list of recognizable entities](https://microsoft.github.io/presidio/supported_entities/). We can select the ones that are important for our use case. In the below example, we redact names, phone numbers, locations, email addresses, and US Social Security Numbers.\n",
    "\n",
    "We'll then encapsulate the Presidio process into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Bh6MgL3g6sc2"
   },
   "outputs": [],
   "source": [
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "\n",
    "# Set up the Analyzer, which loads an NLP module (spaCy model by default) and other PII recognizers.\n",
    "analyzer = AnalyzerEngine()\n",
    "\n",
    "# Set up the Anonymizer, which will use the analyzer results to anonymize the text.\n",
    "anonymizer = AnonymizerEngine()\n",
    "\n",
    "\n",
    "# Encapsulate the Presidio redaction process into a function\n",
    "def redact_with_presidio(text):\n",
    "    # Analyze the text to identify PII data\n",
    "    results = analyzer.analyze(\n",
    "        text=text,\n",
    "        entities=[\"PHONE_NUMBER\", \"PERSON\", \"LOCATION\", \"EMAIL_ADDRESS\", \"US_SSN\"],\n",
    "        language=\"en\",\n",
    "    )\n",
    "    # Anonymize the identified PII data\n",
    "    anonymized_text = anonymizer.anonymize(text=text, analyzer_results=results)\n",
    "    return anonymized_text.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the function with a sample text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text:\n",
      "\tMy phone number is 212-555-5555 and my name is alex\n",
      "Redacted text:\n",
      "\tMy phone number is <PHONE_NUMBER> and my name is <PERSON>\n"
     ]
    }
   ],
   "source": [
    "text = \"My phone number is 212-555-5555 and my name is alex\"\n",
    "\n",
    "# Test the function\n",
    "anonymized_text = redact_with_presidio(text)\n",
    "\n",
    "print(f\"Raw text:\\n\\t{text}\")\n",
    "print(f\"Redacted text:\\n\\t{anonymized_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Anonymization with Replacement using Fakr and Presidio\n",
    "\n",
    "Instead of redacting text, we can anonymize it by swapping PII (like names and phone numbers) with fake data generated using the [Faker](https://faker.readthedocs.io/en/master/) Python library. For example:\n",
    "\n",
    "`\"My name is Raphael and I like to fish. My phone number is 212-555-5555\"` \n",
    "\n",
    "might become\n",
    "\n",
    "`\"My name is Katherine Dixon and I like to fish. My phone number is 667.431.7379\"`\n",
    "\n",
    "To effectively utilize Presidio, we must supply references to our custom operators. These operators will direct Presidio to the functions responsible for swapping PII with fake data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "3XJa7u5T_WYd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text:\n",
      "\tMy name is Raphael and I like to fish. My phone number is 212-555-5555\n",
      "Anonymized text:\n",
      "\tMy name is Jennifer Waters and I like to fish. My phone number is 8007771735\n"
     ]
    }
   ],
   "source": [
    "from faker import Faker\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_anonymizer.entities import OperatorConfig\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "\n",
    "# Create faker functions (note that it has to receive a value)\n",
    "def fake_name(x):\n",
    "    return fake.name()\n",
    "\n",
    "\n",
    "def fake_number(x):\n",
    "    return fake.phone_number()\n",
    "\n",
    "\n",
    "# Create custom operator for the PERSON and PHONE_NUMBER\" entities\n",
    "operators = {\n",
    "    \"PERSON\": OperatorConfig(\"custom\", {\"lambda\": fake_name}),\n",
    "    \"PHONE_NUMBER\": OperatorConfig(\"custom\", {\"lambda\": fake_number}),\n",
    "}\n",
    "\n",
    "text_to_anonymize = (\n",
    "    \"My name is Raphael and I like to fish. My phone number is 212-555-5555\"\n",
    ")\n",
    "\n",
    "# Analyzer output\n",
    "analyzer_results = analyzer.analyze(\n",
    "    text=text_to_anonymize, entities=[\"PHONE_NUMBER\", \"PERSON\"], language=\"en\"\n",
    ")\n",
    "\n",
    "anonymizer = AnonymizerEngine()\n",
    "\n",
    "# do not forget to pass the operators from above to the anonymizer\n",
    "anonymized_results = anonymizer.anonymize(\n",
    "    text=text_to_anonymize, analyzer_results=analyzer_results, operators=operators\n",
    ")\n",
    "\n",
    "print(f\"Raw text:\\n\\t{text_to_anonymize}\")\n",
    "print(f\"Anonymized text:\\n\\t{anonymized_results.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4HJcskpSYdL"
   },
   "source": [
    "Let's consolidate our code into a single class and expand the list of entities to include the additional ones we identified earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "yIaharNAWGh5"
   },
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_anonymizer.entities import OperatorConfig\n",
    "\n",
    "\n",
    "# A custom class for generating fake data that extends Faker\n",
    "class my_faker(Faker):\n",
    "    # Create faker functions (note that it has to receive a value)\n",
    "    def fake_address(x):\n",
    "        return fake.address()\n",
    "\n",
    "    def fake_ssn(x):\n",
    "        return fake.ssn()\n",
    "\n",
    "    def fake_name(x):\n",
    "        return fake.name()\n",
    "\n",
    "    def fake_number(x):\n",
    "        return fake.phone_number()\n",
    "\n",
    "    def fake_email(x):\n",
    "        return fake.email()\n",
    "\n",
    "    # Create custom operators for the entities\n",
    "    operators = {\n",
    "        \"PERSON\": OperatorConfig(\"custom\", {\"lambda\": fake_name}),\n",
    "        \"PHONE_NUMBER\": OperatorConfig(\"custom\", {\"lambda\": fake_number}),\n",
    "        \"EMAIL_ADDRESS\": OperatorConfig(\"custom\", {\"lambda\": fake_email}),\n",
    "        \"LOCATION\": OperatorConfig(\"custom\", {\"lambda\": fake_address}),\n",
    "        \"US_SSN\": OperatorConfig(\"custom\", {\"lambda\": fake_ssn}),\n",
    "    }\n",
    "\n",
    "    def redact_and_anonymize_with_faker(self, text):\n",
    "        anonymizer = AnonymizerEngine()\n",
    "        analyzer_results = analyzer.analyze(\n",
    "            text=text,\n",
    "            entities=[\"PHONE_NUMBER\", \"PERSON\", \"LOCATION\", \"EMAIL_ADDRESS\", \"US_SSN\"],\n",
    "            language=\"en\",\n",
    "        )\n",
    "        anonymized_results = anonymizer.anonymize(\n",
    "            text=text, analyzer_results=analyzer_results, operators=self.operators\n",
    "        )\n",
    "        return anonymized_results.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the function with a sample text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text:\n",
      "\tMy name is Raphael and I like to fish. My phone number is 212-555-5555\n",
      "Anonymized text:\n",
      "\tMy name is Amy Adkins and I like to fish. My phone number is +1-481-464-7524x69850\n"
     ]
    }
   ],
   "source": [
    "faker = my_faker()\n",
    "text_to_anonymize = (\n",
    "    \"My name is Raphael and I like to fish. My phone number is 212-555-5555\"\n",
    ")\n",
    "anonymized_text = faker.redact_and_anonymize_with_faker(text_to_anonymize)\n",
    "\n",
    "print(f\"Raw text:\\n\\t{text_to_anonymize}\")\n",
    "print(f\"Anonymized text:\\n\\t{anonymized_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the Methods to Weave Calls\n",
    "\n",
    "In these examples we will integrate our PII redaction and anonymization methods into Weave Models, and preview the results in Weave Traces.\n",
    "\n",
    "We'll create a [Weave Model](https://wandb.github.io/weave/guides/core-types/models) which is a combination of data (which can include configuration, trained model weights, or other information) and code that defines how the model operates. \n",
    "\n",
    "In this model, we will include our predict function where the Anthropic API will be called. Additionally, we will include our postprocessing functions to ensure that our PII data is redacted or anonymized before it is sent to the LLM.\n",
    "\n",
    "Once you run this code you will receive a links to the Weave project page as well as the specific trace (LLM calls)you ran."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex Method \n",
    "\n",
    "In the simplest case, we can use regex to identify and redact PII data in the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any\n",
    "\n",
    "import anthropic\n",
    "\n",
    "import weave\n",
    "\n",
    "\n",
    "# Define an input postprocessing function that applies our regex redaction for the model prediction Weave Op\n",
    "def postprocess_inputs_regex(inputs: dict[str, Any]) -> dict:\n",
    "    inputs[\"text_block\"] = redact_with_regex(inputs[\"text_block\"])\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# Weave model / predict function\n",
    "class sentiment_analysis_regex_pii_model(weave.Model):\n",
    "    model_name: str\n",
    "    system_prompt: str\n",
    "    temperature: int\n",
    "\n",
    "    @weave.op(\n",
    "        postprocess_inputs=postprocess_inputs_regex,\n",
    "    )\n",
    "    async def predict(self, text_block: str) -> dict:\n",
    "        client = anthropic.AsyncAnthropic()\n",
    "        response = await client.messages.create(\n",
    "            max_tokens=1024,\n",
    "            model=self.model_name,\n",
    "            system=self.system_prompt,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": text_block}]}\n",
    "            ],\n",
    "        )\n",
    "        result = response.content[0].text\n",
    "        if result is None:\n",
    "            raise ValueError(\"No response from model\")\n",
    "        parsed = json.loads(result)\n",
    "        return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  sentiment_analysis_regex_pii_model(name='claude-3-sonnet', description=None, model_name='claude-3-5-sonnet-20240620', system_prompt='You are a Sentiment Analysis classifier. You will be classifying text based on their sentiment. Your input will be a block of text. You will answer with one the following rating option[\"positive\", \"negative\", \"neutral\"]. Your answer should be one word in json format: {classification}. Ensure that it is valid JSON.', temperature=0)\n",
      "🍩 https://wandb.ai/wandb/pii_cookbook/r/call/01924a61-b36e-7f60-a856-6569856128c9\n",
      "🍩 https://wandb.ai/wandb/pii_cookbook/r/call/01924a61-b9e3-7751-95e9-7949b34cd9c3\n",
      "🍩 https://wandb.ai/wandb/pii_cookbook/r/call/01924a61-bd4c-72b3-bd33-3940cb2c3812\n",
      "🍩 https://wandb.ai/wandb/pii_cookbook/r/call/01924a61-c0a8-7390-a5f4-3ed0767d2b87\n",
      "🍩 https://wandb.ai/wandb/pii_cookbook/r/call/01924a61-c442-7fc2-9523-3e20f1e25dc0\n",
      "🍩 https://wandb.ai/wandb/pii_cookbook/r/call/01924a61-c7d3-7a30-9975-29cab0ec0fd7\n",
      "🍩 https://wandb.ai/wandb/pii_cookbook/r/call/01924a61-cb5a-76d2-a7ed-e8cd55a4f280\n",
      "🍩 https://wandb.ai/wandb/pii_cookbook/r/call/01924a61-cef1-7220-852b-2d9a1e30e8ab\n",
      "🍩 https://wandb.ai/wandb/pii_cookbook/r/call/01924a61-d284-7bc3-9181-d2284f291906\n",
      "🍩 https://wandb.ai/wandb/pii_cookbook/r/call/01924a61-d637-7713-8fea-392f83ae8af1\n"
     ]
    }
   ],
   "source": [
    "# create our LLM model with a system prompt\n",
    "model = sentiment_analysis_regex_pii_model(\n",
    "    name=\"claude-3-sonnet\",\n",
    "    model_name=\"claude-3-5-sonnet-20240620\",\n",
    "    system_prompt='You are a Sentiment Analysis classifier. You will be classifying text based on their sentiment. Your input will be a block of text. You will answer with one the following rating option[\"positive\", \"negative\", \"neutral\"]. Your answer should be one word in json format: {classification}. Ensure that it is valid JSON.',\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(\"Model: \", model)\n",
    "# for every block of text, anonymized first and then predict\n",
    "for entry in pii_data:\n",
    "    await model.predict(entry[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presidio Redaction Method\n",
    "\n",
    "Here we will use Presidio to identify and redact PII data in the original text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../../media/pii/redact.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any\n",
    "\n",
    "import anthropic\n",
    "\n",
    "import weave\n",
    "\n",
    "\n",
    "# Define an input postprocessing function that applies our Presidio redaction for the model prediction Weave Op\n",
    "def postprocess_inputs_presidio(inputs: dict[str, Any]) -> dict:\n",
    "    inputs[\"text_block\"] = redact_with_presidio(inputs[\"text_block\"])\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# Weave model / predict function\n",
    "class sentiment_analysis_presidio_pii_model(weave.Model):\n",
    "    model_name: str\n",
    "    system_prompt: str\n",
    "    temperature: int\n",
    "\n",
    "    @weave.op(\n",
    "        postprocess_inputs=postprocess_inputs_presidio,\n",
    "    )\n",
    "    async def predict(self, text_block: str) -> dict:\n",
    "        client = anthropic.AsyncAnthropic()\n",
    "        response = await client.messages.create(\n",
    "            max_tokens=1024,\n",
    "            model=self.model_name,\n",
    "            system=self.system_prompt,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": text_block}]}\n",
    "            ],\n",
    "        )\n",
    "        result = response.content[0].text\n",
    "        if result is None:\n",
    "            raise ValueError(\"No response from model\")\n",
    "        parsed = json.loads(result)\n",
    "        return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  sentiment_analysis_presidio_pii_model(name='claude-3-sonnet', description=None, model_name='claude-3-5-sonnet-20240620', system_prompt='You are a Sentiment Analysis classifier. You will be classifying text based on their sentiment. Your input will be a block of text. You will answer with one the following rating option[\"positive\", \"negative\", \"neutral\"]. Your answer should be one word in json format: {classification}. Ensure that it is valid JSON.', temperature=0)\n",
      "🍩 https://wandb.ai/wandb/pii_cookbook/r/call/01924a61-dd6f-7851-8c99-18529ddc63f4\n",
      "🍩 https://wandb.ai/wandb/pii_cookbook/r/call/01924a61-e17e-7cc3-a28f-7be155d87eb1\n",
      "🍩 https://wandb.ai/wandb/pii_cookbook/r/call/01924a61-e538-76d3-a5a3-0e89bf4ff6b9\n",
      "🍩 https://wandb.ai/wandb/pii_cookbook/r/call/01924a61-e8cf-7fb1-b942-11e5442dcf22\n",
      "🍩 https://wandb.ai/wandb/pii_cookbook/r/call/01924a61-ef00-7793-8799-10ff5a32e129\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# for every block of text, anonymized first and then predict\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m pii_data:\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m model\u001b[38;5;241m.\u001b[39mpredict(entry[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/weave/trace/op.py:464\u001b[0m, in \u001b[0;36mop.<locals>.op_deco.<locals>.create_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m     log_once(\n\u001b[1;32m    460\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror,\n\u001b[1;32m    461\u001b[0m         ASYNC_CALL_CREATE_MSG\u001b[38;5;241m.\u001b[39mformat(traceback\u001b[38;5;241m.\u001b[39mformat_exc()),\n\u001b[1;32m    462\u001b[0m     )\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 464\u001b[0m res, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m _execute_call(wrapper, call, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/weave/trace/op.py:274\u001b[0m, in \u001b[0;36m_execute_call.<locals>._call_async\u001b[0;34m()\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_async\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Coroutine[Any, Any, Any]:\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_exception(e)\n",
      "Cell \u001b[0;32mIn[19], line 23\u001b[0m, in \u001b[0;36msentiment_analysis_presidio_pii_model.predict\u001b[0;34m(self, text_block)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;129m@weave\u001b[39m\u001b[38;5;241m.\u001b[39mop(\n\u001b[1;32m     19\u001b[0m     postprocess_inputs\u001b[38;5;241m=\u001b[39mpostprocess_inputs_presidio,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_block: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m     22\u001b[0m     client \u001b[38;5;241m=\u001b[39m anthropic\u001b[38;5;241m.\u001b[39mAsyncAnthropic()\n\u001b[0;32m---> 23\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m client\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     24\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m     25\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name,\n\u001b[1;32m     26\u001b[0m         system\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msystem_prompt,\n\u001b[1;32m     27\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     28\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: text_block}]}\n\u001b[1;32m     29\u001b[0m         ],\n\u001b[1;32m     30\u001b[0m     )\n\u001b[1;32m     31\u001b[0m     result \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mcontent[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/weave/trace/op.py:464\u001b[0m, in \u001b[0;36mop.<locals>.op_deco.<locals>.create_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m     log_once(\n\u001b[1;32m    460\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror,\n\u001b[1;32m    461\u001b[0m         ASYNC_CALL_CREATE_MSG\u001b[38;5;241m.\u001b[39mformat(traceback\u001b[38;5;241m.\u001b[39mformat_exc()),\n\u001b[1;32m    462\u001b[0m     )\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 464\u001b[0m res, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m _execute_call(wrapper, call, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/weave/trace/op.py:274\u001b[0m, in \u001b[0;36m_execute_call.<locals>._call_async\u001b[0;34m()\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_async\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Coroutine[Any, Any, Any]:\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_exception(e)\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/weave/integrations/anthropic/anthropic_sdk.py:100\u001b[0m, in \u001b[0;36mcreate_wrapper_async.<locals>.wrapper.<locals>._fn_wrapper.<locals>._async_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_async_wrapper\u001b[39m(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;241m*\u001b[39margs: typing\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: typing\u001b[38;5;241m.\u001b[39mAny\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/anthropic/resources/messages.py:1799\u001b[0m, in \u001b[0;36mAsyncMessages.create\u001b[0;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m DEPRECATED_MODELS:\n\u001b[1;32m   1793\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1794\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will reach end-of-life on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEPRECATED_MODELS[model]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1795\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m   1796\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m   1797\u001b[0m     )\n\u001b[0;32m-> 1799\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/v1/messages\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m   1802\u001b[0m         {\n\u001b[1;32m   1803\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   1804\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   1805\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   1806\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m   1807\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop_sequences,\n\u001b[1;32m   1808\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   1809\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m: system,\n\u001b[1;32m   1810\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   1811\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   1812\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   1813\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_k,\n\u001b[1;32m   1814\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   1815\u001b[0m         },\n\u001b[1;32m   1816\u001b[0m         message_create_params\u001b[38;5;241m.\u001b[39mMessageCreateParams,\n\u001b[1;32m   1817\u001b[0m     ),\n\u001b[1;32m   1818\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   1819\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1820\u001b[0m     ),\n\u001b[1;32m   1821\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mMessage,\n\u001b[1;32m   1822\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1823\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[RawMessageStreamEvent],\n\u001b[1;32m   1824\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/anthropic/_base_client.py:1816\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1803\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1804\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1811\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1812\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1813\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1814\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1815\u001b[0m     )\n\u001b[0;32m-> 1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/anthropic/_base_client.py:1510\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m   1502\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1503\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     remaining_retries: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1509\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1511\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1512\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1513\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1514\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1515\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m   1516\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/anthropic/_base_client.py:1549\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1546\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauth\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_auth\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1549\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\n\u001b[1;32m   1550\u001b[0m         request,\n\u001b[1;32m   1551\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream_response_body(request\u001b[38;5;241m=\u001b[39mrequest),\n\u001b[1;32m   1552\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1553\u001b[0m     )\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1555\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/httpx/_client.py:1674\u001b[0m, in \u001b[0;36mAsyncClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m   1672\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m-> 1674\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[1;32m   1675\u001b[0m     request,\n\u001b[1;32m   1676\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[1;32m   1677\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1678\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m   1679\u001b[0m )\n\u001b[1;32m   1680\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1681\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/httpx/_client.py:1702\u001b[0m, in \u001b[0;36mAsyncClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1699\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m auth_flow\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__anext__\u001b[39m()\n\u001b[1;32m   1701\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1702\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[1;32m   1703\u001b[0m         request,\n\u001b[1;32m   1704\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1705\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m   1706\u001b[0m     )\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/httpx/_client.py:1739\u001b[0m, in \u001b[0;36mAsyncClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1736\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[0;32m-> 1739\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[1;32m   1740\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1741\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/httpx/_client.py:1776\u001b[0m, in \u001b[0;36mAsyncClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1772\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an sync request with an AsyncClient instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1773\u001b[0m     )\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1776\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m transport\u001b[38;5;241m.\u001b[39mhandle_async_request(request)\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, AsyncByteStream)\n\u001b[1;32m   1779\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/httpx/_transports/default.py:377\u001b[0m, in \u001b[0;36mAsyncHTTPTransport.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    364\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    365\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    366\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    375\u001b[0m )\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 377\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_async_request(req)\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mAsyncIterable)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    382\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    383\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    384\u001b[0m     stream\u001b[38;5;241m=\u001b[39mAsyncResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    385\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    386\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:216\u001b[0m, in \u001b[0;36mAsyncConnectionPool.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, AsyncIterable)\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:196\u001b[0m, in \u001b[0;36mAsyncConnectionPool.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m connection\u001b[38;5;241m.\u001b[39mhandle_async_request(\n\u001b[1;32m    197\u001b[0m         pool_request\u001b[38;5;241m.\u001b[39mrequest\n\u001b[1;32m    198\u001b[0m     )\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/httpcore/_async/connection.py:101\u001b[0m, in \u001b[0;36mAsyncHTTPConnection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_async_request(request)\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/httpcore/_async/http11.py:143\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/httpcore/_async/http11.py:113\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/httpcore/_async/http11.py:186\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_event(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/httpcore/_async/http11.py:224\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\u001b[38;5;241m.\u001b[39mread(\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREAD_NUM_BYTES, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    226\u001b[0m     )\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/httpcore/_backends/anyio.py:35\u001b[0m, in \u001b[0;36mAnyIOStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39mfail_after(timeout):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream\u001b[38;5;241m.\u001b[39mreceive(max_bytes\u001b[38;5;241m=\u001b[39mmax_bytes)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39mEndOfStream:  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/anyio/streams/tls.py:205\u001b[0m, in \u001b[0;36mTLSStream.receive\u001b[0;34m(self, max_bytes)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreceive\u001b[39m(\u001b[38;5;28mself\u001b[39m, max_bytes: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m65536\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbytes\u001b[39m:\n\u001b[0;32m--> 205\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_sslobject_method(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ssl_object\u001b[38;5;241m.\u001b[39mread, max_bytes)\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m EndOfStream\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/anyio/streams/tls.py:147\u001b[0m, in \u001b[0;36mTLSStream._call_sslobject_method\u001b[0;34m(self, func, *args)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_bio\u001b[38;5;241m.\u001b[39mpending:\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransport_stream\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_bio\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m--> 147\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransport_stream\u001b[38;5;241m.\u001b[39mreceive()\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EndOfStream:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_bio\u001b[38;5;241m.\u001b[39mwrite_eof()\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1198\u001b[0m, in \u001b[0;36mSocketStream.receive\u001b[0;34m(self, max_bytes)\u001b[0m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1193\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol\u001b[38;5;241m.\u001b[39mread_event\u001b[38;5;241m.\u001b[39mis_set()\n\u001b[1;32m   1194\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mis_closing()\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol\u001b[38;5;241m.\u001b[39mis_at_eof\n\u001b[1;32m   1196\u001b[0m ):\n\u001b[1;32m   1197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mresume_reading()\n\u001b[0;32m-> 1198\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol\u001b[38;5;241m.\u001b[39mread_event\u001b[38;5;241m.\u001b[39mwait()\n\u001b[1;32m   1199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mpause_reading()\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/projects/weave/.conda/lib/python3.12/asyncio/locks.py:212\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiters\u001b[38;5;241m.\u001b[39mappend(fut)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create our LLM model with a system prompt\n",
    "model = sentiment_analysis_presidio_pii_model(\n",
    "    name=\"claude-3-sonnet\",\n",
    "    model_name=\"claude-3-5-sonnet-20240620\",\n",
    "    system_prompt='You are a Sentiment Analysis classifier. You will be classifying text based on their sentiment. Your input will be a block of text. You will answer with one the following rating option[\"positive\", \"negative\", \"neutral\"]. Your answer should be one word in json format: {classification}. Ensure that it is valid JSON.',\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(\"Model: \", model)\n",
    "# for every block of text, anonymized first and then predict\n",
    "for entry in pii_data:\n",
    "    await model.predict(entry[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faker + Presidio Replacement Method\n",
    "\n",
    "Here we will have Faker generate anonymized replacement PII data and use Presidio to identify and replace the PII data in the original text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../../media/pii/replace.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any\n",
    "\n",
    "import anthropic\n",
    "\n",
    "import weave\n",
    "\n",
    "# Define an input postprocessing function that applies our Faker anonymization and Presidio redaction for the model prediction Weave Op\n",
    "faker = my_faker()\n",
    "\n",
    "\n",
    "def postprocess_inputs_faker(inputs: dict[str, Any]) -> dict:\n",
    "    inputs[\"text_block\"] = faker.redact_and_anonymize_with_faker(inputs[\"text_block\"])\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# Weave model / predict function\n",
    "class sentiment_analysis_faker_pii_model(weave.Model):\n",
    "    model_name: str\n",
    "    system_prompt: str\n",
    "    temperature: int\n",
    "\n",
    "    @weave.op(\n",
    "        postprocess_inputs=postprocess_inputs_faker,\n",
    "    )\n",
    "    async def predict(self, text_block: str) -> dict:\n",
    "        client = anthropic.AsyncAnthropic()\n",
    "        response = await client.messages.create(\n",
    "            max_tokens=1024,\n",
    "            model=self.model_name,\n",
    "            system=self.system_prompt,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": text_block}]}\n",
    "            ],\n",
    "        )\n",
    "        result = response.content[0].text\n",
    "        if result is None:\n",
    "            raise ValueError(\"No response from model\")\n",
    "        parsed = json.loads(result)\n",
    "        return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  sentiment_analysis_faker_pii_model(name='claude-3-sonnet', description=None, model_name='claude-3-5-sonnet-20240620', system_prompt='You are a Sentiment Analysis classifier. You will be classifying text based on their sentiment. Your input will be a block of text. You will answer with one the following rating option[\"positive\", \"negative\", \"neutral\"]. Your answer should be one word in json format: {classification}. Ensure that it is valid JSON.', temperature=0)\n",
      "🍩 https://wandb.ai/wandb/pii_cookbook/r/call/01924a62-34d0-70a0-a442-d08e59d745de\n",
      "🍩 https://wandb.ai/wandb/pii_cookbook/r/call/01924a62-386a-7632-8785-8102ac495ae8\n",
      "🍩 https://wandb.ai/wandb/pii_cookbook/r/call/01924a62-3c15-70b1-8bd9-b143e0aa8b20\n",
      "🍩 https://wandb.ai/wandb/pii_cookbook/r/call/01924a62-4105-7af1-8446-572485bab118\n",
      "🍩 https://wandb.ai/wandb/pii_cookbook/r/call/01924a62-44c5-7760-8d93-65743f1f2152\n",
      "🍩 https://wandb.ai/wandb/pii_cookbook/r/call/01924a62-48c1-7b71-ab41-0e02dddd795c\n",
      "🍩 https://wandb.ai/wandb/pii_cookbook/r/call/01924a62-4d50-7320-9f1b-03e9a3efeb4b\n",
      "🍩 https://wandb.ai/wandb/pii_cookbook/r/call/01924a62-5155-78e3-a7a5-605fc663a8c3\n",
      "🍩 https://wandb.ai/wandb/pii_cookbook/r/call/01924a62-54fc-71d1-9b6c-775570e84b24\n",
      "🍩 https://wandb.ai/wandb/pii_cookbook/r/call/01924a62-589a-7d22-a8d5-eaeed5e8df40\n"
     ]
    }
   ],
   "source": [
    "# create our LLM model with a system prompt\n",
    "model = sentiment_analysis_faker_pii_model(\n",
    "    name=\"claude-3-sonnet\",\n",
    "    model_name=\"claude-3-5-sonnet-20240620\",\n",
    "    system_prompt='You are a Sentiment Analysis classifier. You will be classifying text based on their sentiment. Your input will be a block of text. You will answer with one the following rating option[\"positive\", \"negative\", \"neutral\"]. Your answer should be one word in json format: {classification}. Ensure that it is valid JSON.',\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(\"Model: \", model)\n",
    "# for every block of text, anonymized first and then predict\n",
    "for entry in pii_data:\n",
    "    await model.predict(entry[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checklist for Safely Using Weave with PII Data\n",
    "\n",
    "### During Testing\n",
    "- Log anonymized data to check PII detection\n",
    "- Track PII handling processes with Weave Traces\n",
    "- Measure anonymization performance without exposing real PII\n",
    "\n",
    "### In Production\n",
    "- Never log raw PII\n",
    "- Encrypt sensitive fields before logging\n",
    "\n",
    "### Encryption Tips\n",
    "- Use reversible encryption for data you need to decrypt later\n",
    "- Apply one-way hashing for unique IDs you don't need to reverse\n",
    "- Consider specialized encryption for data you need to analyze while encrypted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> (Optional) Encrypting our data </summary>\n",
    "![](../../media/pii/encrypt.png)\n",
    "\n",
    "In addition to anonymizing PII, we can add an extra layer of security by encrypting our data using the cryptography library's [Fernet](https://cryptography.io/en/latest/fernet/) symmetric encryption. This approach ensures that even if the anonymized data is intercepted, it remains unreadable without the encryption key.\n",
    "\n",
    "```python\n",
    "import os\n",
    "from cryptography.fernet import Fernet\n",
    "from pydantic import BaseModel, ValidationInfo, model_validator\n",
    "\n",
    "def get_fernet_key():\n",
    "    # Check if the key exists in environment variables\n",
    "    key = os.environ.get('FERNET_KEY')\n",
    "    \n",
    "    if key is None:\n",
    "        # If the key doesn't exist, generate a new one\n",
    "        key = Fernet.generate_key()\n",
    "        # Save the key to an environment variable\n",
    "        os.environ['FERNET_KEY'] = key.decode()\n",
    "    else:\n",
    "        # If the key exists, ensure it's in bytes\n",
    "        key = key.encode()\n",
    "    \n",
    "    return key\n",
    "\n",
    "cipher_suite = Fernet(get_fernet_key())\n",
    "\n",
    "class EncryptedSentimentAnalysisInput(BaseModel):\n",
    "    encrypted_text: str = None\n",
    "\n",
    "    @model_validator(mode=\"before\")\n",
    "    def encrypt_fields(cls, values):\n",
    "        if \"text\" in values and values[\"text\"] is not None:\n",
    "            values[\"encrypted_text\"] = cipher_suite.encrypt(values[\"text\"].encode()).decode()\n",
    "            del values[\"text\"]\n",
    "        return values\n",
    "\n",
    "    @property\n",
    "    def text(self):\n",
    "        if self.encrypted_text:\n",
    "            return cipher_suite.decrypt(self.encrypted_text.encode()).decode()\n",
    "        return None\n",
    "\n",
    "    @text.setter\n",
    "    def text(self, value):\n",
    "        self.encrypted_text = cipher_suite.encrypt(str(value).encode()).decode()\n",
    "\n",
    "    @classmethod\n",
    "    def encrypt(cls, text: str):\n",
    "        return cls(text=text)\n",
    "\n",
    "    def decrypt(self):\n",
    "        return self.text\n",
    "\n",
    "# Modified sentiment_analysis_model to use the new EncryptedSentimentAnalysisInput\n",
    "class sentiment_analysis_model(weave.Model):\n",
    "    model_name: str\n",
    "    system_prompt: str\n",
    "    temperature: int\n",
    "\n",
    "    @weave.op()\n",
    "    async def predict(self, encrypted_input: EncryptedSentimentAnalysisInput) -> dict:\n",
    "        client = AsyncAnthropic()\n",
    "\n",
    "        decrypted_text = encrypted_input.decrypt() # We use the custom class to decrypt the text\n",
    "\n",
    "        response = await client.messages.create(\n",
    "            max_tokens=1024,\n",
    "            model=self.model_name,\n",
    "            system=self.system_prompt,\n",
    "            messages=[\n",
    "                {   \"role\": \"user\",\n",
    "                    \"content\":[\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": decrypted_text\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        result = response.content[0].text\n",
    "        if result is None:\n",
    "            raise ValueError(\"No response from model\")\n",
    "        parsed = json.loads(result)\n",
    "        return parsed\n",
    "\n",
    "model = sentiment_analysis_model(\n",
    "    name=\"claude-3-sonnet\",\n",
    "    model_name=\"claude-3-5-sonnet-20240620\",\n",
    "    system_prompt=\"You are a Sentiment Analysis classifier. You will be classifying text based on their sentiment. Your input will be a block of text. You will answer with one the following rating option[\\\"positive\\\", \\\"negative\\\", \\\"neutral\\\"]. Your answer should one word in json format dict where the key is classification.\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "for entry in pii_data:\n",
    "    encrypted_input = EncryptedSentimentAnalysisInput.encrypt(entry[\"text\"])\n",
    "    await model.predict(encrypted_input)\n",
    "```\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "XiRkYiiIj2KX"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
