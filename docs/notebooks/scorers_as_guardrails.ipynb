{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- docusaurus_head_meta::start\n",
    "---\n",
    "title: Scorers as Guardrails\n",
    "---\n",
    "docusaurus_head_meta::end -->\n",
    "\n",
    "<!--- @wandbcode{prompt-optim-notebook} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EauaASOOUsB"
   },
   "source": [
    "# Scorers as Guardrails\n",
    "\n",
    "Weave Scorers are special classes with a `score` method that can evaluate the performance of a call. They can range from quite simple rules to complex LLMs as judges. \n",
    "\n",
    "In this notebook, we will explore how to use Scorers as guardrails to prevent your LLM from generating harmful or inappropriate content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RJcDCJWWShcZ",
    "outputId": "cace015b-dcaf-4bef-a105-2a62a2361e29"
   },
   "outputs": [],
   "source": [
    "%pip install weave --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oKawLdN3SmJG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as Weights & Biases user: timssweeney.\n",
      "View Weave data at https://wandb.ai/timssweeney/content-safety-guardrails/weave\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Example demonstrating how to implement guardrails in Weave.\n",
    "This example shows a simple content safety checker that prevents\n",
    "potentially harmful or negative responses.\n",
    "\"\"\"\n",
    "\n",
    "import weave\n",
    "\n",
    "# Initialize Weave with a descriptive project name\n",
    "weave.init(\"content-safety-guardrails\")\n",
    "\n",
    "\n",
    "class ContentSafetyScorer(weave.Scorer):\n",
    "    \"\"\"A scorer that evaluates content safety based on presence of specified phrases.\"\"\"\n",
    "\n",
    "    unsafe_phrases: list[str]\n",
    "    case_sensitive: bool = False\n",
    "\n",
    "    @weave.op\n",
    "    def score(self, output: str) -> bool:\n",
    "        \"\"\"\n",
    "        Evaluate output safety based on presence of unsafe phrases.\n",
    "\n",
    "        Args:\n",
    "            output: The text output to evaluate\n",
    "\n",
    "        Returns:\n",
    "            bool: True if output is safe, False if unsafe\n",
    "        \"\"\"\n",
    "        normalized_output = output if self.case_sensitive else output.lower()\n",
    "\n",
    "        for phrase in self.unsafe_phrases:\n",
    "            normalized_phrase = phrase if self.case_sensitive else phrase.lower()\n",
    "            if normalized_phrase in normalized_output:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "\n",
    "@weave.op\n",
    "def generate_response(prompt: str) -> str:\n",
    "    \"\"\"Simulate an LLM response generation.\"\"\"\n",
    "    if \"test\" in prompt.lower():\n",
    "        return \"I'm sorry, I cannot process that request.\"\n",
    "    elif \"help\" in prompt.lower():\n",
    "        return \"I'd be happy to help you with that!\"\n",
    "    else:\n",
    "        return \"Here's what you requested: \" + prompt\n",
    "\n",
    "\n",
    "async def process_with_guardrail(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Process user input with content safety guardrail.\n",
    "    Returns the response if safe, or a fallback message if unsafe.\n",
    "    \"\"\"\n",
    "    # Initialize safety scorer\n",
    "    safety_scorer = ContentSafetyScorer(\n",
    "        name=\"Content Safety Checker\",\n",
    "        unsafe_phrases=[\"sorry\", \"cannot\", \"unable\", \"won't\", \"will not\"],\n",
    "    )\n",
    "\n",
    "    # Generate response and get Call object\n",
    "    response, call = generate_response.call(prompt)\n",
    "\n",
    "    # Apply safety scoring\n",
    "    evaluation = await call.apply_scorer(safety_scorer)\n",
    "\n",
    "    # Return response or fallback based on safety check\n",
    "    if evaluation.result:\n",
    "        return response\n",
    "    else:\n",
    "        return \"I cannot provide that response.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing content safety guardrails:\n",
      "\n",
      "Input: 'Please help me with my homework'\n",
      "游꼴 https://wandb.ai/timssweeney/content-safety-guardrails/r/call/01945209-4b69-7481-86d2-a15e08dac27e\n",
      "游꼴 https://wandb.ai/timssweeney/content-safety-guardrails/r/call/01945209-4b70-7241-aafb-774513b4fc3d\n",
      "Response: I'd be happy to help you with that!\n",
      "\n",
      "Input: 'Can you run a test for me?'\n",
      "游꼴 https://wandb.ai/timssweeney/content-safety-guardrails/r/call/01945209-51bf-7c01-a9b6-958774440844\n",
      "游꼴 https://wandb.ai/timssweeney/content-safety-guardrails/r/call/01945209-51c4-7080-9055-b835ac7a1862\n",
      "Response: I cannot provide that response.\n",
      "\n",
      "Input: 'Tell me a joke'\n",
      "游꼴 https://wandb.ai/timssweeney/content-safety-guardrails/r/call/01945209-53bb-7fb3-a8e6-76987c34114d\n",
      "游꼴 https://wandb.ai/timssweeney/content-safety-guardrails/r/call/01945209-53c5-75e0-8666-c97abddc9fdc\n",
      "Response: Here's what you requested: Tell me a joke\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Example usage of the guardrail system.\"\"\"\n",
    "test_prompts = [\n",
    "    \"Please help me with my homework\",\n",
    "    \"Can you run a test for me?\",\n",
    "    \"Tell me a joke\",\n",
    "]\n",
    "\n",
    "print(\"Testing content safety guardrails:\\n\")\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"Input: '{prompt}'\")\n",
    "    response = await process_with_guardrail(prompt)\n",
    "    print(f\"Response: {response}\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
