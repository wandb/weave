{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- docusaurus_head_meta::start\n",
    "---\n",
    "title: Custom Routing for LLM Prompts with Not Diamond\n",
    "---\n",
    "\n",
    "docusaurus_head_meta::end -->\n",
    "\n",
    "<!--- @wandbcode{cod-notebook} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Routing for LLM Prompts with Not Diamond\n",
    "\n",
    "This notebook demonstrates how to use Weave with [Not Diamond's custom routing](https://docs.notdiamond.ai/docs/router-training-quickstart) to route LLM prompts to the most appropriate model based on evaluation results.\n",
    "\n",
    "## Routing prompts\n",
    "\n",
    "When building complex LLM workflows users may need to prompt different models according to accuracy, cost, or call latency.\n",
    "Users can use [Not Diamond](https://www.notdiamond.ai/) to route prompts in these workflows to the right model for their needs, helping maximize accuracy while saving on model costs.\n",
    "\n",
    "For any given distribution of data, rarely will one single model outperform every other model on every single query. By combining together multiple models into a \"meta-model\" that learns when to call each LLM, you can beat every individual model's performance and even drive down costs and latency in the process.\n",
    "\n",
    "## Custom routing\n",
    "\n",
    "You need three things to train a custom router for your prompts:\n",
    "\n",
    "1. A set of LLM prompts: Prompts must be strings and should be representative of the prompts used in our application.\n",
    "1. LLM responses: The responses from candidate LLMs for each input. Candidate LLMs can include both our supported LLMs and your own custom models.\n",
    "1. Evaluation scores for responses to the inputs from candidate LLMs: Scores are numbers, and can be any metric that fit your needs.\n",
    "\n",
    "By submitting these to the Not Diamond API you can then train a custom router tuned to each of your workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the training data\n",
    "\n",
    "In practice, you will use your own Evaluations to train a custom router. For this example notebook, however, you will use LLM responses\n",
    "for [the HumanEval dataset](https://github.com/openai/human-eval) to train a custom router for coding tasks.\n",
    "\n",
    "We start by downloading the dataset we have prepared for this example, then parsing LLM responses into EvaluationResults for each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L \"https://drive.google.com/uc?export=download&id=1q1zNZHioy9B7M-WRjsJPkfvFosfaHX38\" -o humaneval.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import weave\n",
    "from weave.flow.dataset import Dataset\n",
    "from weave.flow.eval import EvaluationResults\n",
    "from weave.integrations.notdiamond.util import get_model_evals\n",
    "\n",
    "pct_train = 0.8\n",
    "pct_test = 1 - pct_train\n",
    "\n",
    "# In practice, you will build an Evaluation on your dataset and call\n",
    "# `evaluation.get_eval_results(model)`\n",
    "model_evals = get_model_evals(\"./humaneval.csv\")\n",
    "model_train = {}\n",
    "model_test = {}\n",
    "for model, evaluation_results in model_evals.items():\n",
    "    n_results = len(evaluation_results.rows)\n",
    "    all_idxs = list(range(n_results))\n",
    "    train_idxs = random.sample(all_idxs, k=int(n_results * pct_train))\n",
    "    test_idxs = [idx for idx in all_idxs if idx not in train_idxs]\n",
    "\n",
    "    model_train[model] = EvaluationResults(\n",
    "        rows=weave.Table([evaluation_results.rows[idx] for idx in train_idxs])\n",
    "    )\n",
    "    model_test[model] = Dataset(\n",
    "        rows=weave.Table([evaluation_results.rows[idx] for idx in test_idxs])\n",
    "    )\n",
    "    print(\n",
    "        f\"Found {len(train_idxs)} train rows and {len(test_idxs)} test rows for {model}.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a custom router\n",
    "\n",
    "Now that you have EvaluationResults, you can train a custom router. Make sure you have [created an account](https://app.notdiamond.ai/keys) and\n",
    "[generated an API key](https://app.notdiamond.ai/keys), then insert your API key below.\n",
    "\n",
    "![Create an API key](../docs/guides/integrations/imgs/notdiamond/api-keys.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from weave.integrations.notdiamond.custom_router import train_router\n",
    "\n",
    "api_key = os.getenv(\"NOTDIAMOND_API_KEY\", \"<YOUR_API_KEY>\")\n",
    "\n",
    "preference_id = train_router(\n",
    "    model_evals=model_train,\n",
    "    prompt_column=\"prompt\",\n",
    "    response_column=\"actual\",\n",
    "    language=\"en\",\n",
    "    maximize=True,\n",
    "    api_key=api_key,\n",
    "    # Leave this commented out to train your first custom router\n",
    "    # Uncomment this to retrain your custom router in place\n",
    "    # preference_id=preference_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then follow the training process for your custom router via the Not Diamond app.\n",
    "\n",
    "![Check on router training progress](../docs/guides/integrations/imgs/notdiamond/router-preferences.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your custom router has finished training, you can use it to route your prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notdiamond import NotDiamond\n",
    "\n",
    "import weave\n",
    "\n",
    "weave.init(\"notdiamond-quickstart\")\n",
    "\n",
    "llm_configs = [\n",
    "    \"anthropic/claude-3-5-sonnet-20240620\",\n",
    "    \"openai/gpt-4o-2024-05-13\",\n",
    "    \"google/gemini-1.5-pro-latest\",\n",
    "    \"openai/gpt-4-turbo-2024-04-09\",\n",
    "    \"anthropic/claude-3-opus-20240229\",\n",
    "]\n",
    "client = NotDiamond(api_key=api_key, llm_configs=llm_configs)\n",
    "\n",
    "new_prompt = (\n",
    "    \"\"\"\n",
    "You are a helpful coding assistant. Using the provided function signature, write the implementation for the function\n",
    "in Python. Write only the function. Do not include any other text.\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
    "    \"\"\"\n",
    "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
    "    given threshold.\n",
    "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
    "    False\n",
    "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
    "    True\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\"\"\"\n",
    ")\n",
    "session_id, routing_target_model = client.model_select(\n",
    "    messages=[{\"role\": \"user\", \"content\": new_prompt}],\n",
    "    preference_id=preference_id,\n",
    ")\n",
    "\n",
    "print(f\"Session ID: {session_id}\")\n",
    "print(f\"Target Model: {routing_target_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example also used Not Diamond's compatibility with Weave auto-tracing. You can see the results in the Weave UI.\n",
    "\n",
    "![Weave UI for custom routing](../docs/guides/integrations/imgs/notdiamond/weave-trace.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating your custom router\n",
    "\n",
    "Once you have trained your custom router, you can evaluate either its\n",
    "\n",
    "- in-sample performance by submitting the training prompts, or\n",
    "- out-of-sample performance by submitting new or held-out prompts\n",
    "\n",
    "Below, we submit the test set to the custom router to evaluate its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weave.integrations.notdiamond.custom_router import evaluate_router\n",
    "\n",
    "eval_prompt_column = \"prompt\"\n",
    "eval_response_column = \"actual\"\n",
    "\n",
    "best_provider_model, nd_model = evaluate_router(\n",
    "    model_datasets=model_test,\n",
    "    prompt_column=eval_prompt_column,\n",
    "    response_column=eval_response_column,\n",
    "    api_key=api_key,\n",
    "    preference_id=preference_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def is_correct(score: int, model_output: dict) -> dict:\n",
    "    # We hack score, since we already have model responses\n",
    "    return {\"correct\": score}\n",
    "\n",
    "\n",
    "best_provider_eval = weave.Evaluation(\n",
    "    dataset=best_provider_model.model_results.to_dict(orient=\"records\"),\n",
    "    scorers=[is_correct],\n",
    ")\n",
    "await best_provider_eval.evaluate(best_provider_model)\n",
    "\n",
    "nd_eval = weave.Evaluation(\n",
    "    dataset=nd_model.model_results.to_dict(orient=\"records\"), scorers=[is_correct]\n",
    ")\n",
    "await nd_eval.evaluate(nd_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this instance, the Not Diamond \"meta-model\" routes prompts across several different models.\n",
    "\n",
    "Training the custom router via Weave will also run evaluations and upload results to the Weave UI. Once the custom router process is completed, you can review the results in the Weave UI.\n",
    "\n",
    "In the UI we see that the Not Diamond \"meta-model\" outperforms the best-performing model by routing prompts to other models with higher likelihood of answering the prompt accurately.\n",
    "\n",
    "![Evaluating Not Diamond](../docs/guides/integrations/imgs/notdiamond/evaluations.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weave-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
