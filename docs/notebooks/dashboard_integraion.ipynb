{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- docusaurus_head_meta::start\n",
    "---\n",
    "title: Integrating with Weave - Production Dashboard\n",
    "---\n",
    "docusaurus_head_meta::end -->\n",
    "\n",
    "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "<!--- @wandbcode{cod-notebook} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating with Weave: Production Dashboard\n",
    "\n",
    "The GenAI tooling landscape is rapidly evolving - new frameworks, tools, and applications are emerging all the time. Weave aims to be a one-stop-shop for all your GenAI monitoring and evaluation needs. This also means that sometimes it is necessary to integrate with existing platforms or extend Weave to fit the personal needs of the specific company.\n",
    "\n",
    "In this cookbook, we'll demonstrate how to leverage Weave's powerful APIs and functions to create a custom dashboard for production monitoring as an extension to the Traces view in Weave. We'll focus on:\n",
    "* Fetching data from Weave\n",
    "* Creating aggregate views for user feedback and cost distribution\n",
    "* Creating visualizations for token usage and latency over time\n",
    "\n",
    "You can try out the dashboard with your own Weave project by installing streamlit and running [this script](https://github.com/NiWaRe/knowledge-worker-weave/blob/master/prod_dashboard.py)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/NiWaRe/knowledge-worker-weave/blob/master/screenshots/dashboard_weave_preview.jpg?raw=true\" width=\"1000\" alt=\"Example Production Dashboard with Weave\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup\n",
    "To follow along this tutorial you'll only need to install the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install streamlit pandas plotly weave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Initializing Weave Client and Defining Costs\n",
    "First, we'll set up a function to initialize the Weave client and add costs for each model. These costs are used to calculate the total cost of each call in Weave based on the tracked tokens during inference. We can also add an `effective_date` parameter to the function to set the date when the costs should be effective - more information about the `add_cost` function [here](https://weave-docs.wandb.ai/guides/tracking/costs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "import streamlit as st\n",
    "\n",
    "MODEL_NAMES = [\n",
    "    # model name, prompt cost, completion cost\n",
    "    (\"gpt-4o-2024-05-13\", 0.03, 0.06),\n",
    "    (\"gpt-4o-mini-2024-07-18\", 0.03, 0.06),\n",
    "    (\"gemini/gemini-1.5-flash\", 0.00025, 0.0005),\n",
    "    (\"gpt-4o-mini\", 0.03, 0.06),\n",
    "    (\"gpt-4-turbo\", 0.03, 0.06),\n",
    "    (\"claude-3-haiku-20240307\", 0.01, 0.03),\n",
    "    (\"gpt-4o\", 0.03, 0.06)\n",
    "]\n",
    "\n",
    "@st.cache_resource\n",
    "def init_weave_client(project_name):\n",
    "    try:\n",
    "        client = weave.init(project_name)\n",
    "        for model, prompt_cost, completion_cost in MODEL_NAMES:\n",
    "            client.add_cost(llm_id=model, prompt_token_cost=prompt_cost, completion_token_cost=completion_cost)\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        st.error(f\"Failed to initialize Weave client for project '{project_name}': {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Fetching Calls Data from Weave\n",
    "Next, we'll use the `calls_query_stream` API to fetch the calls data from Weave:\n",
    "\n",
    "* `calls_query_stream` API: This API allows us to fetch the calls data from Weave.\n",
    "* `filter` dictionary: This dictionary contains the filter parameters to fetch the calls data - see [here](https://weave-docs.wandb.ai/reference/python-sdk/weave/trace_server/weave.trace_server.trace_server_interface/#class-callschema) for more details.\n",
    "* `expand_columns` list: This list contains the columns to expand in the calls data.\n",
    "* `sort_by` list: This list contains the sorting parameters for the calls data.\n",
    "* `include_costs` boolean: This boolean indicates whether to include the costs in the calls data.\n",
    "* `include_feedback` boolean: This boolean indicates whether to include the feedback in the calls data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def fetch_calls(client, project_id, start_time, trace_roots_only, limit):\n",
    "    filter_params = {\n",
    "        \"project_id\": project_id,\n",
    "        \"filter\": {\"started_at\": start_time, \"trace_roots_only\": trace_roots_only},\n",
    "        \"expand_columns\": [\"inputs.example\", \"inputs.model\"],\n",
    "        \"sort_by\": [{\"field\": \"started_at\", \"direction\": \"desc\"}],\n",
    "        \"include_costs\": True,\n",
    "        \"include_feedback\": True,\n",
    "    }\n",
    "    try:\n",
    "        calls_stream = client.server.calls_query_stream(filter_params)\n",
    "        calls = list(itertools.islice(calls_stream, limit)) # limit the number of calls to fetch if too many\n",
    "        st.write(f\"Fetched {len(calls)} calls.\")\n",
    "        return calls\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error fetching calls: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the cost, we'll use the `query_costs` API to fetch the costs of all used LLMs using a single call from Weave:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cost API to get costs\n",
    "client = init_weave_client(selected_project)\n",
    "if client is None:\n",
    "    st.stop()\n",
    "\n",
    "costs = client.query_costs()\n",
    "df_costs = pd.DataFrame([cost.dict() for cost in costs])\n",
    "df_costs['total_cost'] = df_costs['prompt_token_cost'] + df_costs['completion_token_cost']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Processing Calls Data\n",
    "Processing the calls is very easy with the return from Weave - we'll extract the relevant information and store it in a list of dictionaries. We'll then convert the list of dictionaries to a pandas DataFrame and return it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def process_calls(calls):\n",
    "    records = []\n",
    "    for call in calls:\n",
    "        # For the latency and tokens we'll actually go through each call and extract model name from the inputs or outputs (this will change in the future)\n",
    "        model = next((model for model, _, _ in MODEL_NAMES if model in json.dumps(call.inputs) or model in json.dumps(call.output)), \"N/A\")\n",
    "        costs = call.summary.get(\"weave\", {}).get(\"costs\", {})\n",
    "        total_tokens = sum(cost.get(\"prompt_tokens\", 0) + cost.get(\"completion_tokens\", 0) for cost in costs.values())\n",
    "        feedback = call.summary.get(\"weave\", {}).get(\"feedback\", [])\n",
    "        thumbs_up = sum(1 for item in feedback if isinstance(item, dict) and item.get(\"payload\", {}).get(\"emoji\") == \"üëç\")\n",
    "        thumbs_down = sum(1 for item in feedback if isinstance(item, dict) and item.get(\"payload\", {}).get(\"emoji\") == \"üëé\")\n",
    "        \n",
    "        records.append({\n",
    "            \"Call ID\": call.id,\n",
    "            \"Trace ID\": call.trace_id,\n",
    "            \"Display Name\": call.display_name,\n",
    "            \"Model\": model,\n",
    "            \"Tokens\": total_tokens,\n",
    "            \"Latency (ms)\": call.summary.get(\"weave\", {}).get(\"latency_ms\", 0),\n",
    "            \"Thumbs Up\": thumbs_up,\n",
    "            \"Thumbs Down\": thumbs_down,\n",
    "            \"Started At\": pd.to_datetime(getattr(call, 'started_at', datetime.min)),\n",
    "            \"Inputs\": json.dumps(call.inputs, default=str),\n",
    "            \"Outputs\": json.dumps(call.output, default=str)\n",
    "        })\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Gathering inputs and generating visualizations\n",
    "Next, we'll gather the inputs using streamlit and generate the visualizations using plotly. This is the most basic dashboard, but you can customize it as you like! Check out the complete source code [here](https://github.com/NiWaRe/knowledge-worker-weave/blob/master/prod_dashboard.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_feedback_pie_chart(thumbs_up, thumbs_down):\n",
    "    fig = go.Figure(data=[go.Pie(labels=['Thumbs Up', 'Thumbs Down'], values=[thumbs_up, thumbs_down], marker=dict(colors=['#66b3ff', '#ff9999']), hole=.3)])\n",
    "    fig.update_traces(textinfo='percent+label', hoverinfo='label+percent')\n",
    "    fig.update_layout(showlegend=False, title=\"Feedback Summary\")\n",
    "    return fig\n",
    "\n",
    "def plot_model_cost_distribution(df):\n",
    "    fig = px.bar(df, x=\"llm_id\", y=\"total_cost\", color=\"llm_id\", title=\"Cost Distribution by Model\")\n",
    "    fig.update_layout(xaxis_title=\"Model\", yaxis_title=\"Cost (USD)\")\n",
    "    return fig\n",
    "\n",
    "# See the source code for all the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVAILABLE_PROJECTS = [\n",
    "    # entity name, project name\n",
    "    \"wandb-smle/weave-cookboook-demo\",\n",
    "]\n",
    "\n",
    "def render_dashboard():\n",
    "    # Configs panel\n",
    "    st.markdown(\"<div class='header'>Weave LLM Monitoring Dashboard</div>\", unsafe_allow_html=True)\n",
    "\n",
    "    trace_roots_only = st.sidebar.toggle(\"Trace Roots Only\", value=True)\n",
    "    selected_project = st.sidebar.selectbox(\"Select Weave Project\", AVAILABLE_PROJECTS, index=0)\n",
    "    client = init_weave_client(selected_project)\n",
    "    if client is None:\n",
    "        st.stop()\n",
    "\n",
    "    # [...]: Check the source code for the rest of the dashboard\n",
    "\n",
    "    # First plots - feedback and cost distribution\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        st.plotly_chart(plot_feedback_pie_chart(thumbs_up, thumbs_down), use_container_width=True)\n",
    "    with col2:\n",
    "        st.plotly_chart(plot_model_cost_distribution(df_costs), use_container_width=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this cookbook, we demonstrated how to create a custom production monitoring dashboard using Weave's APIs and functions. Weave currently focuses on fast integrations for easy input of data as well as extraction of the data for custom processes.\n",
    "\n",
    "* **Data Input:** \n",
    "    * Framework-agnostic tracing with [@weave-op()](https://weave-docs.wandb.ai/quickstart#2-log-a-trace-to-a-new-project) decorator and the possibility to import calls from CSV (see related [import cookbook](https://weave-docs.wandb.ai/reference/gen_notebooks/import_from_csv))\n",
    "    * Service API endpoints to log to Weave from for various programming frameworks and languages, see [here](https://weave-docs.wandb.ai/reference/service-api/call-start-call-start-post) for more details.\n",
    "* **Data Output:**\n",
    "    * Easy download of the data in CSV, TSV, JSONL, JSON formats - see [here](https://weave-docs.wandb.ai/guides/tracking/tracing#querying--exporting-calls) for more details.\n",
    "    * Easy export using programmatic access to the data - see \"Use Python\" section in the export panel as described in this cookbook. See [here](https://weave-docs.wandb.ai/guides/tracking/tracing#querying--exporting-calls) for more details.\n",
    "\n",
    "This custom dashboard extends Weave's native Traces view, allowing for tailored monitoring of LLM applications in production. While this dashboard will soon be directly integrated into Weave, the easy programmatic data access will be preserved. This ensures that enterprises can continue to integrate Weave data into their existing monitoring tools and FinOps processes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
