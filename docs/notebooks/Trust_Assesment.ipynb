{"cells":[{"cell_type":"markdown","id":"298501c0","metadata":{"id":"298501c0"},"source":["<!-- docusaurus_head_meta::start\n","---\n","title: Introduction Notebook\n","---\n","docusaurus_head_meta::end -->\n","\n","<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n","<!--- @wandbcode{intro-colab} -->"]},{"cell_type":"markdown","id":"vocyElGYmk06","metadata":{"id":"vocyElGYmk06"},"source":["# Evaluating Trustworthiness in RAG Pipelines with Weave Integration\n","\n","This notebook demonstrates how to add a faithfulness score to evaluate the trustworthiness of answers coming from a Retrieval-Augmented Generation (RAG) response. We will integrate this with Weave for tracking function inputs and outputs, creating objects out of prompts, and running evaluations with different datasets.\n","\n","## Objectives:\n","\n","* Implement a RAG pipeline that includes a faithfulness scoring mechanism.\n","* Integrate Weave to track all function calls, inputs, and outputs.\n","* Create Weave objects for prompts to facilitate reuse and analysis.\n","* Register three different evaluation datasets and showcase evaluation steps.\n","\n","## Stack Used:\n","\n","* LlamaIndex for RAG workflows.\n","* OpenAI API for language models and embeddings.\n","* Weave by Weights & Biases for tracking and evaluation.\n","\n","Note:Ensure you have the necessary API keys set up in your environment.\n","\n"]},{"cell_type":"markdown","id":"hWA5sdk1nST_","metadata":{"id":"hWA5sdk1nST_"},"source":["## ü™Ñ Install Dependencies"]},{"cell_type":"markdown","id":"56dc5c9d","metadata":{"id":"56dc5c9d"},"source":["## ü™Ñ Install `weave` library and login\n","\n","\n","Start by installing the library and logging in to your account.\n","\n","In this example, we're using openai so you should [add an openai API key](https://platform.openai.com/docs/quickstart/step-2-setup-your-api-key).\n","\n"]},{"cell_type":"code","execution_count":null,"id":"WqDMY26fdOyN","metadata":{"id":"WqDMY26fdOyN"},"outputs":[],"source":["%%capture\n","!pip install weave \\\n","openai set-env-colab-kaggle-dotenv \\\n","requests \\\n","python-dotenv==1.0.1 \\\n","PyPDF2 \\\n","unstructured \\\n","pdfminer.six \\\n","llama-index\n"]},{"cell_type":"code","execution_count":null,"id":"0921bbef-d089-4960-b624-505172f90d76","metadata":{"id":"0921bbef-d089-4960-b624-505172f90d76"},"outputs":[],"source":["# Set your OpenAI API key\n","\n","# Put your OPENAI_API_KEY in the secrets panel to the left üóùÔ∏è\n","_ = set_env(\"OPENAI_API_KEY\")\n","# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" # alternatively, put your key here\n","\n","PROJECT = \"Trustworthiness_Check\"\n","from set_env import set_env\n"]},{"cell_type":"code","execution_count":null,"id":"j9wRyq2QuT0G","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":156},"id":"j9wRyq2QuT0G","outputId":"3205119f-d3f6-4ecc-e6c6-80f4e1f279bc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Please login to Weights & Biases (https://wandb.ai/) to continue:\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"]},{"data":{"application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","WARNING:weave.trace.env:There are different credentials in the netrc file and the environment. Using the environment value.\n"]},{"name":"stdout","output_type":"stream","text":["Logged in as Weights & Biases user: mg01.\n","View Weave data at https://wandb.ai/wandb-smle/trustworthiness_check/weave\n"]},{"data":{"text/plain":["<weave.trace.weave_client.WeaveClient at 0x7b2bf2b44b50>"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["weave.init(PROJECT)      # initialize tracking for a specific W&B project\n","import weave                    # import the weave library\n"]},{"cell_type":"markdown","id":"9nkl6QL9oOS0","metadata":{"id":"9nkl6QL9oOS0"},"source":["\n","## üìö Import Necessary Libraries\n","\n","We'll import all the required libraries for our project, including OpenAI, LlamaIndex, and Weave.\n","\n"]},{"cell_type":"code","execution_count":null,"id":"ohVAmScbp2Cx","metadata":{"id":"ohVAmScbp2Cx"},"outputs":[],"source":["\n","# Load environment variables\n","#load_dotenv()\n","from dotenv import load_dotenv\n","from llama_index.core import VectorStoreIndex,SimpleDirectoryReader\n","from llama_index.embeddings.openai import OpenAIEmbedding\n","from openai import OpenAI\n","from typing import List, Dict, Any\n","import os\n","import requests\n","import weave\n"]},{"cell_type":"markdown","id":"FgYcqJNCqEEX","metadata":{"id":"FgYcqJNCqEEX"},"source":["## üîë Initialize OpenAI Client and Embedding Model\n","\n","Create an OpenAI client instance for API calls and set up the embedding model.\n","\n"]},{"cell_type":"code","execution_count":null,"id":"WMCBSnemqImN","metadata":{"id":"WMCBSnemqImN"},"outputs":[],"source":["# Initialize OpenAI client\n","client = OpenAI(\n","    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",")\n","\n","# Set up embedding model\n","embedding_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n"]},{"cell_type":"markdown","id":"vWwvfmZMqLb8","metadata":{"id":"vWwvfmZMqLb8"},"source":["## üì• Download and Load Documents\n","\n","We'll download a PDF document from a URL and create an index using LlamaIndex. Please note taht this can be your own vector database with your data indexed for your RAG Chatbot.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"knrRJg2jqWQV","metadata":{"id":"knrRJg2jqWQV"},"outputs":[],"source":["# Download the PDF from a URL\n","pdf_url = \"https://arxiv.org/pdf/2408.13296v1.pdf\"  # Replace with your PDF URL\n","pdf_filename = \"document.pdf\"\n","\n","response = requests.get(pdf_url)\n","with open(pdf_filename, 'wb') as f:\n","    f.write(response.content)\n","\n","# Load the documents from the PDF\n","documents = SimpleDirectoryReader(input_dir='.', required_exts=['.pdf']).load_data()\n","\n","# Create the index from the documents\n","index = VectorStoreIndex.from_documents(documents, embed_model=embedding_model)\n"]},{"cell_type":"markdown","id":"QpX3QNwmqZ25","metadata":{"id":"QpX3QNwmqZ25"},"source":["## üîé Create Query Engine\n","\n","Set up the query engine with a limit on the number of retrieved documents.\n","\n"]},{"cell_type":"code","execution_count":null,"id":"Lx1hwfJ8qXsf","metadata":{"id":"Lx1hwfJ8qXsf"},"outputs":[],"source":["# Create the query engine\n","query_engine = index.as_query_engine(similarity_top_k=3)\n"]},{"cell_type":"markdown","id":"DgbL1Ex7qfAS","metadata":{"id":"DgbL1Ex7qfAS"},"source":["## üõ†Ô∏è Define Weave-Tracked Functions\n","\n","We'll define our functions for the pipeline and use `@weave.op()` to decorate them, enabling Weave to track their inputs and outputs.\n","\n","### 1. Retrieve Context\n","\n","This function retrieves relevant context for the question using the LlamaIndex query engine.\n","\n"]},{"cell_type":"code","execution_count":null,"id":"vIp1U2-atlXT","metadata":{"id":"vIp1U2-atlXT"},"outputs":[],"source":["@weave.op()\n","def retrieve_context(question: str) -> str:\n","    '''\n","    Retrieves relevant context for the question using LlamaIndex query engine.\n","    '''\n","    response = query_engine.query(question)\n","    context = str(response)\n","    return context\n"]},{"cell_type":"markdown","id":"hzKPXo6atorE","metadata":{"id":"hzKPXo6atorE"},"source":["### 2. Generate Answer\n","\n","This function generates an answer to the question based on the provided context using OpenAI's GPT model.\n","\n"]},{"cell_type":"code","execution_count":null,"id":"vgZ0-KlPtb4t","metadata":{"id":"vgZ0-KlPtb4t"},"outputs":[],"source":["@weave.op()\n","def generate_answer(question: str, context: str, model_name: str) -> str:\n","    '''\n","    Generates an answer to the question based on the provided context using OpenAI's GPT model.\n","    '''\n","\n","    messages = [\n","        {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the provided context.\"},\n","        {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion:\\n{question}\"}\n","    ]\n","    response = client.chat.completions.create(\n","        model=model_name,\n","        messages=messages,\n","        max_tokens=200,\n","        temperature=0.7,\n","        n=1,\n","    )\n","    answer = response.choices[0].message.content.strip()\n","    return answer\n"]},{"cell_type":"markdown","id":"t5nkSEnNtskG","metadata":{"id":"t5nkSEnNtskG"},"source":["### 3. Break Down Answer into Statements\n","\n","This function breaks down the answer into simpler statements without pronouns.\n","\n"]},{"cell_type":"code","execution_count":null,"id":"Vf87agh4tu7a","metadata":{"id":"Vf87agh4tu7a"},"outputs":[],"source":["\n","@weave.op()\n","def break_down_answer_into_statements(answer: str, model_name: str) -> List[str]:\n","    '''\n","    Breaks down the answer into simpler statements without pronouns.\n","    '''\n","\n","    messages = [\n","        {\"role\": \"system\", \"content\": \"You simplify answers into fully understandable statements without pronouns.\"},\n","        {\"role\": \"user\", \"content\": f\"Break down the following answer into a list of simpler statements, ensuring each statement is fully understandable and contains no pronouns.\\n\\nAnswer:\\n{answer}\\n\\nStatements:\"}\n","    ]\n","    response = client.chat.completions.create(\n","        model=model_name,\n","        messages=messages,\n","        max_tokens=300,\n","        temperature=0.5,\n","        n=1,\n","    )\n","    statements_text = response.choices[0].message.content.strip()\n","    # Parse statements as a list\n","    statements = [s.strip().strip('.').strip() for s in statements_text.split('\\n') if s.strip()]\n","    # Remove any numbering or bullets\n","    statements = [s.lstrip('0123456789.- ') for s in statements]\n","    return statements\n"]},{"cell_type":"markdown","id":"8nKLZ_Wp2l9u","metadata":{"id":"8nKLZ_Wp2l9u"},"source":["### 4. Check Statement Faithfulness\n","\n","This function checks if each statement can be directly inferred from the context."]},{"cell_type":"code","execution_count":null,"id":"GyYkI1fU2kLn","metadata":{"id":"GyYkI1fU2kLn"},"outputs":[],"source":["\n","\n","@weave.op()\n","def check_statement_faithfulness(context: str, statement: str, model_name: str) -> Dict[str, Any]:\n","    '''\n","    Checks if the statement can be directly inferred from the context.\n","    Returns a verdict (1 for Yes, 0 for No) and the explanation.\n","    '''\n","\n","    messages = [\n","        {\"role\": \"system\", \"content\": \"You check if statements can be inferred from a given context.\"},\n","        {\"role\": \"user\", \"content\": f\"Given the following context, determine if the statement below can be directly inferred from the context. Answer with 'Yes' or 'No' and provide a brief reason.\\n\\nContext:\\n{context}\\n\\nStatement:\\n{statement}\\n\\nCan the statement be inferred from the context?\"}\n","    ]\n","    response = client.chat.completions.create(\n","        model=model_name,\n","        messages=messages,\n","        max_tokens=150,\n","        temperature=0,\n","        n=1,\n","    )\n","    result = response.choices[0].message.content.strip()\n","    # Parse the result to extract 'Yes' or 'No'\n","    if result.lower().startswith('yes'):\n","        verdict = 1\n","    else:\n","        verdict = 0\n","    return {'verdict': verdict, 'explanation': result}\n"]},{"cell_type":"markdown","id":"47-SMSxKyr_N","metadata":{"id":"47-SMSxKyr_N"},"source":["## üìä Register Evaluation Dataset\n","\n","We'll create and register a single evaluation dataset in Weave. This dataset will be used to evaluate the faithfulness of the generated answers.\n","\n"]},{"cell_type":"code","execution_count":null,"id":"3ZjWDzF6yu_J","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ZjWDzF6yu_J","outputId":"ab22ca51-bcc7-400e-d8f2-1a8c1144b851"},"outputs":[{"name":"stdout","output_type":"stream","text":["üì¶ Published to https://wandb.ai/wandb-smle/trustworthiness_check/weave/objects/Faithfulness_Evaluation_Dataset/versions/4eANf654BpPRx926Hw1asWWdnpXYVatdUcsOcmIZRTg\n"]},{"data":{"text/plain":["ObjectRef(entity='wandb-smle', project='trustworthiness_check', name='Faithfulness_Evaluation_Dataset', digest='4eANf654BpPRx926Hw1asWWdnpXYVatdUcsOcmIZRTg', extra=())"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# Define the dataset\n","dataset = weave.Dataset(\n","    name=\"Faithfulness_Evaluation_Dataset\",\n","    rows=[\n","        {\"question\": \"What are the limitations of the Transformers library and Trainer API?\"},\n","        {\"question\": \"Explain  LORA Technique for fine-tuning\"},\n","        {\"question\": \"Why fine-tuning GPT-4 is more challenging than GPT-3.5\"},\n","        {\"question\": \"Explain why fine-tuning is cheaper compared to few shot learning?\"},\n","        {\"question\": \"what are the key features of Key Features of NVIDIA NeMo\"},\n","    ],\n",")\n","\n","# Publish dataset to Weave\n","weave.publish(dataset)\n"]},{"cell_type":"markdown","id":"PuyLbmf3yydn","metadata":{"id":"PuyLbmf3yydn"},"source":["## üß™ Define End-to-End Pipeline as a Weave Model\n","\n","We'll define an end-to-end pipeline as a Weave Model. This allows us to use it for evaluation later and makes the entire process reproducible and traceable.\n","\n"]},{"cell_type":"code","execution_count":null,"id":"-FzT7b5w3dmq","metadata":{"id":"-FzT7b5w3dmq"},"outputs":[],"source":["\n","\n","class FaithfulnessEvaluator(weave.Model):\n","    model_name: str = \"gpt-3.5-turbo\"\n","\n","    @weave.op()\n","    def predict(self, question: str) -> Dict[str, Any]:\n","        '''\n","        Generates an answer to the question based on retrieved context.\n","        Returns a dict with 'answer', 'context', and 'model_name'.\n","        '''\n","        # Retrieve context\n","        context = retrieve_context(question)\n","        # Generate answer\n","        answer = generate_answer(question, context, self.model_name)\n","        return {'answer': answer, 'context': context, 'model_name': self.model_name}\n"]},{"cell_type":"markdown","id":"AQXTvtiXy24n","metadata":{"id":"AQXTvtiXy24n"},"source":["## üìù Define Scorer Function\n","\n","We'll define a scorer function that computes the faithfulness score of the model's answer. This function will be used by Weave's `Evaluation` class.\n","\n"]},{"cell_type":"code","execution_count":null,"id":"gch0yxmVy1oL","metadata":{"id":"gch0yxmVy1oL"},"outputs":[],"source":["@weave.op()\n","def faithfulness_scorer(model_output: Dict[str, Any]) -> Dict[str, Any]:\n","    '''\n","    Scorer function that computes the faithfulness score of the model's answer.\n","    '''\n","    answer = model_output['answer']\n","    context = model_output['context']\n","    model_name = model_output['model_name']\n","    # Break down the answer into statements\n","    statements = break_down_answer_into_statements(answer, model_name)\n","    # Check each statement for faithfulness\n","    total_statements = len(statements)\n","    faithful_statements = 0\n","    statement_results = []\n","    for statement in statements:\n","        result = check_statement_faithfulness(context, statement, model_name)\n","        faithful_statements += result['verdict']\n","        statement_results.append({\n","            'statement': statement,\n","            'verdict': result['verdict'],\n","            'explanation': result['explanation']\n","        })\n","    # Calculate faithfulness score\n","    if total_statements > 0:\n","        faithfulness_score = faithful_statements / total_statements\n","    else:\n","        faithfulness_score = 0\n","    # Return results\n","    return {\n","        'faithfulness_score': faithfulness_score,\n","        'statements': statements,\n","        'statement_results': statement_results,\n","    }\n"]},{"cell_type":"markdown","id":"RZdp6ed-y60P","metadata":{"id":"RZdp6ed-y60P"},"source":["## üöÄ Run Evaluation Using Weave's `Evaluation` Class\n","\n","We'll use Weave's `Evaluation` class to run the evaluation, ensuring that the results are stored in the **'eval'** section of Weave.\n","\n"]},{"cell_type":"code","execution_count":null,"id":"A63gByJ7y8l4","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":480},"id":"A63gByJ7y8l4","outputId":"ef4462dc-7406-4b6d-f8bb-276a0bd4f3be"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:weave.trace.env:There are different credentials in the netrc file and the environment. Using the environment value.\n"]},{"name":"stdout","output_type":"stream","text":["Logged in as Weights & Biases user: mg01.\n","View Weave data at https://wandb.ai/wandb-smle/trustworthiness_check/weave\n","Running evaluation with model: gpt-3.5-turbo\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples\n","</pre>\n"],"text/plain":["Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m5\u001b[0m examples\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples\n","</pre>\n"],"text/plain":["Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m5\u001b[0m examples\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples\n","</pre>\n"],"text/plain":["Evaluated \u001b[1;36m3\u001b[0m of \u001b[1;36m5\u001b[0m examples\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples\n","</pre>\n"],"text/plain":["Evaluated \u001b[1;36m4\u001b[0m of \u001b[1;36m5\u001b[0m examples\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples\n","</pre>\n"],"text/plain":["Evaluated \u001b[1;36m5\u001b[0m of \u001b[1;36m5\u001b[0m examples\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n","<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'faithfulness_scorer'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'faithfulness_score'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9875</span><span style=\"font-weight: bold\">}}</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.414409875869751</span><span style=\"font-weight: bold\">}}</span>\n","</pre>\n"],"text/plain":["Evaluation summary\n","\u001b[1m{\u001b[0m\u001b[32m'faithfulness_scorer'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'faithfulness_score'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.9875\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m, \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m7.414409875869751\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["üç© https://wandb.ai/wandb-smle/trustworthiness_check/r/call/01927dff-7926-7161-954e-70b809b88a91\n","Completed evaluation with model: gpt-3.5-turbo\n","\n","Running evaluation with model: gpt-4o\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples\n","</pre>\n"],"text/plain":["Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m5\u001b[0m examples\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples\n","</pre>\n"],"text/plain":["Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m5\u001b[0m examples\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples\n","</pre>\n"],"text/plain":["Evaluated \u001b[1;36m3\u001b[0m of \u001b[1;36m5\u001b[0m examples\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples\n","</pre>\n"],"text/plain":["Evaluated \u001b[1;36m4\u001b[0m of \u001b[1;36m5\u001b[0m examples\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples\n","</pre>\n"],"text/plain":["Evaluated \u001b[1;36m5\u001b[0m of \u001b[1;36m5\u001b[0m examples\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n","<span style=\"font-weight: bold\">{</span>\n","    <span style=\"color: #008000; text-decoration-color: #008000\">'faithfulness_scorer'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'faithfulness_score'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9057142857142857</span><span style=\"font-weight: bold\">}}</span>,\n","    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.293869829177856</span><span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","</pre>\n"],"text/plain":["Evaluation summary\n","\u001b[1m{\u001b[0m\n","    \u001b[32m'faithfulness_scorer'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'faithfulness_score'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.9057142857142857\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n","    \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m5.293869829177856\u001b[0m\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["üç© https://wandb.ai/wandb-smle/trustworthiness_check/r/call/01927dff-e680-7a23-932f-47c3f0690c1e\n","Completed evaluation with model: gpt-4o\n","\n"]}],"source":["# Import Weave's Evaluation class\n","from weave import Evaluation\n","import asyncio\n","import nest_asyncio\n","\n","# Initialize Weave\n","weave.init(PROJECT)\n","\n","# Apply nest_asyncio to allow nested event loops in Colab\n","nest_asyncio.apply()\n","\n","# Run the evaluation for both models\n","\n","# Define the models to evaluate\n","model_names = [\"gpt-3.5-turbo\", \"gpt-4o\"]\n","\n","for model_name in model_names:\n","    print(f\"Running evaluation with model: {model_name}\")\n","    # Instantiate the evaluator model with the specified model name\n","    evaluator_model = FaithfulnessEvaluator(model_name=model_name)\n","\n","    # Define the evaluation\n","    evaluation = Evaluation(\n","        dataset=dataset,  # the dataset we have defined earlier\n","        scorers=[faithfulness_scorer],  # the scorer function\n","    )\n","\n","    # Run the evaluation\n","    summary = asyncio.run(evaluation.evaluate(evaluator_model))\n","\n","    print(f\"Completed evaluation with model: {model_name}\\n\")"]},{"cell_type":"markdown","id":"D76yqHtB1Is3","metadata":{"id":"D76yqHtB1Is3"},"source":["## üìå Conclusion\n","\n","**Evaluation of Faithfulness**:\n","\n","In this notebook, we focused on evaluating the **faithfulness** of answers generated by our\n","Retrieval-Augmented Generation (RAG) system. By breaking down the answers into simpler\n","statements and checking each one against the retrieved context, we quantified how much we can\n","**trust** the responses provided by the system.\n","\n"," **How Weave Helps**:\n","\n"," Weave played a crucial role in this process by:\n","\n"," - **Tracking**: Weave's `@weave.op()` decorators allowed us to track the inputs and outputs of our\n","   functions seamlessly. This provided transparency into each step of our pipeline.\n"," - **Evaluation**: Using Weave's `Evaluation` class, we conducted structured evaluations and stored\n","   the results in the **'eval'** section. This made it easy to analyze and compare results.\n"," - **Reproducibility**: By defining our prompts and models as Weave Objects and Models, we ensured\n","   that our pipeline is reproducible and easily shareable.\n","\n"," **Benefits of Weave Integration**:\n","\n"," - **Enhanced Trust**: By integrating faithfulness evaluation, we added an extra layer of **trust** to\n","   our system. Users can be more confident in the accuracy of the responses.\n"," - **Debugging and Improvement**: Weave's tracking capabilities make it easier to identify areas\n","  where the model may not be performing as expected, facilitating targeted improvements.\n","- **Comprehensive Insights**: The ability to store and analyze evaluation results within Weave\n","   provides comprehensive insights into model performance over time.\n","\n"," ---\n","\n"," ## üîö Final Thoughts\n","\n"," By integrating **Weave** into our code, we've enhanced the transparency, reliability, and\n"," **trustworthiness** of our RAG system. We can:\n","\n"," - Track function inputs and outputs.\n"," - Reuse prompt templates as Weave Objects.\n"," - Perform comprehensive evaluations focused on faithfulness.\n"," - Define an end-to-end pipeline as a Weave Model for easier evaluation.\n"," - Store evaluation results in the **'eval'** section of Weave for better analysis.\n","\n"," This approach not only provides valuable insights into the trustworthiness of the generated\n"," answers but also contributes to building systems that users can rely on with confidence.\n","\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}
