{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excelling at BIG-Bench Hard tasks Using DSPy and Weave\n",
    "\n",
    "The [BIG-bench (Beyond the Imitation Game Benchmark)](https://github.com/google/BIG-bench) is a collaborative benchmark intended to probe large language models and extrapolate their future capabilities consisting of more than 200 tasks. The [BIG-Bench Hard (BBH)](https://github.com/suzgunmirac/BIG-Bench-Hard) is a suite of 23 most challenging BIG-Bench tasks that can be quite difficult to be solved using the current generation of language models.\n",
    "\n",
    "This tutorial demonstrates how we can improve the performance of our LLM workflow implemented  on the **causal judgement task** from the BIG-bench Hard benchmark and evaluate our prompting strategies. We will use [DSPy](https://dspy-docs.vercel.app/) for implementing our LLM workflow and optimizing our prompting strategy. We would also use[ Weave](../../quickstart.md) to track our LLM workflow and evaluate our prompting strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the Dependencies\n",
    "\n",
    "We need the following libraries for this tutorial:\n",
    "\n",
    "- [DSPy](https://dspy-docs.vercel.app/) for building the LLM workflow and optimizing it.\n",
    "- [Weave](../../quickstart.md) to track our LLM workflow and evaluate our prompting strategies.\n",
    "- [datasets](https://huggingface.co/docs/datasets/index) to access the Big-Bench Hard dataset from HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU dspy-ai weave datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we'll be using [OpenAI API](https://openai.com/index/openai-api/) as the LLM Vendor, we will also need an OpenAI API key. You can [sign up](https://platform.openai.com/signup) on the OpenAI platform to get your own API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "api_key = getpass(\"Enter you OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable Tracking using Weave\n",
    "\n",
    "Weave is currently integrated with DSPy, and including [`weave.init`](../../api-reference/python/weave.md#function-init) at the start of our code lets us automatically trace our DSPy functions which can be explored in the Weave UI. Check out the [Weave integration docs for DSPy](../../guides/integrations/dspy.md) to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "\n",
    "weave.init(project_name=\"dspy-bigbench-hard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we use a metadata class inherited from [`weave.Model`](../../guides/core-types/models.md) to manage our metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metadata(weave.Model): \n",
    "    big_bench_hard_task: str = \"causal_judgement\"\n",
    "    num_train_examples: int = 50\n",
    "    openai_model: str = \"gpt-3.5-turbo\"\n",
    "    max_bootstrapped_demos: int = 8\n",
    "    max_labeled_demos: int = 8\n",
    "    validation_size_for_optimization: int = 10\n",
    "\n",
    "\n",
    "metadata = Metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the BIG-Bench Hard Dataset\n",
    "\n",
    "We're gonna load this dataset from HuggingFace Hub, split into training and validation sets, and [publish](../../guides/core-types/datasets.md) them on Weave, this would let us version the datasets, and also use [`weave.Evaluation`](../../guides/core-types/evaluations.md) to evaluate our prompting strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def get_dataset(metadata: Metadata):\n",
    "    # load the BIG-Bench Hard dataset corresponding to the task from Huggingface Hug\n",
    "    dataset = load_dataset(\"maveriq/bigbenchhard\", metadata.big_bench_hard_task)[\"train\"]\n",
    "    \n",
    "    # create the training and validation datasets\n",
    "    rows = [{\"question\": data[\"input\"], \"answer\": data[\"target\"]} for data in dataset]\n",
    "    train_rows = rows[0:metadata.num_train_examples]\n",
    "    val_rows = rows[metadata.num_train_examples:]\n",
    "\n",
    "    # create the training and validation examples consisting of `dspy.Example` objects\n",
    "    dspy_train_examples = [dspy.Example(row).with_inputs(\"question\") for row in train_rows]\n",
    "    dspy_val_examples = [dspy.Example(row).with_inputs(\"question\") for row in val_rows]\n",
    "\n",
    "    # publish the datasets to the Weave, this would let us version the data and use for evaluation\n",
    "    weave.publish(weave.Dataset(name=f\"bigbenchhard_{metadata.big_bench_hard_task}_train\", rows=train_rows))\n",
    "    weave.publish(weave.Dataset(name=f\"bigbenchhard_{metadata.big_bench_hard_task}_val\", rows=val_rows))\n",
    "    \n",
    "    return dspy_train_examples, dspy_val_examples\n",
    "\n",
    "\n",
    "dspy_train_examples, dspy_val_examples = get_dataset(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ![](../assets/dspy_prompt_optimization/datasets.gif) |\n",
    "|---|\n",
    "| The datasets, once published, can be explored in the Weave UI |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The DSPy Program\n",
    "\n",
    "We're gonna use the [`dspy.OpenAI`](https://dspy-docs.vercel.app/api/language_model_clients/OpenAI) abstraction to make LLM calls to [GPT3.5 Turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an expert in the field of causal reasoning and judgement.\n",
    "You are to analyze the a given question carefully and answer in `Yes` or `No`.\n",
    "You should also provide a detailed explanation justifying your answer.\n",
    "\"\"\"\n",
    "\n",
    "llm = dspy.OpenAI(model=metadata.openai_model, system_prompt=system_prompt)\n",
    "dspy.settings.configure(lm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DSPy is a framework that pushes building new LM pipelines away from manipulating free-form strings and closer to programming (composing modular operators to build text transformation graphs) where a compiler automatically generates optimized LM invocation strategies and prompts from a program.\n",
    "\n",
    "According to the DSPy programming model, first string-based prompting techniques are translated into declarative modules that carry natural-language typed signatures. Then, each module is the parameterized so that it can learn its desired behavior by iteratively bootstrapping useful demonstrations within the pipeline.\n",
    "\n",
    "Check the following papers to learn more about the DSPy paradigm:\n",
    "\n",
    "- [DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines](https://arxiv.org/abs/2310.03714)\n",
    "- [DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines](https://arxiv.org/abs/2312.13382)\n",
    "- [Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs](https://arxiv.org/abs/2406.11695v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the Causal Reasoning Signature\n",
    "\n",
    "A [signature](https://dspy-docs.vercel.app/docs/building-blocks/signatures) is a declarative specification of input/output behavior of a [DSPy module](https://dspy-docs.vercel.app/docs/building-blocks/modules). Signatures allow you to tell the LM what it needs to do, rather than specify how we should ask the LM to do it. This enables us to organize our prompting strategy using modular and clean code, in which LM calls can be optimized into high-quality prompts (or automatic finetunes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class CausalReasoningInput(BaseModel):\n",
    "    query: str = Field(description=\"The question to be answered\")\n",
    "\n",
    "\n",
    "class CausalReasoningOutput(BaseModel):\n",
    "    answer: str = Field(description=\"The answer for the question\")\n",
    "    confidence: float = Field(ge=0, le=1, description=\"The confidence score for the answer\")\n",
    "    explanation: str = Field(description=\"The explanation for the answer\")\n",
    "\n",
    "\n",
    "class CausalReasoningSignature(dspy.Signature):\n",
    "    input: CausalReasoningInput = dspy.InputField()\n",
    "    output: CausalReasoningOutput = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[DSPy modules](https://dspy-docs.vercel.app/docs/building-blocks/modules) are task-adaptive components—akin to neural network layers—that abstract any particular text transformation, in this case returning a structured question-answering result by executing causal reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalReasoningModule(dspy.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.prog = dspy.TypedPredictor(CausalReasoningSignature)\n",
    "    \n",
    "    @weave.op()\n",
    "    def forward(self, question) -> dict:\n",
    "        program_input = CausalReasoningInput(query=question)\n",
    "        return self.prog(input=program_input).output.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we use [`dspy.TypedPredictor`](https://dspy-docs.vercel.app/docs/building-blocks/typed_predictors) which enforces the type constraints on the inputs and outputs of the fields in a DSPy signature. This enables us to always get the output in the form of a `pydantic.BaseModel` structuring the output of the LLM workflow according a fixed schema, making it easy to parse.\n",
    "\n",
    "Let's test our LLM workflow, i.e., the `CausalReasoningModule` on an example from the causal reasoning subset of Big-Bench Hard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rich\n",
    "\n",
    "baseline_module = CausalReasoningModule()\n",
    "\n",
    "prediction = baseline_module(dspy_train_examples[0][\"question\"])\n",
    "rich.print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our DSPy Program\n",
    "\n",
    "Now that we have a baseline prompting strategy, let's evaluate it on our validation set using [`weave.Evaluation`](../../guides/core-types/evaluations.md) on a simple metric that matches the predicted answer with the ground truth. Weave will take each example, pass it through your application and score the output on multiple custom scoring functions. By doing this, you'll have a view of the performance of your application, and a rich UI to drill into individual outputs and scores.\n",
    "\n",
    "First, we need to create a simple evalaution metric function that tells whether the answer from the baseline module's output is the same as the ground truth answer or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def weave_evaluation_metric(answer: str, model_output: CausalReasoningOutput) -> dict:\n",
    "    return {\n",
    "        \"match\": int(answer.lower() == model_output[\"answer\"].lower())\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation and run it\n",
    "evaluation = weave.Evaluation(\n",
    "    name=\"baseline_causal_reasoning_module\",\n",
    "    dataset=weave.ref(\"bigbenchhard_causal_judgement_val:v0\").get(),\n",
    "    scorers=[weave_evaluation_metric]\n",
    ")\n",
    "\n",
    "await evaluation.evaluate(baseline_module.forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're running from a python script, you can use the following code to run the evaluation:\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "asyncio.run(evaluation.evaluate(baseline_module.forward))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing our DSPy Program\n",
    "\n",
    "Now, that we have a baseline DSPy program, let us try to improve its performance for causal reasoning. We would do this using a [DSPy teleprompter](https://dspy-docs.vercel.app/docs/building-blocks/optimizers) is an algorithm that can tune the parameters of a DSPy program (i.e., the prompts and/or the LM weights) to maximize the metrics you specify, like accuracy. When compiling a DSPy program, we generally invoke a teleprompter, which is an optimizer that takes the program, a training set, and a metric—and returns a new optimized program. In this tutorial, we use the [BootstrapFewShot](https://dspy-docs.vercel.app/api/category/optimizers) teleprompter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def get_optimized_program(model: dspy.Module, metadata: Metadata) -> dspy.Module:\n",
    "    \n",
    "    @weave.op()\n",
    "    def dspy_evaluation_metric(true, prediction, trace=None):\n",
    "        return prediction[\"answer\"].lower() == true.answer.lower()\n",
    "\n",
    "\n",
    "    teleprompter = BootstrapFewShotWithRandomSearch(\n",
    "        metric=dspy_evaluation_metric, \n",
    "        max_bootstrapped_demos=metadata.max_bootstrapped_demos,\n",
    "        max_labeled_demos=metadata.max_labeled_demos,\n",
    "    )\n",
    "    valset = dspy_val_examples[:metadata.validation_size_for_optimization]\n",
    "    return teleprompter.compile(model, trainset=dspy_train_examples, valset=valset)\n",
    "\n",
    "\n",
    "optimized_module = get_optimized_program(baseline_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our optimized program (the optimized prompting strategy), let's evaluate it once again on our validation set and compare it with our baseline DSPy program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = weave.Evaluation(\n",
    "    name=\"optimized_causal_reasoning_module\",\n",
    "    dataset=weave.ref(\"bigbenchhard_causal_judgement_val:v0\").get(),\n",
    "    scorers=[weave_evaluation_metric]\n",
    ")\n",
    "await evaluation.evaluate(optimized_module.forward)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
