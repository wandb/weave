Eval Playground Docs Assertion Spec:

1. Navigate to the Evaluation Playground by clicking "Playground" on the sidebar to navigate to the Playground Page, then the "Evaluate" tab.
2. First, decide wether to start from scratch or load a demo evaluation. This will enter the user into configuration mode.
3. Once the evaluation is configured, click "Run Eval" to launch the evaluation run(s)
4. Once the evaluation run(s) are complete, the view will update to show predictions, scores, and metrics relevant to the evaluation
5. Click "Back to config" to return to the configuration mode

Details of configuration mode:
1. There are a few sections: Evaluation name/description, dataset, model(s), scorer(s)
2. Evaluation name/description are exactly what they sound like and help you to locate these evaluations in the future
3. Dataset is the dataset used for the evaluation. Users can create a dataset from scratch, upload a CSV, or load an existing dataset in the system. Users can edit the dataset to add/remove/modify rows until they are happy with the data
4. Scorers are the LLM-as-a-judge scorers that evaluate model quality. You can use string interpolations (eg. {my_dataset_column} or {output} ) to reference dataset columns and/or model outputs inside the scoring function
    a. Similar to datasets, users can load existing, start from scratch, or edit existing.
    b. Users can add new scorers using the "add scorer" button, and delete scorers with the trash can
5. Models are very similar to scorers and the user simply needs to define the system prompt for the LLM model
6. In both models and scorers, the user can select the underlying LLM model of interest.
7. Once complete, clicking "run Evaluation" will save all the edits and launch an eval.