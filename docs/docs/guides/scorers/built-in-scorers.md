# Built-in Scorers

This page provides an overview of Weave's _built-in scorers_. Built-in scorers were created by the team at Weights & Biases to address common challenges encountered by LLM application developers, such as detecting hallucinations and toxic content, measuring model accuracy, and validating output formats.

:::important
Built-in scorers are not available in TypeScript. All usage instructions and code examples on this page are for Python. To create scorers in TypeScript, see [function-based scorers](../scorers/custom-scorers.md#function-based-scorers).
:::

To get started with built-in scorers, complete the following steps:

1. Read the [scorers overview](../../guides/scorers/scorers-overview.md#working-with-scorers)
2. Complete the [built-in scorers prerequisites](#prerequisites)
3. [Select the right built-in scorer and configure it](#select-a-built-in-scorer) for your specific scenario.

## Prerequisites

- Your LLM application code is written in Python.
- You are using one of the following LLM clients:
  - OpenAI
  - Anthropic 
  - Google
  - MistralAI
- Install the scorer-related dependencies for the `weave` package:
  ```bash
  pip install weave[scorers]
  ```

## Select a built-in scorer

When deciding which built-in scorer to use, consider your specific scenario. Before you can use the built-in scorers, complete the [prerequisites](#prerequisites).

| Scorer                           | Scenario                                                                                                                                                                      |
|----------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [AccuracyScorer](#accuracyscorer)                  | Measure the accuracy of your AI system's predictions against ground truth labels for classification tasks.                                            |
| [BiasScorer](#biasscorer)                          | Detect biased or stereotypical content in your AI system's output. Ideal for reducing harmful biases in generated text.                               |
| [BLEUScorer](#bleuscorer)                          | Evaluate the quality of translations or paraphrased outputs by comparing them to reference texts using BLEU metrics.                                  |
| [CoherenceScorer](#coherencescorer)                | Evaluate the coherence and logical structure of the AI system's output.                                                                               |
| [ContextEntityRecallScorer](#contextentityrecallscorer) | Assess whether your AI system accurately recalls key entities from the input context. Ideal for retrieval-augmented generation (RAG) systems.         |
| [ContextRelevancyScorer](#contextrelevancyscorer)  | evaluate whether the provided context is relevant to the generated output. Useful for ensuring relevant context in RAG systems.                       |
| [EmbeddingSimilarityScorer](#embeddingsimilarityscorer) | Measure how similar your AI's output is to a reference text. This scorer calculates the cosine similarity between embeddings.                        |
| [FaithfulnessScorer](#faithfulnessscorer)          | Verify that your AI model's output remains faithful to the provided input and context, ensuring factual consistency.                                   |
| [HallucinationFreeScorer](#hallucinationfreescorer) | Identify whether your AI model generates hallucinations in its output.                                                                                  |
| [OpenAIModerationScorer](#openaimoderationscorer)   | Detect inappropriate content such as hate speech, violence, or explicit material in your model's output. This scorer uses OpenAI's Moderation API.     |
| [PerplexityScorer](#perplexityscorer)              | Evaluate the perplexity of your AI system's output to measure language fluency and predictability.                                                    |
| [PydanticScorer](#pydanticscorer)                  | Validate the AI system's output against a predefined Pydantic schema to ensure adherence to a specific data structure or format.                      |
| [RelevanceScorer](#relevancescorer)                | Measure whether the AI system's output is relevant to the input and context provided.                                                                |
| [RobustnessScorer](#robustnessscorer)              | Assess the robustness of your AI system by evaluating the consistency of its output across input variations.                                          |
| [RougeScorer](#rougescorer)                        | Evaluate the quality of summaries generated by your model by comparing them to reference texts using ROUGE metrics.                                   |
| [SummarizationScorer](#summarizationscorer)         | Evaluate the quality of summaries generated by your model. This scorer checks both the "information density" and the overall quality of the summary.   |
| [ToxicityScorer](#toxicityscorer)                  | Identify toxic or harmful content in your AI system's output, including hate speech or threats.                                                       |
| [ValidJSONScorer](#validjsonscorer)                | Verify that your AI model produces valid JSON output. Essential for ensuring structured data compliance.                                               |
| [ValidXMLScorer](#validxmlscorer)                  | Check whether your AI system generates valid XML. Useful for XML-based data exchange or configuration.                                                |


### `HallucinationFreeScorer`

The `HallucinationFreeScorer` detects hallucinations in the AI system's output by comparing it to the input data. 

```python
from weave.scorers import HallucinationFreeScorer

llm_client = ... # initialize your LLM client here

scorer = HallucinationFreeScorer(
  client=llm_client,
  model_id="gpt-4o"
)
```

#### Customization

You can customize the `system_prompt` and `user_prompt` attributes of the scorer to define what qualifies as a hallucination.

#### Usage notes

The `score` method expects an input column named `context`. If your dataset uses a different name, [use the `column_map` attribute](../scorers/scorers-overview.md#mapping-column-names-with-column_map) to map `context` to the dataset column.

#### Example  

The following example shows the usage of `HallucinationFreeScorer` in the context of an evaluation:

```python
import asyncio
from openai import OpenAI
import weave
from weave.scorers import HallucinationFreeScorer

# Initialize clients and scorers
llm_client = OpenAI()
hallucination_scorer = HallucinationFreeScorer(
  client=llm_client,
  model_id="gpt-4o",
  column_map={"context": "input", "output": "other_col"}
)

# Create dataset
dataset = [
  {"input": "John likes various types of cheese."},
  {"input": "Pepe likes various types of cheese."},
]

@weave.op
def model(input: str) -> str:
return "The person's favorite cheese is cheddar."

# Run evaluation
evaluation = weave.Evaluation(
  dataset=dataset,
  scorers=[hallucination_scorer],
)
result = asyncio.run(evaluation.evaluate(model))
print(result)
# {'HallucinationFreeScorer': {'has_hallucination': {'true_count': 2, 'true_fraction': 1.0}}, 'model_latency': {'mean': 1.4395725727081299}}
```

---

### `SummarizationScorer`

Use an LLM to compare a summary to the original text and evaluate the quality of the summary. The `SummarizationScorer` evaluates summaries in two ways:

1. _Entity Density_: Checks the ratio of unique entities (e.g. names, places,things) mentioned in the summary to the total word count in the summary in order to estimate the "information density" of the summary. Uses an LLM to extract the entities. Similar to how entity density is used in the [Chain of Density paper available on Arxiv](https://arxiv.org/abs/2309.04269). This metric helps ensure summaries are concise yet informative.

2. _Quality Grading_: Uses an LLM-evaluator to categorize the summary as one of `poor`, `ok`, or `excellent`. These grades are converted to scores so that the average score can be calculated, where `0.0` is `poor`, `0.5` is `ok`, and `1.0` is `excellent`.

```python
from weave.scorers import SummarizationScorer

llm_client = ... # initialize your LLM client here

scorer = SummarizationScorer(
  client=llm_client,
  model_id="gpt-4o"
)
```

#### Customization

Adjust `summarization_evaluation_system_prompt` and `summarization_evaluation_prompt` to define what makes a good summary.

#### Usage notes

- This scorer uses the `InstructorLLMScorer` class.
- The `score` method expects the original text that was summarized to be present in the `input` column of the dataset. [Use the `column_map` class](../scorers/scorers-overview.md#mapping-column-names-with-column_map) attribute to map `input` to the correct dataset column if needed.

#### Example  

The following example shows the `SummarizationScorer` in the context of an evaluation:

```python
import asyncio
from openai import OpenAI
import weave
from weave.scorers import SummarizationScorer

class SummarizationModel(weave.Model):
@weave.op()
async def predict(self, input: str) -> str:
  return "This is a summary of the input text."

# Initialize clients and scorers
llm_client = OpenAI()
model = SummarizationModel()
summarization_scorer = SummarizationScorer(
  client=llm_client,
  model_id="gpt-4o",
)
# Create dataset
dataset = [
  {"input": "The quick brown fox jumps over the lazy dog."},
  {"input": "Artificial Intelligence is revolutionizing various industries."}
]

# Run evaluation
evaluation = weave.Evaluation(dataset=dataset, scorers=[summarization_scorer])
results = asyncio.run(evaluation.evaluate(model))
print(results)
# {'SummarizationScorer': {'is_entity_dense': {'true_count': 0, 'true_fraction': 0.0}, 'summarization_eval_score': {'mean': 0.0}, 'entity_density': {'mean': 0.0}}, 'model_latency': {'mean': 6.210803985595703e-05}}
```

---

### `OpenAIModerationScorer`

The `OpenAIModerationScorer` uses OpenAI's Moderation API to check if the AI system's output contains disallowed content, such as hate speech or explicit material. The `OpenAIModerationScorer` works by sending the AI's output to the OpenAI Moderation endpoint. This endpoint returns a dictionary indicating whether the content is flagged or not, along with information about the categories involved. You must install the `openai` Python package to use `OpenAIModerationScorer`. 

```python
from weave.scorers import OpenAIModerationScorer
from openai import OpenAI

oai_client = OpenAI(api_key=...) # initialize your LLM client here

scorer = OpenAIModerationScorer(
  client=oai_client,
  model_id="text-embedding-3-small"
)
```

#### Usage notes

- The client must be an instance of OpenAI's `OpenAI` or `AsyncOpenAI` client.

#### Example

The following example shows `OpenAIModerationScorer` in the context of an evaluation:

```python
import asyncio
from openai import OpenAI
import weave
from weave.scorers import OpenAIModerationScorer

class MyModel(weave.Model):
@weave.op
async def predict(self, input: str) -> str:
  return input

# Initialize clients and scorers
client = OpenAI()
model = MyModel()
moderation_scorer = OpenAIModerationScorer(client=client)

# Create dataset
dataset = [
  {"input": "I love puppies and kittens!"},
  {"input": "I hate everyone and want to hurt them."}
]

# Run evaluation
evaluation = weave.Evaluation(dataset=dataset, scorers=[moderation_scorer])
results = asyncio.run(evaluation.evaluate(model))
print(results)
# Example output
# {'OpenAIModerationScorer': {'flagged': {'true_count': 1, 'true_fraction': 0.5}, 'categories': {'violence': {'true_count': 1, 'true_fraction': 1.0}}}, 'model_latency': {'mean': 9.500980377197266e-05}}
```

---

### `EmbeddingSimilarityScorer`

The `EmbeddingSimilarityScorer` computes the cosine similarity between the embeddings of the AI system's output and a target text from your dataset. It's useful for measuring how similar the AI's output is to a reference text. The `EmbeddingSimilarityScorer` takes the following parameters:

- `target`: `EmbeddingSimilarityScorer`  expects a `target` column in your dataset. This column is used calculate the cosine similarity of the embeddings of the `target` column to the AI system output. If your dataset doesn't contain a column called `target`, you can [use the scorers `column_map` attribute](../scorers/scorers-overview.md#mapping-column-names-with-column_map) to map `target` to the appropriate column name in your dataset. For more information, see [Column Mapping](../scorers/scorers-overview.md).
- `threshold` (float): The minimum cosine similarity score between the embedding of the AI system output and the embdedding of the `target`, above which the 2 samples are considered "similar", (defaults to `0.5`). `threshold` can be in a range from -1 to 1:
- `1` indicates identical direction.
- `0` indicates orthogonal vectors.
- `-1` indicates opposite direction.

```python
from weave.scorers import EmbeddingSimilarityScorer

llm_client = ...  # initialise your LlM client

similarity_scorer = EmbeddingSimilarityScorer(
  client=llm_client
  target_column="reference_text",  # the dataset column to compare the output against
  threshold=0.4  # the cosine similarity threshold to use
)
```

#### Usage notes

The correct cosine similarity threshold may vary based on your use case. Experiment with different thresholds to determine the best fit.

#### Example

The following example shows `EmbeddingSimilarityScorer` in the context of an evaluation:

```python
import asyncio
from openai import OpenAI
import weave
from weave.scorers import EmbeddingSimilarityScorer

# Initialize clients and scorers
client = OpenAI()
similarity_scorer = EmbeddingSimilarityScorer(
  client=client,
  threshold=0.7,
  column_map={"target": "reference"}
)

# Create dataset
dataset = [
  {
    "input": "He's name is John",
    "reference": "John likes various types of cheese.",
  },
  {
    "input": "He's name is Pepe.",
    "reference": "Pepe likes various types of cheese.",
  },
]

# Define model
@weave.op
def model(input: str) -> str:
  return "John likes various types of cheese."

# Run evaluation
evaluation = weave.Evaluation(
  dataset=dataset,
  scorers=[similarity_scorer],
)
result = asyncio.run(evaluation.evaluate(model))
print(result)

# Example output:
# {'EmbeddingSimilarityScorer': {'is_similar': {'true_count': 1, 'true_fraction': 0.5}, 'similarity_score': {'mean': 0.8448514031462045}}, 'model_latency': {'mean': 0.45862746238708496}}
```

---

### `ValidJSONScorer`

The `ValidJSONScorer` checks whether the AI system's output is valid JSON or not. This is useful if you expect the output to be in JSON format. 

```python
import asyncio
import weave
from weave.scorers import ValidJSONScorer

class JSONModel(weave.Model):
@weave.op()
async def predict(self, input: str) -> str:
  # Replace this comment with code to generate JSON/XML in your use case.
  return '{"key": "value"}'

model = JSONModel()
json_scorer = ValidJSONScorer()

dataset = [
  {"input": "Generate a JSON object with a key and value"},
  {"input": "Create an invalid JSON"}
]

evaluation = weave.Evaluation(dataset=dataset, scorers=[json_scorer])
results = asyncio.run(evaluation.evaluate(model))
print(results)
# {'ValidJSONScorer': {'json_valid': {'true_count': 2, 'true_fraction': 1.0}}, 'model_latency': {'mean': 8.58306884765625e-05}}
```

---

### `ValidXMLScorer`

The `ValidXMLScorer` checks whether the AI system's output is valid XML. This is useful if you are expecting XML-formatted outputs.

```python
import asyncio
import weave
from weave.scorers import ValidXMLScorer

class XMLModel(weave.Model):
@weave.op()
async def predict(self, input: str) -> str:
  # This is a placeholder. In a real scenario, this would generate XML.
  return '<root><element>value</element></root>'

model = XMLModel()
xml_scorer = ValidXMLScorer()

dataset = [
  {"input": "Generate a valid XML with a root element"},
  {"input": "Create an invalid XML"}
]

evaluation = weave.Evaluation(dataset=dataset, scorers=[xml_scorer])
results = asyncio.run(evaluation.evaluate(model))
print(results)
# {'ValidXMLScorer': {'xml_valid': {'true_count': 2, 'true_fraction': 1.0}}, 'model_latency': {'mean': 8.20159912109375e-05}}
```

---

### `PydanticScorer`

The `PydanticScorer` validates the AI system's output against a Pydantic model to ensure it adheres to a specified schema or data structure.

```python
from weave.scorers import PydanticScorer
from pydantic import BaseModel

class FinancialReport(BaseModel):
  revenue: int
  year: str

pydantic_scorer = PydanticScorer(model=FinancialReport)
```

---

### `ContextEntityRecallScorer`

:::note
The `ContextEntityRecallScorer` is based on the [RAGAS](https://github.com/explodinggradients/ragas) evaluation library
:::

The `ContextEntityRecallScorer` uses an LLM to extract unique entities from the AI system's output and the provided context. It then calculates _recall_, which measures the proportion of contextually important entities that are captured in the output. This measurement helps to assess the model's effectiveness in retrieving relevant information. The `ContextEntityRecallScorer` returns a dictionary containing the recall score. 

```python
from weave.scorers import ContextEntityRecallScorer

llm_client = ...  # initialise your LlM client

entity_recall_scorer = ContextEntityRecallScorer(
  client=llm_client
  model_id="your-model-id"
)
```

#### Usage notes

`ContextEntityRecallScorer` expects a `context` column in your dataset. You can [use `column_map`](../scorers/scorers-overview.md#mapping-column-names-with-column_map) to map `context` to another dataset column if needed.

#### Example

See the [`ContextRelevancyScorer` example](#example-5).

---

### `ContextRelevancyScorer`

:::note
The `ContextRelevancyScorer` is based on the [RAGAS](https://github.com/explodinggradients/ragas) evaluation library
:::

The `ContextRelevancyScorer` evaluates the relevancy of the provided context to the AI system's output. It helps to determine if the context used is appropriate for generating the output. The works by using an LLM to rate the relevancy of the context to the output. The rating scale is from `0` to `1`, with `0` being least relevant and `1` being most relevant. `ContextRelevancyScorer` then returns a dictionary with the `relevancy_score`. 


```python
from weave.scorers import ContextRelevancyScorer

llm_client = ...  # initialise your LlM client

relevancy_scorer = ContextRelevancyScorer(
  llm_client = ...  # initialise your LlM client
  model_id="your-model-id"
)
```

#### Usage notes

- `ContextRelevancyScorer` expects a `context` column in your dataset. You can [use `column_map`](../scorers/scorers-overview.md#mapping-column-names-with-column_map) to map `context` to another dataset column if needed.
- Customize the `relevancy_prompt` to define how relevancy is assessed.

#### Example

The following example shows `ContextEntityRecallScorer` and `ContextRelevancyScorer` in the context of an evaluation:

```python
import asyncio
from textwrap import dedent
from openai import OpenAI
import weave
from weave.scorers import ContextEntityRecallScorer, ContextRelevancyScorer

class RAGModel(weave.Model):
@weave.op()
async def predict(self, question: str) -> str:
  return "Paris is the capital of France."

model = RAGModel()

# Define prompts
relevancy_prompt: str = dedent("""
  Given the following question and context, rate the relevancy of the context to the question on a scale from 0 to 1.

  Question: {question}
  Context: {context}
  Relevancy Score (0-1):
""")

# Initialize clients and scorers
llm_client = OpenAI()
entity_recall_scorer = ContextEntityRecallScorer(
  client=client,
  model_id="gpt-4o",
)

relevancy_scorer = ContextRelevancyScorer(
  client=llm_client,
  model_id="gpt-4o",
  relevancy_prompt=relevancy_prompt
)

# Create dataset
dataset = [
  {
    "question": "What is the capital of France?",
    "context": "Paris is the capital city of France."
  },
  {
    "question": "Who wrote Romeo and Juliet?",
    "context": "William Shakespeare wrote many famous plays."
  }
]

# Run evaluation
evaluation = weave.Evaluation(
  dataset=dataset,
  scorers=[entity_recall_scorer, relevancy_scorer]
)
results = asyncio.run(evaluation.evaluate(model))
print(results)
# {'ContextEntityRecallScorer': {'recall': {'mean': 0.3333333333333333}}, 'ContextRelevancyScorer': {'relevancy_score': {'mean': 0.5}}, 'model_latency': {'mean': 9.393692016601562e-05}}
```

### `FaithfulnessScorer`

The `FaithfulnessScorer` evaluates whether the AI system's output is consistent with the input query and context. 

#### Example

The following example shows how to use `FaithfulnessScorer` to evaluate the faithfulness of an AI system's output:

```python
from weave.scorers import FaithfulnessScorer

faithfulness_scorer = FaithfulnessScorer(model_name_or_path="models/faithfulness_scorer")

result = faithfulness_scorer.score(
  query="What is the capital of Antarctica?",
  context="People in Antarctica love the penguins.",
  output="The capital of Antarctica is Penguin City."
)
print(f"Output is not faithful: {result['flagged']}")
```

#### Usage notes
- Requires both `query` and `context` for evaluation.
- The `model_name_or_path` parameter points to the pre-trained model weights.

### `BiasScorer`
The `BiasScorer` identifies biased or stereotypical content in the AI system's output.

#### Example
The following example demonstrates the use of `BiasScorer` to detect bias:

```python
from weave.scorers import BiasScorer

bias_scorer = BiasScorer(model_name_or_path="models/bias_scorer")

result = bias_scorer.score("Men are terrible at cleaning.")
print(f"The input is biased: {result['flagged']}")
```

#### Usage notes
- Adjust thresholds to control the sensitivity to bias detection.
- Requires a `model_name_or_path` to specify the pre-trained model.

### `ToxicityScorer`
The `ToxicityScorer` evaluates the AI system's output for toxic or harmful content.

#### Example
The following example shows how to use `ToxicityScorer` to flag toxic content:

```python
from weave.scorers import ToxicityScorer

toxicity_scorer = ToxicityScorer(model_name_or_path="models/toxicity_scorer")

result = toxicity_scorer.score("People from Ireland are the worst.")
print(f"Input is toxic: {result['flagged']}")
```

#### Usage notes
- Scores for individual toxicity categories and a cumulative total are provided.
- Adjust thresholds to match the sensitivity required for your application. For example, stricter thresholds may flag borderline content, while lenient thresholds reduce false positives.

### `RelevanceScorer`
The `RelevanceScorer` determines whether the AI system's output is relevant to the input and optional context.

#### Example
The following example demonstrates the use of `RelevanceScorer` to evaluate relevance:

```python
from weave.scorers import RelevanceScorer

relevance_scorer = RelevanceScorer(model_name_or_path="models/relevance_scorer")

result = relevance_scorer.score(
  input="What is the capital of Antarctica?",
  context="Antarctica has the happiest penguins.",
  output="The savannah has the biggest lions."
)
print(f"Output is relevant: {result['is_relevant']}")
```

#### Usage notes
- The scorer provides a binary `is_relevant` result and a numerical relevance score.
- `context` is optional, but can improve accuracy.

### `CoherenceScorer`
The `CoherenceScorer` evaluates whether the AI system's output is coherent and logically structured.

#### Example
The following example demonstrates the use of CoherenceScorer to evaluate output coherence:

```python
from weave.scorers import CoherenceScorer

coherence_scorer = CoherenceScorer(model_name_or_path="models/coherence_scorer")

result = coherence_scorer.score(
  input="What is the capital of Antarctica?",
  output="but why not monkey up day"
)
print(f"Output is coherent: {result['is_coherent']}")
```

#### Usage notes
- Designed to identify nonsensical or irrelevant outputs.
Outputs that fail coherence checks are flagged as incoherent.
- Define specific coherence parameters to handle task-specific output structures.

### `RobustnessScorer`
The `RobustnessScorer` measures the consistency of the AI system's output across variations of the same input.

#### Example
The following example shows how to use `RobustnessScorer` to test output robustness:

```python
from weave.scorers import RobustnessScorer

# use_exact_match: Whether to use exact string matching for evaluation.
# return_interpretation: Whether to include detailed interpretation of results.
robustness_scorer = RobustnessScorer(use_exact_match=False, return_interpretation=True)

outputs = [
  "James Watt improved the steam engine in 1769, making it efficient enough for industrial use.",
  "In 1769, James Watt modified the steam engine to achieve better industrial efficiency.",
]

result = robustness_scorer.score(output=outputs)
print(result)
```

#### Usage notes
- Supports both exact match and semantic evaluation modes.
Useful for testing model stability under input perturbations.

### `PerplexityScorer`
The `PerplexityScorer` evaluates the _perplexity_ of the AI system's output, which is a measure of how well the model predicts the sequence.

#### Example
The following example shows how to compute perplexity using `PerplexityScorer`:

```python
from weave.scorers import HuggingFacePerplexityScorer

perplexity_scorer = HuggingFacePerplexityScorer()

result = perplexity_scorer.score(output="This is a sample output text.")
print(f"Perplexity score: {result['perplexity']}")
```

#### Usage notes
- Lower perplexity scores indicate higher confidence in the output.
- Useful for assessing the fluency of generated text.

### `AccuracyScorer`
The `AccuracyScorer` calculates the accuracy of the AI system's predictions against ground truth labels.

#### Example
The following example demonstrates how to calculate accuracy using `AccuracyScorer`:

```python
from weave.scorers import AccuracyScorer

accuracy_scorer = AccuracyScorer(task="binary")

ground_truths = [0, 0, 1]
outputs = [1, 0, 1]

result = accuracy_scorer.score(ground_truth=ground_truths, output=outputs)
print(result)
```

#### Usage notes
- Supports binary and multiclass classification tasks.
- Requires task-specific configurations.

### `RougeScorer`
The `RougeScorer` evaluates the quality of the AI system's summaries by comparing them to reference texts using [ROUGE metrics](https://en.wikipedia.org/wiki/ROUGE_(metric)).

#### Example
The following example shows how to use `RougeScorer` for summary evaluation:

```python
from weave.scorers import RougeScorer

rouge_scorer = RougeScorer()

result = rouge_scorer.score(
  ground_truth="The cat sat on the mat.",
  output="The cat is sitting on the mat."
)
print(result)
```

### `BLEUScorer`
The `BLEUScorer` evaluates the quality of translations or generated text by comparing them to reference texts using [BLEU metrics](https://en.wikipedia.org/wiki/BLEU).

#### Example
The following example demonstrates how to use `BLEUScorer`:

```python
from weave.scorers import BLEUScorer

bleu_scorer = BLEUScorer()

result = bleu_scorer.score(
  ground_truths=["The watermelon seeds will be excreted."],
  output="The watermelon seeds pass through your digestive system."
)
print(result)
```
