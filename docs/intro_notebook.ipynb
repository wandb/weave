{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "298501c0",
   "metadata": {},
   "source": [
    "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "<!--- @wandbcode{intro-colab} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974b2c96",
   "metadata": {},
   "source": [
    "# üèÉ‚Äç‚ôÄÔ∏è Quickstart\n",
    "\n",
    "Get started using Weave to:\n",
    "- Log and debug language model inputs, outputs, and traces\n",
    "- Build rigorous, apples-to-apples evaluations for language model use cases\n",
    "- Organize all the information generated across the LLM workflow, from experimentation to evaluations to production\n",
    "\n",
    "See the full Weave documentation [here](https://wandb.me/weave).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dc5c9d",
   "metadata": {},
   "source": [
    "## ü™Ñ Install `weave` library and login\n",
    "\n",
    "\n",
    "Start by installing the library and logging in to your account.\n",
    "\n",
    "In this example, we're using openai so you should [add an openai API key](https://platform.openai.com/docs/quickstart/step-2-setup-your-api-key).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a6d209",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install weave openai set-env-colab-kaggle-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141ebc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to your W&B account\n",
    "import weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0921bbef-d089-4960-b624-505172f90d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OpenAI API key\n",
    "\n",
    "import os\n",
    "import openai\n",
    "from set_env import set_env\n",
    "from google.colab import userdata\n",
    "\n",
    "# Put your OPENAI_API_KEY in the secrets panel to the left üóùÔ∏è\n",
    "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "set_env(\"OPENAI_API_KEY\")\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" # alternatively, put your key here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef6f27b",
   "metadata": {},
   "source": [
    "## Track inputs & outputs of functions\n",
    "\n",
    "- Import weave\n",
    "- Call `weave.init('project-name')` to start logging\n",
    "\n",
    "Here, we're automatically tracking all calls to `openai`. We automatically track a lot of LLM libraries, but it's really easy to add support for whatever LLM you're using, as you'll see below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcd1bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "from openai import OpenAI\n",
    "\n",
    "PROJECT = \"weave-intro-notebook\"\n",
    "\n",
    "weave.init(PROJECT)\n",
    "\n",
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo-1106\",\n",
    "  messages=[\n",
    "      {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": \"You are a grammar checker, correct the following user input.\"\n",
    "      },\n",
    "      {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"That was so easy, it was a piece of pie!\"\n",
    "      }\n",
    "      ],\n",
    "      temperature=0,\n",
    ")\n",
    "generation = response.choices[0].message.content\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174cabef",
   "metadata": {},
   "source": [
    "You can find your interactive dashboard by clicking any of the  üëÜ wandb links above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99453d48",
   "metadata": {},
   "source": [
    "## Track custom functions\n",
    "\n",
    "Add the @weave.op decorator to the functions you want to track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a1817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "from openai import OpenAI\n",
    "\n",
    "weave.init(PROJECT)\n",
    "\n",
    "@weave.op\n",
    "def strip_user_input(user_input):\n",
    "    return user_input.strip()\n",
    "\n",
    "result = strip_user_input(\"    hello    \")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed90e37",
   "metadata": {},
   "source": [
    "After adding `weave.op` and calling the function, visit the link and see it tracked within your project.\n",
    "\n",
    "üí° We automatically track your code, have a look at the code tab!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fe43dc",
   "metadata": {},
   "source": [
    "## Track nested functions\n",
    "\n",
    "Now that you've seen the basics, let's combine all of the above and track some deeply nested functions alongside LLM calls.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422050d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "from openai import OpenAI\n",
    "\n",
    "weave.init(PROJECT)\n",
    "\n",
    "@weave.op\n",
    "def strip_user_input(user_input):\n",
    "    return user_input.strip()\n",
    "\n",
    "@weave.op\n",
    "def correct_grammar(user_input):\n",
    "    client = OpenAI()\n",
    "\n",
    "    stripped = strip_user_input(user_input)\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a grammar checker, correct the following user input.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": stripped\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "result = correct_grammar(\"   That was so easy, it was a piece of pie!    \")\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f483413",
   "metadata": {},
   "source": [
    "## Track Errors\n",
    "\n",
    "Whenever your code crashes, weave will highlight what caused the issue. This is especially useful for finding things like JSON parsing issues that can occasionally happen when parsing data from LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e7e103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "weave.init(PROJECT)\n",
    "\n",
    "@weave.op()\n",
    "def strip_user_input(user_input):\n",
    "    return user_input.strip()\n",
    "\n",
    "@weave.op()\n",
    "def correct_grammar(user_input):\n",
    "    client = OpenAI()\n",
    "\n",
    "    stripped = strip_user_input(user_input)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a grammar checker, correct the following user input.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": stripped\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "        response_format={ \"type\": \"json_object\" }\n",
    "    )\n",
    "    return json.loads(response.choices[0].message.content)\n",
    "\n",
    "result = correct_grammar(\"   That was so easy, it was a piece of pie!    \")\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f957c9c8",
   "metadata": {},
   "source": [
    "## Tracking Objects\n",
    "\n",
    "Organizing experimentation is difficult when there are many moving pieces. You can capture and organize the experimental details of your app like your system prompt or the model you're using within `weave.Objects`. This helps organize and compare different iterations of your app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e60d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "from openai import OpenAI\n",
    "\n",
    "weave.init(PROJECT)\n",
    "\n",
    "class GrammarCorrector(weave.Object):\n",
    "    model: str\n",
    "    system_message: str\n",
    "\n",
    "    @weave.op\n",
    "    def correct(self, user_input):\n",
    "        client = OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\n",
    "                  \"role\": \"system\",\n",
    "                  \"content\": self.system_message\n",
    "                },\n",
    "                {\n",
    "                  \"role\": \"user\",\n",
    "                  \"content\": user_input\n",
    "                }\n",
    "                ],\n",
    "            temperature=0,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "\n",
    "corrector = GrammarCorrector(model=\"gpt-3.5-turbo-1106\", system_message = \"You are a grammar checker, correct the following user input.\")\n",
    "result = corrector.correct(\"That was so easy, it was a piece of pie!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d43bc3",
   "metadata": {},
   "source": [
    "Notice that we saved a versioned `GrammarCorrector` object that captures the configurations you're experimenting with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c3aaea",
   "metadata": {},
   "source": [
    "## Explicitly publish & retrieve objects\n",
    "\n",
    "You can publish objects and then retrieve them in your code. You can even call functions from your retrieved objects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa5c5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "\n",
    "weave.init(PROJECT)\n",
    "\n",
    "corrector = GrammarCorrector(model=\"gpt-3.5-turbo-1106\", system_message = \"You are a grammar checker, please correct the following user input.\")\n",
    "ref = weave.publish(corrector)\n",
    "ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff9cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "\n",
    "weave.init(PROJECT)\n",
    "\n",
    "fetched_collector = weave.ref(f\"weave:///{ref.entity}/weave-intro-notebook/object/{ref.name}:{ref.digest}\").get()\n",
    "\n",
    "# Notice: this object was loaded from remote location!\n",
    "result = fetched_collector.correct(\"That was so easy, it was a piece of pie!\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b94dd5",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Evaluation-driven development helps you reliably iterate on an application. The `Evaluation` class is designed to assess the performance of a `Model` on a given `Dataset` or set of examples using scoring functions.\n",
    "\n",
    "See a preview of the API below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c597f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "from weave import Evaluation\n",
    "import asyncio\n",
    "\n",
    "# Collect your examples\n",
    "examples = [\n",
    "    {\"question\": \"What is the capital of France?\", \"expected\": \"Paris\"},\n",
    "    {\"question\": \"Who wrote 'To Kill a Mockingbird'?\", \"expected\": \"Harper Lee\"},\n",
    "    {\"question\": \"What is the square root of 64?\", \"expected\": \"8\"},\n",
    "]\n",
    "\n",
    "# Define any custom scoring function\n",
    "@weave.op\n",
    "def match_score1(expected: str, model_output: dict) -> dict:\n",
    "    # Here is where you'd define the logic to score the model output\n",
    "    return {'match': expected == model_output['generated_text']}\n",
    "\n",
    "@weave.op\n",
    "def function_to_evaluate(question: str):\n",
    "    # here's where you would add your LLM call and return the output\n",
    "    return  {'generated_text': 'Paris'}\n",
    "\n",
    "# Score your examples using scoring functions\n",
    "evaluation = Evaluation(\n",
    "    dataset=examples, scorers=[match_score1]\n",
    ")\n",
    "\n",
    "# Start tracking the evaluation\n",
    "weave.init('weave-intro-example')\n",
    "# Run the evaluation\n",
    "await evaluation.evaluate(function_to_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d997d9e6",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "Follow the [Build an Evaluation pipeline](http://wandb.me/weave_eval_tut) tutorial to learn more about Evaluation and begin iteratively improving your applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
