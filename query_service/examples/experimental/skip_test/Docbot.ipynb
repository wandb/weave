{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef0c98ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import weave\n",
        "weave.use_frontend_devmode()\n",
        "from weave.legacy.weave.ecosystem import langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f7192c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "neg_feedback = [\"Is there a way that I can embed an iframe component in Weights and Biases report?\",\n",
        "                \"how do i create the pretty sweeps plot?\",\n",
        "                \"I'm getting error 400. What can I do?\",\n",
        "                \"How can I move a column in a table to the right?\",\n",
        "                \"In Prompts, how can I resize the Trace Timeline to make it bigger or full screen?\",\n",
        "                \"I have a question about exporting CSV files from a web panel. Whenever I do this, I always get two extra columns for MAX and MIN values, even if I only have one data curve. Does anyone know how to solve this issue?\",\n",
        "                \"artifacts cli command to upload a folder of images\",\n",
        "                \"Is there a recommended way to use Launch in an SLURM environment?\",\n",
        "                \"How to export a single chart's data using the API?\",\n",
        "                \"how can i login with a different wandb user?\"\n",
        "]\n",
        "\n",
        "pos_feedback = [\"my logging doesn't seem to include errors when the training crashes, how do I change the logging level for wandb logging?\",\n",
        "                \"how can i get the data from my wandb run by querying my logs using python?\",\n",
        "                \"how do I fix an error with wandb Table construction from pandas dataframe: TypeError: Data row contained incompatible types\",\n",
        "                \"how can i make a heatmap using vega and plot it to wandb? what is the vegaspec?\",\n",
        "                \"is there a good way to join 2 tables together programmatically?\",\n",
        "                \"I have a question about sweeps. How can you constrain relationship between parameters. For example, I now that if num_layers * hidden_dim is large, I'll run out of GPU memory. So, also I would like to explore some hyperparameter space, there are some combination I know will fail. optuna as a way to do that: you can throw an special exception to cancel a run during a sweep, so that it is not recorded. Is there something similar in W&B, or another way of pruning unwanted combination of hyperparameters?\",\n",
        "                \"where can I find my run_id\",\n",
        "                \"How do I group runs?\",\n",
        "                \"I am using the Hugging Face trainer to train a GPT-2 model. How can I log in wandb the results of the model in each evaluation?\",\n",
        "                \"I am looking to finetune LLAMA on my own dataset using OpenAI, can you give me examples on how to do this?\"\n",
        "               ]\n",
        "\n",
        "all_qs = [\n",
        "    {'question': q, 'feedback': 'positive'} for q in pos_feedback] + [\n",
        "    {'question': q, 'feedback': 'negative'} for q in neg_feedback]\n",
        "\n",
        "questions = weave.save(all_qs, 'eval_questions')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03a68bf9",
      "metadata": {},
      "outputs": [],
      "source": [
        "questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6e1c366",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import (\n",
        "    MarkdownTextSplitter,\n",
        "    PythonCodeTextSplitter,\n",
        "    TokenTextSplitter,\n",
        ")\n",
        "# Get markdown files from our docs repo\n",
        "\n",
        "# Checkout of our docs repo: https://github.com/wandb/docodile/\n",
        "DOC_DIR = '/Users/shawn/code2/docodile'\n",
        "DOC_SUFFIX = '.md'\n",
        "\n",
        "docs = []\n",
        "for file in pathlib.Path(DOC_DIR).glob('**/*' + DOC_SUFFIX):\n",
        "    with file.open('r') as f:\n",
        "        # store them as langchain Document objects\n",
        "        docs.append(Document(page_content=f.read(), metadata={'path': file.name}))\n",
        "docs = MarkdownTextSplitter().split_documents(docs)\n",
        "docs = TokenTextSplitter().split_documents(docs)\n",
        "\n",
        "docs = weave.save(docs, 'wandb-docs')\n",
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "624e0b9e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vector store with langchain\n",
        "from langchain.vectorstores import VectorStore, FAISS\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vector_store = FAISS.from_documents(weave.use(docs), embeddings)\n",
        "\n",
        "weave.save(vector_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a97a3a9c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vector store with Weave\n",
        "\n",
        "# Stuff you can do:\n",
        "#   - .similarity_search(<query>)\n",
        "#     - but currently there is a crash because Row.Group tries to render and fails. Need to switch the\n",
        "#       panel to Table and then change column to row.__getattr__('page_content')\n",
        "#   - .document_embeddings\n",
        "#     - this gets the embeddings out of FAISS, and also performs FAISS' k-means with 20 clusters\n",
        "#     - switch to projection.plot\n",
        "# TODO:\n",
        "#   - give control over k for k-means\n",
        "\n",
        "from weave.legacy.weave.ecosystem import langchain\n",
        "\n",
        "vector_store_node = langchain.faiss_from_documents(docs, langchain.openai_embeddings())\n",
        "vector_store_node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd31e988",
      "metadata": {},
      "outputs": [],
      "source": [
        "# With langchain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "model_gpt_35_temp07 = RetrievalQA.from_chain_type(\n",
        "        llm=ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.7),\n",
        "        chain_type='stuff',\n",
        "        retriever=vector_store.as_retriever()\n",
        "    )\n",
        "model = model_gpt_35_temp07\n",
        "model = weave.save(model_gpt_35_temp07, 'mymodel')\n",
        "\n",
        "#model.run('hello')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "694ce5d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# With weave\n",
        "from weave.legacy.weave.ecosystem import langchain\n",
        "qa = langchain.retrieval_qa_from_chain_type(\n",
        "    langchain.chat_openai('gpt-3.5-turbo', 0.7),\n",
        "    'stuff',\n",
        "    vector_store_node)\n",
        "qa.run('hello')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27fc1618",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from langchain.chat_models import ChatOpenAI\n",
        "# from langchain.chains import RetrievalQA\n",
        "model_02 = RetrievalQA.from_chain_type(\n",
        "        llm=ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.2),\n",
        "        chain_type='stuff',\n",
        "        retriever=vector_store.as_retriever()\n",
        "    )\n",
        "model_07 = RetrievalQA.from_chain_type(\n",
        "        llm=ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.7),\n",
        "        chain_type='stuff',\n",
        "        retriever=vector_store.as_retriever()\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f18ae43f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Two with LC and weave.save\n",
        "\n",
        "models = [model_02, model_07]\n",
        "models = weave.save(models, 'docbot-models')\n",
        "models.run('hello')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34c3e345",
      "metadata": {},
      "outputs": [],
      "source": [
        "# You can map over questions\n",
        "weave.legacy.weave.panels.Table(questions.limit(3), columns=[\n",
        "    lambda q: q['question'],\n",
        "    lambda q: models.run(q['question'])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "876bc738",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive evaluation and exploration dashboard\n",
        "\n",
        "weave.legacy.weave.panels.Board(\n",
        "    vars={\n",
        "        'documents': docs,\n",
        "        'questions': questions.limit(2),\n",
        "        'embeddings': langchain.openai_embeddings(),\n",
        "        'vector_store': lambda embeddings, documents: langchain.faiss_from_documents(documents, embeddings),\n",
        "        'doc_embeddings': lambda vector_store: vector_store.document_embeddings(),\n",
        "        #'models': models,\n",
        "        'model_a': lambda vector_store: langchain.retrieval_qa_from_chain_type(\n",
        "            langchain.chat_openai('gpt-3.5-turbo', 0.2),\n",
        "            'stuff',\n",
        "            vector_store),\n",
        "        'model_b': lambda vector_store: langchain.retrieval_qa_from_chain_type(\n",
        "            langchain.chat_openai('gpt-3.5-turbo', 0.7),\n",
        "            'stuff',\n",
        "            vector_store),\n",
        "        'models': lambda model_a, model_b: weave.legacy.weave.ops.make_list(a=model_a, b=model_b),\n",
        "        'projection': lambda doc_embeddings: doc_embeddings.projection2D(\n",
        "                                                                  'pca',\n",
        "                                                                  'single',\n",
        "                                                                  ['embedding'],\n",
        "                                                                  {'pca': {},\n",
        "                                                                   'tsne': {\n",
        "                                                                       'perplexity': 30,\n",
        "                                                                       'learningRate': 10,\n",
        "                                                                       'iterations': 25\n",
        "                                                                   },\n",
        "                                                                   'umap': {\n",
        "                                                                       'neighbors': 15,\n",
        "                                                                       'minDist': 0.1,\n",
        "                                                                       'spread': 1.0\n",
        "                                                                   }\n",
        "                                                                  }),\n",
        "    },\n",
        "    panels=[    \n",
        "        weave.legacy.weave.panels.BoardPanel(\n",
        "            lambda models: weave.legacy.weave.panels.Each(models.run(\"What is Weave?\")),\n",
        "            layout=weave.legacy.weave.panels.BoardPanelLayout(x=0, y=0, w=24, h=6)\n",
        "        ),\n",
        "        weave.legacy.weave.panels.BoardPanel(\n",
        "            lambda model_a, model_b: weave.legacy.weave.panels.Table(questions,\n",
        "                                              columns=[\n",
        "                                                  lambda question: question['question'],\n",
        "                                                  lambda question: question['feedback'],\n",
        "                                                  weave.legacy.weave.panels.TableColumn(\n",
        "                                                      lambda question: model_a.run(question['question']).result,\n",
        "                                                      name='model_a'\n",
        "                                                  ),\n",
        "                                                  weave.legacy.weave.panels.TableColumn(\n",
        "                                                      lambda question: model_b.run(question['question']).result,\n",
        "                                                      name='model_b'\n",
        "                                                  ),\n",
        "                                              ]),\n",
        "            layout=weave.legacy.weave.panels.BoardPanelLayout(x=0, y=6, w=24, h=6)\n",
        "        ),       \n",
        "        weave.legacy.weave.panels.BoardPanel(\n",
        "            id='docs_projection',\n",
        "            panel=lambda projection: weave.legacy.weave.panels.Plot(\n",
        "                projection,\n",
        "                x=lambda row: row['projection.x'],\n",
        "                y=lambda row: row['projection.y'],\n",
        "                color=lambda row: row['source.cluster']\n",
        "            ),\n",
        "            layout=weave.legacy.weave.panels.BoardPanelLayout(x=0, y=12, w=12, h=6)\n",
        "        ),\n",
        "        weave.legacy.weave.panels.BoardPanel(\n",
        "            lambda docs_projection: docs_projection.selected_data(),\n",
        "            layout=weave.legacy.weave.panels.BoardPanelLayout(x=12, y=12, w=12, h=6)\n",
        "        ),\n",
        "        weave.legacy.weave.panels.BoardPanel(\n",
        "            lambda documents: weave.legacy.weave.panels.Table(documents,\n",
        "                                                 columns=[\n",
        "                                                     lambda doc: doc.page_content,\n",
        "                                                     lambda doc: doc.metadata['path']\n",
        "                                                 ]),\n",
        "            layout=weave.legacy.weave.panels.BoardPanelLayout(x=0, y=18, w=12, h=6)\n",
        "        ),\n",
        "        weave.legacy.weave.panels.BoardPanel(\n",
        "            lambda vector_store: weave.legacy.weave.panels.Table(vector_store.similarity_search('weave'),\n",
        "                                                    columns=[\n",
        "                                                     lambda doc: doc.page_content,\n",
        "                                                     lambda doc: doc.metadata['path']  \n",
        "                                                    ]),\n",
        "            layout=weave.legacy.weave.panels.BoardPanelLayout(x=12, y=18, w=12, h=6)\n",
        "        ),\n",
        "    ]\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
