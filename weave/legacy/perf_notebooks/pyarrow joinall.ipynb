{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c09866c",
   "metadata": {},
   "source": [
    "Shows a 40x+ performance improvement over duckdb for our joinall op.\n",
    "\n",
    "Example output for make_tables(5, 100000, 10):\n",
    "```\n",
    "Test data construction time: 0.023s\n",
    "\n",
    "Total duck time: 3.076s\n",
    "\n",
    "Hybrid join time: 0.120s\n",
    "Hybrid zip time 0.030s\n",
    "Total hybrid time: 0.156s\n",
    "\n",
    "Pyarrow join time: 0.024s\n",
    "Pyarrow zip time: 0.043s\n",
    "Total pyarrow time: 0.069s\n",
    "\n",
    "Hybrid speedup over duck: 19.74x\n",
    "Pyarrow speedup over duck: 44.84x\n",
    "```\n",
    "\n",
    "For very large inputs I saw Duck and Hybrid converging to about the same time, with the pyarrow version achieving a speed up of ~150x. My guess is the duck join implementation is less memory efficient and starts to swap at some point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7daaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import duckdb\n",
    "import numpy as np\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ff1d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test tables\n",
    "\n",
    "\n",
    "def make_tables(n_tables, n_rows, n_cols):\n",
    "    base_col = np.arange(n_rows, dtype=\"int64\")\n",
    "    tables = []\n",
    "    for t in range(n_tables):\n",
    "        cols = []\n",
    "        for c in range(n_cols):\n",
    "            if c == 0:\n",
    "                cols.append(base_col)\n",
    "            else:\n",
    "                col_start = c * n_rows + t * n_cols * n_rows\n",
    "                col_vals = base_col + col_start\n",
    "                cols.append(\n",
    "                    pa.StructArray.from_arrays(\n",
    "                        [-col_vals, col_vals], names=[\"tag\", \"val\"]\n",
    "                    )\n",
    "                )\n",
    "        tables.append(\n",
    "            pa.Table.from_arrays(cols, names=[f\"c{c}\" for c in range(n_cols)])\n",
    "        )\n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fee51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joinall implemented fully using duckdb. This is the baseline.\n",
    "\n",
    "\n",
    "def duck_query(tables):\n",
    "    n_tables = len(tables)\n",
    "    n_cols = tables[0].num_columns\n",
    "    selects = []\n",
    "    for col in range(n_cols):\n",
    "        select_cols = \", \".join(f\"t{t}.c{col}\" for t in range(n_tables))\n",
    "        selects.append(f\"list_value({select_cols}) as c{col}\")\n",
    "    select = \", \".join(selects)\n",
    "    joins = []\n",
    "    for t in range(1, n_tables):\n",
    "        joins.append(f\"inner join t{t} on t0.c0 = t{t}.c0\")\n",
    "    join = \" \".join(joins)\n",
    "    # Must sort to get stable results for comparison, but it crashes the\n",
    "    # notebook kernel when the tables are over a certain size!\n",
    "    return f\"SELECT {select} FROM t0 {join}\"\n",
    "    # return f'SELECT {select} FROM t0 {join} ORDER BY t0.c0 ASC'\n",
    "\n",
    "\n",
    "def duck_join(tables):\n",
    "    conn = duckdb.connect()\n",
    "    for i, t in enumerate(tables):\n",
    "        conn.register(f\"t{i}\", t)\n",
    "    query = duck_query(tables)\n",
    "    return conn.execute(query).arrow()\n",
    "\n",
    "\n",
    "# tables = make_tables(2, 100, 3)\n",
    "# old_res = duck_join(tables)\n",
    "# old_res['c0'].to_pylist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed709de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrow_zip(*arrs):\n",
    "    n_arrs = len(arrs)\n",
    "    output_len = min(len(a) for a in arrs)\n",
    "    array_indexes = np.tile(np.arange(n_arrs, dtype=\"int64\"), output_len)\n",
    "    item_indexes = np.floor(np.arange(0, output_len, 1.0 / n_arrs)).astype(\"int64\")\n",
    "    indexes = item_indexes + array_indexes * output_len\n",
    "    concatted = pa.concat_arrays(arrs)\n",
    "    interleaved = concatted.take(indexes)\n",
    "    offsets = np.arange(0, len(interleaved) + len(arrs), len(arrs), dtype=\"int64\")\n",
    "    return pa.ListArray.from_arrays(offsets, interleaved)\n",
    "\n",
    "\n",
    "def new_duck_query(tables):\n",
    "    n_tables = len(tables)\n",
    "    n_cols = tables[0].num_columns\n",
    "    selects = []\n",
    "    for col in range(n_cols):\n",
    "        for t in range(n_tables):\n",
    "            selects.append(f\"t{t}.c{col} as t{t}c{col}\")\n",
    "    select = \", \".join(selects)\n",
    "    joins = []\n",
    "    for t in range(1, n_tables):\n",
    "        joins.append(f\"inner join t{t} on t0.c0 = t{t}.c0\")\n",
    "    join = \" \".join(joins)\n",
    "    # Must sort to get stable results for comparison, but it crashes the\n",
    "    # notebook kernel when the tables are over a certain size!\n",
    "    return f\"SELECT {select} FROM t0 {join}\"\n",
    "    # return f'SELECT {select} FROM t0 {join} ORDER BY t0.c0 ASC'\n",
    "\n",
    "\n",
    "# This implements the join part using duckdb, but the zip step using\n",
    "# pyarrow operations.\n",
    "def hybrid_join(tables):\n",
    "    n_tables = len(tables)\n",
    "    n_cols = tables[0].num_columns\n",
    "\n",
    "    # join with duckdb\n",
    "    start_time = time.time()\n",
    "    conn = duckdb.connect()\n",
    "    for i, t in enumerate(tables):\n",
    "        conn.register(f\"t{i}\", t)\n",
    "    query = new_duck_query(tables)\n",
    "    joined = conn.execute(query).arrow()\n",
    "    print(\"Hybrid join time: %.03fs\" % (time.time() - start_time))\n",
    "\n",
    "    # zip cols\n",
    "    start_time = time.time()\n",
    "    zipped_cols = []\n",
    "    for c in range(n_cols):\n",
    "        col_cols = [joined[f\"t{t}c{c}\"].combine_chunks() for t in range(n_tables)]\n",
    "        zipped_cols.append(arrow_zip(*col_cols))\n",
    "    result = pa.Table.from_arrays(zipped_cols, names=[f\"c{c}\" for c in range(n_cols)])\n",
    "    print(\"Hybrid zip time %.03fs\" % (time.time() - start_time))\n",
    "    return result\n",
    "\n",
    "\n",
    "# This implements everything using pyarrow operations. (fastest version)\n",
    "def pyarrow_join(tables):\n",
    "    n_tables = len(tables)\n",
    "    n_cols = tables[0].num_columns\n",
    "\n",
    "    # join\n",
    "    start_time = time.time()\n",
    "    table0 = tables[0]\n",
    "    joined = pa.Table.from_arrays(\n",
    "        [table0[\"c0\"], np.arange(len(table0), dtype=\"int64\")],\n",
    "        names=[\"join\", \"index_t0\"],\n",
    "    )\n",
    "    for i, t in enumerate(tables[1:]):\n",
    "        other = pa.Table.from_arrays(\n",
    "            [t[\"c0\"], np.arange(len(t), dtype=\"int64\")], names=[\"join\", f\"index_t{i+1}\"]\n",
    "        )\n",
    "        joined = joined.join(other, [\"join\"], join_type=\"inner\", use_threads=False)\n",
    "    print(\"Pyarrow join time: %.03fs\" % (time.time() - start_time))\n",
    "\n",
    "    # zip\n",
    "    zipped_cols = []\n",
    "    start_time = time.time()\n",
    "    for c in range(n_cols):\n",
    "        col_cols = []\n",
    "        for t, table in enumerate(tables):\n",
    "            t_indexes = joined[f\"index_t{t}\"]\n",
    "            col_cols.append(table[f\"c{c}\"].take(t_indexes).combine_chunks())\n",
    "        zipped_cols.append(arrow_zip(*col_cols))\n",
    "    print(\"Pyarrow zip time: %.03fs\" % (time.time() - start_time))\n",
    "\n",
    "    result = pa.Table.from_arrays(zipped_cols, names=[f\"c{c}\" for c in range(n_cols)])\n",
    "    return result\n",
    "\n",
    "\n",
    "# new_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1c34c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "tables = make_tables(5, 100000, 10)\n",
    "print(\"Test data construction time: %.03fs\" % (time.time() - start_time))\n",
    "print()\n",
    "\n",
    "start_time = time.time()\n",
    "old_res = duck_join(tables)\n",
    "old_time = time.time() - start_time\n",
    "print(\"Total duck time: %.03fs\" % old_time)\n",
    "print()\n",
    "\n",
    "start_time = time.time()\n",
    "hybrid_res = hybrid_join(tables)\n",
    "hybrid_time = time.time() - start_time\n",
    "print(\"Total hybrid time: %.03fs\" % hybrid_time)\n",
    "print()\n",
    "\n",
    "start_time = time.time()\n",
    "pyarrow_res = pyarrow_join(tables)\n",
    "pyarrow_time = time.time() - start_time\n",
    "print(\"Total pyarrow time: %.03fs\" % pyarrow_time)\n",
    "print()\n",
    "\n",
    "print(\"Hybrid speedup over duck: %.02fx\" % (old_time / hybrid_time))\n",
    "print(\"Pyarrow speedup over duck: %.02fx\" % (old_time / pyarrow_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06328121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old_res.to_pylist() == new_res.to_pylist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615394c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old_res.to_pylist() == new2_res.to_pylist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
