{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaa2b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import tiktoken\n",
    "import faiss\n",
    "import numpy as np\n",
    "from tenacity import (\n",
    "    before_sleep_log,\n",
    "    retry,\n",
    "    retry_if_exception_type,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    ")\n",
    "import openai\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9210100",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f739a144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API functions\n",
    "retry_openai_decorator = retry(\n",
    "        reraise=True,\n",
    "        stop=stop_after_attempt(4),\n",
    "        wait=wait_exponential(multiplier=1, min=4, max=10),\n",
    "        retry=(\n",
    "            retry_if_exception_type(openai.error.Timeout)\n",
    "            | retry_if_exception_type(openai.error.APIError)\n",
    "            | retry_if_exception_type(openai.error.APIConnectionError)\n",
    "            | retry_if_exception_type(openai.error.RateLimitError)\n",
    "            | retry_if_exception_type(openai.error.ServiceUnavailableError)\n",
    "        ),\n",
    "        before_sleep=before_sleep_log(logger, logging.WARNING),\n",
    "    )\n",
    "\n",
    "@retry_openai_decorator\n",
    "def openai_embed(model, input):\n",
    "    return openai.Embedding.create(input = input, model=model)\n",
    "\n",
    "@retry_openai_decorator\n",
    "def openai_chatcompletion(model, messages):\n",
    "    return openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\", # The deployment name you chose when you deployed the ChatGPT or GPT-4 model.\n",
    "        messages = messages\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b325d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to efficiently embed a set of documents using the OpenAI embedding API\n",
    "# This is from langchain\n",
    "\n",
    "embedding_ctx_length = 8191\n",
    "OPENAI_EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "chunk_size = 1000\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def embed_texts(texts: List[str], embedding_model: str) -> List[List[float]]:\n",
    "    embeddings: List[List[float]] = [[] for _ in range(len(texts))]\n",
    "    tokens = []\n",
    "    indices = []\n",
    "    encoding = tiktoken.model.encoding_for_model(embedding_model)\n",
    "    for i, text in enumerate(texts):\n",
    "        if embedding_model.endswith(\"001\"):\n",
    "            # See: https://github.com/openai/openai-python/issues/418#issuecomment-1525939500\n",
    "            # replace newlines, which can negatively affect performance.\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "        token = encoding.encode(\n",
    "            text,\n",
    "            disallowed_special=\"all\",\n",
    "        )\n",
    "        for j in range(0, len(token), embedding_ctx_length):\n",
    "            tokens += [token[j : j + embedding_ctx_length]]\n",
    "            indices += [i]\n",
    "\n",
    "    batched_embeddings = []\n",
    "    _chunk_size = chunk_size\n",
    "    for i in range(0, len(tokens), _chunk_size):\n",
    "        response = openai_embed(\n",
    "            embedding_model,\n",
    "            input=tokens[i : i + _chunk_size],\n",
    "        )\n",
    "        batched_embeddings += [r[\"embedding\"] for r in response[\"data\"]]\n",
    "\n",
    "    results: List[List[List[float]]] = [[] for _ in range(len(texts))]\n",
    "    num_tokens_in_batch: List[List[int]] = [[] for _ in range(len(texts))]\n",
    "    for i in range(len(indices)):\n",
    "        results[indices[i]].append(batched_embeddings[i])\n",
    "        num_tokens_in_batch[indices[i]].append(len(tokens[i]))\n",
    "\n",
    "    for i in range(len(texts)):\n",
    "        _result = results[i]\n",
    "        if len(_result) == 0:\n",
    "            average = embed_with_retry(\n",
    "                embedding_model,\n",
    "                input=\"\",\n",
    "            )[\"data\"][0][\"embedding\"]\n",
    "        else:\n",
    "            average = np.average(\n",
    "                _result, axis=0, weights=num_tokens_in_batch[i]\n",
    "            )\n",
    "        embeddings[i] = (average / np.linalg.norm(average)).tolist()\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb096bd1",
   "metadata": {},
   "source": [
    "# Miminal end-to-end version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ae6b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get markdown files from our docs repo\n",
    "\n",
    "DOC_DIR = '/Users/shawn/code2/docodile'\n",
    "DOC_SUFFIX = '.md'\n",
    "\n",
    "docs = []\n",
    "for file in Path(DOC_DIR).glob('**/*' + DOC_SUFFIX):\n",
    "    with file.open('r') as f:\n",
    "        docs.append({'path': file.name, 'contents': f.read()})\n",
    "        \n",
    "docs = docs[:100]\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4db55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually embed the docs\n",
    "\n",
    "doc_embeddings = embed_texts([d['contents'] for d in docs], OPENAI_EMBEDDING_MODEL)\n",
    "#doc_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e34ec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector index using FAISS\n",
    "\n",
    "faiss_index = faiss.IndexFlatL2(len(doc_embeddings[0]))\n",
    "doc_embeddings_vector = np.array(doc_embeddings, dtype=np.float32)\n",
    "faiss_index.add(doc_embeddings_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7619b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed our query\n",
    "\n",
    "query = 'Who are you?'\n",
    "query_embedding = openai_embed(OPENAI_EMBEDDING_MODEL, query)['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bc76d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most similar documents to our embedded query\n",
    "\n",
    "query_vector = np.array([query_embedding], dtype=np.float32)\n",
    "query_result_scores, query_result_indices = faiss_index.search(query_vector, 4)\n",
    "query_result_docs = [docs[i] for i in query_result_indices[0]]\n",
    "# query_result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d74f7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the documents into a prompt along with our question\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "prompt = prompt_template.format(\n",
    "    context='\\n\\n'.join([d['contents'] for d in query_result_docs]),\n",
    "    question=query\n",
    ")\n",
    "#prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5235e7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the response\n",
    "response = openai_chatcompletion(\n",
    "    model=\"gpt-3.5-turbo\", # The deployment name you chose when you deployed the ChatGPT or GPT-4 model.\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5df8513",
   "metadata": {},
   "source": [
    "# Object-oriented version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fcee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import typing\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Document:\n",
    "    contents: str\n",
    "    metadata: typing.Any\n",
    "        \n",
    "class Embeddings:\n",
    "    def embed_texts(self, texts: list[str]) -> List[List[float]]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class OpenAIEmbeddings(Embeddings):\n",
    "    model: str = \"text-embedding-ada-002\"\n",
    "\n",
    "    def embed_texts(self, texts: list[str]) -> List[List[float]]:\n",
    "        return embed_texts(texts, self.model)\n",
    "\n",
    "    def embed_query(self, query: str) -> List[float]:\n",
    "        return self.embed_texts(query)[0]\n",
    "\n",
    "class ChatMessage(typing.TypedDict):\n",
    "    role: str\n",
    "    content: str\n",
    "    \n",
    "@dataclasses.dataclass\n",
    "class ChatModel:\n",
    "    temperature: float = 0.7\n",
    "    def complete(self, messages: list[ChatMessage]) -> typing.Any:\n",
    "        return openai_chatcompletion(\n",
    "            model=\"gpt-3.5-turbo\", # The deployment name you chose when you deployed the ChatGPT or GPT-4 model.\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "class ChatOpenAI(ChatModel):\n",
    "    def complete(self, messages: list[str]) -> typing.Any:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class VectorIndex:\n",
    "    def search(self, query: str) -> list[int]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class FAISS(VectorIndex):\n",
    "    faiss_index: faiss.IndexFlatL2\n",
    "    \n",
    "    def search(self, query: str) -> list[list[int], list[int]]:\n",
    "        return self.faiss_index.search(query, 4)\n",
    "    \n",
    "def make_faiss(docs: list[Document], embedder: Embeddings) -> FAISS:\n",
    "    doc_embeddings = embedder.embed_texts([d['contents'] for d in docs])\n",
    "    faiss_index = faiss.IndexFlatL2(len(doc_embeddings[0]))\n",
    "    doc_embeddings_vector = np.array(doc_embeddings, dtype=np.float32)\n",
    "    faiss_index.add(doc_embeddings_vector)\n",
    "    return VectorStore(FAISS(faiss_index), docs, embedder.embed_query)\n",
    "    \n",
    "@dataclasses.dataclass\n",
    "class VectorStore:\n",
    "    index: VectorIndex\n",
    "    docs: list[Document]\n",
    "    embed_fn: typing.Callable[[str], Document]\n",
    "    \n",
    "    def search(self, query: str) -> list[Document]:\n",
    "        embedded_query = self.embed_fn(query)\n",
    "        query_vector = np.array([embedded_query], dtype=np.float32)\n",
    "        scores, indices = self.index.search(query_vector)\n",
    "        return [self.docs[i] for i in indices[0]]\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class QA:\n",
    "    vector_store: VectorStore\n",
    "    chat_model: ChatModel\n",
    "    prompt_template: str\n",
    "        \n",
    "    def query(self, query: str) -> typing.Any:    \n",
    "        return openai_chatcompletion(\n",
    "            model=\"gpt-3.5-turbo\", # The deployment name you chose when you deployed the ChatGPT or GPT-4 model.\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "def make_qa(vector_store: VectorStore, chat_model: ChatModel, prompt_template: str) -> QA:\n",
    "    return QA(vector_store, chat_model, prompt_template)\n",
    "    \n",
    "def doc_qa(vector_store: VectorStore, prompt_template: str, query: str):\n",
    "    query_result_docs = vector_store.search(query)\n",
    "    prompt = prompt_template.format(\n",
    "        context='\\n\\n'.join([d['contents'] for d in query_result_docs]),\n",
    "        question=query\n",
    "    )\n",
    "    return openai_chatcompletion(\n",
    "        model=\"gpt-3.5-turbo\", # The deployment name you chose when you deployed the ChatGPT or GPT-4 model.\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a17fd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vs = make_faiss(docs, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92514722",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatOpenAI()\n",
    "qa = make_qa(vs, chat_model, prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc75994",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_qa(vs, prompt_template, \"Who are you?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
