{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "36a9f92a",
      "metadata": {},
      "source": [
        "# W&B Production Monitoring Overview\n",
        "\n",
        "This notebook demonstrates how to monitor production models with W&B through an illustrative example. We will train a model to correctly identify handwritten digits, then monitor a locally deployed version of the model. We create a gradio app which runs in the notebook and lets a user draw/\"handwrite\" characters with the mouse and give live feedback by labeling the character as the digit 0-9.\n",
        "\n",
        "_Note: To keep the example focused on important code, much of the dataset manipulation, modelling, and other utilities are packaged in local files and imported here_\n",
        "\n",
        "# Step 0: Setup & import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3af52605",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install tensorflow\n",
        "!pip install gradio\n",
        "!pip install weave"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55afcd6c",
      "metadata": {},
      "source": [
        "Log in to W&B to sync these examples to your W&B account, where you can view, interact with, and customize the resulting Tables and Boards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9e3e332",
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48354a22",
      "metadata": {},
      "source": [
        "Set your W&B entity (username or team name) and optionally rename the destination project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "424dc619",
      "metadata": {},
      "outputs": [],
      "source": [
        "WB_ENTITY = \"shawn\"\n",
        "WB_PROJECT = \"prodmon_mnist\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8daaf18d",
      "metadata": {},
      "source": [
        "# Step 1: Get data\n",
        "In this example, we will use `keras.datasets.mnist.load_data()` to load in the MNIST dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de553636",
      "metadata": {},
      "outputs": [],
      "source": [
        "import model_util\n",
        "\n",
        "dataset = model_util.get_dataset()\n",
        "model_util.image_from_array(dataset[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6822a544",
      "metadata": {},
      "source": [
        "# Step 2: Train model\n",
        "Next we will train a classic NN to predict the digits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39bf1bc2",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = model_util.train_model(*dataset, conv_layers=0, epochs=1) # 1 epoch so we can actually see some errors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68a38ddd",
      "metadata": {},
      "source": [
        "# Step 3: Query model\n",
        "Now, let's query the model! Normally there is a little pre- and post- processing needed to make a prediction - we will write a short function to handle this for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b8afe99",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "\n",
        "def simple_predict(image_arr):\n",
        "    # Prepare image for model\n",
        "    tensor = (image_arr.astype(\"float32\")).reshape(1, 28, 28, 1)\n",
        "\n",
        "    # Make the prediction\n",
        "    prediction = model.predict(tensor, verbose=False)\n",
        "\n",
        "    # In this application, we need to reshape the output:\n",
        "    raw_predictions = prediction[0].tolist()\n",
        "    logits = {\n",
        "        str(k): v for k, v in zip(range(10), raw_predictions)\n",
        "    }\n",
        "    \n",
        "    prediction = np.argmax(raw_predictions).tolist()\n",
        "    \n",
        "    return {\"logits\": logits, \"prediction\": prediction}\n",
        "\n",
        "_, _, x_test, y_test = dataset\n",
        "for i in range(10):\n",
        "    image_arr = x_test[i]\n",
        "    truth = y_test[i]\n",
        "    preds = simple_predict(image_arr)\n",
        "    \n",
        "    print(f\"Input: {truth}\")\n",
        "    display(model_util.image_from_array(image_arr))\n",
        "    print(f\"Prediction: {preds['prediction']}\")\n",
        "    print(f\"Logits: {json.dumps(preds['logits'], indent=2)}\")\n",
        "    print(\"\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8826acf8",
      "metadata": {},
      "source": [
        "# Step 3A: Save predictions with W&B Weave using StreamTable\n",
        "With W&B's Weave library, we can stream any data to W&B for storage and further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c59bccb",
      "metadata": {},
      "outputs": [],
      "source": [
        "import weave\n",
        "weave.use_frontend_devmode()\n",
        "from weave.legacy.weave.monitoring import StreamTable\n",
        "\n",
        "# Initialize a stream table\n",
        "# (optionally change the name argument to any string\n",
        "# that follows the wandbentity_name/project_name/table_name format)\n",
        "st = StreamTable(f\"{WB_ENTITY}/{WB_PROJECT}/logged_predictions\")\n",
        "_, _, x_test, y_test = dataset\n",
        "for i in range(100):\n",
        "    image_arr = x_test[i]\n",
        "    truth = y_test[i].tolist()\n",
        "    preds = simple_predict(image_arr)\n",
        "    \n",
        "    # Log the data\n",
        "    st.log({\n",
        "        **preds,\n",
        "        \"image\": model_util.image_from_array(image_arr),\n",
        "        \"truth\": truth\n",
        "    })\n",
        "\n",
        "# Optional: wait for the logs to finish uploading (nicer for live demos)\n",
        "st.finish()\n",
        "\n",
        "# Show the StreamTable\n",
        "st    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcb0cc29",
      "metadata": {},
      "source": [
        "# Step 3B: Save predictions with W&B Weave using `monitor` decorator\n",
        "This pattern of logging inputs and outputs of a functions is so common, that we provide a decorator which automatically logs a function's I/O."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90ab5fe4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from weave.legacy.weave.monitoring import monitor\n",
        "import numpy as np\n",
        "\n",
        "mon = monitor.init_monitor(f\"{WB_ENTITY}/{WB_PROJECT}/monitor_predict_function\")\n",
        "\n",
        "def preprocess(span):\n",
        "    span.inputs['image'] = model_util.image_from_array(span.inputs['image_arr'])\n",
        "    del span.inputs['image_arr']\n",
        "\n",
        "@mon.trace(\n",
        "    # An preprocessor allows the function arguments to be pre-processed before logging.\n",
        "    preprocess = preprocess\n",
        ")\n",
        "def monitor_predict(image_arr):\n",
        "    # Prepare image for model\n",
        "    tensor = (image_arr.astype(\"float32\")).reshape(1, 28, 28, 1)\n",
        "\n",
        "    # Make the prediction\n",
        "    prediction = model.predict(tensor, verbose=False)\n",
        "\n",
        "    # In this application, we need to reshape the output:\n",
        "    raw_predictions = prediction[0].tolist()\n",
        "    logits = {\n",
        "        str(k): v for k, v in zip(range(10), raw_predictions)\n",
        "    }\n",
        "    \n",
        "    prediction = np.argmax(raw_predictions).tolist()\n",
        "    \n",
        "    return {\"logits\": logits, \"prediction\": prediction}\n",
        "\n",
        "_, _, x_test, y_test = dataset\n",
        "for i in range(100):\n",
        "    image_arr = x_test[i]\n",
        "    truth = y_test[i].tolist()\n",
        "    # Use the added monitor_attributes argument to add additional data\n",
        "    preds = monitor_predict(image_arr, monitor_attributes={'truth': truth})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3896993",
      "metadata": {},
      "source": [
        "# Step 4: End-to-end example\n",
        "Typically a production application will contain a prediction service that provides predictions to a client. To demonstrate this in a notebook, we will create a `PredictionService` and an `AppUI`: a small interface which lets the user to draw an image, view the prediction, and give feedback on a result (in this case, correctly label a handdrawn digit 0-9). These communicate via `predict` and `record_feedback` methods. \n",
        "\n",
        "Note: this is purely for example purposesâ€”your production systems may widely vary in structure_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efc97d6a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: not yet working with new API\n",
        "\n",
        "# import app_util\n",
        "# from weave.legacy.weave.monitoring import monitor\n",
        "# import PIL\n",
        "# import numpy as np\n",
        "\n",
        "# class PredictionService(app_util.PredictionServiceInterface):\n",
        "#     def __init__(self, model):\n",
        "#         self.model = model\n",
        "#         self.last_prediction = {}\n",
        "    \n",
        "#     @monitor(auto_log = False,  entity_name=WB_ENTITY, project_name=WB_PROJECT)\n",
        "#     def _raw_predict(self, pil_image: PIL.Image) -> dict:\n",
        "#         # Prepare image for model\n",
        "#         tensor = (np.array(pil_image.resize((28, 28))).astype(\"float32\") / 255).reshape(1, 28, 28, 1)\n",
        "\n",
        "#         # Make the prediction\n",
        "#         prediction = self.model.predict(tensor, verbose=False)\n",
        "\n",
        "#         # In this application, we need to reshape the output:\n",
        "#         raw_predictions = prediction[0].tolist()\n",
        "#         logits = {\n",
        "#             str(k): v for k, v in zip(range(10), raw_predictions)\n",
        "#         }\n",
        "\n",
        "#         prediction = np.argmax(raw_predictions).tolist()\n",
        "\n",
        "#         return {\"logits\": logits, \"prediction\": prediction}\n",
        "    \n",
        "#     def _update_last_prediction(self, prediction) -> None:\n",
        "#         if len(self.last_prediction) > 0:\n",
        "#             last_pred = self.last_prediction.pop(list(self.last_prediction.keys())[0])\n",
        "#             last_pred.finalize()\n",
        "#         self.last_prediction[prediction.id] = prediction\n",
        "\n",
        "    \n",
        "#     def predict(self, pil_image: PIL.Image) -> app_util.Prediction:\n",
        "#         record = self._raw_predict(pil_image)\n",
        "        \n",
        "#         # Cache the last prediction for ground_truth recording\n",
        "#         self._update_last_prediction(record)\n",
        "        \n",
        "#         # Return the prediction\n",
        "#         return app_util.Prediction(record.get()['logits'], record.id)\n",
        "    \n",
        "#     def record_feedback(self, prediction_id: str, feedback: int) -> None:\n",
        "#         if prediction_id not in self.last_prediction:\n",
        "#             return\n",
        "\n",
        "#         # Get the past prediction\n",
        "#         prediction = self.last_prediction.pop(prediction_id)\n",
        "        \n",
        "#         # Save the user feedback\n",
        "#         prediction.add_data({'user_feedback': feedback})\n",
        "        \n",
        "#         # Log the results\n",
        "#         prediction.finalize()\n",
        "        \n",
        "# app_util.render_app(PredictionService(model))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
