{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENTITY = 'timssweeney'\n",
        "PROJECT =  'weave'\n",
        "STREAM = 'custom_llm_monitoring_example'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import uuid\n",
        "import time\n",
        "import typing\n",
        "from datetime import datetime \n",
        "\n",
        "from weave.stream_data_interfaces import LLMCompletionDict, _LLMCompletionInputs, _LLMCompletionOutput, _LLMCompletionSummary, _LLMCompletionMessage, _LLMCompletionChoice\n",
        "import tiktoken\n",
        "\n",
        "def count_chat_completion_tokens(\n",
        "    model_name: typing.Optional[str] = None,\n",
        "    prompt_input_messages: typing.List[_LLMCompletionMessage] = [],\n",
        "    completion_choices: typing.List[_LLMCompletionChoice] = []\n",
        ") -> dict:\n",
        "        \n",
        "    summary = {}\n",
        "    if not model_name:\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    else:\n",
        "        encoding = tiktoken.encoding_for_model(model_name)\n",
        "\n",
        "    prompt_tokens = (encoding.encode(m[\"content\"]) for m in prompt_input_messages)\n",
        "    summary[\"prompt_tokens\"] = sum(len(c) for c in prompt_tokens)\n",
        "\n",
        "    completion_tokens = (\n",
        "        encoding.encode(c[\"message\"][\"content\"]) for c in completion_choices\n",
        "    )\n",
        "    summary[\"completion_tokens\"] = sum(len(c) for c in completion_tokens)\n",
        "    summary[\"total_tokens\"] = summary[\"prompt_tokens\"] + summary[\"completion_tokens\"]\n",
        "    return summary\n",
        "\n",
        "def create_llm_record(\n",
        "\n",
        "        # The end-format is that of `_LLMCompletionInputs`, but we can add helper processing to convert from other formats\n",
        "        inputs: typing.Union[_LLMCompletionInputs, typing.List[_LLMCompletionMessage], typing.List[str], str] = None,\n",
        "\n",
        "        # the end-format is that of `_LLMCompletionOutput`, but we can add helper processing to convert from other formats\n",
        "        output: typing.Union[_LLMCompletionOutput, str] = None,\n",
        "\n",
        "        # Required for cost analysis\n",
        "        model_name: str = \"\",\n",
        "\n",
        "        # Optional fields\n",
        "        span_id: typing.Optional[str] = None,\n",
        "        name: typing.Optional[str] = None,\n",
        "        trace_id: typing.Optional[str] = None,\n",
        "        status_code: typing.Optional[str] = None,\n",
        "        start_time_s: typing.Optional[float] = None,\n",
        "        end_time_s: typing.Optional[float] = None,\n",
        "        parent_id: typing.Optional[str] = None,\n",
        "\n",
        "        # Must contain dictionaries, lists, and primitives (ie json serializable)\n",
        "        attributes: typing.Optional[typing.Dict[str, typing.Any]] = None,\n",
        "\n",
        "        summary: typing.Optional[_LLMCompletionSummary] = None,\n",
        "\n",
        "        exception: typing.Optional[str] = None\n",
        "):\n",
        "    span_id = span_id or str(uuid.uuid4())\n",
        "    name = name or \"llm_completion\"\n",
        "    trace_id = trace_id or span_id\n",
        "    status_code = status_code or \"UNSET\"\n",
        "    start_time_s = start_time_s or time.time()\n",
        "    end_time_s = end_time_s or (start_time_s + 1)\n",
        "    latency_s = end_time_s - start_time_s\n",
        "    # parent_id can be None\n",
        "    attributes = attributes or {}\n",
        "    \n",
        "\n",
        "    # Input handling\n",
        "    if isinstance(inputs, dict):\n",
        "        # Assume correct format\n",
        "        inputs = inputs\n",
        "    elif isinstance(inputs, list):\n",
        "        messages = []\n",
        "        for item in inputs:\n",
        "            if isinstance(item, str):\n",
        "                messages.append(_LLMCompletionMessage(content=item))\n",
        "            elif isinstance(item, dict):\n",
        "                # Assume correct format\n",
        "                messages.append(item)\n",
        "            else:\n",
        "                raise ValueError(f\"Invalid type for item in inputs: {type(item)}\")\n",
        "        inputs = _LLMCompletionInputs(messages=messages)\n",
        "    elif isinstance(inputs, str):\n",
        "        inputs = _LLMCompletionInputs(messages=[_LLMCompletionMessage(content=inputs)])\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid type for inputs: {type(inputs)}\")\n",
        "    \n",
        "\n",
        "    # Output handling\n",
        "    if isinstance(output, dict):\n",
        "        # Assume correct format\n",
        "        output = output\n",
        "    elif isinstance(output, str):\n",
        "        output = _LLMCompletionOutput(model=model_name, choices=[_LLMCompletionChoice(message=_LLMCompletionMessage(content=output))])\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid type for output: {type(output)}\")\n",
        "\n",
        "    # Sort of odd, but we need at least one key for now\n",
        "    summary = summary or {}\n",
        "    summary = {\n",
        "        **(summary or {}),\n",
        "        \"latency_s\": latency_s,\n",
        "        **count_chat_completion_tokens(\n",
        "            model_name,\n",
        "            inputs['messages'],\n",
        "            output['choices'])\n",
        "    }\n",
        "    \n",
        "\n",
        "    # exception can be None\n",
        "\n",
        "    assert status_code in [\"SUCCESS\", \"ERROR\", \"UNSET\"]\n",
        "\n",
        "    return LLMCompletionDict(\n",
        "        span_id = span_id,\n",
        "        name = name,\n",
        "        trace_id = trace_id,\n",
        "        status_code = status_code,\n",
        "        start_time_s = start_time_s,\n",
        "        end_time_s = end_time_s,\n",
        "        parent_id = parent_id,\n",
        "        attributes = attributes,\n",
        "        inputs = inputs,\n",
        "        output = output,\n",
        "        summary = summary,\n",
        "        exception = exception,\n",
        "        # Manually set timestamp - else it will be set to the time of the function call\n",
        "        timestamp = datetime.fromtimestamp(start_time_s)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from weave.legacy.weave.monitoring import StreamTable\n",
        "st = StreamTable(f\"{ENTITY}/{PROJECT}/{STREAM}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Base Case\n",
        "record = create_llm_record(\n",
        "    inputs=\"hello\",\n",
        "    output=\"world\"\n",
        ")\n",
        "st.log(record)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "wandb-weave",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
