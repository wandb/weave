{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "from weave.trace.refs import ObjectRef\n",
    "from weave.trace_server.trace_server_interface import (\n",
    "    EvaluateModelReq,\n",
    "    EvaluateModelRes,\n",
    "    ObjCreateReq,\n",
    "    TableCreateReq,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ec1235f0044c0bbe370e4752aecb5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: retry_attempt\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: retry_attempt\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: retry_failed\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Logged in as Weights & Biases user: timssweeney.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: View Weave data at https://app.wandb.test/wandb/eval_test_project/weave\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: ðŸ© https://app.wandb.test/wandb/eval_test_project/r/call/0197eda7-68dc-78ea-b286-82673706ba6c\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WF_TRACE_SERVER_URL\"] = \"http://localhost:6345\"\n",
    "os.environ[\"WANDB_BASE_URL\"] = \"https://api.wandb.test\"\n",
    "\n",
    "\n",
    "weave.init(\"wandb/eval_test_project\")\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def simple_test():\n",
    "    return 1\n",
    "\n",
    "\n",
    "simple_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weave.trace.context.weave_client_context import require_weave_client\n",
    "\n",
    "\n",
    "def create_model() -> str:\n",
    "    \"\"\"Create a test model and return its reference URI.\"\"\"\n",
    "    client = require_weave_client()\n",
    "    project_id = f\"{client.entity}/{client.project}\"\n",
    "    model_object_id = \"test_model_for_eval\"\n",
    "    llm_model_val = {\n",
    "        \"llm_model_id\": \"gpt-4o-mini\",\n",
    "        \"default_params\": {\n",
    "            \"messages_template\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a scorer. You will be given a user input and a model output. You will return a score from 0 to 10. Please return the score in a JSON object with the key 'score'.\",\n",
    "                },\n",
    "            ],\n",
    "            \"response_format\": \"json_object\",\n",
    "        },\n",
    "    }\n",
    "    model_create_res = client.server.obj_create(\n",
    "        ObjCreateReq.model_validate(\n",
    "            {\n",
    "                \"obj\": {\n",
    "                    \"project_id\": project_id,\n",
    "                    \"object_id\": model_object_id,\n",
    "                    \"val\": llm_model_val,\n",
    "                    \"builtin_object_class\": \"LLMStructuredCompletionModel\",\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    return ObjectRef(\n",
    "        entity=client.entity,\n",
    "        project=client.project,\n",
    "        name=model_object_id,\n",
    "        _digest=model_create_res.digest,\n",
    "    ).uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ref_uri = create_model()\n",
    "print(model_ref_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset() -> str:\n",
    "    \"\"\"Create a test dataset and return its reference URI.\"\"\"\n",
    "    client = require_weave_client()\n",
    "    project_id = f\"{client.entity}/{client.project}\"\n",
    "    dataset_table_val = [\n",
    "        {\"user_input\": \"How are you?\", \"expected\": \"I'm doing well, thank you!\"},\n",
    "        # {\"user_input\": \"What's 2+2?\", \"expected\": \"4\"},\n",
    "        # {\n",
    "        #     \"user_input\": \"Tell me a joke\",\n",
    "        #     \"expected\": \"Why did the chicken cross the road?\",\n",
    "        # },\n",
    "    ]\n",
    "    dataset_table_res = client.server.table_create(\n",
    "        TableCreateReq.model_validate(\n",
    "            {\n",
    "                \"table\": {\n",
    "                    \"project_id\": project_id,\n",
    "                    \"rows\": dataset_table_val,\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    dataset_object_id = \"test_eval_dataset\"\n",
    "    dataset_val = {\n",
    "        \"_type\": \"Dataset\",\n",
    "        \"_class_name\": \"Dataset\",\n",
    "        \"_bases\": [\"Dataset\", \"Object\", \"BaseModel\"],\n",
    "        \"rows\": f\"weave:///{project_id}/table/{dataset_table_res.digest}\",\n",
    "    }\n",
    "    dataset_create_res = client.server.obj_create(\n",
    "        ObjCreateReq.model_validate(\n",
    "            {\n",
    "                \"obj\": {\n",
    "                    \"project_id\": project_id,\n",
    "                    \"object_id\": dataset_object_id,\n",
    "                    \"val\": dataset_val,\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    return ObjectRef(\n",
    "        entity=client.entity,\n",
    "        project=client.project,\n",
    "        name=dataset_object_id,\n",
    "        _digest=dataset_create_res.digest,\n",
    "    ).uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weave:///wandb/eval_test_project/object/test_eval_dataset:bvnAZ3fAkmD5S8Cl77B0Z4KCa7Wh7gQVZPWXZDev4EM\n"
     ]
    }
   ],
   "source": [
    "dataset_ref_uri = create_dataset()\n",
    "print(dataset_ref_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scorer() -> str:\n",
    "    \"\"\"Create a test scorer and return its reference URI.\"\"\"\n",
    "    client = require_weave_client()\n",
    "    project_id = f\"{client.entity}/{client.project}\"\n",
    "\n",
    "    # First create the model for the scorer\n",
    "    scorer_model_object_id = \"test_eval_scorer_model\"\n",
    "    scorer_model_val = {\n",
    "        \"llm_model_id\": \"gpt-4o-mini\",\n",
    "        \"default_params\": {\n",
    "            \"messages_template\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an expert judge. Compare the model output to the expected output and return a score from 0 to 10. Please return the score in a JSON object with the key 'score'.\",\n",
    "                },\n",
    "            ],\n",
    "            \"response_format\": \"json_object\",\n",
    "        },\n",
    "    }\n",
    "    scorer_model_create_res = client.server.obj_create(\n",
    "        ObjCreateReq.model_validate(\n",
    "            {\n",
    "                \"obj\": {\n",
    "                    \"project_id\": project_id,\n",
    "                    \"object_id\": scorer_model_object_id,\n",
    "                    \"val\": scorer_model_val,\n",
    "                    \"builtin_object_class\": \"LLMStructuredCompletionModel\",\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    scorer_model_ref = ObjectRef(\n",
    "        entity=client.entity,\n",
    "        project=client.project,\n",
    "        name=scorer_model_object_id,\n",
    "        _digest=scorer_model_create_res.digest,\n",
    "    ).uri()\n",
    "\n",
    "    # Then create the scorer\n",
    "    scorer_object_id = \"test_eval_llm_judge_scorer\"\n",
    "    scorer_val = {\n",
    "        \"_type\": \"LLMAsAJudgeScorer\",\n",
    "        \"_class_name\": \"LLMAsAJudgeScorer\",\n",
    "        \"_bases\": [\"LLMAsAJudgeScorer\", \"Scorer\", \"Object\", \"BaseModel\"],\n",
    "        \"model\": scorer_model_ref,\n",
    "        \"scoring_prompt\": \"User input: {user_input}\\nModel output: {output}\\nExpected output: {expected}\\n\\nScore the similarity (0-10).\",\n",
    "    }\n",
    "    scorer_create_res = client.server.obj_create(\n",
    "        ObjCreateReq.model_validate(\n",
    "            {\n",
    "                \"obj\": {\n",
    "                    \"project_id\": project_id,\n",
    "                    \"object_id\": scorer_object_id,\n",
    "                    \"val\": scorer_val,\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    return ObjectRef(\n",
    "        entity=client.entity,\n",
    "        project=client.project,\n",
    "        name=scorer_object_id,\n",
    "        _digest=scorer_create_res.digest,\n",
    "    ).uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weave:///wandb/eval_test_project/object/test_eval_llm_judge_scorer:1YZq2QWeNqyX3YUmSN6bQC4lcBlC7qmoAsAUDzkW4a8\n"
     ]
    }
   ],
   "source": [
    "scorer_ref_uri = create_scorer()\n",
    "print(scorer_ref_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluation(dataset_ref: str, scorer_ref: str) -> str:\n",
    "    \"\"\"Create a test evaluation and return its reference URI.\"\"\"\n",
    "    client = require_weave_client()\n",
    "    project_id = f\"{client.entity}/{client.project}\"\n",
    "    evaluation_object_id = \"test_evaluation\"\n",
    "    evaluation_val = {\n",
    "        \"_type\": \"Evaluation\",\n",
    "        \"_class_name\": \"Evaluation\",\n",
    "        \"_bases\": [\"Evaluation\", \"Object\", \"BaseModel\"],\n",
    "        \"dataset\": dataset_ref,\n",
    "        \"scorers\": [scorer_ref],\n",
    "        # Note: You might need to add more fields depending on the Evaluation class structure\n",
    "    }\n",
    "    evaluation_create_res = client.server.obj_create(\n",
    "        ObjCreateReq.model_validate(\n",
    "            {\n",
    "                \"obj\": {\n",
    "                    \"project_id\": project_id,\n",
    "                    \"object_id\": evaluation_object_id,\n",
    "                    \"val\": evaluation_val,\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    return ObjectRef(\n",
    "        entity=client.entity,\n",
    "        project=client.project,\n",
    "        name=evaluation_object_id,\n",
    "        _digest=evaluation_create_res.digest,\n",
    "    ).uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weave:///wandb/eval_test_project/object/test_evaluation:JUmWVWMcH84zeu1VUPTTeCVx6Y1lizEG3TZdYyZRSdo\n"
     ]
    }
   ],
   "source": [
    "evaluation_ref_uri = create_evaluation(dataset_ref_uri, scorer_ref_uri)\n",
    "print(evaluation_ref_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_harness(\n",
    "    evaluation_ref: str,\n",
    "    model_ref: str,\n",
    ") -> EvaluateModelRes:\n",
    "    \"\"\"Run an evaluation on a model.\"\"\"\n",
    "    client = require_weave_client()\n",
    "    project_id = f\"{client.entity}/{client.project}\"\n",
    "    eval_res = client.server.evaluate_model(\n",
    "        EvaluateModelReq.model_validate(\n",
    "            {\n",
    "                \"project_id\": project_id,\n",
    "                \"evaluation_ref\": evaluation_ref,\n",
    "                \"model_ref\": model_ref,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    return eval_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call_id='0197edb3-5051-7c37-9310-c59839d61578'\n"
     ]
    }
   ],
   "source": [
    "res = evaluate_model_harness(evaluation_ref_uri, model_ref_uri)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
