{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "from weave.trace.refs import ObjectRef\n",
    "from weave.trace_server.trace_server_interface import (\n",
    "    EvaluateModelReq,\n",
    "    EvaluateModelRes,\n",
    "    ObjCreateReq,\n",
    "    TableCreateReq,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: retry_attempt\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: retry_attempt\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: retry_failed\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: weave version 0.51.55-dev0 has been retired!  Please upgrade.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Logged in as Weights & Biases user: timssweeney.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: View Weave data at https://app.wandb.test/wandb/eval_test_project_2/weave\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: ðŸ© https://app.wandb.test/wandb/eval_test_project_2/r/call/0197f16c-92da-7a20-814f-113e2b49c596\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WF_TRACE_SERVER_URL\"] = \"http://localhost:6345\"\n",
    "os.environ[\"WANDB_BASE_URL\"] = \"https://api.wandb.test\"\n",
    "\n",
    "\n",
    "weave.init(\"wandb/eval_test_project_2\")\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def simple_test():\n",
    "    return 1\n",
    "\n",
    "\n",
    "simple_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weave.trace.context.weave_client_context import require_weave_client\n",
    "\n",
    "\n",
    "def create_model() -> str:\n",
    "    \"\"\"Create a test model and return its reference URI.\"\"\"\n",
    "    client = require_weave_client()\n",
    "    project_id = f\"{client.entity}/{client.project}\"\n",
    "    model_object_id = \"test_model_for_eval\"\n",
    "    llm_model_val = {\n",
    "        \"llm_model_id\": \"gpt-4o-mini\",\n",
    "        \"default_params\": {\n",
    "            \"messages_template\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful assistant\",\n",
    "                },\n",
    "            ],\n",
    "            \"response_format\": \"text\",\n",
    "        },\n",
    "    }\n",
    "    model_create_res = client.server.obj_create(\n",
    "        ObjCreateReq.model_validate(\n",
    "            {\n",
    "                \"obj\": {\n",
    "                    \"project_id\": project_id,\n",
    "                    \"object_id\": model_object_id,\n",
    "                    \"val\": llm_model_val,\n",
    "                    \"builtin_object_class\": \"LLMStructuredCompletionModel\",\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    return ObjectRef(\n",
    "        entity=client.entity,\n",
    "        project=client.project,\n",
    "        name=model_object_id,\n",
    "        _digest=model_create_res.digest,\n",
    "    ).uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weave:///wandb/eval_test_project_2/object/test_model_for_eval:opsX9CNP3gzaPq6jU2aeXb3R35h5S9zEY2BT4X7hGXk\n"
     ]
    }
   ],
   "source": [
    "model_ref_uri = create_model()\n",
    "print(model_ref_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset() -> str:\n",
    "    \"\"\"Create a test dataset and return its reference URI.\"\"\"\n",
    "    client = require_weave_client()\n",
    "    project_id = f\"{client.entity}/{client.project}\"\n",
    "    dataset_table_val = [\n",
    "        {\"user_input\": \"How are you?\", \"expected\": \"I'm doing well, thank you!\"},\n",
    "        {\"user_input\": \"What's 2+2?\", \"expected\": \"4\"},\n",
    "        {\n",
    "            \"user_input\": \"Tell me a joke\",\n",
    "            \"expected\": \"Why did the chicken cross the road?\",\n",
    "        },\n",
    "    ]\n",
    "    dataset_table_res = client.server.table_create(\n",
    "        TableCreateReq.model_validate(\n",
    "            {\n",
    "                \"table\": {\n",
    "                    \"project_id\": project_id,\n",
    "                    \"rows\": dataset_table_val,\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    dataset_object_id = \"test_eval_dataset\"\n",
    "    dataset_val = {\n",
    "        \"_type\": \"Dataset\",\n",
    "        \"_class_name\": \"Dataset\",\n",
    "        \"_bases\": [\"Dataset\", \"Object\", \"BaseModel\"],\n",
    "        \"rows\": f\"weave:///{project_id}/table/{dataset_table_res.digest}\",\n",
    "    }\n",
    "    dataset_create_res = client.server.obj_create(\n",
    "        ObjCreateReq.model_validate(\n",
    "            {\n",
    "                \"obj\": {\n",
    "                    \"project_id\": project_id,\n",
    "                    \"object_id\": dataset_object_id,\n",
    "                    \"val\": dataset_val,\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    return ObjectRef(\n",
    "        entity=client.entity,\n",
    "        project=client.project,\n",
    "        name=dataset_object_id,\n",
    "        _digest=dataset_create_res.digest,\n",
    "    ).uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weave:///wandb/eval_test_project_2/object/test_eval_dataset:X7H8wj2rDp5q93XJbKqXqX0rYugBW5T0MxjwU0i8U3E\n"
     ]
    }
   ],
   "source": [
    "dataset_ref_uri = create_dataset()\n",
    "print(dataset_ref_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scorer() -> str:\n",
    "    \"\"\"Create a test scorer and return its reference URI.\"\"\"\n",
    "    client = require_weave_client()\n",
    "    project_id = f\"{client.entity}/{client.project}\"\n",
    "\n",
    "    # First create the model for the scorer\n",
    "    scorer_model_object_id = \"test_eval_scorer_model\"\n",
    "    scorer_model_val = {\n",
    "        \"llm_model_id\": \"gpt-4o-mini\",\n",
    "        \"default_params\": {\n",
    "            \"messages_template\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an expert judge. Compare the model output to the expected output and return a score from 0 to 1. Please return the score in a JSON object with the key 'score'.\",\n",
    "                },\n",
    "            ],\n",
    "            \"response_format\": \"json_object\",\n",
    "        },\n",
    "    }\n",
    "    scorer_model_create_res = client.server.obj_create(\n",
    "        ObjCreateReq.model_validate(\n",
    "            {\n",
    "                \"obj\": {\n",
    "                    \"project_id\": project_id,\n",
    "                    \"object_id\": scorer_model_object_id,\n",
    "                    \"val\": scorer_model_val,\n",
    "                    \"builtin_object_class\": \"LLMStructuredCompletionModel\",\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    scorer_model_ref = ObjectRef(\n",
    "        entity=client.entity,\n",
    "        project=client.project,\n",
    "        name=scorer_model_object_id,\n",
    "        _digest=scorer_model_create_res.digest,\n",
    "    ).uri()\n",
    "\n",
    "    # Then create the scorer\n",
    "    scorer_object_id = \"test_eval_llm_judge_scorer\"\n",
    "    scorer_val = {\n",
    "        \"_type\": \"LLMAsAJudgeScorer\",\n",
    "        \"_class_name\": \"LLMAsAJudgeScorer\",\n",
    "        \"_bases\": [\"LLMAsAJudgeScorer\", \"Scorer\", \"Object\", \"BaseModel\"],\n",
    "        \"model\": scorer_model_ref,\n",
    "        \"scoring_prompt\": \"User input: {user_input}\\nModel output: {output}\\nExpected output: {expected}\\n\\nScore the similarity (0-1).\",\n",
    "    }\n",
    "    scorer_create_res = client.server.obj_create(\n",
    "        ObjCreateReq.model_validate(\n",
    "            {\n",
    "                \"obj\": {\n",
    "                    \"project_id\": project_id,\n",
    "                    \"object_id\": scorer_object_id,\n",
    "                    \"val\": scorer_val,\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    return ObjectRef(\n",
    "        entity=client.entity,\n",
    "        project=client.project,\n",
    "        name=scorer_object_id,\n",
    "        _digest=scorer_create_res.digest,\n",
    "    ).uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weave:///wandb/eval_test_project_2/object/test_eval_llm_judge_scorer:n1f572k0CDURbHAzYyhS0TYWB3YFUYL2PMOSUeKVXKs\n"
     ]
    }
   ],
   "source": [
    "scorer_ref_uri = create_scorer()\n",
    "print(scorer_ref_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluation(dataset_ref: str, scorer_ref: str) -> str:\n",
    "    \"\"\"Create a test evaluation and return its reference URI.\"\"\"\n",
    "    client = require_weave_client()\n",
    "    project_id = f\"{client.entity}/{client.project}\"\n",
    "    evaluation_object_id = \"test_evaluation\"\n",
    "    evaluation_val = {\n",
    "        \"_type\": \"Evaluation\",\n",
    "        \"_class_name\": \"Evaluation\",\n",
    "        \"_bases\": [\"Evaluation\", \"Object\", \"BaseModel\"],\n",
    "        \"dataset\": dataset_ref,\n",
    "        \"scorers\": [scorer_ref],\n",
    "        # Note: You might need to add more fields depending on the Evaluation class structure\n",
    "    }\n",
    "    evaluation_create_res = client.server.obj_create(\n",
    "        ObjCreateReq.model_validate(\n",
    "            {\n",
    "                \"obj\": {\n",
    "                    \"project_id\": project_id,\n",
    "                    \"object_id\": evaluation_object_id,\n",
    "                    \"val\": evaluation_val,\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    return ObjectRef(\n",
    "        entity=client.entity,\n",
    "        project=client.project,\n",
    "        name=evaluation_object_id,\n",
    "        _digest=evaluation_create_res.digest,\n",
    "    ).uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weave:///wandb/eval_test_project_2/object/test_evaluation:gObJe2APNUgjgX2JTvrHDPOmaVuYUhzCXys6MYCas90\n"
     ]
    }
   ],
   "source": [
    "evaluation_ref_uri = create_evaluation(dataset_ref_uri, scorer_ref_uri)\n",
    "print(evaluation_ref_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_harness(\n",
    "    evaluation_ref: str,\n",
    "    model_ref: str,\n",
    ") -> EvaluateModelRes:\n",
    "    \"\"\"Run an evaluation on a model.\"\"\"\n",
    "    client = require_weave_client()\n",
    "    project_id = f\"{client.entity}/{client.project}\"\n",
    "    eval_res = client.server.evaluate_model(\n",
    "        EvaluateModelReq.model_validate(\n",
    "            {\n",
    "                \"project_id\": project_id,\n",
    "                \"evaluation_ref\": evaluation_ref,\n",
    "                \"model_ref\": model_ref,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    return eval_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call_id='0197f173-3f50-7b58-814e-7b4850829f53'\n"
     ]
    }
   ],
   "source": [
    "res = evaluate_model_harness(evaluation_ref_uri, model_ref_uri)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
