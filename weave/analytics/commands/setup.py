"""Setup command for weave analytics configuration."""

import os
from pathlib import Path

import click

from weave.analytics.header import get_header_for_rich


def get_config_path() -> Path:
    """Get the path to the weave analytics config file."""
    config_dir = Path.home() / ".weave"
    config_dir.mkdir(exist_ok=True)
    return config_dir / "analytics_config"


def load_config() -> dict:
    """Load existing configuration from file."""
    config_path = get_config_path()
    config: dict = {}
    if config_path.exists():
        with open(config_path) as f:
            for line in f:
                line = line.strip()
                if "=" in line and not line.startswith("#"):
                    key, value = line.split("=", 1)
                    config[key.strip()] = value.strip()
    return config


def save_config(config: dict) -> None:
    """Save configuration to file."""
    config_path = get_config_path()
    with open(config_path, "w") as f:
        f.write("# Weave Analytics Configuration\n")
        f.write("# Generated by 'weave analytics setup'\n\n")

        # Group settings
        f.write("# W&B Authentication\n")
        if "WANDB_API_KEY" in config:
            f.write(f"WANDB_API_KEY={config['WANDB_API_KEY']}\n")

        f.write("\n# LLM Configuration\n")
        if "LLM_MODEL" in config:
            f.write(f"LLM_MODEL={config['LLM_MODEL']}\n")
        # Note: W&B inference uses WANDB_API_KEY, not a separate key
        for key in ["GOOGLE_API_KEY", "OPENAI_API_KEY", "ANTHROPIC_API_KEY"]:
            if key in config:
                f.write(f"{key}={config[key]}\n")

        f.write("\n# Sampling Configuration\n")
        if "MAX_SAMPLE_SIZE" in config:
            f.write(f"MAX_SAMPLE_SIZE={config['MAX_SAMPLE_SIZE']}\n")

        f.write("\n# Debug Configuration (for tracing with Weave)\n")
        if "DEBUG_WEAVE_ENTITY" in config:
            f.write(f"DEBUG_WEAVE_ENTITY={config['DEBUG_WEAVE_ENTITY']}\n")
        if "DEBUG_WEAVE_PROJECT" in config:
            f.write(f"DEBUG_WEAVE_PROJECT={config['DEBUG_WEAVE_PROJECT']}\n")


@click.command()
@click.option(
    "--wandb-api-key",
    envvar="WANDB_API_KEY",
    help="W&B API key for authentication",
)
@click.option(
    "--llm-model",
    default="gemini/gemini-2.5-flash",
    help="LiteLLM-compatible model name (e.g., gemini/gemini-2.5-flash, openai/gpt-4o, wandb/meta-llama/Llama-4-Scout-17B-16E-Instruct)",
)
@click.option(
    "--llm-api-key",
    help="API key for the LLM provider (e.g., GOOGLE_API_KEY, OPENAI_API_KEY)",
)
@click.option(
    "--llm-provider",
    type=click.Choice(["google", "openai", "anthropic", "wandb", "auto"]),
    default="auto",
    help="LLM provider (auto-detected from model name if not specified)",
)
@click.option(
    "--debug-entity",
    help="W&B entity for debug tracing (used with --debug flag)",
)
@click.option(
    "--debug-project",
    default="weave-analytics-debug",
    help="W&B project for debug tracing (used with --debug flag)",
)
@click.option(
    "--max-sample-size",
    default=500,
    type=int,
    help="Default maximum number of traces to sample for analysis (default: 500)",
)
def setup(
    wandb_api_key: str | None,
    llm_model: str,
    llm_api_key: str | None,
    llm_provider: str,
    debug_entity: str | None,
    debug_project: str,
    max_sample_size: int,
) -> None:
    """Configure weave analytics with required API keys.

    This command sets up the necessary credentials for running analytics
    commands. You need:

    \b
    1. A W&B API key (for fetching traces from Weave)
    2. An LLM API key (for AI-powered clustering)
    3. (Optional) Max sample size for large trace sets
    4. (Optional) Debug entity/project for tracing with --debug

    Configuration is stored in ~/.weave/analytics_config

    Examples:

    \b
        # Interactive setup
        weave analytics setup

    \b
        # Set specific options
        weave analytics setup --llm-model openai/gpt-4o --llm-api-key sk-...

    \b
        # Configure debug tracing
        weave analytics setup --debug-entity my-team --debug-project analytics-debug
    """
    # Import rich here to avoid slow startup
    from rich.console import Console
    from rich.panel import Panel

    console = Console(stderr=True)

    # Print header
    console.print(get_header_for_rich())
    console.print()

    console.print("[bold cyan]ðŸ”§ Setup Configuration[/bold cyan]\n")

    # Load existing config
    config = load_config()

    # Check WANDB_API_KEY
    current_wandb_key = (
        wandb_api_key or os.environ.get("WANDB_API_KEY") or config.get("WANDB_API_KEY")
    )
    if current_wandb_key:
        console.print(f"[green]âœ“[/green] WANDB_API_KEY: [dim]{'*' * 8}...{current_wandb_key[-4:]}[/dim]")
    else:
        console.print("[yellow]âš [/yellow] WANDB_API_KEY not found")
        wandb_api_key = click.prompt(
            "  Enter your W&B API key (from https://wandb.ai/authorize)",
            hide_input=True,
        )
        config["WANDB_API_KEY"] = wandb_api_key
        console.print("[green]âœ“[/green] WANDB_API_KEY saved")

    console.print()

    # Configure LLM model
    current_model = config.get("LLM_MODEL", llm_model)
    new_model = click.prompt(
        "  LLM model (LiteLLM format)",
        default=current_model,
    )
    config["LLM_MODEL"] = new_model

    # Detect provider from model name
    detected_provider = llm_provider
    if llm_provider == "auto":
        if new_model.startswith("gemini/") or new_model.startswith("google/"):
            detected_provider = "google"
        elif new_model.startswith("openai/") or new_model.startswith("gpt"):
            detected_provider = "openai"
        elif new_model.startswith("anthropic/") or new_model.startswith("claude"):
            detected_provider = "anthropic"
        elif new_model.startswith("wandb/") or new_model.startswith("wandb_ai/"):
            detected_provider = "wandb"
        else:
            detected_provider = click.prompt(
                "  LLM provider",
                type=click.Choice(["google", "openai", "anthropic", "wandb"]),
            )

    # Get the appropriate API key environment variable
    # Note: W&B inference uses the standard WANDB_API_KEY
    provider_env_map = {
        "google": "GOOGLE_API_KEY",
        "openai": "OPENAI_API_KEY",
        "anthropic": "ANTHROPIC_API_KEY",
        "wandb": "WANDB_API_KEY",  # W&B inference uses the same API key
    }
    env_var = provider_env_map.get(detected_provider, "OPENAI_API_KEY")

    # Check for existing LLM API key
    current_llm_key = llm_api_key or os.environ.get(env_var) or config.get(env_var)

    if current_llm_key:
        console.print(f"[green]âœ“[/green] {env_var}: [dim]{'*' * 8}...{current_llm_key[-4:]}[/dim]")
    else:
        console.print(f"[yellow]âš [/yellow] {env_var} not found")
        llm_api_key = click.prompt(
            f"  Enter your {detected_provider.title()} API key",
            hide_input=True,
        )
        config[env_var] = llm_api_key
        console.print(f"[green]âœ“[/green] {env_var} saved")

    console.print()

    # Sampling configuration
    console.print("[bold cyan]Sampling Configuration[/bold cyan]")
    console.print("[dim]Used to limit traces when analyzing large datasets[/dim]\n")

    current_sample_size = config.get("MAX_SAMPLE_SIZE", str(max_sample_size))
    new_sample_size = click.prompt(
        "  Max sample size (traces to analyze)",
        default=current_sample_size,
        type=int,
    )
    config["MAX_SAMPLE_SIZE"] = str(new_sample_size)
    console.print(f"[green]âœ“[/green] MAX_SAMPLE_SIZE: {new_sample_size}")

    console.print()

    # Debug configuration
    console.print("[bold cyan]Debug Configuration (Optional)[/bold cyan]")
    console.print("[dim]Used when running with --debug to trace LLM calls to Weave[/dim]\n")

    current_debug_entity = debug_entity or config.get("DEBUG_WEAVE_ENTITY")
    current_debug_project = config.get("DEBUG_WEAVE_PROJECT", debug_project)

    configure_debug = click.confirm(
        "  Configure debug tracing?",
        default=bool(current_debug_entity),
    )

    if configure_debug:
        new_debug_entity = click.prompt(
            "  Debug W&B entity (team or username)",
            default=current_debug_entity or "",
        )
        if new_debug_entity:
            config["DEBUG_WEAVE_ENTITY"] = new_debug_entity

        new_debug_project = click.prompt(
            "  Debug W&B project",
            default=current_debug_project,
        )
        config["DEBUG_WEAVE_PROJECT"] = new_debug_project
        console.print("[green]âœ“[/green] Debug configuration saved")
    elif current_debug_entity:
        console.print(f"[dim]  Using existing: {current_debug_entity}/{current_debug_project}[/dim]")

    # Save configuration
    save_config(config)

    console.print()
    config_info = f"""[bold]Configuration saved to:[/bold] {get_config_path()}

[bold]Settings:[/bold]
  â€¢ LLM Model: [cyan]{config.get('LLM_MODEL', 'not set')}[/cyan]
  â€¢ LLM Provider: [cyan]{detected_provider}[/cyan]
  â€¢ Max Sample Size: [cyan]{config.get('MAX_SAMPLE_SIZE', '500')}[/cyan]"""

    if config.get("DEBUG_WEAVE_ENTITY"):
        config_info += f"""
  â€¢ Debug Entity: [cyan]{config.get('DEBUG_WEAVE_ENTITY')}[/cyan]
  â€¢ Debug Project: [cyan]{config.get('DEBUG_WEAVE_PROJECT')}[/cyan]"""

    console.print(Panel(config_info, title="[green]âœ“ Setup Complete[/green]", border_style="green"))
    console.print("\n[dim]You can now run:[/dim] weave analytics cluster <trace-url>")
    console.print("[dim]For debug mode:[/dim] weave analytics cluster <trace-url> --debug")
