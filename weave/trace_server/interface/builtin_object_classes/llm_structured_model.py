from typing import Optional, Union, Literal, Any, Annotated
import json
from pydantic import BaseModel, Field, BeforeValidator

from weave.trace_server.interface.builtin_object_classes import base_object_def
from weave import Model, op
from weave.trace_server.trace_server_interface import (
    CompletionsCreateReq,
    CompletionsCreateRequestInputs,
)
from weave.trace.context.weave_client_context import require_weave_client


# TODO: Fast follow up
# JSON_SCHEMA = "json_schema"
ResponseFormat = Literal["json_object", "text"]


def is_response_format(value: Any) -> bool:
    return isinstance(value, str) and value in ["json_object", "text"]


class Message(BaseModel):
    """A message in a conversation with an LLM.

    Attributes:
        role: The role of the message's author. Can be: system, user, assistant, function or tool.
        content: The contents of the message. Required for all messages, but may be null for assistant messages with function calls.
        name: The name of the author of the message. Required if role is "function". Must match the name of the function represented in content.
              Can contain characters (a-z, A-Z, 0-9), and underscores, with a maximum length of 64 characters.
        function_call: The name and arguments of a function that should be called, as generated by the model.
        tool_call_id: Tool call that this message is responding to.
    """

    role: str
    content: Optional[Union[str, list[dict]]] = None
    name: Optional[str] = None
    function_call: Optional[dict] = None
    tool_call_id: Optional[str] = None


class LLMStructuredCompletionModelDefaultParams(BaseModel):
    # Could use Prompt objects for the message template
    # This is a list of Messages, loosely following litellm's message format
    # https://docs.litellm.ai/docs/completion/input#properties-of-messages
    messages_template: list[Message]

    temperature: Optional[float] = None
    top_p: Optional[float] = None
    max_tokens: Optional[int] = None
    presence_penalty: Optional[float] = None
    frequency_penalty: Optional[float] = None
    stop: Optional[list[str]] = None
    n_times: Optional[int] = None
    functions: Optional[list[dict]] = None

    # Either json, text, or json_schema
    response_format: Optional[ResponseFormat] = None

    # TODO: Currently not used. Fast follow up with json_schema
    # if default_params.response_format is set to JSON_SCHEMA, this will be used
    # response_format_schema: dict | None = None


def cast_to_message_list(obj: Any) -> list[Message]:
    if isinstance(obj, Message):
        return [obj]
    elif isinstance(obj, dict):
        return [Message.model_validate(obj)]
    elif isinstance(obj, list):
        return [cast_to_message(item) for item in obj]
    raise TypeError("Unable to cast to Message")


def cast_to_message(obj: Any) -> Message:
    if isinstance(obj, Message):
        return obj
    elif isinstance(obj, dict):
        return Message.model_validate(obj)
    elif isinstance(obj, str):
        return Message(content=obj, role="user")
    raise TypeError("Unable to cast to Message")


def cast_to_llm_structured_model_params(
    obj: Any,
) -> LLMStructuredCompletionModelDefaultParams:
    if isinstance(obj, LLMStructuredCompletionModelDefaultParams):
        return obj
    elif isinstance(obj, dict):
        return LLMStructuredCompletionModelDefaultParams.model_validate(obj)
    raise TypeError("Unable to cast to LLMStructuredCompletionModelDefaultParams")


MessageListLike = Annotated[list[Message], BeforeValidator(cast_to_message_list)]
MessageLike = Annotated[Message, BeforeValidator(cast_to_message)]
LLMStructuredModelParamsLike = Annotated[
    LLMStructuredCompletionModelDefaultParams,
    BeforeValidator(cast_to_llm_structured_model_params),
]


class LLMStructuredCompletionModel(Model):
    # <provider>/<model> or ref to a provider model
    llm_model_id: Union[str, base_object_def.RefStr]

    default_params: LLMStructuredCompletionModelDefaultParams = Field(
        default_factory=LLMStructuredCompletionModelDefaultParams
    )

    @op
    def predict(
        self,
        user_input: MessageListLike,
        config: Optional[LLMStructuredModelParamsLike] = None,
        return_type: Literal["string", "message", "json"] = "message",
    ) -> Union[Message, str, dict[str, Any]]:
        """
        Generates a prediction by preparing messages (template + user_input)
        and calling the LLM completions endpoint with overridden config, using the provided client.
        """
        current_client = require_weave_client()

        # 1. Prepare messages
        template_msgs = None
        if self.llm.default_params and self.llm.default_params.messages_template:
            template_msgs = self.llm.default_params.messages_template

        prepared_messages_dicts = _prepare_llm_messages(template_msgs, user_input)

        # 2. Prepare completion parameters, starting with defaults from LLMStructuredCompletionModel
        completion_params: dict[str, Any] = {}
        default_p_model = self.llm.default_params
        if default_p_model:
            completion_params = parse_params_to_litellm_params(default_p_model)

        # 3. Override parameters with the provided config dictionary
        if config:
            completion_params = {
                **completion_params,
                **parse_params_to_litellm_params(config),
            }

        # 4. Create the completion inputs
        model_id_str = str(self.llm.llm_model_id)
        completion_inputs = CompletionsCreateRequestInputs(
            model=model_id_str, messages=prepared_messages_dicts, **completion_params
        )
        req = CompletionsCreateReq(
            project_id=f"{current_client.entity}/{current_client.project}",
            inputs=completion_inputs,
        )

        # 5. Call the LLM API
        try:
            api_response = current_client.server.completions_create(req=req)
        except Exception as e:
            raise RuntimeError("Failed to call LLM completions endpoint.") from e

        # 6. Extract the message from the API response
        try:
            # The 'response' attribute of CompletionsCreateRes is a dict
            response_payload = api_response.response
            if response_payload.get("error"):
                # Or handle more gracefully depending on desired behavior
                raise RuntimeError(
                    f"LLM API returned an error: {response_payload['error']}"
                )

            # Assuming OpenAI-like structure: a list of choices, first choice has the message
            output_message_dict = response_payload["choices"][0]["message"]

            if self.return_type == "string":
                return output_message_dict["content"]
            elif self.return_type == "message":
                return Message.model_validate(output_message_dict)
            elif self.return_type == "json":
                return json.loads(output_message_dict["content"])
            else:
                raise ValueError(f"Invalid return_type: {self.return_type}")
        except (
            KeyError,
            IndexError,
            TypeError,
            AttributeError,
            json.JSONDecodeError,
        ) as e:
            raise RuntimeError(
                f"Failed to extract message from LLM response payload. Response: {api_response.response}"
            ) from e


def _prepare_llm_messages(
    template_messages: Optional[list[Message]],
    user_input: list[Message],
) -> list[dict[str, Any]]:
    """
    Prepares a list of message dictionaries for the LLM API from a message template and user input.
    Helper function for PlaygroundModel.predict.
    Returns a list of message dictionaries.
    """
    final_messages_dicts: list[dict[str, Any]] = []

    # 1. Initialize messages from template
    if template_messages:
        for msg_template in template_messages:
            msg_dict = msg_template.model_dump(exclude_none=True)
            final_messages_dicts.append(msg_dict)

    # 2. Append user_input messages
    for u_msg in user_input:
        final_messages_dicts.append(u_msg.model_dump(exclude_none=True))

    return final_messages_dicts


def parse_params_to_litellm_params(
    params_source: LLMStructuredCompletionModelDefaultParams,
) -> dict[str, Any]:
    final_params: dict[str, Any] = {}
    source_dict_to_iterate: dict[str, Any] = params_source.model_dump(exclude_none=True)

    for key, value in source_dict_to_iterate.items():
        if key == "response_format":
            litellm_response_format_value = None
            if isinstance(value, str) and is_response_format(value):
                litellm_response_format_value = {"type": value}
            elif (
                isinstance(value, dict)
                and "type" in value
                and is_response_format(value["type"])
            ):  # Pre-formed dict with valid type
                litellm_response_format_value = value

            if litellm_response_format_value is not None:
                final_params["response_format"] = litellm_response_format_value
        elif key == "n_times":
            final_params["n"] = value
        elif key == "messages_template":
            pass
        elif key == "functions" or key == "stop":
            if isinstance(value, list) and len(value) > 0:
                final_params[key] = value
        else:
            final_params[key] = value

    return final_params
