interactions:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate, zstd
      connection:
      - keep-alive
      host:
      - raw.githubusercontent.com
      user-agent:
      - python-httpx/0.28.1
    method: GET
    uri: https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA+19W4/jOLLm+/wKoWaBAQ6Kti6+ZQGD6e65nd7t3jO7OHMW85SQZWZak7LkluSs
        rD6Y/76k5IssSyRDoiTKZj1UZ2fRsvh9EcFgMBjx378xyJ9PibvbB/g52WPv0xfjv7NfZv+wcz+e
        0+gNhwn5/aef/vzX7//4D2Pvxu4OpzieGAlOjTQy6LDokO4P6XG04b8Y+zh69zc4Nuhj/Rcfk9+m
        E+PHvxhhlBY/6YeXD36u/+Snz8b1ixU/R1+P/M7IfmcUnpVucd2bkH+lb7LBL+4hOL/N8Xmfr7/r
        ananL8t/KeXbClPLp+VFSfq8x3E+gnyjOTHJn8JrHV+pemBhXOCnOAh2z6f3om8fhdiIXoxtmu6T
        L9PpJvKSyXHcxPWz/5+exl9jEW1w4QHe1k0/G3i3xpuNH75+NryISlLqRyHBY+e+4udXHOLYzX/j
        HjZ+9JzGbph4sb8v/pKAhb1t8auSw34fxWny/HIIPTr02XODgHwL+f40PuCqoVQ0gwAHkM+8+wkZ
        yBiQv2BGy2mUUTssZ4X1ijGBiNDmelv2e8U42UdhQtTS2+KdyxiZfEtSvHve4SQhiCe3Izd4H2Mv
        I+F546YZg/S/xtctDjOhpbwGxhoT/oi8nsbjDVGo7N9fonjnpsY/yB/088/oT3/6lD39X/mXfIp2
        oY/oM3KmUUA+m6QMW+LYy8XqM1Ofq4aU1dAUUxwxreHpzB6Hrl+lDJeJi6CC/DDFYZpodK7RsU17
        hswnZC8eHZnXfYpmDBBm5tOCsxiurCebg0H5Kcx1xxFdeMxFYzjoYtJ6ARC1rlybeR6ZRlHw7G0j
        38PHUbdkRQy2rIWzmrFF1rJX12t2FV/l5zD5Mu25MGOWyXnq89pNyRqUXJ5u8R9/+6H51RQpRZgs
        cu6miEb2kLrvGUCcGvgT4us21/MYSppR5ssg4gS8+/ir2rJdGnp0MYsfsJppQnloxZPt0QlnhS/b
        3JXtVOpyf8CykbUcuwCas84k0FxpEexaBE1kWmMXQW0DRyKAO5/sikZjCE0LaAnF5fB6JyFiCrUk
        SpdExQWOK0HwnY1pLhtsbUwHvLUpfdE4BPYe9jaZhc2DTUtkrbSIaxG/GxGPmI6qad7Kakme7aox
        N/JcHsQUaIA4L6ASNkL5GkJqxGWaK188t2AxnzsLCTaz/By2zbQAfuVsBrZj8w7ErJkcXLPhcJ20
        YVR+jHxIUvsXN0jYel87ooOTYZ46O2dvaI5MCzkKLh9alkYiS1bRs35Clq3cKiF+iGnZUDmyFF0i
        LIFTHIFDdSE2ys+R5haCdx5d+IVS2RBTE01Mf8SIRVnV30IpQYneQ9UvlBS0YyiKmyaX5ei01//S
        Y9irJCBxREuOvLBkviDMkeXchzyIxSKb5BFpgyVd7FbIZGVdKnDErUoKG3hfoDPYhpNri7p1Wq61
        XN+PXBNagtTfYViO0tB+AyxBSUgAr+Qv/4RHPpKJQv6pwrdcf9YW1j/WSJ0L1besj1rATXBoGrBO
        AOVUZ432YpQ5wSQts1pmB040VdzCmoyhvOxSsdM/5+YTok4E5DiJWAOgsGtPonthV9dKa7nXci9B
        7lF6iNci1/gGkHHxpA5ndCIxUHAAmY7FvTbOZFn01rggx4NeGh+MhEXLE5x7ImEQpamqdpJnuC3o
        MUdjZh37rRWxIoUpIMyy7kddx9Y6WFWHUi9CQns7p5mQxURbY6eZkOXmaffu0Q/zj+5+fqJPkwu1
        SDy8SFgWcXr0BrBncRhs72ERL1ez/SBs50ZH832DXo/WuGavmYWV2+w1M7utCb5Xgp3JXMRrX3JI
        XTiruUROQZUHBo7UD2Jzz7Qh0+HlW3G4qxgxZuoG54MbiHgsPkakSnSpY1BXZeQ6t4PCddYJec3r
        /z6Ww1pQVrJNUYxxwC2XMV57Gppxa8E6OhmCcEASw0w+3QrwwVsyNSndk/KSfgHsBnhbvAHZ4D20
        4ppKg1sqHewkO6OSt8iNh0/lkeY4kBppeTLd9ri3YoQG+gy0Th66Ak3Av40OcYbop/8gj//+RyPE
        eJPQ3lruZmPsY98jH6O9i4x06yfGS5r3OfpsfPUD2u3IOOw3WZejrA8S+Qz5cf3NyB82Mf6U9+qi
        zyCPXLvJqU/S8cn9CIXSd7QdfinR8+AG5QGsVZNSpcvrTiPj2KJJvJzalQiqeZ36WgQrk9lzOSld
        YBYQrWrRfTh5GmxjNIZqzQAX5CrnTLBWszXnxiIrPrWAt1rSMt6/jG/cdz/0fGSarCpwIoJdMaSV
        Yy1+Rd6ECzX37Jj9kSrZTPFHii7NRxFDVs+DPpXpWLvrNeFUQToAHYSuxwpaGS7dnM90Q0n2hHNz
        WeSgwI1fMYMasvux2MyURhzn+o69NIqfE/9X+nTHXPJkumg2AQtAk1r9iya1+s0WDvAZcA4ZyY7Y
        587JsGiBV9EulTDD1YQN/sngIGy4G5djtQZhA9Ltpx+E0LtUkLqZv6gwNnINu4K60Mk3Sd11wLLT
        0rr4CjPRhyBeEEjY+Jgm68L5g4PTX+vwceBjzxfoA5G/pxuy3iKMygbs+AB/R1zX51ccnlpK189v
        73/goOjJzWYWy8EsjBeb39UE5pZNJkD+7m4C5mLFuvjV7v0tGnz4QPQ/Hc7AeupsAtvN9DSH5dOZ
        hfI5gwwW5k63syCvX2JijLO4lacOZrFc2N3Ngizw4caN+5KrmTPvYy69SFdPc+lFxpyVNVt0N5uv
        Wz8hT0HlROvjmx/Lx8RumHixv+e8fIK9KNyc3p7lmJdHGpA3N45OTZqwX5tMDHvbT4y9lLd1Y9dL
        sy+BHUJcu1f0TdB2I/tlnCaMur8eYjzVXYcEug5lUDW7FNO+KRAncp3TOIyM57jUvc4Qgl77Rp3a
        L6NeOoEGrPb9dXs5NmQDKae0RiRHlhVt6Thosy44wX0VGIDJTz3fuk/bI1Kvni0ftrWVumS3zlMQ
        Nf6675kWjlrh0L0jRyUXMpwD3aDyYdnPUy4ZdA/dsQCQpCupiZG6bPezBqh/A8GGpHGDC6WPUEKG
        SIoFy5KaVwkay9KjCQbcdtxPY9NRcN23agfR2g3Q+fxxBMtGwyaMetVQSLQUXUUaitajyUkb+nkH
        U8PcBYMU/brem2ru6x0IRbnmX8wokl10S8ROIUcYilB3ERnJZVItVGMTqocpuq+cXEhcYx6icLpy
        BAL4eYQ2BmPm537rFnXKCgDhO++2BIcZhp2GDQyb1ucu9fmue3YdQTa6QbnjtSx36B+j90YDouS5
        3Q6gGrHaRTx5xYjVMDqO1IKpUuvbAPL55S+J3Zie+i60Dr3xJIfJu257oT6TNj3UlsSkag1lmpYA
        00zKKPwtt3qYzOYW6nMpwb7KreKuyVRisWQ3IXk0DxOMXyvspK9Oo9ECuNHRQPcENPLDhIzzUmQ+
        WZx4F8z76tlJfqZFtCCVNSus4wkLDcMji8TOJ9N3g7zSqkgtttvI1k3w3ATc7F8Jw2D3FnThwmTP
        mKU3HxUkImk7N9ygGO2DQ8Jb1QwmQPKzvwGXzHtb1ipAdDcuAzlFCpaCQKkpInqs4tJ5XduRIdBB
        0WseAg0LWncIgfRS0xwImlaRbgcB/bsIA6CaW+tKbYVJic2n4sjoppRcBbVC9Q9b1jbsay6AKogt
        qwL2PyNuLcSRzEio9mnLiqB9zqU1L4rMhWXbbNEZDT+PZ9efbjDeJxi/oZh1gFORENPExS09pn4d
        4wx6ztPFt34qG8rnrrruisR3KB//dHdrVzCcwQmmLU0+I51F08xlF3CLoni1w23lelVtfyvEWtw9
        E++FI5Y/B4VQckyyiHZ7R1c62oANkVC2UQltAKrdyDaN3rBKXw1xMx2w/RDKd+wA83Pn1W2a7pMv
        02n2RTs3fsPpPnA9PNn5Xhwl0Us68aLdFIfTwj9O3f0+mZLXJ7t7c3Liw/ULlKDo5QXHf0jd9e//
        4x3HWRZeO9LD49c4a8XohrX3EjsLVJ7yCxuEbstsTTf9u0T5T4G7c5EzsZFl/YD+K8/o/JHvGNjm
        TEZRq9JjODLgLAEy4DTxDZosYHXZnp0KyQ6nxPMOXX9Cf0JBTiOiNK5PibnncxrJZuIkMw5amj8o
        KyxLSJhyKVSpRlGDcZGFkxhQZtad82+jJ1N1m2FD1g2763VDUZvxZHZuM36m33gUHHlmQ+AKDUha
        IEXTmy0w0hz0IqATC63UNcSQs5JFEzvMUxpxnUGHRFhtCOaVJvZvgRsm34ebv8WUQokUK7zY2gvx
        83bTmTexskORXLOQdsbyzJwrS/PcAWQENNl1D0UyAb1jlv+29bOkuqy8ipTAawd7bNCx9ryTwGF9
        pd5qD6iS+D2hi/hf0w1h2Q/I/1Pwp1L4K7lJmsF6Bsu+rxoE/hz9WV32hG+9U1dJ6io6Cv3Lrefs
        TQ6BFSPuR/nUpY/ojSQC79x+KklhdvqGZDEouQQGtLrko9KntgpqCtlGFG/8w07hVRBySHMVN3gs
        AtVWQk1iHYletMUxRjGO3fANvRM+D0Hq03j/wWXlpXShinTILwccf2sQ0KkflD3xyK9gmkULAciB
        rEkcvAEbh6+Bn2w1znJx/mOOc5ZvLwbz3OLkal4POM7pHXtpFD8n/q/0nazry1dyrp20gOhy3UB+
        7DGX5IlXDXRtzFGMLkETpDnrn7O1u167r5hzRUuklrb0KhmQjCpesCm7j3S5N4uOre2rFvyay7Ub
        990PPV9BoCRW+WkDE/07h6r6hr5SO3GZl7Xby1ZXJQ3EUBvminsz1K5g8wL3sMEZXm6YIs7dDU7W
        dWV/3HZp1wtAtG4+56YWhOk2jva+V7tNuQLnmJF7zpRO/fBbK4Ckp6VDuuyUBlfhc5woaBPnJomf
        CQ95En7xs8T9hhcBynjf9z2ACrBlXgPomhd+WQxNzzD05NEntYixAdGmFc+MK2amjtE+FfXhAWC3
        HWZvbg26VNAFKyJVxqubxLRvBvV2ZawJ8ID08M45smcWy5vXDCnAUMuaPirfKb4Dclj3YYdSH8B2
        +OnuGNr7H3oRGoqjth0vqknUq9SIKbRsemuZ2exxKAJhOUbc2oZ3SCINFqPTmrdkFQPQcU1J69cR
        8lx5Vh/KgQ7JCOLdjFRIY5qxY9vt6FnMHYeXgwILK9zlmiRIjkde7+Seqxdm6zbsPISpuuBNFnhW
        7w+NtjzTkwMe4h2r+UHvHlWjW94ygb9JBj1+eOL60xR72zAKotdv1SmffZClXKxAM1bB2MWi7Whp
        RAZf9nzB5atyzM3N6/IgZZ1exQgrccVf8zVlQ1N2MohZCiXbX+BkqAoXd71xBoCQ1lRfP1WyLZS0
        xW4Shdnzmk9rMZ+L7AcAU5/POWPL5W1zwCDVkixeEPmEkZSthCFdVKtGVZba5UhANhW12Ld5OYeV
        7F/t2/MBRLo3xdfNHsP+iEc+kxFY/TFxAbN46nuX8nVe3hTf1gKI5OX3nyfabG3bRF4yKSxwnrt3
        1z75Ih8nGYiFAuWdL3lV9Km3S9bkVZJXzpl+QF2shYCTOS6RZDAld61fqhHyQYa/xtEbWuOUtVG2
        HMtc8vo5VI25CWyUB3GKlAlDzz3Y++iu6n3bg7ozC/apJJVlM1MdHXu54BSMqxpSJqM8RvxUojQy
        6xxSPZLD2j2RJtA9UtOmHG2ar3HwdVQyzkIFDjq13p4zQjMlvh5uMeOsYkM5FaPQG2GUNcAdAyyQ
        fqpxboxzRQQ22jAD8MPUUOLVyaiOwUJC8KbNK710lyFSomS/FHsKog15BZ/eOs0qOi9NVnLWUJpn
        LiGngU+8sxWKAYjU5FuS4t3zDicJcTHoBG4KazH4z8dWCUCMk30UJvg5IcK8c1nP5eh1RmtQaC6z
        Ru84TlxCLW7lv8ltG5k7ZoIdQnPmpZMJMKa3/DQ0uxX0JHvsbbCnlHMNo0a+ngmjaGcQ0jWl1col
        vYDjEhK8XPHClyMTbYd29aDyppZMg5qimjynYGSc5O3p1og4GFkfIrWYkdqXcIzMOKoyA6mmz82K
        HyMztBdgdnykJD8WoGfQ9eB74ucYo9QM9cRQ21hliULaOk9dFQN5wqOgD86Oygp2f/zIU698h6me
        K/7oO/9z30M3bJf1qjdJ0pmRFzLTaiOdnKzJYH5bIH8Bzc7g7BTv26Ms6YLByyC5Hdfl1rnnQfcT
        YHjFO3qWQ9Ya1ZYZSCUEbimEkRFCfGr1GLEhEWv77iLW9Oezs4zoB9EhwWrueFaQhWZ1PwtNkarV
        KJiyIExxrwKOgikPx3gdu8mRrWyrwyBluPJVEGY4xJzmPFAuzy3knOyNoTAHteZVG/OX2MfhJvBd
        f1rsg37Z2XO6iQxhjCRK/GX6XeG/d8meIsAB5DO36Tmdm7t6OaDOhJqCIFMNtSCcjHCpz8ukXeVh
        +Z1eYK1epPR6aQ6jauCJH+JxowfAFjknQCbtOgfZ44akuSQ5aOv6bwdkk9cwHWYZKX6emBCKsEwx
        SH0b62qwUKWI7I9z8yluSQpesapmlEm4D5OxfqAGO7fxx0znC0WLWdXjICnWlcnTjRaPDSbf5uX8
        bNw0A4cI4hyZDjKtVlI9L8i1ZdrtDvhErYO4kwFwNhvJdJUq8IS6mwWNL9P3IbKW2VZko/0hye2w
        bbNKWg9ghwHFrJdN5NVaVXyMI6/c24jK2mDnaT4CgW5vg5MoDHGqqEgDalOOV9Ksq0NXZSVtiey2
        q31R1ha2qdhy31DWhJd7B24+x+vCjkSoF+1d2IJQK+jDaqFWTKg3Lzk0I3WEsxzntZ8wi5/Y5oyT
        HyMQDy49pCTmRAxi10szgREKQ1R9gOcyvOOYzPfZ9fPUbiqCQfVhjWC5LS+IDpvJaxS9BnhCPjTN
        vwHRggHHOlvvmP4frcs1DbAbh9P8W3/7Eh3CTc7k8T1qePmOqDKDG+s6lNWMm9JDNDfi3DCLbmhu
        huPGsd+GpebmrANUNvx6tGZdnPXhlVIz3y/zh9D3opjlwfROunismVvqZaRsDO+3aEby7RXXv+dH
        AgW4ABZRGq1JzBBlUM6tjC2V7CaxjrNIcFRUi8XDigXLgdJicQ9iUR9DnSHzqZ0AIfbeix83Fazb
        Dkk01RI0vGFB/N2Zlo3Hkg3ySa57yt8qLKxZecSj7tgzRHnbkhHJRettpJYNmGwMv2Utct8ukqe5
        Hy33bcP3mvrRUi/gJmr+75P/V+y9RZwlf8FhviJbocz8oife744Xlk5qXgbiRXOiHif8TiGamt6p
        KYS4+Y1ctIcBFIG7CEYJnZdq2Xhw2dBBKS0fHPnQu1ctH7XyoQ9ItYBwBUSflGohKZccxDs/9BGZ
        YkvBWC7aCUaxnS2LZcpElP2UYC8KN6ePiIvcvNicTkCGqNAZwjLKvGNf9fjr+3FsmQvc8PVAcIKI
        HOD2W3Pp3Me+R5/ZXAKtiXmSQkOLoRbDNmL423xoe7N4FEp03JxpwdSCOdJUyoJAH4I07rzLPftu
        rhZmhYT5+8SIXoz/eQg/G7REwcT4zy2OseEnRhgZ7rvrB+46wAYRYSMKjfz9DNc3jsbWKIsWtZYT
        4+9J6d/IJE8fmRh/jVLjInSGH75EBv3WL4ZijvHVrLTSaKXRSgNxm2ztNmmluQe3ad5+c2qbT0tr
        3q7IdVGcTce+rrdXGuseNn6FSJPPMcNoNZrA+7ayvEIjdeyXyub97K6jd/xs2au3C17Z5xfzJUv9
        ylNq/KAypPUPMhc260HZYManTeanz/ixnpC9gbB9AZoXgbAt491YF6eF5gbIJWlj27gtKkVqNYlX
        aW/Vt6EoLrzyT0ZvxplhTFHbwwhtT7U91fb0ruzpA9nKkmcp5MU+IXvGMakt49R5Twl5Wy5tU7VN
        1TZV29Sh/E8hszrnmdVj10Uy0ppp+yoeEjOXK0vMSJSNbP1HHs7Kmtac/QRxSysebizLR+fmlrcU
        qGRyH9qiCtlJ25prO6ntpLaT2k5qO8mwkzOT1U5Hm0ltJrWZrH/+nZjJdsZvTLbvJXCTbf8GL3sO
        1fMk4z5vnkKGObdPyhS6NMyqHPQc4PA1pbMp96TNdTn/5+ctQTp7pUn59t1F5S/fVBpCjxcT/1f8
        vFtnrytoxFlZK3WmmPWZWqsPSY8xWZa+ynDn3QRbWDqrtZFrs3KYjKuU4ssG6ymANePqMVxbDTPU
        JWYbWOq2dprf2U9hf5Ybv+X3blf1UtnZ5CP8sUfmyma1pda2/45tvzlbrFZAx13b/4Hsf8YW06bX
        09VmHTDNJ0fsa9mP6SvzVS8HTZeDtplfljlbzZfllgF6JRjBSqB3ASNZBfQu4IHN/k3MRiiPIktP
        k7M+tLtqp3cKx3/W64NeH/T6oNeHXrcFnEQ7GQtEOSFPrxR6pdArhaIrhY4n5Qqo1sKhapSI5sfg
        D0KPv8Nh6gb9m/c6wyMi8oImSVQXepG7Cml6cYOkSfZVS6HaHYLUJ+/rBtOjNFxJQpW4nM+XtMBo
        gRERGGpfzkaxTlAqOiWUBWXhrHj1lKueUuE2WuVAdqXTyPYZm9YpgVQdYbiIgEypHPuO86RGt+qd
        KuZoydSSqbJkckKzWjq1dKpWwmmHN8EOkb/9w66Vb0iLkfH6J7E9Q17BcZG06E49POWMUE5e4MaZ
        yjTvfiFQfJHd/aKCChhzHUeflWPuuHzYx+UjywC0kTlvpYKNSv48UrgWFqiFhWhhO17mbrZRFLZ5
        +LVZ1LVtvLVtpBUYPhCNCwACqc1jp3cYMD0NyMnLGGU+hooIb5RI/Tj+yZ7Y7aD2zR3q7Ps5Btcy
        9KaT+rJ/1rZd23Zt2xXNorheBXLkOjG2jV3uadkya6vcgVXOH1aOES3Fw0ms+mLlsTOOyuWMi+hX
        vN99Op5yFX6bXn57/Q/3oo4NPSTXP2nsBr+fmwvZz2auV2x/qH0Sq9a87J9FNM8CKB6k4eCCo3l6
        sVNm/5Fu/fCNDNMbkfIwvRHRGxG9EXlM29zBraDLxqLxHqXGbCPTQjbLa1rM507ZMDey3pUP0uZb
        m+9a5LT5fjzzXZvV2MB+1z6rZMBvxqlpwW+iTHq/Wx6mI00PEGmCJD7LDTXdaqDe+JaGac9Je04P
        6DmJG+mxWt3SsHxVmd0sKqc1aIzuFCW2UFC/beactu3HfxbyrpiXi9mDIXUTZHhYC+1hST5u4Ot5
        ppxiPpk+ldDOmXbOtHOmnbMxOmfnGCR5ETJLjByURGGIU4Ypn5lPnCMKW+SOeekxbB+EVYzpeijg
        Goobpts42vtexS2QXi6dneUrSfwkJa9DXhS/+EHAGNuU0O9s056Zts3qx6KZHRezc76yilw5EqBU
        sCCEplQWpbm6LmxTc3t33KL3dtWhNauKspoprUUWWU3v3dC7df23A89nMtoTCnKaTH7vsMtgC1CE
        4ZFIzZdYx2Q1bdHsjpDdOVdphzDCgCwF9RjthTBVl0/NXC1z0f6QqBVQANx+AZQ9fRDLSelUNFCk
        eYXzusOpOw0Cd+c6aGbO18gPEzLcS9HOdVl669i35N0Ui+LzWx5TT7AYt8K0ZnOGUNr67tZv926c
        kt9XpPC2421patpGSNtKszY61pAzsdET0bdjKUJRAi17xWWwcgynLqS6DMIPbgGFF6Mwiagc1AvF
        /rAO/GSL4yRnL3tz9OrGGxyKUNlOXsg6HbtBXrPvu8BNccI5gCnnfjQRjdJj2BsWW0w+6B/x+9/H
        eXfsKDWkgewbLf4FDU1E10QgSoQmYXBtYAY1NQndkhDiXcRfGKR5DTeDZNYHARwIKcbGP93d2s1a
        atE0qe84db3nCy4ZlWNuXLjyIF6oH8AF7+5aIazh21YTHhqAm1scNdEFgLtSEFsFER2/vGbJqRrY
        jgyBgsiO0whc+RIcf24oT0I8rYR7KUhZL8IjX5q9kMIOHcR0jNa5vhBBtIG1NGoaeqEB2XOmz6dp
        6JKG7GrTKTYehcT/XlxzUZ55oSmPLR4Yzj7FqEqT37C6vIfcMP8n1uRD5ExMdPwcvo39MQAQd5+U
        n/+Lm6TNQLBHCQIZmSK8W+PNht45Ja4wy+PkNusqjTjC9Y69NIqz65lkzHVPJNbVx9N1aRtwt1oo
        t0iYqgsy9XSdx3TVXofDGHNbqxlThLGs8ShdrQ5kqS/Sx0rO0/QpQt/ltV6x9xYxKHPMJa8r3PUI
        TVlHLch4JF6ppGb03hjlhNA1q2NhVRN5P0Q6msgxEll403OBsZnJSmjvgcwyN6a54FL5vHZTb/sc
        418OOEmTq08LneIrzW39Dr9m93FNpsJcjpMPsK7t3WA3pXFPtPaTLLuz+YURgQbGsEuUFuQWpcA1
        SjpZBToVswjgZEBqErojITNbPC3ooY33YAREu32AJYZ/29LA0QVNRY9UJO4LTr+h6OVFM6IUIzH2
        /DT7lCZnOHKuS6vSXKpTw4l2N+EfvKJqoWyH53pbTLZR7qaIUSZNRS/eWl1dHc4/5ZGPZdTVfxLQ
        nJFfeLm66GaT3G1ufhbzq3i5EXdY5zOXrWfKe+V3sit9lu5fCrbAKAK1wWSH6+XSRuxF9mHbtOfI
        fEL2rH3x9aJp0b1stGnRpkWbFnMu17RosyLFrGhdH4muG/3pcNtFX+SuulbQwhCtoHegoAouxm0V
        ebXWStypEjdSXRGNbaapWkNhGjpTQEPzdt9PNitBXauqVlWtqoOoKtVOy7K0dmrt7Ek72d38BtFh
        XueqtqpJi7Nt3NS9UrLsX8IozcD69H/JntQI/J2fJgb5pbGJvMMOhyneGC9RbJS0dWJ8nyQH8qtX
        I3F32HATo7D0Eionha/nwX8e+C+enbDLtyuBdsI2n5bWnHc8qu1E7ZBHshOtrgtrO5Fpa/924uL6
        Z37/yuYUYNOeRWGIthjj8fvVj43J0sDyDLUGag1UXwOH2lATrFvpm2MvF0Bt4xzBOPNmvaiPH18W
        P81rwmTOuaNZX2ZbxY9LEl++UMb7zdE4mTfyZdmV4uUsINJ1lA13f8yKzLMfp7eTaW7128pdo70Z
        py5aK8EDyF1LsetE6oYxmhU7IaNiWNsz/IZbHb4It8711WKsxbhqaLvofPutPSelVUgz2qWqas3Q
        mjEuzZgLaka+z1VSPdpoB0Q5WunG/aqG/G1ld56PlGiNVCnWgQ0tp7dyOlT2rCKuiPZFxr/ZpGJ8
        nl3zIo60fAq3SL9oKzsdqpMjnR3E8qTeeG4d7CP/2bnIRvZyjXygHZYZSIbJlzoCMhZ+nzS990Ev
        +cqdG25QzCCTX5JIqEcAsCgR5HIN73qMF21xjJ/L1PbYFeCMM9lGIdq8XePdD95kIbLs8SNuOkte
        7ZQOQb8xW9QaTfIvzK2WnRuoAuxtrFHgv25Zywufr4oR7dgC9FHqTD2Ebcw+OCSKiTugUBAv6KeQ
        dcmQVtSk3yXiIbUMwTce1EavxkGogu9to7t6pDklsIrInKHRkJyFJSZ+c/iG3h1me84uFhE65JcD
        jr81CLLUD8qeeARMsHNWY0xz7CrxxOFr4CdbiqupcZWH61UhZQ1uR0Jra1y7E1oNrhxws2rhZ5nN
        tkE8i5AV7GQv7sCankLHpJPyeQhs8pey6BXzhxjEO5w+xGSBvLrRzPwo+Oz53yHzEKNKex+wZw/q
        jjD85AXdy25pL40t9OIE7WkkIFS1Sc67eGbvyNhKn5/BHi9wofN3Vd/8O8NPDNc4pcbRy50+DjYT
        4+8JNn7Hfgvy2TBJsbuZfDp/5fWNzRjvA58+dkrfbxoEbnbgZDmsckoDxAMBwmDyIrfnOcuMBtYD
        ibJHajQloLk0FRPLBeBQweYeKfQMpYKCOWI8FZNMsV5Yp/7waiGpoGCOFU6HYzP5CZrSc0vGquMZ
        lIg6VPHBa5f0qjE9Y8qu22mueN25bkfcIHk95G4VfSUmnBpSNqQ+gdANXP/0E12P3s1JuyZyD74e
        VWF6klUKLuuerwZXHNyPDNzVRwlexWQXkNvDy5ToBNtoj8OYvBiOpxuM9wnGb+cfUNzuauFi7jhS
        L7uYc+7NwryxytYvNl+xZuIk2NYTh4YLYlLyVapujrhJQgTdDckbxvjFD4JPkOSWqlGVl13FBYGz
        RxERhblkUYCQatqrRqQK3xXuVAu9KH/JFugvTNkJpneK/s734iiJXtLpV/9XN94EO7JJX33Y9vpL
        6KfMUjNlEZeWxMRbFxoaJHFQ8isExUtkFjP7qaPiatJSEpm9OYrHMvaCe9lD8mpQMbTBdUQxIu2J
        qbvByaiVVytL+cPK5nAJMLMAKzvrxkwoWgTv4itlGOfgSFGSqj0Gsf8i8QbYEgAKZF0PHmQZyE+W
        p8JXAW7Szjk32YXBsAaHgp5tr2Pfe0umm3X8ISIbtL4hIGdiARGNoeEgO5VtHO19b+oFRB8xctDW
        9d8ODDjssgPAwQOyvF/3gmalXczuanmvoGEumwjAitSRlsq7+CEgwdnlG9MxWdVp+FGlG4zbb8Ca
        64NaYp5RdKALfe5nHPd6F+gWs66UIuPWMu22hRdl72TGoGID0DaZoyQKQ9wuECWfruH9lh7U0Jo/
        VT2uXaCyAfdf1riUVagFYEgBaGWHueoM9E0aEsFwEGerz8Y9eYi3h5OBG2ezrd+2QBhYCTNgdxSu
        gGxnX0OflkChVz0PaVbthGznomC/9UNUPF/k7OqehOEBnbUOvsm9Dftya4fN5itu3+NieAvixZbK
        VLDCtvP70toXP8Zfo/gtyX4KAvfdhSWt8/YSABJ4N6o7l0qaBoSyNKCqZKAvLzFmmTPhhV0MEsXB
        wB8pDjeYdX/fWjgrwIUWG6Kxdlf7zlYAFRMbuYd6sDPpJ4AidZRmIA0aaaDMIagsB4eF/kj8o4h1
        ImZV7SGa7DNuHsTEElDVqWj985SYGLub4jtmn7wgz82T7X7F3LvE4QpwIHWV5eUONDjnEj+baiB1
        KKsdKH7EVHUaK1LuB3ZUBdhZdeQg9CVHL26QSOUyizLSbhcWK86oaR0RrfsYv/v4K2dbyCmiLEQn
        6NAQsjx05LmOmU4xRdXMjoXZ132KZizffojCeYC6eXfqkDXk8aidc2Q5mtK7oPQYTRRYTa1yVXu2
        sRTmw6l/ZCGcaM1m95UFWiCBHqylh3jNMZPiIV1I4eHBQ4lVQCBr8cYJlzHEprF/3VX2ZjPVlBT7
        EZ++ehlsNAxGfonewXGgirNkS1x/FgCZmc8HvxpwgxvLd5YOl2UBzi4ce9HsHkq3iZLRvmU9benJ
        ZQAbrkDUsGlCg3O1qjU/Ht272R0hOhW09hN2a6X5ygQctozxtPgEB3mwGCREiABpwKOC5OaEpUmJ
        rLs+u21SmkmeAzr8PYICIFRjjudwsy5O4cakOGF0SGKcYDf2ttn/oC2OdzjJoQJW7LsrBXJDj/zn
        K3bf2VeCAftlc76AHOqXRg+z1sTf9ls83X1LtxGZNwp4IgHLv77OYeIZkVVXTpA4Hv+Mwg3ZvPrh
        1PXjaB3FUUIxobbVhhTEYLuFDgSX8uhBcDmEG/9pPo3xboeSAMd7vqQsrJm4hzY+Sdl/e925AXGY
        aQooUZ/sZ5n2dHyQMAv2SFt/LUiTp6vB6oHyYEl2v3zFYfYXMabzvBoHcsQuBDvOkncfoWrIzWFh
        aQxP1sTTsEuD+8P3nzY6BESwxMWo+9KCgL0Dd+vg+rZViVdNX6F/uru1m/Uap0kj37ErMtjzioIu
        5bBQ1ZgyOjeDJDrLvIh2LUJNJeqMYHanQVEIAQjyFLMzABWEbSSSx0ne0+jx9VZB+FTWWZvIHCu/
        v/9FVHwNlb2E2tzmrv1XKwa4/TynH4rH8SobA47y5Y+qfJmKITdZ7LA7JDJDkWGwf876nEOAocqm
        waEW5wqW4ECWLxpsXbsJ0wxn12IFZyQu/w5X/gO83z67hDcXwvbVtMj2LUzjKJA2PUC0Y8avw8Gb
        Yj1nAle9QBObAeb11HZaPOZOk5PNHijyvRAIVjXnjyz4Md7JUjtLXCzJYtY1fce5SWbPhoQa7ZnJ
        P7cAs0dX5EnugrEThbKkFnZjsaohlbkxosINuVzNt0xrvIkj7w2GSxbhURCZlXiA7HqsJGSyHc+l
        aLn1pV2H06XJ3/LADkUhXsiyJT5Vu57qy3h1SFrouHfkQXkHG8im0kYxyq6jKQqR1BAFDKSd+2sU
        TlI/dUOUElcD0UeyrdaMTo3TH9UWUcrrx8jrJdA1RPiDLOlJwrHtclAqP4ZzAAnpuNDWdnFQom6P
        j2OezuU1dtoDdfMceVaee97RHKm8Q2uGF89TsPmewtWQ46TesZdGcVazmW7IQTV6BcNZougYt/DU
        NK6txchmypI0lEB9buX0M5eDUnZ1h5O3b/P6G5dGSICIcdnzqiStaOXfBlie3Su5vX6Z7YWjQ5xF
        rj9t03SffJlO6XbeTVJkTch2MIkCPHG/JpMjj2QrOT2++XQb7fAfYvxK9pW/P3/ot9PTJJM/0C8O
        fn8lATn3rzjEsUuYolJQfFMluxIfEywm1e2ivnCqk5aL05ezC0TWV8gFFVCCLTdhssnegHMicoGz
        ukHUF+5utGdAAZG2bjZbwoAWKgYie2baPL9mADQlFiBsAqW82tS1qC/boy50BxoEO8Dn5hV4Ugv2
        ZEeeqKqwAwpF885WhkX9tOSTZf4rJsu8PdVWXLIVL0Cce1Ia4q4gxodcip3xQQyq1ffES+DoVoxv
        LIV2nzu2Exrgjq3EKAAGXZjjFXYYRIL1DmZop07z0D8P9QZHZR4sU7zTrMOtUjksE8dQZRi9uyhr
        NCt09M8OVGelseQe/psOZAm1BKWfPCF8x3EC614OKW0mWum3un7vFVOHc0Rak6U8WUWmjkf3bYmS
        XyUQ1tFRdEnpnCd+fe3NC/cgqUPN1HzfOd9FsveqGmHAfVtH0EV5PKZLmq3Jvmeyz4XwJqVucHnD
        Vdt+kkC+/LarElv9dbwVEW9S0baybAWV8ysyF7apybwTMmm7XKEUtPZcDtWDs2MuuQxBeqpWm+Sq
        lC9pTWsqJOS6U7aSui65X/Z96/t1d2yBmGDv6g5xwrqIzkJ0tBPehX0xOP+0IDHMC+tXl2WWJ74b
        PaabJzkutXbCFCWzmVOt6RwDndqtHoFb3aVXXS0jDRxr7VcrrfPatR7ItVZFpbV3PUJtxgftXd85
        mdq7vl86tXf94N51tYxo7/redH4E3vWoGB2jVmsHe4QKfcNj25px8jtz9ngf4Aqa2+ssGizplwg1
        pJIgdfcoJOO3WlQ7xdVCu4hAi7xot/PTHQ5TJaFOsBeFmxzA2Zy18NWPlGZASxAuxgehPbeXghiW
        hkoDER+QR6Cit8YG1W7hIndDL9tXgKmltvUYXmkj02GtH9kNgmpprRiCtsOstMgY2oHzOEYRZFcz
        rx/ZAXxjlD/zacm6s88YKhPAo6et5a8VfFr+YIVmb7DhVtO4Zw+l3V65dk+nMW2MaW38QWPaDFNu
        AEIDKwVYoXVcAayHCkHIQ1poyVcM6X4jFS2w5sQzFIB1lMYCHvVQAOmhoh6ycB6loeg1NiLF/x2j
        OPe5g5UC8hhlud99rpR9h5blHkDWstwBzBUI8qspC2Eo0mnwvrw1ZmxHo9oc1frojka1Iar8+I6G
        Vg60Yr6BCmg/SoxHNazvJsqjArCjNBkN4jwqYP0YkR7VkL7bWI9qQI9nhzzq5U/5PXLTeI9qQI9H
        nkERH9VgVl6ebzCkbXLIL2VlPMn03yBlNBSN/Wh0e/YkVAL8yqqyGk3WDlTam1AVanMhDHZpqNI+
        hapwj0WyAX6FqlCPULKVxnakKyIvfq8wxLYNaHO9VCi6PGoLPRPf/M2G3P01iOirirlJhrK67rHG
        KhXZVwngGxsMMNgrZ6UIxKM2JQtmrzzGUNVj/aoibj4R0RUOKV2P7QlzL9riGE8oum64QSn+oDDO
        GDgK9K+oKCtUWTJIOOsb0tTaNluiWCmT/1al+oqCN8wOrwa2Cv0dAWzmYmEKH+7fDG4NXwmhwH/d
        psrgVN55AXYFV00he9VNtSC8kh7ridlTnjW2Vz1VGUJrZrH6jTIHy9bWGO2DQ6JkV1bHFNZVucVM
        biBSER1megbLI5EFD96RTyEcvgZ+skXvDgOguWVz8CmN4Eyd5SaUxzafeTbBDa26WD/93SFIfVqZ
        8UD8/8fBYIdTdxIE7s51kIOW5jrb1cQHL1VTV5hn5czBvTWCreoDeL0denGDBNfQYCPLWSP6pex9
        pehqx9lVAp0uZlSwbKtkmqoiQFRKFQXIeoJsGedS/YGiKq/E9TgrpMzGqWpIRXgCYvQG994vB9ca
        NyBu2Tmdxg1cBzMhX7wdHXCsDU7TBRcGnecWArPjAg+wGpiLpy7Aw4dx6ivEz1t0VQHzmJEwLuSe
        IArb9oSrErnEHeXaCvFtu/JuYXuwQXCyFwCgWh/oAZw3jVxT900j19iBUx46x4K0luzXgVMfPMii
        MDPbnqQDPDjlobNXgJ2D040jUu3CKQ+dM4NIXTd9G6p9OOWhm0Ggm6+kSl0RKAvi7lbGyZvE0m3z
        Kr+N4+/akD2W3cXNM5kh9EMy0QQMScA1+hBLMRD8T5Cd8lPb6Ezv8q8ZGFQBZuZ8APxBp6lzQJSt
        de5K7/KvCRhO/G1kqY49KO+ii4txHQq/hr9f+PFhfPBDTkStLq4IdGd7HPXRhxwM9tqGXor10QQM
        bH9GQADE9bfG5PoT66+++QdlJnRyK69b90czMKQCPA0RegDBD4i8qR53uxF+jf5w6Osc+sGYmFs2
        +kDk7+ncREmK98k0Sd21T17624T+FGC08V9eDgn5QvQRoHcWNcslh5jrAWUY/Z37inMYrRaHTNlj
        nl9xiGOX4vSpZsbk/VSastMiVsWeMp3oByJ/K0ZyHzNWjOQ2yb7sKVumPSNzpv8Rppl1FUTenFvU
        XBGfsjDP/cy5M/NVmN3GQYEbv2Legjm6Wc3vcV65LGbDkRfFPc6uM/3jzG7k1qVmdocgjd3eyLMG
        m14v7HU4PfKPO/cNx1kqVlYK4TIUZRsPsutbrtttNGRfhhTdY3BQO0++cl8R7fYBbo4YenlAzG7y
        1sTQshwtYHDItISJw7U0tYTBIUNaxrJ/SaNXnG5xjFwfHfYojdCsJE49JiScXubZ9eFvP5tYa7SC
        vTwok7TLl1/Rl7ctliZb19xLvLnNS1NoNTPbolOblafGeX3A5Rvu3q2dVGWvvyrbWF6GJuD1eYe0
        7eQqe33Lgr0/5O4TN07b6v3P9XeOxsmamzsQE8Ki1G4WNXWCqqdiza0dnYwDngyzblpfkyGfylfX
        bDmd/kx//ClbWZ2JhVY/oB9P5zj/eYjXEWw1gSh+U9HjnqNUDN27MRmDA8hnYpzsozDBz4m3JWvy
        7ci29WDFGFmarShZgWyxpkSEkpk5b8MJJAmCmwMxDCMd4XyC2NFSr4DUs9hAf4kxFqNEhAlNwIUA
        P6GXxslPP/sf2fXx1ceyAP+7ObFAugCpZcQ70H1UIrKfEJuGR8fr9DMN7hxSHE//SB5+tCGz9Rk5
        VUFrC0WUG02PPOYV7zKwlbkkLmXbkU8QEs67ggR/sOpYO/ZyUVyGKyCpGlIJiQqIZEJUldBVIYi3
        WV1H4DYY7xOM3xBFMEbv9jnHTiNZawbEgFy7CeZpKF9BRwLhRT3lA0lfRYulVDS1bDZC0w+J0xEG
        O/t5juxjtWQtkCAI81rTDNB6P1jrA7V6JL6olV8yMBiKJUMMLRpqHdwPiwYiNoa8cRTjzSOBwt73
        5LdjHmYlZ0qI86V8tv+4SLAtxyNBUQ6hQR20sSHR1j87hiSP6fXnC3f2zFwygFzM5065A1gJyKoh
        SiPJQOdhdIu9+uyEgtZ3DA9HTMrAsLY9kvaK5TEqYiN25PTo6Ni2ODySzG95jHrw0ChW/ql+tgAq
        YMG2wVHsuWjnh75G5IjIu+8dQpaAlGppVqBRMaJxQc6BwKCBXz98iYkf7P+6j7Hn0wz3eBr8Grw/
        k+3B88veWjxvuSnufKmRJDR5VQTxjqa0xToHtTMGoNoJnEPYC7B/jb/tt3j687d0G/3sfqCfhC7z
        9Auo3FLIHQMKTYcoe5YWG1zBWLsljq7cYn9do1tOuyp0fNR2oAWwXvQa+qn/jvO0lOwmUTLdRMF+
        64fInixQweflxGV6F+GrTvdcA7EcGumj+1eT+KOcHC8gcrwYWI6LhviDE+bgY+nYpuT6Sn2KKter
        +hvR7U3+NyoK4w/onXu6ycTNWjir2eMKITw3VZvQZkru+nG0juIoEbkgrJf/WkRNCxFp/YefKf8f
        2RkhegkSxHLxAyLrx/9qBaWdTUkqmH06/Nwl6J9RuDnEaz8sqHKQO/Wv+3SGrMmMaSu1YoM3TLTy
        hJIbplHt8wmWv0bhaZP/E3kB1VZ0iJja6onpUlEpHWu8pHg5s3h/uZXYVoxohy6oJgK3KMJA6Bbv
        AaoF7xzUAJxXtGEYeMs3ixkI937kmq/5EP+gE4g7vi/XbFuhoi/c58aC6wtHexxSSs8/PDuTuV78
        gNJH3oTg/OGn3y6h1izAWrx9Y9RAWhW4ahDbKo/hlTGas8pIlkZbM/OzYbCBvWBwlUt3Ba3BR61Y
        rl491Mg+TBw1e9UxakGhEokgao51XSe7CraqMTe4lQdx6h5BhI2jwZIgW6mNmGkDIKOD+xG0JArd
        GG0PrxhZ9uoNRSFZtzETQHvJB7BqzA2A5UFsACH2jVfJRQS8wiM2mCaOZOeZzxs326V/sk17jkwb
        2fYnUZTzVGLFYe5bszuF+XQvUTVjMH6Qkx3x8NUGeQiL2ynQ6hqNQRa39lhnCKuIJ8w+dOoq5FJI
        HsfCqXJX3WTnvYLsJB1xlKx5DyhpSdKS1Bql/T74OJ1bsIDq9wLRcbcMsvKrz93jVKg9YdQhNczx
        LjCs0INE8d2HAZCCAFUSqdKTY/zLASfpcbiMHSBf9vqEVLA4PhDTkvANh2lQSsVWDVSwSneI1fla
        pVjIaxAZBOUC8s5gW6J1SToXKZ81hBXsDK0asNg+Wr7n5BR1GiZgryRSl2Wg1vvnOrVVQ259f1Oo
        s0VTiKRZ/0ZY7vDGP+zUFDtAYV9urX1ZQI1X6qzBhO7Fj/HXKH5LaBVd1/OiQ5gm0/Nvp/STQXJ0
        Rt6dvY0soTVjGKmEbO65ceQiNEIBziaZItwqyUl0iLMz+U/bNN0nX6YXciaEsn3se/Qbqp7NOdOH
        U+9o6h+T+pvDZE39o1BPDD6x+O9+1gNbWREAHrcoIAI5omrKiJBYPJnqi8UTRCx4WaP3JRZNjAL9
        xWku7NusAwTjx8e1+qtAsejUxQFg3x7pveRUtqpDFgBLiQVAffZ/+YpDGy1tIddvqFsBWuk7oX0/
        PzYGcMToHyBGrLmXz/03P88WVE3RAfkI3J7EmuqM6kYNfvou56vX9q7Zf3daufGWveIHzKGOfFdm
        HUD2MAxWUBhGO99D5x+yBssoxR8pereYF9nAWzDeQSPoFjGAl0J761w42Y2lZW51WeBqaFtB+/+2
        OMY/Jt//OP37939GP2VXEP6LBerc4mB6PYAHqWi/8xFBmm5xGJDXn76mmOumaThBcHL6nklG8870
        nVhQdNijNELWolQwjBu3F8cB5NnVv6i1mFhr+q6rcnGzofyS+nfdRfgI7BwILKDNOzenQPBlyStm
        IrCEvaoFkAGYd1//qhv84h6CVPQ1xV5QzqtdNPioUXNzB2K+R+MiMgv6/nQeDnge/S05xXmQB7jh
        t4Ts8XC7stMDnYVDbgXT0WzcTlBI3ynfLDebyCMrzfHrJl60I2sOfvfx1ykON/vIp7vYzDN/xSGO
        s5tt0+MXEp/9fF5xJAzVEnZ8Bw7bDQqMar6H5buWMhjjSndXyX0L0BURbq3EO6D8lrN6zv/98PpK
        3vcvrof/fTb9Fe+332J6b2CNU1ZrkAfR7nrcXqPolf4H72gNmzXyOUXY+j7+H9QQdiPqOeSoCDnD
        lo2o2COUKs5lwDZiPapSriDc7L5xU7VnCPDGd3eYja1JhdLAUWnTwNWsRIbspehSvq6eASFj0axg
        rvLORAccSneczySgWhLgFIpX5e2ZQwt067s0Wlk9rCPxioYbFr0gOmxeAjfG0++8l+whl+vlx0Wb
        dv1jEOjQGisGk8GqITdZP6UxHA6fbEjiT3l4FYsXKAALkQh+hL8VA7+sKyQbv6ohZfzKY8aJX76F
        r7q1L9Qlmg1j1ZBKSzJqGLcv9PxyHZCvLtaWLSDpfv2F5xSxgRTu/Do6IN+jb+4rnub/QabUVijc
        u2rifgUnRJG/PvuE9zeFiZemTR/5sHPP0lFYF1ishcm9Kl4awps95Ky72+m/+KEbemwAqrr03aT5
        jhWAXPaFMl5nXBhmMBTUAcH9+rgqQNfMx509uxHlvQq8I3C3QbrRk5iM0Xb6vU4cUiGme97ZncQ6
        mL06up5Zun65V0jod4cg9cknyDazZwwUkIAYx274JnuZqz7Xvh3zywHH3xqvlldZiaWx2ZPrxjbP
        0KuHOQeSiTHPxlRckykHLvgwVwwpobyCgcyIp96ALCjQUkHeuKm7jn3vLZlefkSFKKiDssZlYtVp
        qu4qNbnPdBwkHPe4FeXN+lA3emnNbPFUY2teQ81mXfMRmzx/teCwdAFb6NbUzYnF169fJ5dnHKPd
        0Ybwc0oln75Eh3CThbnz4DZKcPxeykqnRNPHUFI/hVGKKfqffqQQTvN5GnSSBpmkkU3S8BODYJv/
        9t+M/0FmTOuU0uz+jRGFxuWdjCx0bjgTy1iaP5APhO84prU3Jsb/PhCrGhtHPDbGSxQbMX7BMSab
        5s/G7/6tgr7fGQf6FX5ouF56cAPDcwPvEGTzm3z6VxEs0YZvIqJfboukmuRb1UaGIf5l6eQVln2i
        f6rEv/YbyBfMub3WtfwrJP9FoZ84Wui10D+A0BeM/mYdf/RegwVQCXz29HRbyoEl7tTJAYi7PXuq
        CiMxvsEhXg63UaYWdzXF/crH0cZeG/uHkv6qvg81Qj9EHwO637Qg4k5tPaDBsVURy2E83S7tlrWo
        j0jUh2tEoY68m8S4V8dvhBcQLfNjkvl9ihwxn0Ze0qy4uJelUbY3o8X98cRdzLgPIe1dGndxGdey
        PU7ZXp8zCTMmJdUOOgrLO/bSKH5O/F/pG1mmPQPINd2UOiC5Fj906kmuBasUaeG+Fm4BsT1XD2OL
        LdgeSxFc+zrWISC416EOLbj3ILiJu1u7YfR+fdlsYgneGJSa1AIQXoAbzCtkdUagYfUNbgEPzopX
        y4Dghb/B4rGgznSjZWFmzhXWBPHicxbP1irKgI0shfE3AYdrJq87o7IMOEIMgNPZq2OGIPgB6a8W
        L7KhDvz/h/aFmMyzCgC0L4TYxW8u/KJpj+IncoDal7wzYvXQXyqLOyDheaY47G6SEDc++EYrg5P3
        KOF8fLh72PjRcxq7YeLF/p6+UfG7SmAlmDjQmxMEZFu0ZFmJq9HcygTnt/1UN4t13kFW8ixMh/zp
        bBa/+ddv/j/D9V5OiR8FAA==
    headers:
      Accept-Ranges:
      - bytes
      Access-Control-Allow-Origin:
      - '*'
      Cache-Control:
      - max-age=300
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Length:
      - '16033'
      Content-Security-Policy:
      - default-src 'none'; style-src 'unsafe-inline'; sandbox
      Content-Type:
      - text/plain; charset=utf-8
      Cross-Origin-Resource-Policy:
      - cross-origin
      Date:
      - Wed, 19 Feb 2025 16:41:20 GMT
      ETag:
      - W/"c48ac84a72a3006ab9f57be21afaa3b1b5b91d845ba602bf600dfe1339f2418d"
      Expires:
      - Wed, 19 Feb 2025 16:46:20 GMT
      Source-Age:
      - '12'
      Strict-Transport-Security:
      - max-age=31536000
      Vary:
      - Authorization,Accept-Encoding,Origin
      Via:
      - 1.1 varnish
      X-Cache:
      - HIT
      X-Cache-Hits:
      - '1'
      X-Content-Type-Options:
      - nosniff
      X-Fastly-Request-ID:
      - e8d226012586fa485056275e531f42b7c0a90af5
      X-Frame-Options:
      - deny
      X-GitHub-Request-Id:
      - 094D:160F2A:47787A:68CA54:67B558B5
      X-Served-By:
      - cache-ccu830048-CCU
      X-Timer:
      - S1739983280.325377,VS0,VE1
      X-XSS-Protection:
      - 1; mode=block
    status:
      code: 200
      message: OK
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate, zstd
      Connection:
      - keep-alive
      X-Amzn-Trace-Id:
      - 83636dff-e722-4866-8004-41f858460c72
      user-agent:
      - unknown/None; hf_hub/0.29.0; python/3.12.8; torch/2.6.0
    method: GET
    uri: https://huggingface.co/api/tasks
  response:
    body:
      string: "{\"any-to-any\":{\"datasets\":[],\"demo\":{\"inputs\":[],\"outputs\":[]},\"isPlaceholder\":true,\"metrics\":[],\"models\":[],\"spaces\":[],\"summary\":\"\",\"widgetModels\":[],\"id\":\"any-to-any\",\"label\":\"Any-to-Any\",\"libraries\":[\"transformers\"]},\"audio-classification\":{\"datasets\":[{\"description\":\"A
        benchmark of 10 different audio tasks.\",\"id\":\"s3prl/superb\"},{\"description\":\"A
        dataset of YouTube clips and their sound categories.\",\"id\":\"agkphysics/AudioSet\"}],\"demo\":{\"inputs\":[{\"filename\":\"audio.wav\",\"type\":\"audio\"}],\"outputs\":[{\"data\":[{\"label\":\"Up\",\"score\":0.2},{\"label\":\"Down\",\"score\":0.8}],\"type\":\"chart\"}]},\"metrics\":[{\"description\":\"\",\"id\":\"accuracy\"},{\"description\":\"\",\"id\":\"recall\"},{\"description\":\"\",\"id\":\"precision\"},{\"description\":\"\",\"id\":\"f1\"}],\"models\":[{\"description\":\"An
        easy-to-use model for command recognition.\",\"id\":\"speechbrain/google_speech_command_xvector\"},{\"description\":\"An
        emotion recognition model.\",\"id\":\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\"},{\"description\":\"A
        language identification model.\",\"id\":\"facebook/mms-lid-126\"}],\"spaces\":[{\"description\":\"An
        application that can classify music into different genre.\",\"id\":\"kurianbenoy/audioclassification\"}],\"summary\":\"Audio
        classification is the task of assigning a label or class to a given audio.
        It can be used for recognizing which command a user is giving or the emotion
        of a statement, as well as identifying a speaker.\",\"widgetModels\":[\"MIT/ast-finetuned-audioset-10-10-0.4593\"],\"youtubeId\":\"KWwzcmG98Ds\",\"id\":\"audio-classification\",\"label\":\"Audio
        Classification\",\"libraries\":[\"speechbrain\",\"transformers\",\"transformers.js\"]},\"audio-to-audio\":{\"datasets\":[{\"description\":\"512-element
        X-vector embeddings of speakers from CMU ARCTIC dataset.\",\"id\":\"Matthijs/cmu-arctic-xvectors\"}],\"demo\":{\"inputs\":[{\"filename\":\"input.wav\",\"type\":\"audio\"}],\"outputs\":[{\"filename\":\"label-0.wav\",\"type\":\"audio\"},{\"filename\":\"label-1.wav\",\"type\":\"audio\"}]},\"metrics\":[{\"description\":\"The
        Signal-to-Noise ratio is the relationship between the target signal level
        and the background noise level. It is calculated as the logarithm of the target
        signal divided by the background noise, in decibels.\",\"id\":\"snri\"},{\"description\":\"The
        Signal-to-Distortion ratio is the relationship between the target signal and
        the sum of noise, interference, and artifact errors\",\"id\":\"sdri\"}],\"models\":[{\"description\":\"A
        speech enhancement model.\",\"id\":\"ResembleAI/resemble-enhance\"},{\"description\":\"A
        model that can change the voice in a speech recording.\",\"id\":\"microsoft/speecht5_vc\"}],\"spaces\":[{\"description\":\"An
        application for speech separation.\",\"id\":\"younver/speechbrain-speech-separation\"},{\"description\":\"An
        application for audio style transfer.\",\"id\":\"nakas/audio-diffusion_style_transfer\"}],\"summary\":\"Audio-to-Audio
        is a family of tasks in which the input is an audio and the output is one
        or multiple generated audios. Some example tasks are speech enhancement and
        source separation.\",\"widgetModels\":[\"speechbrain/sepformer-wham\"],\"youtubeId\":\"iohj7nCCYoM\",\"id\":\"audio-to-audio\",\"label\":\"Audio-to-Audio\",\"libraries\":[\"asteroid\",\"fairseq\",\"speechbrain\"]},\"audio-text-to-text\":{\"datasets\":[],\"demo\":{\"inputs\":[],\"outputs\":[]},\"isPlaceholder\":true,\"metrics\":[],\"models\":[],\"spaces\":[],\"summary\":\"\",\"widgetModels\":[],\"id\":\"audio-text-to-text\",\"label\":\"Audio-Text-to-Text\",\"libraries\":[]},\"automatic-speech-recognition\":{\"datasets\":[{\"description\":\"31,175
        hours of multilingual audio-text dataset in 108 languages.\",\"id\":\"mozilla-foundation/common_voice_17_0\"},{\"description\":\"Multilingual
        and diverse audio dataset with 101k hours of audio.\",\"id\":\"amphion/Emilia-Dataset\"},{\"description\":\"A
        dataset with 44.6k hours of English speaker data and 6k hours of other language
        speakers.\",\"id\":\"parler-tts/mls_eng\"},{\"description\":\"A multilingual
        audio dataset with 370K hours of audio.\",\"id\":\"espnet/yodas\"}],\"demo\":{\"inputs\":[{\"filename\":\"input.flac\",\"type\":\"audio\"}],\"outputs\":[{\"label\":\"Transcript\",\"content\":\"Going
        along slushy country roads and speaking to damp audiences in...\",\"type\":\"text\"}]},\"metrics\":[{\"description\":\"\",\"id\":\"wer\"},{\"description\":\"\",\"id\":\"cer\"}],\"models\":[{\"description\":\"A
        powerful ASR model by OpenAI.\",\"id\":\"openai/whisper-large-v3\"},{\"description\":\"A
        good generic speech model by MetaAI for fine-tuning.\",\"id\":\"facebook/w2v-bert-2.0\"},{\"description\":\"An
        end-to-end model that performs ASR and Speech Translation by MetaAI.\",\"id\":\"facebook/seamless-m4t-v2-large\"},{\"description\":\"A
        powerful multilingual ASR and Speech Translation model by Nvidia.\",\"id\":\"nvidia/canary-1b\"},{\"description\":\"Powerful
        speaker diarization model.\",\"id\":\"pyannote/speaker-diarization-3.1\"}],\"spaces\":[{\"description\":\"A
        powerful general-purpose speech recognition application.\",\"id\":\"hf-audio/whisper-large-v3\"},{\"description\":\"Latest
        ASR model from Useful Sensors.\",\"id\":\"mrfakename/Moonshinex\"},{\"description\":\"A
        high quality speech and text translation model by Meta.\",\"id\":\"facebook/seamless_m4t\"},{\"description\":\"A
        powerful multilingual ASR and Speech Translation model by Nvidia\",\"id\":\"nvidia/canary-1b\"}],\"summary\":\"Automatic
        Speech Recognition (ASR), also known as Speech to Text (STT), is the task
        of transcribing a given audio to text. It has many applications, such as voice
        user interfaces.\",\"widgetModels\":[\"openai/whisper-large-v3\"],\"youtubeId\":\"TksaY_FDgnk\",\"id\":\"automatic-speech-recognition\",\"label\":\"Automatic
        Speech Recognition\",\"libraries\":[\"espnet\",\"nemo\",\"speechbrain\",\"transformers\",\"transformers.js\"]},\"depth-estimation\":{\"datasets\":[{\"description\":\"NYU
        Depth V2 Dataset: Video dataset containing both RGB and depth sensor data.\",\"id\":\"sayakpaul/nyu_depth_v2\"},{\"description\":\"Monocular
        depth estimation benchmark based without noise and errors.\",\"id\":\"depth-anything/DA-2K\"}],\"demo\":{\"inputs\":[{\"filename\":\"depth-estimation-input.jpg\",\"type\":\"img\"}],\"outputs\":[{\"filename\":\"depth-estimation-output.png\",\"type\":\"img\"}]},\"metrics\":[],\"models\":[{\"description\":\"Cutting-edge
        depth estimation model.\",\"id\":\"depth-anything/Depth-Anything-V2-Large\"},{\"description\":\"A
        strong monocular depth estimation model.\",\"id\":\"jingheya/lotus-depth-g-v1-0\"},{\"description\":\"A
        depth estimation model that predicts depth in videos.\",\"id\":\"tencent/DepthCrafter\"},{\"description\":\"A
        robust depth estimation model.\",\"id\":\"apple/DepthPro\"}],\"spaces\":[{\"description\":\"An
        application that predicts the depth of an image and then reconstruct the 3D
        model as voxels.\",\"id\":\"radames/dpt-depth-estimation-3d-voxels\"},{\"description\":\"An
        application for bleeding-edge depth estimation.\",\"id\":\"akhaliq/depth-pro\"},{\"description\":\"An
        application on cutting-edge depth estimation in videos.\",\"id\":\"tencent/DepthCrafter\"},{\"description\":\"A
        human-centric depth estimation application.\",\"id\":\"facebook/sapiens-depth\"}],\"summary\":\"Depth
        estimation is the task of predicting depth of the objects present in an image.\",\"widgetModels\":[\"\"],\"youtubeId\":\"\",\"id\":\"depth-estimation\",\"label\":\"Depth
        Estimation\",\"libraries\":[\"transformers\",\"transformers.js\"]},\"document-question-answering\":{\"datasets\":[{\"description\":\"Largest
        document understanding dataset.\",\"id\":\"HuggingFaceM4/Docmatix\"},{\"description\":\"Dataset
        from the 2020 DocVQA challenge. The documents are taken from the UCSF Industry
        Documents Library.\",\"id\":\"eliolio/docvqa\"}],\"demo\":{\"inputs\":[{\"label\":\"Question\",\"content\":\"What
        is the idea behind the consumer relations efficiency team?\",\"type\":\"text\"},{\"filename\":\"document-question-answering-input.png\",\"type\":\"img\"}],\"outputs\":[{\"label\":\"Answer\",\"content\":\"Balance
        cost efficiency with quality customer service\",\"type\":\"text\"}]},\"metrics\":[{\"description\":\"The
        evaluation metric for the DocVQA challenge is the Average Normalized Levenshtein
        Similarity (ANLS). This metric is flexible to character regognition errors
        and compares the predicted answer with the ground truth answer.\",\"id\":\"anls\"},{\"description\":\"Exact
        Match is a metric based on the strict character match of the predicted answer
        and the right answer. For answers predicted correctly, the Exact Match will
        be 1. Even if only one character is different, Exact Match will be 0\",\"id\":\"exact-match\"}],\"models\":[{\"description\":\"A
        robust document question answering model.\",\"id\":\"impira/layoutlm-document-qa\"},{\"description\":\"A
        document question answering model specialized in invoices.\",\"id\":\"impira/layoutlm-invoices\"},{\"description\":\"A
        special model for OCR-free document question answering.\",\"id\":\"microsoft/udop-large\"},{\"description\":\"A
        powerful model for document question answering.\",\"id\":\"google/pix2struct-docvqa-large\"}],\"spaces\":[{\"description\":\"A
        robust document question answering application.\",\"id\":\"impira/docquery\"},{\"description\":\"An
        application that can answer questions from invoices.\",\"id\":\"impira/invoices\"},{\"description\":\"An
        application to compare different document question answering models.\",\"id\":\"merve/compare_docvqa_models\"}],\"summary\":\"Document
        Question Answering (also known as Document Visual Question Answering) is the
        task of answering questions on document images. Document question answering
        models take a (document, question) pair as input and return an answer in natural
        language. Models usually rely on multi-modal features, combining text, position
        of words (bounding-boxes) and image.\",\"widgetModels\":[\"impira/layoutlm-invoices\"],\"youtubeId\":\"\",\"id\":\"document-question-answering\",\"label\":\"Document
        Question Answering\",\"libraries\":[\"transformers\",\"transformers.js\"]},\"visual-document-retrieval\":{\"datasets\":[],\"demo\":{\"inputs\":[],\"outputs\":[]},\"isPlaceholder\":true,\"metrics\":[],\"models\":[],\"spaces\":[],\"summary\":\"\",\"widgetModels\":[],\"id\":\"visual-document-retrieval\",\"label\":\"Visual
        Document Retrieval\",\"libraries\":[\"transformers\"]},\"feature-extraction\":{\"datasets\":[{\"description\":\"Wikipedia
        dataset containing cleaned articles of all languages. Can be used to train
        `feature-extraction` models.\",\"id\":\"wikipedia\"}],\"demo\":{\"inputs\":[{\"label\":\"Input\",\"content\":\"India,
        officially the Republic of India, is a country in South Asia.\",\"type\":\"text\"}],\"outputs\":[{\"table\":[[\"Dimension
        1\",\"Dimension 2\",\"Dimension 3\"],[\"2.583383083343506\",\"2.757075071334839\",\"0.9023529887199402\"],[\"8.29393482208252\",\"1.1071064472198486\",\"2.03399395942688\"],[\"-0.7754912972450256\",\"-1.647324562072754\",\"-0.6113331913948059\"],[\"0.07087723910808563\",\"1.5942802429199219\",\"1.4610432386398315\"]],\"type\":\"tabular\"}]},\"metrics\":[],\"models\":[{\"description\":\"A
        powerful feature extraction model for natural language processing tasks.\",\"id\":\"thenlper/gte-large\"},{\"description\":\"A
        strong feature extraction model for retrieval.\",\"id\":\"Alibaba-NLP/gte-Qwen1.5-7B-instruct\"}],\"spaces\":[{\"description\":\"A
        leaderboard to rank text feature extraction models based on a benchmark.\",\"id\":\"mteb/leaderboard\"},{\"description\":\"A
        leaderboard to rank best feature extraction models based on human feedback.\",\"id\":\"mteb/arena\"}],\"summary\":\"Feature
        extraction is the task of extracting features learnt in a model.\",\"widgetModels\":[\"facebook/bart-base\"],\"id\":\"feature-extraction\",\"label\":\"Feature
        Extraction\",\"libraries\":[\"sentence-transformers\",\"transformers\",\"transformers.js\"]},\"fill-mask\":{\"datasets\":[{\"description\":\"A
        common dataset that is used to train models for many languages.\",\"id\":\"wikipedia\"},{\"description\":\"A
        large English dataset with text crawled from the web.\",\"id\":\"c4\"}],\"demo\":{\"inputs\":[{\"label\":\"Input\",\"content\":\"The
        <mask> barked at me\",\"type\":\"text\"}],\"outputs\":[{\"type\":\"chart\",\"data\":[{\"label\":\"wolf\",\"score\":0.487},{\"label\":\"dog\",\"score\":0.061},{\"label\":\"cat\",\"score\":0.058},{\"label\":\"fox\",\"score\":0.047},{\"label\":\"squirrel\",\"score\":0.025}]}]},\"metrics\":[{\"description\":\"Cross
        Entropy is a metric that calculates the difference between two probability
        distributions. Each probability distribution is the distribution of predicted
        words\",\"id\":\"cross_entropy\"},{\"description\":\"Perplexity is the exponential
        of the cross-entropy loss. It evaluates the probabilities assigned to the
        next word by the model. Lower perplexity indicates better performance\",\"id\":\"perplexity\"}],\"models\":[{\"description\":\"State-of-the-art
        masked language model.\",\"id\":\"answerdotai/ModernBERT-large\"},{\"description\":\"A
        multilingual model trained on 100 languages.\",\"id\":\"FacebookAI/xlm-roberta-base\"}],\"spaces\":[],\"summary\":\"Masked
        language modeling is the task of masking some of the words in a sentence and
        predicting which words should replace those masks. These models are useful
        when we want to get a statistical understanding of the language in which the
        model is trained in.\",\"widgetModels\":[\"distilroberta-base\"],\"youtubeId\":\"mqElG5QJWUg\",\"id\":\"fill-mask\",\"label\":\"Fill-Mask\",\"libraries\":[\"transformers\",\"transformers.js\"]},\"image-classification\":{\"datasets\":[{\"description\":\"Benchmark
        dataset used for image classification with images that belong to 100 classes.\",\"id\":\"cifar100\"},{\"description\":\"Dataset
        consisting of images of garments.\",\"id\":\"fashion_mnist\"}],\"demo\":{\"inputs\":[{\"filename\":\"image-classification-input.jpeg\",\"type\":\"img\"}],\"outputs\":[{\"type\":\"chart\",\"data\":[{\"label\":\"Egyptian
        cat\",\"score\":0.514},{\"label\":\"Tabby cat\",\"score\":0.193},{\"label\":\"Tiger
        cat\",\"score\":0.068}]}]},\"metrics\":[{\"description\":\"\",\"id\":\"accuracy\"},{\"description\":\"\",\"id\":\"recall\"},{\"description\":\"\",\"id\":\"precision\"},{\"description\":\"\",\"id\":\"f1\"}],\"models\":[{\"description\":\"A
        strong image classification model.\",\"id\":\"google/vit-base-patch16-224\"},{\"description\":\"A
        robust image classification model.\",\"id\":\"facebook/deit-base-distilled-patch16-224\"},{\"description\":\"A
        strong image classification model.\",\"id\":\"facebook/convnext-large-224\"}],\"spaces\":[{\"description\":\"A
        leaderboard to evaluate different image classification models.\",\"id\":\"timm/leaderboard\"}],\"summary\":\"Image
        classification is the task of assigning a label or class to an entire image.
        Images are expected to have only one class for each image. Image classification
        models take an image as input and return a prediction about which class the
        image belongs to.\",\"widgetModels\":[\"google/vit-base-patch16-224\"],\"youtubeId\":\"tjAIM7BOYhw\",\"id\":\"image-classification\",\"label\":\"Image
        Classification\",\"libraries\":[\"keras\",\"timm\",\"transformers\",\"transformers.js\"]},\"image-feature-extraction\":{\"datasets\":[{\"description\":\"ImageNet-1K
        is a image classification dataset in which images are used to train image-feature-extraction
        models.\",\"id\":\"imagenet-1k\"}],\"demo\":{\"inputs\":[{\"filename\":\"mask-generation-input.png\",\"type\":\"img\"}],\"outputs\":[{\"table\":[[\"Dimension
        1\",\"Dimension 2\",\"Dimension 3\"],[\"0.21236686408519745\",\"1.0919708013534546\",\"0.8512550592422485\"],[\"0.809657871723175\",\"-0.18544459342956543\",\"-0.7851548194885254\"],[\"1.3103108406066895\",\"-0.2479034662246704\",\"-0.9107287526130676\"],[\"1.8536205291748047\",\"-0.36419737339019775\",\"0.09717650711536407\"]],\"type\":\"tabular\"}]},\"metrics\":[],\"models\":[{\"description\":\"A
        powerful image feature extraction model.\",\"id\":\"timm/vit_large_patch14_dinov2.lvd142m\"},{\"description\":\"A
        strong image feature extraction model.\",\"id\":\"nvidia/MambaVision-T-1K\"},{\"description\":\"A
        robust image feature extraction model.\",\"id\":\"facebook/dino-vitb16\"},{\"description\":\"Cutting-edge
        image feature extraction model.\",\"id\":\"apple/aimv2-large-patch14-336-distilled\"},{\"description\":\"Strong
        image feature extraction model that can be used on images and documents.\",\"id\":\"OpenGVLab/InternViT-6B-448px-V1-2\"}],\"spaces\":[{\"description\":\"A
        leaderboard to evaluate different image-feature-extraction models on classification
        performances\",\"id\":\"timm/leaderboard\"}],\"summary\":\"Image feature extraction
        is the task of extracting features learnt in a computer vision model.\",\"widgetModels\":[],\"id\":\"image-feature-extraction\",\"label\":\"Image
        Feature Extraction\",\"libraries\":[\"timm\",\"transformers\"]},\"image-segmentation\":{\"datasets\":[{\"description\":\"Scene
        segmentation dataset.\",\"id\":\"scene_parse_150\"}],\"demo\":{\"inputs\":[{\"filename\":\"image-segmentation-input.jpeg\",\"type\":\"img\"}],\"outputs\":[{\"filename\":\"image-segmentation-output.png\",\"type\":\"img\"}]},\"metrics\":[{\"description\":\"Average
        Precision (AP) is the Area Under the PR Curve (AUC-PR). It is calculated for
        each semantic class separately\",\"id\":\"Average Precision\"},{\"description\":\"Mean
        Average Precision (mAP) is the overall average of the AP values\",\"id\":\"Mean
        Average Precision\"},{\"description\":\"Intersection over Union (IoU) is the
        overlap of segmentation masks. Mean IoU is the average of the IoU of all semantic
        classes\",\"id\":\"Mean Intersection over Union\"},{\"description\":\"AP\u03B1
        is the Average Precision at the IoU threshold of a \u03B1 value, for example,
        AP50 and AP75\",\"id\":\"AP\u03B1\"}],\"models\":[{\"description\":\"Solid
        semantic segmentation model trained on ADE20k.\",\"id\":\"openmmlab/upernet-convnext-small\"},{\"description\":\"Background
        removal model.\",\"id\":\"briaai/RMBG-1.4\"},{\"description\":\"A multipurpose
        image segmentation model for high resolution images.\",\"id\":\"ZhengPeng7/BiRefNet\"},{\"description\":\"Powerful
        human-centric image segmentation model.\",\"id\":\"facebook/sapiens-seg-1b\"},{\"description\":\"Panoptic
        segmentation model trained on the COCO (common objects) dataset.\",\"id\":\"facebook/mask2former-swin-large-coco-panoptic\"}],\"spaces\":[{\"description\":\"A
        semantic segmentation application that can predict unseen instances out of
        the box.\",\"id\":\"facebook/ov-seg\"},{\"description\":\"One of the strongest
        segmentation applications.\",\"id\":\"jbrinkma/segment-anything\"},{\"description\":\"A
        human-centric segmentation model.\",\"id\":\"facebook/sapiens-pose\"},{\"description\":\"An
        instance segmentation application to predict neuronal cell types from microscopy
        images.\",\"id\":\"rashmi/sartorius-cell-instance-segmentation\"},{\"description\":\"An
        application that segments videos.\",\"id\":\"ArtGAN/Segment-Anything-Video\"},{\"description\":\"An
        panoptic segmentation application built for outdoor environments.\",\"id\":\"segments/panoptic-segment-anything\"}],\"summary\":\"Image
        Segmentation divides an image into segments where each pixel in the image
        is mapped to an object. This task has multiple variants such as instance segmentation,
        panoptic segmentation and semantic segmentation.\",\"widgetModels\":[\"nvidia/segformer-b0-finetuned-ade-512-512\"],\"youtubeId\":\"dKE8SIt9C-w\",\"id\":\"image-segmentation\",\"label\":\"Image
        Segmentation\",\"libraries\":[\"transformers\",\"transformers.js\"]},\"image-to-image\":{\"datasets\":[{\"description\":\"Synthetic
        dataset, for image relighting\",\"id\":\"VIDIT\"},{\"description\":\"Multiple
        images of celebrities, used for facial expression translation\",\"id\":\"huggan/CelebA-faces\"},{\"description\":\"12M
        image-caption pairs.\",\"id\":\"Spawning/PD12M\"}],\"demo\":{\"inputs\":[{\"filename\":\"image-to-image-input.jpeg\",\"type\":\"img\"}],\"outputs\":[{\"filename\":\"image-to-image-output.png\",\"type\":\"img\"}]},\"isPlaceholder\":false,\"metrics\":[{\"description\":\"Peak
        Signal to Noise Ratio (PSNR) is an approximation of the human perception,
        considering the ratio of the absolute intensity with respect to the variations.
        Measured in dB, a high value indicates a high fidelity.\",\"id\":\"PSNR\"},{\"description\":\"Structural
        Similarity Index (SSIM) is a perceptual metric which compares the luminance,
        contrast and structure of two images. The values of SSIM range between -1
        and 1, and higher values indicate closer resemblance to the original image.\",\"id\":\"SSIM\"},{\"description\":\"Inception
        Score (IS) is an analysis of the labels predicted by an image classification
        model when presented with a sample of the generated images.\",\"id\":\"IS\"}],\"models\":[{\"description\":\"An
        image-to-image model to improve image resolution.\",\"id\":\"fal/AuraSR-v2\"},{\"description\":\"A
        model that increases the resolution of an image.\",\"id\":\"keras-io/super-resolution\"},{\"description\":\"A
        model for applying edits to images through image controls.\",\"id\":\"Yuanshi/OminiControl\"},{\"description\":\"A
        model that generates images based on segments in the input image and the text
        prompt.\",\"id\":\"mfidabel/controlnet-segment-anything\"},{\"description\":\"Strong
        model for inpainting and outpainting.\",\"id\":\"black-forest-labs/FLUX.1-Fill-dev\"},{\"description\":\"Strong
        model for image editing using depth maps.\",\"id\":\"black-forest-labs/FLUX.1-Depth-dev-lora\"}],\"spaces\":[{\"description\":\"Image
        enhancer application for low light.\",\"id\":\"keras-io/low-light-image-enhancement\"},{\"description\":\"Style
        transfer application.\",\"id\":\"keras-io/neural-style-transfer\"},{\"description\":\"An
        application that generates images based on segment control.\",\"id\":\"mfidabel/controlnet-segment-anything\"},{\"description\":\"Image
        generation application that takes image control and text prompt.\",\"id\":\"hysts/ControlNet\"},{\"description\":\"Colorize
        any image using this app.\",\"id\":\"ioclab/brightness-controlnet\"},{\"description\":\"Edit
        images with instructions.\",\"id\":\"timbrooks/instruct-pix2pix\"}],\"summary\":\"Image-to-image
        is the task of transforming an input image through a variety of possible manipulations
        and enhancements, such as super-resolution, image inpainting, colorization,
        and more.\",\"widgetModels\":[\"stabilityai/stable-diffusion-2-inpainting\"],\"youtubeId\":\"\",\"id\":\"image-to-image\",\"label\":\"Image-to-Image\",\"libraries\":[\"diffusers\",\"transformers\",\"transformers.js\"]},\"image-text-to-text\":{\"datasets\":[{\"description\":\"Instructions
        composed of image and text.\",\"id\":\"liuhaotian/LLaVA-Instruct-150K\"},{\"description\":\"Collection
        of image-text pairs on scientific topics.\",\"id\":\"DAMO-NLP-SG/multimodal_textbook\"},{\"description\":\"A
        collection of datasets made for model fine-tuning.\",\"id\":\"HuggingFaceM4/the_cauldron\"},{\"description\":\"Screenshots
        of websites with their HTML/CSS codes.\",\"id\":\"HuggingFaceM4/WebSight\"}],\"demo\":{\"inputs\":[{\"filename\":\"image-text-to-text-input.png\",\"type\":\"img\"},{\"label\":\"Text
        Prompt\",\"content\":\"Describe the position of the bee in detail.\",\"type\":\"text\"}],\"outputs\":[{\"label\":\"Answer\",\"content\":\"The
        bee is sitting on a pink flower, surrounded by other flowers. The bee is positioned
        in the center of the flower, with its head and front legs sticking out.\",\"type\":\"text\"}]},\"metrics\":[],\"models\":[{\"description\":\"Small
        and efficient yet powerful vision language model.\",\"id\":\"HuggingFaceTB/SmolVLM-Instruct\"},{\"description\":\"A
        screenshot understanding model used to control computers.\",\"id\":\"showlab/ShowUI-2B\"},{\"description\":\"Cutting-edge
        vision language model.\",\"id\":\"allenai/Molmo-7B-D-0924\"},{\"description\":\"Small
        yet powerful model.\",\"id\":\"vikhyatk/moondream2\"},{\"description\":\"Strong
        image-text-to-text model.\",\"id\":\"Qwen/Qwen2.5-VL-7B-Instruct\"},{\"description\":\"Image-text-to-text
        model with reasoning capabilities.\",\"id\":\"Qwen/QVQ-72B-Preview\"},{\"description\":\"Strong
        image-text-to-text model focused on documents.\",\"id\":\"stepfun-ai/GOT-OCR2_0\"}],\"spaces\":[{\"description\":\"Leaderboard
        to evaluate vision language models.\",\"id\":\"opencompass/open_vlm_leaderboard\"},{\"description\":\"Vision
        language models arena, where models are ranked by votes of users.\",\"id\":\"WildVision/vision-arena\"},{\"description\":\"Powerful
        vision-language model assistant.\",\"id\":\"akhaliq/Molmo-7B-D-0924\"},{\"description\":\"An
        image-text-to-text application focused on documents.\",\"id\":\"stepfun-ai/GOT_official_online_demo\"},{\"description\":\"An
        application for chatting with an image-text-to-text model.\",\"id\":\"GanymedeNil/Qwen2-VL-7B\"},{\"description\":\"An
        application that parses screenshots into actions.\",\"id\":\"showlab/ShowUI\"},{\"description\":\"An
        application that detects gaze.\",\"id\":\"moondream/gaze-demo\"}],\"summary\":\"Image-text-to-text
        models take in an image and text prompt and output text. These models are
        also called vision-language models, or VLMs. The difference from image-to-text
        models is that these models take an additional text input, not restricting
        the model to certain use cases like image captioning, and may also be trained
        to accept a conversation as input.\",\"widgetModels\":[\"Qwen/Qwen2-VL-7B-Instruct\"],\"youtubeId\":\"IoGaGfU1CIg\",\"id\":\"image-text-to-text\",\"label\":\"Image-Text-to-Text\",\"libraries\":[\"transformers\"]},\"image-to-text\":{\"datasets\":[{\"description\":\"Dataset
        from 12M image-text of Reddit\",\"id\":\"red_caps\"},{\"description\":\"Dataset
        from 3.3M images of Google\",\"id\":\"datasets/conceptual_captions\"}],\"demo\":{\"inputs\":[{\"filename\":\"savanna.jpg\",\"type\":\"img\"}],\"outputs\":[{\"label\":\"Detailed
        description\",\"content\":\"a herd of giraffes and zebras grazing in a field\",\"type\":\"text\"}]},\"metrics\":[],\"models\":[{\"description\":\"A
        robust image captioning model.\",\"id\":\"Salesforce/blip2-opt-2.7b\"},{\"description\":\"A
        powerful and accurate image-to-text model that can also localize concepts
        in images.\",\"id\":\"microsoft/kosmos-2-patch14-224\"},{\"description\":\"A
        strong optical character recognition model.\",\"id\":\"facebook/nougat-base\"},{\"description\":\"A
        powerful model that lets you have a conversation with the image.\",\"id\":\"llava-hf/llava-1.5-7b-hf\"}],\"spaces\":[{\"description\":\"An
        application that compares various image captioning models.\",\"id\":\"nielsr/comparing-captioning-models\"},{\"description\":\"A
        robust image captioning application.\",\"id\":\"flax-community/image-captioning\"},{\"description\":\"An
        application that transcribes handwritings into text.\",\"id\":\"nielsr/TrOCR-handwritten\"},{\"description\":\"An
        application that can caption images and answer questions about a given image.\",\"id\":\"Salesforce/BLIP\"},{\"description\":\"An
        application that can caption images and answer questions with a conversational
        agent.\",\"id\":\"Salesforce/BLIP2\"},{\"description\":\"An image captioning
        application that demonstrates the effect of noise on captions.\",\"id\":\"johko/capdec-image-captioning\"}],\"summary\":\"Image
        to text models output a text from a given image. Image captioning or optical
        character recognition can be considered as the most common applications of
        image to text.\",\"widgetModels\":[\"Salesforce/blip-image-captioning-large\"],\"youtubeId\":\"\",\"id\":\"image-to-text\",\"label\":\"Image-to-Text\",\"libraries\":[\"transformers\",\"transformers.js\"]},\"keypoint-detection\":{\"datasets\":[{\"description\":\"A
        dataset of hand keypoints of over 500k examples.\",\"id\":\"Vincent-luo/hagrid-mediapipe-hands\"}],\"demo\":{\"inputs\":[{\"filename\":\"keypoint-detection-input.png\",\"type\":\"img\"}],\"outputs\":[{\"filename\":\"keypoint-detection-output.png\",\"type\":\"img\"}]},\"metrics\":[],\"models\":[{\"description\":\"A
        robust keypoint detection model.\",\"id\":\"magic-leap-community/superpoint\"},{\"description\":\"Strong
        keypoint detection model used to detect human pose.\",\"id\":\"facebook/sapiens-pose-1b\"},{\"description\":\"Powerful
        keypoint detection model used to detect human pose.\",\"id\":\"usyd-community/vitpose-plus-base\"}],\"spaces\":[{\"description\":\"An
        application that detects hand keypoints in real-time.\",\"id\":\"datasciencedojo/Hand-Keypoint-Detection-Realtime\"},{\"description\":\"An
        application to try a universal keypoint detection model.\",\"id\":\"merve/SuperPoint\"}],\"summary\":\"Keypoint
        detection is the task of identifying meaningful distinctive points or features
        in an image.\",\"widgetModels\":[],\"youtubeId\":\"\",\"id\":\"keypoint-detection\",\"label\":\"Keypoint
        Detection\",\"libraries\":[\"transformers\"]},\"mask-generation\":{\"datasets\":[{\"description\":\"Widely
        used benchmark dataset for multiple Vision tasks.\",\"id\":\"merve/coco2017\"},{\"description\":\"Medical
        Imaging dataset of the Human Brain for segmentation and mask generating tasks\",\"id\":\"rocky93/BraTS_segmentation\"}],\"demo\":{\"inputs\":[{\"filename\":\"mask-generation-input.png\",\"type\":\"img\"}],\"outputs\":[{\"filename\":\"mask-generation-output.png\",\"type\":\"img\"}]},\"metrics\":[{\"description\":\"IoU
        is used to measure the overlap between predicted mask and the ground truth
        mask.\",\"id\":\"Intersection over Union (IoU)\"}],\"models\":[{\"description\":\"Small
        yet powerful mask generation model.\",\"id\":\"Zigeng/SlimSAM-uniform-50\"},{\"description\":\"Very
        strong mask generation model.\",\"id\":\"facebook/sam2-hiera-large\"}],\"spaces\":[{\"description\":\"An
        application that combines a mask generation model with a zero-shot object
        detection model for text-guided image segmentation.\",\"id\":\"merve/OWLSAM2\"},{\"description\":\"An
        application that compares the performance of a large and a small mask generation
        model.\",\"id\":\"merve/slimsam\"},{\"description\":\"An application based
        on an improved mask generation model.\",\"id\":\"SkalskiP/segment-anything-model-2\"},{\"description\":\"An
        application to remove objects from videos using mask generation models.\",\"id\":\"SkalskiP/SAM_and_ProPainter\"}],\"summary\":\"Mask
        generation is the task of generating masks that identify a specific object
        or region of interest in a given image. Masks are often used in segmentation
        tasks, where they provide a precise way to isolate the object of interest
        for further processing or analysis.\",\"widgetModels\":[],\"youtubeId\":\"\",\"id\":\"mask-generation\",\"label\":\"Mask
        Generation\",\"libraries\":[\"transformers\"]},\"object-detection\":{\"datasets\":[{\"description\":\"Widely
        used benchmark dataset for multiple vision tasks.\",\"id\":\"merve/coco2017\"},{\"description\":\"Multi-task
        computer vision benchmark.\",\"id\":\"merve/pascal-voc\"}],\"demo\":{\"inputs\":[{\"filename\":\"object-detection-input.jpg\",\"type\":\"img\"}],\"outputs\":[{\"filename\":\"object-detection-output.jpg\",\"type\":\"img\"}]},\"metrics\":[{\"description\":\"The
        Average Precision (AP) metric is the Area Under the PR Curve (AUC-PR). It
        is calculated for each class separately\",\"id\":\"Average Precision\"},{\"description\":\"The
        Mean Average Precision (mAP) metric is the overall average of the AP values\",\"id\":\"Mean
        Average Precision\"},{\"description\":\"The AP\u03B1 metric is the Average
        Precision at the IoU threshold of a \u03B1 value, for example, AP50 and AP75\",\"id\":\"AP\u03B1\"}],\"models\":[{\"description\":\"Solid
        object detection model pre-trained on the COCO 2017 dataset.\",\"id\":\"facebook/detr-resnet-50\"},{\"description\":\"Real-time
        and accurate object detection model.\",\"id\":\"jameslahm/yolov10x\"},{\"description\":\"Fast
        and accurate object detection model trained on COCO and Object365 datasets.\",\"id\":\"PekingU/rtdetr_r18vd_coco_o365\"},{\"description\":\"Object
        detection model for low-lying objects.\",\"id\":\"StephanST/WALDO30\"}],\"spaces\":[{\"description\":\"Leaderboard
        to compare various object detection models across several metrics.\",\"id\":\"hf-vision/object_detection_leaderboard\"},{\"description\":\"An
        application that contains various object detection models to try from.\",\"id\":\"Gradio-Blocks/Object-Detection-With-DETR-and-YOLOS\"},{\"description\":\"A
        cutting-edge object detection application.\",\"id\":\"Ultralytics/YOLO11\"},{\"description\":\"An
        object tracking, segmentation and inpainting application.\",\"id\":\"VIPLab/Track-Anything\"},{\"description\":\"Very
        fast object tracking application based on object detection.\",\"id\":\"merve/RT-DETR-tracking-coco\"}],\"summary\":\"Object
        Detection models allow users to identify objects of certain defined classes.
        Object detection models receive an image as input and output the images with
        bounding boxes and labels on detected objects.\",\"widgetModels\":[\"facebook/detr-resnet-50\"],\"youtubeId\":\"WdAeKSOpxhw\",\"id\":\"object-detection\",\"label\":\"Object
        Detection\",\"libraries\":[\"transformers\",\"transformers.js\",\"ultralytics\"]},\"video-classification\":{\"datasets\":[{\"description\":\"Benchmark
        dataset used for video classification with videos that belong to 400 classes.\",\"id\":\"kinetics400\"}],\"demo\":{\"inputs\":[{\"filename\":\"video-classification-input.gif\",\"type\":\"img\"}],\"outputs\":[{\"type\":\"chart\",\"data\":[{\"label\":\"Playing
        Guitar\",\"score\":0.514},{\"label\":\"Playing Tennis\",\"score\":0.193},{\"label\":\"Cooking\",\"score\":0.068}]}]},\"metrics\":[{\"description\":\"\",\"id\":\"accuracy\"},{\"description\":\"\",\"id\":\"recall\"},{\"description\":\"\",\"id\":\"precision\"},{\"description\":\"\",\"id\":\"f1\"}],\"models\":[{\"description\":\"Strong
        Video Classification model trained on the Kinetics 400 dataset.\",\"id\":\"google/vivit-b-16x2-kinetics400\"},{\"description\":\"Strong
        Video Classification model trained on the Kinetics 400 dataset.\",\"id\":\"microsoft/xclip-base-patch32\"}],\"spaces\":[{\"description\":\"An
        application that classifies video at different timestamps.\",\"id\":\"nateraw/lavila\"},{\"description\":\"An
        application that classifies video.\",\"id\":\"fcakyon/video-classification\"}],\"summary\":\"Video
        classification is the task of assigning a label or class to an entire video.
        Videos are expected to have only one class for each video. Video classification
        models take a video as input and return a prediction about which class the
        video belongs to.\",\"widgetModels\":[],\"youtubeId\":\"\",\"id\":\"video-classification\",\"label\":\"Video
        Classification\",\"libraries\":[\"transformers\"]},\"question-answering\":{\"datasets\":[{\"description\":\"A
        famous question answering dataset based on English articles from Wikipedia.\",\"id\":\"squad_v2\"},{\"description\":\"A
        dataset of aggregated anonymized actual queries issued to the Google search
        engine.\",\"id\":\"natural_questions\"}],\"demo\":{\"inputs\":[{\"label\":\"Question\",\"content\":\"Which
        name is also used to describe the Amazon rainforest in English?\",\"type\":\"text\"},{\"label\":\"Context\",\"content\":\"The
        Amazon rainforest, also known in English as Amazonia or the Amazon Jungle\",\"type\":\"text\"}],\"outputs\":[{\"label\":\"Answer\",\"content\":\"Amazonia\",\"type\":\"text\"}]},\"metrics\":[{\"description\":\"Exact
        Match is a metric based on the strict character match of the predicted answer
        and the right answer. For answers predicted correctly, the Exact Match will
        be 1. Even if only one character is different, Exact Match will be 0\",\"id\":\"exact-match\"},{\"description\":\"
        The F1-Score metric is useful if we value both false positives and false negatives
        equally. The F1-Score is calculated on each word in the predicted sequence
        against the correct answer\",\"id\":\"f1\"}],\"models\":[{\"description\":\"A
        robust baseline model for most question answering domains.\",\"id\":\"deepset/roberta-base-squad2\"},{\"description\":\"Small
        yet robust model that can answer questions.\",\"id\":\"distilbert/distilbert-base-cased-distilled-squad\"},{\"description\":\"A
        special model that can answer questions from tables.\",\"id\":\"google/tapas-base-finetuned-wtq\"}],\"spaces\":[{\"description\":\"An
        application that can answer a long question from Wikipedia.\",\"id\":\"deepset/wikipedia-assistant\"}],\"summary\":\"Question
        Answering models can retrieve the answer to a question from a given text,
        which is useful for searching for an answer in a document. Some question answering
        models can generate answers without context!\",\"widgetModels\":[\"deepset/roberta-base-squad2\"],\"youtubeId\":\"ajPx5LwJD-I\",\"id\":\"question-answering\",\"label\":\"Question
        Answering\",\"libraries\":[\"adapter-transformers\",\"allennlp\",\"transformers\",\"transformers.js\"]},\"reinforcement-learning\":{\"datasets\":[{\"description\":\"A
        curation of widely used datasets for Data Driven Deep Reinforcement Learning
        (D4RL)\",\"id\":\"edbeeching/decision_transformer_gym_replay\"}],\"demo\":{\"inputs\":[{\"label\":\"State\",\"content\":\"Red
        traffic light, pedestrians are about to pass.\",\"type\":\"text\"}],\"outputs\":[{\"label\":\"Action\",\"content\":\"Stop
        the car.\",\"type\":\"text\"},{\"label\":\"Next State\",\"content\":\"Yellow
        light, pedestrians have crossed.\",\"type\":\"text\"}]},\"metrics\":[{\"description\":\"Accumulated
        reward across all time steps discounted by a factor that ranges between 0
        and 1 and determines how much the agent optimizes for future relative to immediate
        rewards. Measures how good is the policy ultimately found by a given algorithm
        considering uncertainty over the future.\",\"id\":\"Discounted Total Reward\"},{\"description\":\"Average
        return obtained after running the policy for a certain number of evaluation
        episodes. As opposed to total reward, mean reward considers how much reward
        a given algorithm receives while learning.\",\"id\":\"Mean Reward\"},{\"description\":\"Measures
        how good a given algorithm is after a predefined time. Some algorithms may
        be guaranteed to converge to optimal behavior across many time steps. However,
        an agent that reaches an acceptable level of optimality after a given time
        horizon may be preferable to one that ultimately reaches optimality but takes
        a long time.\",\"id\":\"Level of Performance After Some Time\"}],\"models\":[{\"description\":\"A
        Reinforcement Learning model trained on expert data from the Gym Hopper environment\",\"id\":\"edbeeching/decision-transformer-gym-hopper-expert\"},{\"description\":\"A
        PPO agent playing seals/CartPole-v0 using the stable-baselines3 library and
        the RL Zoo.\",\"id\":\"HumanCompatibleAI/ppo-seals-CartPole-v0\"}],\"spaces\":[{\"description\":\"An
        application for a cute puppy agent learning to catch a stick.\",\"id\":\"ThomasSimonini/Huggy\"},{\"description\":\"An
        application to play Snowball Fight with a reinforcement learning agent.\",\"id\":\"ThomasSimonini/SnowballFight\"}],\"summary\":\"Reinforcement
        learning is the computational approach of learning from action by interacting
        with an environment through trial and error and receiving rewards (negative
        or positive) as feedback\",\"widgetModels\":[],\"youtubeId\":\"q0BiUn5LiBc\",\"id\":\"reinforcement-learning\",\"label\":\"Reinforcement
        Learning\",\"libraries\":[\"transformers\",\"stable-baselines3\",\"ml-agents\",\"sample-factory\"]},\"sentence-similarity\":{\"datasets\":[{\"description\":\"Bing
        queries with relevant passages from various web sources.\",\"id\":\"ms_marco\"}],\"demo\":{\"inputs\":[{\"label\":\"Source
        sentence\",\"content\":\"Machine learning is so easy.\",\"type\":\"text\"},{\"label\":\"Sentences
        to compare to\",\"content\":\"Deep learning is so straightforward.\",\"type\":\"text\"},{\"label\":\"\",\"content\":\"This
        is so difficult, like rocket science.\",\"type\":\"text\"},{\"label\":\"\",\"content\":\"I
        can't believe how much I struggled with this.\",\"type\":\"text\"}],\"outputs\":[{\"type\":\"chart\",\"data\":[{\"label\":\"Deep
        learning is so straightforward.\",\"score\":0.623},{\"label\":\"This is so
        difficult, like rocket science.\",\"score\":0.413},{\"label\":\"I can't believe
        how much I struggled with this.\",\"score\":0.256}]}]},\"metrics\":[{\"description\":\"Reciprocal
        Rank is a measure used to rank the relevancy of documents given a set of documents.
        Reciprocal Rank is the reciprocal of the rank of the document retrieved, meaning,
        if the rank is 3, the Reciprocal Rank is 0.33. If the rank is 1, the Reciprocal
        Rank is 1\",\"id\":\"Mean Reciprocal Rank\"},{\"description\":\"The similarity
        of the embeddings is evaluated mainly on cosine similarity. It is calculated
        as the cosine of the angle between two vectors. It is particularly useful
        when your texts are not the same length\",\"id\":\"Cosine Similarity\"}],\"models\":[{\"description\":\"This
        model works well for sentences and paragraphs and can be used for clustering/grouping
        and semantic searches.\",\"id\":\"sentence-transformers/all-mpnet-base-v2\"},{\"description\":\"A
        multilingual robust sentence similarity model.\",\"id\":\"BAAI/bge-m3\"},{\"description\":\"A
        robust sentence similarity model.\",\"id\":\"HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1.5\"}],\"spaces\":[{\"description\":\"An
        application that leverages sentence similarity to answer questions from YouTube
        videos.\",\"id\":\"Gradio-Blocks/Ask_Questions_To_YouTube_Videos\"},{\"description\":\"An
        application that retrieves relevant PubMed abstracts for a given online article
        which can be used as further references.\",\"id\":\"Gradio-Blocks/pubmed-abstract-retriever\"},{\"description\":\"An
        application that leverages sentence similarity to summarize text.\",\"id\":\"nickmuchi/article-text-summarizer\"},{\"description\":\"A
        guide that explains how Sentence Transformers can be used for semantic search.\",\"id\":\"sentence-transformers/Sentence_Transformers_for_semantic_search\"}],\"summary\":\"Sentence
        Similarity is the task of determining how similar two texts are. Sentence
        similarity models convert input texts into vectors (embeddings) that capture
        semantic information and calculate how close (similar) they are between them.
        This task is particularly useful for information retrieval and clustering/grouping.\",\"widgetModels\":[\"BAAI/bge-small-en-v1.5\"],\"youtubeId\":\"VCZq5AkbNEU\",\"id\":\"sentence-similarity\",\"label\":\"Sentence
        Similarity\",\"libraries\":[\"sentence-transformers\",\"spacy\",\"transformers.js\"]},\"summarization\":{\"canonicalId\":\"text2text-generation\",\"datasets\":[{\"description\":\"News
        articles in five different languages along with their summaries. Widely used
        for benchmarking multilingual summarization models.\",\"id\":\"mlsum\"},{\"description\":\"English
        conversations and their summaries. Useful for benchmarking conversational
        agents.\",\"id\":\"samsum\"}],\"demo\":{\"inputs\":[{\"label\":\"Input\",\"content\":\"The
        tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey
        building, and the tallest structure in Paris. Its base is square, measuring
        125 metres (410 ft) on each side. It was the first structure to reach a height
        of 300 metres. Excluding transmitters, the Eiffel Tower is the second tallest
        free-standing structure in France after the Millau Viaduct.\",\"type\":\"text\"}],\"outputs\":[{\"label\":\"Output\",\"content\":\"The
        tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey
        building. It was the first structure to reach a height of 300 metres.\",\"type\":\"text\"}]},\"metrics\":[{\"description\":\"The
        generated sequence is compared against its summary, and the overlap of tokens
        are counted. ROUGE-N refers to overlap of N subsequent tokens, ROUGE-1 refers
        to overlap of single tokens and ROUGE-2 is the overlap of two subsequent tokens.\",\"id\":\"rouge\"}],\"models\":[{\"description\":\"A
        strong summarization model trained on English news articles. Excels at generating
        factual summaries.\",\"id\":\"facebook/bart-large-cnn\"},{\"description\":\"A
        summarization model trained on medical articles.\",\"id\":\"Falconsai/medical_summarization\"}],\"spaces\":[{\"description\":\"An
        application that can summarize long paragraphs.\",\"id\":\"pszemraj/summarize-long-text\"},{\"description\":\"A
        much needed summarization application for terms and conditions.\",\"id\":\"ml6team/distilbart-tos-summarizer-tosdr\"},{\"description\":\"An
        application that summarizes long documents.\",\"id\":\"pszemraj/document-summarization\"},{\"description\":\"An
        application that can detect errors in abstractive summarization.\",\"id\":\"ml6team/post-processing-summarization\"}],\"summary\":\"Summarization
        is the task of producing a shorter version of a document while preserving
        its important information. Some models can extract text from the original
        input, while other models can generate entirely new text.\",\"widgetModels\":[\"facebook/bart-large-cnn\"],\"youtubeId\":\"yHnr5Dk2zCI\",\"id\":\"summarization\",\"label\":\"Summarization\",\"libraries\":[\"transformers\",\"transformers.js\"]},\"table-question-answering\":{\"datasets\":[{\"description\":\"The
        WikiTableQuestions dataset is a large-scale dataset for the task of question
        answering on semi-structured tables.\",\"id\":\"wikitablequestions\"},{\"description\":\"WikiSQL
        is a dataset of 80654 hand-annotated examples of questions and SQL queries
        distributed across 24241 tables from Wikipedia.\",\"id\":\"wikisql\"}],\"demo\":{\"inputs\":[{\"table\":[[\"Rank\",\"Name\",\"No.of
        reigns\",\"Combined days\"],[\"1\",\"lou Thesz\",\"3\",\"3749\"],[\"2\",\"Ric
        Flair\",\"8\",\"3103\"],[\"3\",\"Harley Race\",\"7\",\"1799\"]],\"type\":\"tabular\"},{\"label\":\"Question\",\"content\":\"What
        is the number of reigns for Harley Race?\",\"type\":\"text\"}],\"outputs\":[{\"label\":\"Result\",\"content\":\"7\",\"type\":\"text\"}]},\"metrics\":[{\"description\":\"Checks
        whether the predicted answer(s) is the same as the ground-truth answer(s).\",\"id\":\"Denotation
        Accuracy\"}],\"models\":[{\"description\":\"A table question answering model
        that is capable of neural SQL execution, i.e., employ TAPEX to execute a SQL
        query on a given table.\",\"id\":\"microsoft/tapex-base\"},{\"description\":\"A
        robust table question answering model.\",\"id\":\"google/tapas-base-finetuned-wtq\"}],\"spaces\":[{\"description\":\"An
        application that answers questions based on table CSV files.\",\"id\":\"katanaml/table-query\"}],\"summary\":\"Table
        Question Answering (Table QA) is the answering a question about an information
        on a given table.\",\"widgetModels\":[\"google/tapas-base-finetuned-wtq\"],\"id\":\"table-question-answering\",\"label\":\"Table
        Question Answering\",\"libraries\":[\"transformers\"]},\"tabular-classification\":{\"datasets\":[{\"description\":\"A
        comprehensive curation of datasets covering all benchmarks.\",\"id\":\"inria-soda/tabular-benchmark\"}],\"demo\":{\"inputs\":[{\"table\":[[\"Glucose\",\"Blood
        Pressure \",\"Skin Thickness\",\"Insulin\",\"BMI\"],[\"148\",\"72\",\"35\",\"0\",\"33.6\"],[\"150\",\"50\",\"30\",\"0\",\"35.1\"],[\"141\",\"60\",\"29\",\"1\",\"39.2\"]],\"type\":\"tabular\"}],\"outputs\":[{\"table\":[[\"Diabetes\"],[\"1\"],[\"1\"],[\"0\"]],\"type\":\"tabular\"}]},\"metrics\":[{\"description\":\"\",\"id\":\"accuracy\"},{\"description\":\"\",\"id\":\"recall\"},{\"description\":\"\",\"id\":\"precision\"},{\"description\":\"\",\"id\":\"f1\"}],\"models\":[{\"description\":\"Breast
        cancer prediction model based on decision trees.\",\"id\":\"scikit-learn/cancer-prediction-trees\"}],\"spaces\":[{\"description\":\"An
        application that can predict defective products on a production line.\",\"id\":\"scikit-learn/tabular-playground\"},{\"description\":\"An
        application that compares various tabular classification techniques on different
        datasets.\",\"id\":\"scikit-learn/classification\"}],\"summary\":\"Tabular
        classification is the task of classifying a target category (a group) based
        on set of attributes.\",\"widgetModels\":[\"scikit-learn/tabular-playground\"],\"youtubeId\":\"\",\"id\":\"tabular-classification\",\"label\":\"Tabular
        Classification\",\"libraries\":[\"sklearn\"]},\"tabular-regression\":{\"datasets\":[{\"description\":\"A
        comprehensive curation of datasets covering all benchmarks.\",\"id\":\"inria-soda/tabular-benchmark\"}],\"demo\":{\"inputs\":[{\"table\":[[\"Car
        Name\",\"Horsepower\",\"Weight\"],[\"ford torino\",\"140\",\"3,449\"],[\"amc
        hornet\",\"97\",\"2,774\"],[\"toyota corolla\",\"65\",\"1,773\"]],\"type\":\"tabular\"}],\"outputs\":[{\"table\":[[\"MPG
        (miles per gallon)\"],[\"17\"],[\"18\"],[\"31\"]],\"type\":\"tabular\"}]},\"metrics\":[{\"description\":\"\",\"id\":\"mse\"},{\"description\":\"Coefficient
        of determination (or R-squared) is a measure of how well the model fits the
        data. Higher R-squared is considered a better fit.\",\"id\":\"r-squared\"}],\"models\":[{\"description\":\"Fish
        weight prediction based on length measurements and species.\",\"id\":\"scikit-learn/Fish-Weight\"}],\"spaces\":[{\"description\":\"An
        application that can predict weight of a fish based on set of attributes.\",\"id\":\"scikit-learn/fish-weight-prediction\"}],\"summary\":\"Tabular
        regression is the task of predicting a numerical value given a set of attributes.\",\"widgetModels\":[\"scikit-learn/Fish-Weight\"],\"youtubeId\":\"\",\"id\":\"tabular-regression\",\"label\":\"Tabular
        Regression\",\"libraries\":[\"sklearn\"]},\"text-classification\":{\"datasets\":[{\"description\":\"A
        widely used dataset used to benchmark multiple variants of text classification.\",\"id\":\"nyu-mll/glue\"},{\"description\":\"A
        text classification dataset used to benchmark natural language inference models\",\"id\":\"stanfordnlp/snli\"}],\"demo\":{\"inputs\":[{\"label\":\"Input\",\"content\":\"I
        love Hugging Face!\",\"type\":\"text\"}],\"outputs\":[{\"type\":\"chart\",\"data\":[{\"label\":\"POSITIVE\",\"score\":0.9},{\"label\":\"NEUTRAL\",\"score\":0.1},{\"label\":\"NEGATIVE\",\"score\":0}]}]},\"metrics\":[{\"description\":\"\",\"id\":\"accuracy\"},{\"description\":\"\",\"id\":\"recall\"},{\"description\":\"\",\"id\":\"precision\"},{\"description\":\"The
        F1 metric is the harmonic mean of the precision and recall. It can be calculated
        as: F1 = 2 * (precision * recall) / (precision + recall)\",\"id\":\"f1\"}],\"models\":[{\"description\":\"A
        robust model trained for sentiment analysis.\",\"id\":\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"},{\"description\":\"A
        sentiment analysis model specialized in financial sentiment.\",\"id\":\"ProsusAI/finbert\"},{\"description\":\"A
        sentiment analysis model specialized in analyzing tweets.\",\"id\":\"cardiffnlp/twitter-roberta-base-sentiment-latest\"},{\"description\":\"A
        model that can classify languages.\",\"id\":\"papluca/xlm-roberta-base-language-detection\"},{\"description\":\"A
        model that can classify text generation attacks.\",\"id\":\"meta-llama/Prompt-Guard-86M\"}],\"spaces\":[{\"description\":\"An
        application that can classify financial sentiment.\",\"id\":\"IoannisTr/Tech_Stocks_Trading_Assistant\"},{\"description\":\"A
        dashboard that contains various text classification tasks.\",\"id\":\"miesnerjacob/Multi-task-NLP\"},{\"description\":\"An
        application that analyzes user reviews in healthcare.\",\"id\":\"spacy/healthsea-demo\"}],\"summary\":\"Text
        Classification is the task of assigning a label or class to a given text.
        Some use cases are sentiment analysis, natural language inference, and assessing
        grammatical correctness.\",\"widgetModels\":[\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"],\"youtubeId\":\"leNG9fN9FQU\",\"id\":\"text-classification\",\"label\":\"Text
        Classification\",\"libraries\":[\"adapter-transformers\",\"setfit\",\"spacy\",\"transformers\",\"transformers.js\"]},\"text-generation\":{\"datasets\":[{\"description\":\"Multilingual
        dataset used to evaluate text generation models.\",\"id\":\"CohereForAI/Global-MMLU\"},{\"description\":\"High
        quality multilingual data used to train text-generation models.\",\"id\":\"HuggingFaceFW/fineweb-2\"},{\"description\":\"Truly
        open-source, curated and cleaned dialogue dataset.\",\"id\":\"HuggingFaceH4/ultrachat_200k\"},{\"description\":\"A
        reasoning dataset.\",\"id\":\"open-r1/OpenThoughts-114k-math\"},{\"description\":\"A
        multilingual instruction dataset with preference ratings on responses.\",\"id\":\"allenai/tulu-3-sft-mixture\"},{\"description\":\"A
        large synthetic dataset for alignment of text generation models.\",\"id\":\"HuggingFaceTB/smoltalk\"},{\"description\":\"A
        dataset made for training text generation models solving math questions.\",\"id\":\"HuggingFaceTB/finemath\"}],\"demo\":{\"inputs\":[{\"label\":\"Input\",\"content\":\"Once
        upon a time,\",\"type\":\"text\"}],\"outputs\":[{\"label\":\"Output\",\"content\":\"Once
        upon a time, we knew that our ancestors were on the verge of extinction. The
        great explorers and poets of the Old World, from Alexander the Great to Chaucer,
        are dead and gone. A good many of our ancient explorers and poets have\",\"type\":\"text\"}]},\"metrics\":[{\"description\":\"Cross
        Entropy is a metric that calculates the difference between two probability
        distributions. Each probability distribution is the distribution of predicted
        words\",\"id\":\"Cross Entropy\"},{\"description\":\"The Perplexity metric
        is the exponential of the cross-entropy loss. It evaluates the probabilities
        assigned to the next word by the model. Lower perplexity indicates better
        performance\",\"id\":\"Perplexity\"}],\"models\":[{\"description\":\"A text-generation
        model trained to follow instructions.\",\"id\":\"google/gemma-2-2b-it\"},{\"description\":\"Smaller
        variant of one of the most powerful models.\",\"id\":\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"},{\"description\":\"Very
        powerful text generation model trained to follow instructions.\",\"id\":\"meta-llama/Meta-Llama-3.1-8B-Instruct\"},{\"description\":\"Powerful
        text generation model by Microsoft.\",\"id\":\"microsoft/phi-4\"},{\"description\":\"A
        very powerful model with reasoning capabilities.\",\"id\":\"PowerInfer/SmallThinker-3B-Preview\"},{\"description\":\"Strong
        conversational model that supports very long instructions.\",\"id\":\"Qwen/Qwen2.5-7B-Instruct-1M\"},{\"description\":\"Text
        generation model used to write code.\",\"id\":\"Qwen/Qwen2.5-Coder-32B-Instruct\"},{\"description\":\"Powerful
        reasoning based open large language model.\",\"id\":\"deepseek-ai/DeepSeek-R1\"}],\"spaces\":[{\"description\":\"A
        leaderboard to compare different open-source text generation models based
        on various benchmarks.\",\"id\":\"open-llm-leaderboard/open_llm_leaderboard\"},{\"description\":\"A
        leaderboard for comparing chain-of-thought performance of models.\",\"id\":\"logikon/open_cot_leaderboard\"},{\"description\":\"An
        text generation based application based on a very powerful LLaMA2 model.\",\"id\":\"ysharma/Explore_llamav2_with_TGI\"},{\"description\":\"An
        text generation based application to converse with Zephyr model.\",\"id\":\"HuggingFaceH4/zephyr-chat\"},{\"description\":\"A
        leaderboard that ranks text generation models based on blind votes from people.\",\"id\":\"lmsys/chatbot-arena-leaderboard\"},{\"description\":\"An
        chatbot to converse with a very powerful text generation model.\",\"id\":\"mlabonne/phixtral-chat\"}],\"summary\":\"Generating
        text is the task of generating new text given another text. These models can,
        for example, fill in incomplete text or paraphrase.\",\"widgetModels\":[\"mistralai/Mistral-Nemo-Instruct-2407\"],\"youtubeId\":\"e9gNEAlsOvU\",\"id\":\"text-generation\",\"label\":\"Text
        Generation\",\"libraries\":[\"transformers\",\"transformers.js\"]},\"text-to-image\":{\"datasets\":[{\"description\":\"RedCaps
        is a large-scale dataset of 12M image-text pairs collected from Reddit.\",\"id\":\"red_caps\"},{\"description\":\"Conceptual
        Captions is a dataset consisting of ~3.3M images annotated with captions.\",\"id\":\"conceptual_captions\"},{\"description\":\"12M
        image-caption pairs.\",\"id\":\"Spawning/PD12M\"}],\"demo\":{\"inputs\":[{\"label\":\"Input\",\"content\":\"A
        city above clouds, pastel colors, Victorian style\",\"type\":\"text\"}],\"outputs\":[{\"filename\":\"image.jpeg\",\"type\":\"img\"}]},\"metrics\":[{\"description\":\"The
        Inception Score (IS) measure assesses diversity and meaningfulness. It uses
        a generated image sample to predict its label. A higher score signifies more
        diverse and meaningful images.\",\"id\":\"IS\"},{\"description\":\"The Fr\xE9chet
        Inception Distance (FID) calculates the distance between distributions between
        synthetic and real samples. A lower FID score indicates better similarity
        between the distributions of real and generated images.\",\"id\":\"FID\"},{\"description\":\"R-precision
        assesses how the generated image aligns with the provided text description.
        It uses the generated images as queries to retrieve relevant text descriptions.
        The top 'r' relevant descriptions are selected and used to calculate R-precision
        as r/R, where 'R' is the number of ground truth descriptions associated with
        the generated images. A higher R-precision value indicates a better model.\",\"id\":\"R-Precision\"}],\"models\":[{\"description\":\"One
        of the most powerful image generation models that can generate realistic outputs.\",\"id\":\"black-forest-labs/FLUX.1-dev\"},{\"description\":\"A
        powerful yet fast image generation model.\",\"id\":\"latent-consistency/lcm-lora-sdxl\"},{\"description\":\"Text-to-image
        model for photorealistic generation.\",\"id\":\"Kwai-Kolors/Kolors\"},{\"description\":\"A
        powerful text-to-image model.\",\"id\":\"stabilityai/stable-diffusion-3-medium-diffusers\"}],\"spaces\":[{\"description\":\"A
        powerful text-to-image application.\",\"id\":\"stabilityai/stable-diffusion-3-medium\"},{\"description\":\"A
        text-to-image application to generate comics.\",\"id\":\"jbilcke-hf/ai-comic-factory\"},{\"description\":\"An
        application to match multiple custom image generation models.\",\"id\":\"multimodalart/flux-lora-lab\"},{\"description\":\"A
        powerful yet very fast image generation application.\",\"id\":\"latent-consistency/lcm-lora-for-sdxl\"},{\"description\":\"A
        gallery to explore various text-to-image models.\",\"id\":\"multimodalart/LoraTheExplorer\"},{\"description\":\"An
        application for `text-to-image`, `image-to-image` and image inpainting.\",\"id\":\"ArtGAN/Stable-Diffusion-ControlNet-WebUI\"},{\"description\":\"An
        application to generate realistic images given photos of a person and a prompt.\",\"id\":\"InstantX/InstantID\"}],\"summary\":\"Text-to-image
        is the task of generating images from input text. These pipelines can also
        be used to modify and edit images based on text prompts.\",\"widgetModels\":[\"black-forest-labs/FLUX.1-dev\"],\"youtubeId\":\"\",\"id\":\"text-to-image\",\"label\":\"Text-to-Image\",\"libraries\":[\"diffusers\"]},\"text-to-speech\":{\"canonicalId\":\"text-to-audio\",\"datasets\":[{\"description\":\"10K
        hours of multi-speaker English dataset.\",\"id\":\"parler-tts/mls_eng_10k\"},{\"description\":\"Multi-speaker
        English dataset.\",\"id\":\"mythicinfinity/libritts_r\"},{\"description\":\"Multi-lingual
        dataset.\",\"id\":\"facebook/multilingual_librispeech\"}],\"demo\":{\"inputs\":[{\"label\":\"Input\",\"content\":\"I
        love audio models on the Hub!\",\"type\":\"text\"}],\"outputs\":[{\"filename\":\"audio.wav\",\"type\":\"audio\"}]},\"metrics\":[{\"description\":\"The
        Mel Cepstral Distortion (MCD) metric is used to calculate the quality of generated
        speech.\",\"id\":\"mel cepstral distortion\"}],\"models\":[{\"description\":\"A
        prompt based, powerful TTS model.\",\"id\":\"parler-tts/parler-tts-large-v1\"},{\"description\":\"A
        powerful TTS model that supports English and Chinese.\",\"id\":\"SWivid/F5-TTS\"},{\"description\":\"A
        massively multi-lingual TTS model.\",\"id\":\"fishaudio/fish-speech-1.5\"},{\"description\":\"A
        powerful TTS model.\",\"id\":\"OuteAI/OuteTTS-0.1-350M\"},{\"description\":\"Small
        yet powerful TTS model.\",\"id\":\"hexgrad/Kokoro-82M\"}],\"spaces\":[{\"description\":\"An
        application for generate high quality speech in different languages.\",\"id\":\"hexgrad/Kokoro-TTS\"},{\"description\":\"A
        multilingual text-to-speech application.\",\"id\":\"fishaudio/fish-speech-1\"},{\"description\":\"An
        application that generates speech in different styles in English and Chinese.\",\"id\":\"mrfakename/E2-F5-TTS\"},{\"description\":\"An
        application that synthesizes emotional speech for diverse speaker prompts.\",\"id\":\"parler-tts/parler-tts-expresso\"}],\"summary\":\"Text-to-Speech
        (TTS) is the task of generating natural sounding speech given text input.
        TTS models can be extended to have a single model that generates speech for
        multiple speakers and multiple languages.\",\"widgetModels\":[\"suno/bark\"],\"youtubeId\":\"NW62DpzJ274\",\"id\":\"text-to-speech\",\"label\":\"Text-to-Speech\",\"libraries\":[\"espnet\",\"tensorflowtts\",\"transformers\",\"transformers.js\"]},\"text-to-video\":{\"datasets\":[{\"description\":\"Microsoft
        Research Video to Text is a large-scale dataset for open domain video captioning\",\"id\":\"iejMac/CLIP-MSR-VTT\"},{\"description\":\"UCF101
        Human Actions dataset consists of 13,320 video clips from YouTube, with 101
        classes.\",\"id\":\"quchenyuan/UCF101-ZIP\"},{\"description\":\"A high-quality
        dataset for human action recognition in YouTube videos.\",\"id\":\"nateraw/kinetics\"},{\"description\":\"A
        dataset of video clips of humans performing pre-defined basic actions with
        everyday objects.\",\"id\":\"HuggingFaceM4/something_something_v2\"},{\"description\":\"This
        dataset consists of text-video pairs and contains noisy samples with irrelevant
        video descriptions\",\"id\":\"HuggingFaceM4/webvid\"},{\"description\":\"A
        dataset of short Flickr videos for the temporal localization of events with
        descriptions.\",\"id\":\"iejMac/CLIP-DiDeMo\"}],\"demo\":{\"inputs\":[{\"label\":\"Input\",\"content\":\"Darth
        Vader is surfing on the waves.\",\"type\":\"text\"}],\"outputs\":[{\"filename\":\"text-to-video-output.gif\",\"type\":\"img\"}]},\"metrics\":[{\"description\":\"Inception
        Score uses an image classification model that predicts class labels and evaluates
        how distinct and diverse the images are. A higher score indicates better video
        generation.\",\"id\":\"is\"},{\"description\":\"Frechet Inception Distance
        uses an image classification model to obtain image embeddings. The metric
        compares mean and standard deviation of the embeddings of real and generated
        images. A smaller score indicates better video generation.\",\"id\":\"fid\"},{\"description\":\"Frechet
        Video Distance uses a model that captures coherence for changes in frames
        and the quality of each frame. A smaller score indicates better video generation.\",\"id\":\"fvd\"},{\"description\":\"CLIPSIM
        measures similarity between video frames and text using an image-text similarity
        model. A higher score indicates better video generation.\",\"id\":\"clipsim\"}],\"models\":[{\"description\":\"A
        strong model for consistent video generation.\",\"id\":\"tencent/HunyuanVideo\"},{\"description\":\"A
        text-to-video model with high fidelity motion and strong prompt adherence.\",\"id\":\"Lightricks/LTX-Video\"},{\"description\":\"A
        text-to-video model focusing on physics-aware applications like robotics.\",\"id\":\"nvidia/Cosmos-1.0-Diffusion-7B-Text2World\"}],\"spaces\":[{\"description\":\"An
        application that generates video from text.\",\"id\":\"VideoCrafter/VideoCrafter\"},{\"description\":\"Consistent
        video generation application.\",\"id\":\"TIGER-Lab/T2V-Turbo-V2\"},{\"description\":\"A
        cutting edge video generation application.\",\"id\":\"Pyramid-Flow/pyramid-flow\"}],\"summary\":\"Text-to-video
        models can be used in any application that requires generating consistent
        sequence of images from text. \",\"widgetModels\":[],\"id\":\"text-to-video\",\"label\":\"Text-to-Video\",\"libraries\":[\"diffusers\"]},\"token-classification\":{\"datasets\":[{\"description\":\"A
        widely used dataset useful to benchmark named entity recognition models.\",\"id\":\"eriktks/conll2003\"},{\"description\":\"A
        multilingual dataset of Wikipedia articles annotated for named entity recognition
        in over 150 different languages.\",\"id\":\"unimelb-nlp/wikiann\"}],\"demo\":{\"inputs\":[{\"label\":\"Input\",\"content\":\"My
        name is Omar and I live in Z\xFCrich.\",\"type\":\"text\"}],\"outputs\":[{\"text\":\"My
        name is Omar and I live in Z\xFCrich.\",\"tokens\":[{\"type\":\"PERSON\",\"start\":11,\"end\":15},{\"type\":\"GPE\",\"start\":30,\"end\":36}],\"type\":\"text-with-tokens\"}]},\"metrics\":[{\"description\":\"\",\"id\":\"accuracy\"},{\"description\":\"\",\"id\":\"recall\"},{\"description\":\"\",\"id\":\"precision\"},{\"description\":\"\",\"id\":\"f1\"}],\"models\":[{\"description\":\"A
        robust performance model to identify people, locations, organizations and
        names of miscellaneous entities.\",\"id\":\"dslim/bert-base-NER\"},{\"description\":\"A
        strong model to identify people, locations, organizations and names in multiple
        languages.\",\"id\":\"FacebookAI/xlm-roberta-large-finetuned-conll03-english\"},{\"description\":\"A
        token classification model specialized on medical entity recognition.\",\"id\":\"blaze999/Medical-NER\"},{\"description\":\"Flair
        models are typically the state of the art in named entity recognition tasks.\",\"id\":\"flair/ner-english\"}],\"spaces\":[{\"description\":\"An
        application that can recognizes entities, extracts noun chunks and recognizes
        various linguistic features of each token.\",\"id\":\"spacy/gradio_pipeline_visualizer\"}],\"summary\":\"Token
        classification is a natural language understanding task in which a label is
        assigned to some tokens in a text. Some popular token classification subtasks
        are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models
        could be trained to identify specific entities in a text, such as dates, individuals
        and places; and PoS tagging would identify, for example, which words in a
        text are verbs, nouns, and punctuation marks.\",\"widgetModels\":[\"FacebookAI/xlm-roberta-large-finetuned-conll03-english\"],\"youtubeId\":\"wVHdVlPScxA\",\"id\":\"token-classification\",\"label\":\"Token
        Classification\",\"libraries\":[\"adapter-transformers\",\"flair\",\"spacy\",\"span-marker\",\"stanza\",\"transformers\",\"transformers.js\"]},\"translation\":{\"canonicalId\":\"text2text-generation\",\"datasets\":[{\"description\":\"A
        dataset of copyright-free books translated into 16 different languages.\",\"id\":\"Helsinki-NLP/opus_books\"},{\"description\":\"An
        example of translation between programming languages. This dataset consists
        of functions in Java and C#.\",\"id\":\"google/code_x_glue_cc_code_to_code_trans\"}],\"demo\":{\"inputs\":[{\"label\":\"Input\",\"content\":\"My
        name is Omar and I live in Z\xFCrich.\",\"type\":\"text\"}],\"outputs\":[{\"label\":\"Output\",\"content\":\"Mein
        Name ist Omar und ich wohne in Z\xFCrich.\",\"type\":\"text\"}]},\"metrics\":[{\"description\":\"BLEU
        score is calculated by counting the number of shared single or subsequent
        tokens between the generated sequence and the reference. Subsequent n tokens
        are called \u201Cn-grams\u201D. Unigram refers to a single token while bi-gram
        refers to token pairs and n-grams refer to n subsequent tokens. The score
        ranges from 0 to 1, where 1 means the translation perfectly matched and 0
        did not match at all\",\"id\":\"bleu\"},{\"description\":\"\",\"id\":\"sacrebleu\"}],\"models\":[{\"description\":\"Very
        powerful model that can translate many languages between each other, especially
        low-resource languages.\",\"id\":\"facebook/nllb-200-1.3B\"},{\"description\":\"A
        general-purpose Transformer that can be used to translate from English to
        German, French, or Romanian.\",\"id\":\"google-t5/t5-base\"}],\"spaces\":[{\"description\":\"An
        application that can translate between 100 languages.\",\"id\":\"Iker/Translate-100-languages\"},{\"description\":\"An
        application that can translate between many languages.\",\"id\":\"Geonmo/nllb-translation-demo\"}],\"summary\":\"Translation
        is the task of converting text from one language to another.\",\"widgetModels\":[\"facebook/mbart-large-50-many-to-many-mmt\"],\"youtubeId\":\"1JvfrvZgi6c\",\"id\":\"translation\",\"label\":\"Translation\",\"libraries\":[\"transformers\",\"transformers.js\"]},\"unconditional-image-generation\":{\"datasets\":[{\"description\":\"The
        CIFAR-100 dataset consists of 60000 32x32 colour images in 100 classes, with
        600 images per class.\",\"id\":\"cifar100\"},{\"description\":\"Multiple images
        of celebrities, used for facial expression translation.\",\"id\":\"CelebA\"}],\"demo\":{\"inputs\":[{\"label\":\"Seed\",\"content\":\"42\",\"type\":\"text\"},{\"label\":\"Number
        of images to generate:\",\"content\":\"4\",\"type\":\"text\"}],\"outputs\":[{\"filename\":\"unconditional-image-generation-output.jpeg\",\"type\":\"img\"}]},\"metrics\":[{\"description\":\"The
        inception score (IS) evaluates the quality of generated images. It measures
        the diversity of the generated images (the model predictions are evenly distributed
        across all possible labels) and their 'distinction' or 'sharpness' (the model
        confidently predicts a single label for each image).\",\"id\":\"Inception
        score (IS)\"},{\"description\":\"The Fr\xE9chet Inception Distance (FID) evaluates
        the quality of images created by a generative model by calculating the distance
        between feature vectors for real and generated images.\",\"id\":\"Fre\u0107het
        Inception Distance (FID)\"}],\"models\":[{\"description\":\"High-quality image
        generation model trained on the CIFAR-10 dataset. It synthesizes images of
        the ten classes presented in the dataset using diffusion probabilistic models,
        a class of latent variable models inspired by considerations from nonequilibrium
        thermodynamics.\",\"id\":\"google/ddpm-cifar10-32\"},{\"description\":\"High-quality
        image generation model trained on the 256x256 CelebA-HQ dataset. It synthesizes
        images of faces using diffusion probabilistic models, a class of latent variable
        models inspired by considerations from nonequilibrium thermodynamics.\",\"id\":\"google/ddpm-celebahq-256\"}],\"spaces\":[{\"description\":\"An
        application that can generate realistic faces.\",\"id\":\"CompVis/celeba-latent-diffusion\"}],\"summary\":\"Unconditional
        image generation is the task of generating images with no condition in any
        context (like a prompt text or another image). Once trained, the model will
        create images that resemble its training data distribution.\",\"widgetModels\":[\"\"],\"youtubeId\":\"\",\"id\":\"unconditional-image-generation\",\"label\":\"Unconditional
        Image Generation\",\"libraries\":[\"diffusers\"]},\"video-text-to-text\":{\"datasets\":[{\"description\":\"Multiple-choice
        questions and answers about videos.\",\"id\":\"lmms-lab/Video-MME\"},{\"description\":\"A
        dataset of instructions and question-answer pairs about videos.\",\"id\":\"lmms-lab/VideoChatGPT\"},{\"description\":\"Large
        video understanding dataset.\",\"id\":\"HuggingFaceFV/finevideo\"}],\"demo\":{\"inputs\":[{\"filename\":\"video-text-to-text-input.gif\",\"type\":\"img\"},{\"label\":\"Text
        Prompt\",\"content\":\"What is happening in this video?\",\"type\":\"text\"}],\"outputs\":[{\"label\":\"Answer\",\"content\":\"The
        video shows a series of images showing a fountain with water jets and a variety
        of colorful flowers and butterflies in the background.\",\"type\":\"text\"}]},\"metrics\":[],\"models\":[{\"description\":\"A
        robust video-text-to-text model.\",\"id\":\"Vision-CAIR/LongVU_Qwen2_7B\"},{\"description\":\"Strong
        video-text-to-text model with reasoning capabilities.\",\"id\":\"GoodiesHere/Apollo-LMMs-Apollo-7B-t32\"}],\"spaces\":[{\"description\":\"An
        application to chat with a video-text-to-text model.\",\"id\":\"llava-hf/video-llava\"},{\"description\":\"A
        leaderboard for various video-text-to-text models.\",\"id\":\"opencompass/openvlm_video_leaderboard\"}],\"summary\":\"Video-text-to-text
        models take in a video and a text prompt and output text. These models are
        also called video-language models.\",\"widgetModels\":[\"\"],\"youtubeId\":\"\",\"id\":\"video-text-to-text\",\"label\":\"Video-Text-to-Text\",\"libraries\":[\"transformers\"]},\"visual-question-answering\":{\"datasets\":[{\"description\":\"A
        widely used dataset containing questions (with answers) about images.\",\"id\":\"Graphcore/vqa\"},{\"description\":\"A
        dataset to benchmark visual reasoning based on text in images.\",\"id\":\"facebook/textvqa\"}],\"demo\":{\"inputs\":[{\"filename\":\"elephant.jpeg\",\"type\":\"img\"},{\"label\":\"Question\",\"content\":\"What
        is in this image?\",\"type\":\"text\"}],\"outputs\":[{\"type\":\"chart\",\"data\":[{\"label\":\"elephant\",\"score\":0.97},{\"label\":\"elephants\",\"score\":0.06},{\"label\":\"animal\",\"score\":0.003}]}]},\"isPlaceholder\":false,\"metrics\":[{\"description\":\"\",\"id\":\"accuracy\"},{\"description\":\"Measures
        how much a predicted answer differs from the ground truth based on the difference
        in their semantic meaning.\",\"id\":\"wu-palmer similarity\"}],\"models\":[{\"description\":\"A
        visual question answering model trained to convert charts and plots to text.\",\"id\":\"google/deplot\"},{\"description\":\"A
        visual question answering model trained for mathematical reasoning and chart
        derendering from images.\",\"id\":\"google/matcha-base\"},{\"description\":\"A
        strong visual question answering that answers questions from book covers.\",\"id\":\"google/pix2struct-ocrvqa-large\"}],\"spaces\":[{\"description\":\"An
        application that compares visual question answering models across different
        tasks.\",\"id\":\"merve/pix2struct\"},{\"description\":\"An application that
        can answer questions based on images.\",\"id\":\"nielsr/vilt-vqa\"},{\"description\":\"An
        application that can caption images and answer questions about a given image.
        \",\"id\":\"Salesforce/BLIP\"},{\"description\":\"An application that can
        caption images and answer questions about a given image. \",\"id\":\"vumichien/Img2Prompt\"}],\"summary\":\"Visual
        Question Answering is the task of answering open-ended questions based on
        an image. They output natural language responses to natural language questions.\",\"widgetModels\":[\"dandelin/vilt-b32-finetuned-vqa\"],\"youtubeId\":\"\",\"id\":\"visual-question-answering\",\"label\":\"Visual
        Question Answering\",\"libraries\":[\"transformers\",\"transformers.js\"]},\"zero-shot-classification\":{\"datasets\":[{\"description\":\"A
        widely used dataset used to benchmark multiple variants of text classification.\",\"id\":\"nyu-mll/glue\"},{\"description\":\"The
        Multi-Genre Natural Language Inference (MultiNLI) corpus is a crowd-sourced
        collection of 433k sentence pairs annotated with textual entailment information.\",\"id\":\"nyu-mll/multi_nli\"},{\"description\":\"FEVER
        is a publicly available dataset for fact extraction and verification against
        textual sources.\",\"id\":\"fever/fever\"}],\"demo\":{\"inputs\":[{\"label\":\"Text
        Input\",\"content\":\"Dune is the best movie ever.\",\"type\":\"text\"},{\"label\":\"Candidate
        Labels\",\"content\":\"CINEMA, ART, MUSIC\",\"type\":\"text\"}],\"outputs\":[{\"type\":\"chart\",\"data\":[{\"label\":\"CINEMA\",\"score\":0.9},{\"label\":\"ART\",\"score\":0.1},{\"label\":\"MUSIC\",\"score\":0}]}]},\"metrics\":[],\"models\":[{\"description\":\"Powerful
        zero-shot text classification model.\",\"id\":\"facebook/bart-large-mnli\"},{\"description\":\"Cutting-edge
        zero-shot multilingual text classification model.\",\"id\":\"MoritzLaurer/ModernBERT-large-zeroshot-v2.0\"}],\"spaces\":[],\"summary\":\"Zero-shot
        text classification is a task in natural language processing where a model
        is trained on a set of labeled examples but is then able to classify new examples
        from previously unseen classes.\",\"widgetModels\":[\"facebook/bart-large-mnli\"],\"id\":\"zero-shot-classification\",\"label\":\"Zero-Shot
        Classification\",\"libraries\":[\"transformers\",\"transformers.js\"]},\"zero-shot-image-classification\":{\"datasets\":[{\"description\":\"\",\"id\":\"\"}],\"demo\":{\"inputs\":[{\"filename\":\"image-classification-input.jpeg\",\"type\":\"img\"},{\"label\":\"Classes\",\"content\":\"cat,
        dog, bird\",\"type\":\"text\"}],\"outputs\":[{\"type\":\"chart\",\"data\":[{\"label\":\"Cat\",\"score\":0.664},{\"label\":\"Dog\",\"score\":0.329},{\"label\":\"Bird\",\"score\":0.008}]}]},\"metrics\":[{\"description\":\"Computes
        the number of times the correct label appears in top K labels predicted\",\"id\":\"top-K
        accuracy\"}],\"models\":[{\"description\":\"Multilingual image classification
        model for 80 languages.\",\"id\":\"visheratin/mexma-siglip\"},{\"description\":\"Strong
        zero-shot image classification model.\",\"id\":\"google/siglip-so400m-patch14-224\"},{\"description\":\"Robust
        zero-shot image classification model.\",\"id\":\"microsoft/LLM2CLIP-EVA02-L-14-336\"},{\"description\":\"Powerful
        zero-shot image classification model supporting 94 languages.\",\"id\":\"jinaai/jina-clip-v2\"},{\"description\":\"Strong
        image classification model for biomedical domain.\",\"id\":\"microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\"}],\"spaces\":[{\"description\":\"An
        application that leverages zero-shot image classification to find best captions
        to generate an image. \",\"id\":\"pharma/CLIP-Interrogator\"},{\"description\":\"An
        application to compare different zero-shot image classification models. \",\"id\":\"merve/compare_clip_siglip\"}],\"summary\":\"Zero-shot
        image classification is the task of classifying previously unseen classes
        during training of a model.\",\"widgetModels\":[\"google/siglip-so400m-patch14-224\"],\"youtubeId\":\"\",\"id\":\"zero-shot-image-classification\",\"label\":\"Zero-Shot
        Image Classification\",\"libraries\":[\"transformers\",\"transformers.js\"]},\"zero-shot-object-detection\":{\"datasets\":[],\"demo\":{\"inputs\":[{\"filename\":\"zero-shot-object-detection-input.jpg\",\"type\":\"img\"},{\"label\":\"Classes\",\"content\":\"cat,
        dog, bird\",\"type\":\"text\"}],\"outputs\":[{\"filename\":\"zero-shot-object-detection-output.jpg\",\"type\":\"img\"}]},\"metrics\":[{\"description\":\"The
        Average Precision (AP) metric is the Area Under the PR Curve (AUC-PR). It
        is calculated for each class separately\",\"id\":\"Average Precision\"},{\"description\":\"The
        Mean Average Precision (mAP) metric is the overall average of the AP values\",\"id\":\"Mean
        Average Precision\"},{\"description\":\"The AP\u03B1 metric is the Average
        Precision at the IoU threshold of a \u03B1 value, for example, AP50 and AP75\",\"id\":\"AP\u03B1\"}],\"models\":[{\"description\":\"Solid
        zero-shot object detection model.\",\"id\":\"IDEA-Research/grounding-dino-base\"},{\"description\":\"Cutting-edge
        zero-shot object detection model.\",\"id\":\"google/owlv2-base-patch16-ensemble\"}],\"spaces\":[{\"description\":\"A
        demo to try the state-of-the-art zero-shot object detection model, OWLv2.\",\"id\":\"merve/owlv2\"},{\"description\":\"A
        demo that combines a zero-shot object detection and mask generation model
        for zero-shot segmentation.\",\"id\":\"merve/OWLSAM\"}],\"summary\":\"Zero-shot
        object detection is a computer vision task to detect objects and their classes
        in images, without any prior training or knowledge of the classes. Zero-shot
        object detection models receive an image as input, as well as a list of candidate
        classes, and output the bounding boxes and labels where the objects have been
        detected.\",\"widgetModels\":[],\"youtubeId\":\"\",\"id\":\"zero-shot-object-detection\",\"label\":\"Zero-Shot
        Object Detection\",\"libraries\":[\"transformers\",\"transformers.js\"]},\"text-to-3d\":{\"datasets\":[{\"description\":\"A
        large dataset of over 10 million 3D objects.\",\"id\":\"allenai/objaverse-xl\"},{\"description\":\"Descriptive
        captions for 3D objects in Objaverse.\",\"id\":\"tiange/Cap3D\"}],\"demo\":{\"inputs\":[{\"label\":\"Prompt\",\"content\":\"a
        cat statue\",\"type\":\"text\"}],\"outputs\":[{\"label\":\"Result\",\"content\":\"text-to-3d-3d-output-filename.glb\",\"type\":\"text\"}]},\"metrics\":[],\"models\":[{\"description\":\"Text-to-3D
        mesh model by OpenAI\",\"id\":\"openai/shap-e\"},{\"description\":\"Generative
        3D gaussian splatting model.\",\"id\":\"ashawkey/LGM\"}],\"spaces\":[{\"description\":\"Text-to-3D
        demo with mesh outputs.\",\"id\":\"hysts/Shap-E\"},{\"description\":\"Text/image-to-3D
        demo with splat outputs.\",\"id\":\"ashawkey/LGM\"}],\"summary\":\"Text-to-3D
        models take in text input and produce 3D output.\",\"widgetModels\":[],\"youtubeId\":\"\",\"id\":\"text-to-3d\",\"label\":\"Text-to-3D\",\"libraries\":[\"diffusers\"]},\"image-to-3d\":{\"datasets\":[{\"description\":\"A
        large dataset of over 10 million 3D objects.\",\"id\":\"allenai/objaverse-xl\"},{\"description\":\"A
        dataset of isolated object images for evaluating image-to-3D models.\",\"id\":\"dylanebert/iso3d\"}],\"demo\":{\"inputs\":[{\"filename\":\"image-to-3d-image-input.png\",\"type\":\"img\"}],\"outputs\":[{\"label\":\"Result\",\"content\":\"image-to-3d-3d-output-filename.glb\",\"type\":\"text\"}]},\"metrics\":[],\"models\":[{\"description\":\"Fast
        image-to-3D mesh model by Tencent.\",\"id\":\"TencentARC/InstantMesh\"},{\"description\":\"Fast
        image-to-3D mesh model by StabilityAI\",\"id\":\"stabilityai/TripoSR\"},{\"description\":\"A
        scaled up image-to-3D mesh model derived from TripoSR.\",\"id\":\"hwjiang/Real3D\"},{\"description\":\"Consistent
        image-to-3d generation model.\",\"id\":\"stabilityai/stable-point-aware-3d\"}],\"spaces\":[{\"description\":\"Leaderboard
        to evaluate image-to-3D models.\",\"id\":\"dylanebert/3d-arena\"},{\"description\":\"Image-to-3D
        demo with mesh outputs.\",\"id\":\"TencentARC/InstantMesh\"},{\"description\":\"Image-to-3D
        demo.\",\"id\":\"stabilityai/stable-point-aware-3d\"},{\"description\":\"Image-to-3D
        demo with mesh outputs.\",\"id\":\"hwjiang/Real3D\"},{\"description\":\"Image-to-3D
        demo with splat outputs.\",\"id\":\"dylanebert/LGM-mini\"}],\"summary\":\"Image-to-3D
        models take in image input and produce 3D output.\",\"widgetModels\":[],\"youtubeId\":\"\",\"id\":\"image-to-3d\",\"label\":\"Image-to-3D\",\"libraries\":[\"diffusers\"]}}"
    headers:
      Access-Control-Allow-Origin:
      - https://huggingface.co
      Access-Control-Expose-Headers:
      - X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Xet-Access-Token,X-Xet-Token-Expiration,X-Xet-Refresh-Route,X-Xet-Cas-Url,X-Xet-Hash
      Connection:
      - keep-alive
      Content-Length:
      - '76131'
      Content-Type:
      - application/json; charset=utf-8
      Date:
      - Wed, 19 Feb 2025 16:41:21 GMT
      ETag:
      - W/"12963-vyIYgItlbQqfuVxsEg2yioEcYRQ"
      Referrer-Policy:
      - strict-origin-when-cross-origin
      Vary:
      - Origin
      Via:
      - 1.1 f2413ce30d7e0433695f69f9e33492d4.cloudfront.net (CloudFront)
      X-Amz-Cf-Id:
      - CPoapzcMMv69pjDeJQoYFS-ZRS044UZ4m_lAN9xuWYM-J1W0m88j2A==
      X-Amz-Cf-Pop:
      - MAA51-P3
      X-Cache:
      - Miss from cloudfront
      X-Powered-By:
      - huggingface-moon
      X-Request-Id:
      - Root=1-67b609b1-6490eedc37f3399c57d73fba;83636dff-e722-4866-8004-41f858460c72
      cross-origin-opener-policy:
      - same-origin
    status:
      code: 200
      message: OK
- request:
    body: '{"inputs": "The goal of life is <mask>.", "parameters": {}}'
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate, zstd
      Connection:
      - keep-alive
      Content-Length:
      - '59'
      Content-Type:
      - application/json
      X-Amzn-Trace-Id:
      - 776fd86c-a384-42ad-b595-75ec9a1df3d4
      user-agent:
      - unknown/None; hf_hub/0.29.0; python/3.12.8; torch/2.6.0
    method: POST
    uri: https://router.huggingface.co/hf-inference/models/distilroberta-base
  response:
    body:
      string: "<!DOCTYPE html>\n<html class=\"\" lang=\"en\">\n<head>\n    <meta charset=\"utf-8\"
        />\n    <meta\n            name=\"viewport\"\n            content=\"width=device-width,
        initial-scale=1.0, user-scalable=no\"\n    />\n    <meta\n            name=\"description\"\n
        \           content=\"We're on a journey to advance and democratize artificial
        intelligence through open source and open science.\"\n    />\n    <meta property=\"fb:app_id\"
        content=\"1321688464574422\" />\n    <meta name=\"twitter:card\" content=\"summary_large_image\"
        />\n    <meta name=\"twitter:site\" content=\"@huggingface\" />\n    <meta\n
        \           property=\"og:title\"\n            content=\"Hugging Face - The
        AI community building the future.\"\n    />\n    <meta property=\"og:type\"
        content=\"website\" />\n\n    <title>Hugging Face - The AI community building
        the future.</title>\n    <style>\n        body {\n            margin: 0;\n
        \       }\n\n        main {\n            background-color: white;\n            min-height:
        100vh;\n            padding: 7rem 1rem 8rem 1rem;\n            text-align:
        center;\n            font-family: Source Sans Pro, ui-sans-serif, system-ui,
        -apple-system,\n            BlinkMacSystemFont, Segoe UI, Roboto, Helvetica
        Neue, Arial, Noto Sans,\n            sans-serif, Apple Color Emoji, Segoe
        UI Emoji, Segoe UI Symbol,\n            Noto Color Emoji;\n        }\n\n        img
        {\n            width: 6rem;\n            height: 6rem;\n            margin:
        0 auto 1rem;\n        }\n\n        h1 {\n            font-size: 3.75rem;\n
        \           line-height: 1;\n            color: rgba(31, 41, 55, 1);\n            font-weight:
        700;\n            box-sizing: border-box;\n            margin: 0 auto;\n        }\n\n
        \       p, a {\n            color: rgba(107, 114, 128, 1);\n            font-size:
        1.125rem;\n            line-height: 1.75rem;\n            max-width: 28rem;\n
        \           box-sizing: border-box;\n            margin: 0 auto;\n        }\n\n
        \       .dark main {\n            background-color: rgb(11, 15, 25);\n        }\n
        \       .dark h1 {\n            color: rgb(209, 213, 219);\n        }\n        .dark
        p, .dark a {\n            color: rgb(156, 163, 175);\n        }\n    </style>\n
        \   <script>\n        // On page load or when changing themes, best to add
        inline in `head` to avoid FOUC\n        const key = \"_tb_global_settings\";\n
        \       let theme = window.matchMedia(\"(prefers-color-scheme: dark)\").matches\n
        \           ? \"dark\"\n            : \"light\";\n        try {\n            const
        storageTheme = JSON.parse(window.localStorage.getItem(key)).theme;\n            if
        (storageTheme) {\n                theme = storageTheme === \"dark\" ? \"dark\"
        : \"light\";\n            }\n        } catch (e) {}\n        if (theme ===
        \"dark\") {\n            document.documentElement.classList.add(\"dark\");\n
        \       } else {\n            document.documentElement.classList.remove(\"dark\");\n
        \       }\n    </script>\n</head>\n\n<body>\n<main>\n    <img\n            src=\"https://cdn-media.huggingface.co/assets/huggingface_logo.svg\"\n
        \           alt=\"\"\n    />\n    <div>\n        <h1>503</h1>\n        <p>Service
        Unavailable</p>\n    </div>\n</main>\n</body>\n</html>"
    headers:
      Age:
      - '86181'
      Connection:
      - close
      Content-Encoding:
      - gzip
      Content-Type:
      - text/html
      Date:
      - Tue, 18 Feb 2025 21:09:01 GMT
      ETag:
      - W/"b80cd6c64b33bebd8ec8b17e96b2ed52"
      Last-Modified:
      - Mon, 03 Jul 2023 19:25:16 GMT
      Server:
      - AmazonS3
      Transfer-Encoding:
      - chunked
      Vary:
      - accept-encoding
      Via:
      - 1.1 b54ed05222d6420c578fc810016a0a96.cloudfront.net (CloudFront)
      X-Amz-Cf-Id:
      - Jzd-FYipyQnvh6JCMr64BV_cI7JCPtKuatcWNRna97MPfDY4xri1PA==
      X-Amz-Cf-Pop:
      - MAA51-P3
      X-Cache:
      - Error from cloudfront
      x-amz-server-side-encryption:
      - AES256
      x-amz-version-id:
      - cECRvO7FI25puKWtzk1ZkYa6IdZfd2SY
    status:
      code: 503
      message: Service Temporarily Unavailable
version: 1
